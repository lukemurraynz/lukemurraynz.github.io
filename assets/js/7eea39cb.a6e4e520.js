"use strict";(self.webpackChunklukemurraynz=self.webpackChunklukemurraynz||[]).push([[16782],{34480:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var o=t(46452),r=t(74848),i=t(28453);const s={title:"Deploying Large Language Models on AKS with Kaito",metaDescription:"Learn how to deploy large language models (or Small Language Models) on Azure Kubernetes Service (AKS) using Kaito, an operator that automates the deployment of AI/ML inference models in a Kubernetes cluster.",description:"Deploy large language models on AKS using Kaito, an operator that simplifies the deployment of AI/ML inference models in a Kubernetes cluster.",date:new Date("2024-05-08T21:49:34.752Z"),tags:["Azure"],categories:["Azure"],authors:["Luke"],header:"kaito-arch.png",slug:"azure/run-local-llm-aks",keywords:["azure","large language models","AKS","Kaito","Kubernetes","operator","AI/ML inference models","GPU-enabled nodes","deployment","model provisioning","model configuration","model tuning","model deployment","model management","open-source LLMs","cost reduction","data security","AI Shared responsibility model","AKS cluster","quota","GPU compute nodes","Azure CLI","GitHub Codespace","infrastructure as code","Terraform","Azure Developer CLI","deployment","AKS cluster deployment","region","environment configuration","deployment completion","microservices","Pets app","AKS demos","experiments","polyglot architecture","event-driven design","open-source back-end services","RabbitMQ","MongoDB","OpenAI's GPT-3 models","generative text","graphics creation","MongoDB instance","RabbitMQ","prerequisites","Azure subscription","GPU compute workload","Standard NCSv3","Azure Developer CLI","Terraform","AKS cluster deployment"]},a=void 0,l={authorsImageUrls:[void 0]},c=[{value:"\ud83d\udc40 Overview",id:"-overview",level:2},{value:"\ud83d\udc36 Example App - Pets",id:"-example-app---pets",level:3},{value:"\ud83d\ude80 Overview",id:"-overview-1",level:3},{value:"\ud83d\udce6 Prerequisites",id:"-prerequisites",level:3},{value:"\ud83d\udc1d Deploy AKS cluster",id:"-deploy-aks-cluster",level:3},{value:"\ud83e\uddea Test the Pet-store app",id:"-test-the-pet-store-app",level:3},{value:"\ud83d\udc26 Implement Kaito and Falcon LLM",id:"-implement-kaito-and-falcon-llm",level:3},{value:"\ud83d\ude80 Update the Pet-store app",id:"-update-the-pet-store-app",level:3}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["Today, we are going to look at deploying a large language model ",(0,r.jsx)(n.em,{children:"(LLM)"})," directly into your AKS ",(0,r.jsx)(n.em,{children:"(Azure Kubernetes Service)"})," cluster, running on GPU-enabled nodes, using ",(0,r.jsxs)(n.a,{href:"https://github.com/Azure/kaito",children:["Kaito ",(0,r.jsx)(n.em,{children:"(Kubernetes AI Toolchain Operator)"})]}),"."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"KAITO is an open-source operator that transforms how you deploy AI models on Kubernetes. It streamlines the process, automating critical tasks like infrastructure provisioning and resource optimization. It intelligently selects the optimal hardware configuration for your specific model, using available CPU and GPU resources on AKS. KAITO eliminates the manual setup complexities, accelerating your deployment time and reducing associated costs."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"-overview",children:"\ud83d\udc40 Overview"}),"\n",(0,r.jsxs)(n.admonition,{type:"info",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/Azure/kaito",children:"Kaito"})," is an operator that automates the deployment of the AI/ML inference model in a Kubernetes cluster. The target models are popular open-sourced inference models such as Falcon and llama2. Kaito has the following critical differentiations compared to most of the mainstream model deployment methodologies built on top of virtual machine infrastructures:"]}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Manage large model files using container images. An HTTP server is provided to perform inference calls using the model library."}),"\n",(0,r.jsx)(n.li,{children:"Avoid tuning deployment parameters to fit GPU hardware by providing preset configurations."}),"\n",(0,r.jsx)(n.li,{children:"Auto-provision GPU nodes based on model requirements."}),"\n",(0,r.jsx)(n.li,{children:"Host large model images in the public Microsoft Container Registry (MCR) if the license allows."}),"\n"]}),(0,r.jsx)(n.p,{children:"Kaito follows the classic Kubernetes Custom Resource Definition(CRD)/controller design pattern. The user manages a workspace custom resource that describes the GPU requirements and the inference specification. Kaito controllers automate the deployment by reconciling the workspace custom resource."}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Kaito - Architecture",src:t(10700).A+"",width:"2666",height:"1123"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Workspace controller: It reconciles the workspace custom resource, creates machine custom resources to trigger node auto-provisioning, and creates the inference workload (deployment or stateful set) based on the model preset configurations."}),"\n",(0,r.jsxs)(n.li,{children:["Node provisioner controller: The controller's name is gpu-provisioner in ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/kaito/tree/main/charts/kaito/gpu-provisioner",children:"Kaito helm chart"}),". It uses the machine CRD originated from Karpenter to interact with the workspace controller. It integrates with Azure Kubernetes Service(AKS) APIs to add new GPU nodes to the AKS cluster. Note that the gpu-provisioner is an open-sourced component. It can be replaced by other controllers if they support Karpenter-core APIs."]}),"\n"]}),(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["At the time of this article, ",(0,r.jsx)(n.a,{href:"https://azure.microsoft.com/updates/public-preview-kubernetes-ai-toolchain-operator-kaito-addon-for-aks/?WT.mc_id=AZ-MVP-5004796",children:"Kaito is in preview"})," and ",(0,r.jsx)(n.strong,{children:"is not recommended for production use"}),"."]}),"\n"]}),(0,r.jsx)(n.p,{children:"There are some significant benefits of running open-source LLMs with Kaito. Some advantages include:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Automated GPU node provisioning and configuration: Kaito will automatically provision and configure GPU nodes for you. This can help reduce the operational burden of managing GPU nodes, configuring them for Kubernetes, and tuning model deployment parameters to fit GPU profiles."}),"\n",(0,r.jsx)(n.li,{children:"Reduced cost: Kaito can help you save money by splitting inferencing across lower-end GPU nodes, which may also be more readily available and cost less than high-end GPU nodes."}),"\n",(0,r.jsx)(n.li,{children:"Support for popular open-source LLMs: Kaito offers preset configurations for popular open-source LLMs. This can help you deploy and manage open-source LLMs on AKS and integrate them with your intelligent applications."}),"\n",(0,r.jsx)(n.li,{children:"Fine-grained control: You can fully control data security and privacy, model development and configuration transparency, and fine-tune the model to fit your specific use case."}),"\n",(0,r.jsx)(n.li,{children:"Network and data security: You can ensure these models are ring-fenced within your organization's network, and the data never leaves the Kubernetes cluster."}),"\n"]})]}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["A word of warning: when looking at running your model in your own AKS cluster, be aware of the ",(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/security/fundamentals/shared-responsibility-ai?WT.mc_id=AZ-MVP-5004796",children:"AI Shared responsibility model"}),". There is a difference in the Azure AI PaaS ",(0,r.jsx)(n.em,{children:"(Platform as a Service)"})," offerings, i.e., Model safety, compute, versioning, etc. and running your model in your own AKS cluster. So, although the Kaito operator helps you run some SLM models in your AKS cluster, you are responsible for the security, compliance, and governance of the entire system versus consuming a Microsoft-provided model as a service offering."]})}),"\n",(0,r.jsxs)(n.admonition,{type:"info",children:[(0,r.jsxs)(n.p,{children:["If you want to follow the guide and set up Kaito, make sure you have approved ",(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?WT.mc_id=AZ-MVP-5004796",children:"quota"})," in your region for the GPU compute nodes. In this example, I am using a 12 vCPU Standard NCSv3 instance."]}),(0,r.jsx)(n.p,{children:"You can use the following Azure CLI command to check your use and limit:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"az vm list-usage \\\n  --location ${AZURE_LOCATION} \\\n  --query \"[? contains(localName, 'Standard NCSv3')]\" \\\n  -o table\n"})}),(0,r.jsxs)(n.p,{children:["You can follow this Microsoft guide for requesting a quota increase: ",(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/azure/quotas/regional-quota-requests?WT.mc_id=AZ-MVP-5004796",children:"Request a quota increase"}),"."]})]}),"\n",(0,r.jsx)(n.h3,{id:"-example-app---pets",children:"\ud83d\udc36 Example App - Pets"}),"\n",(0,r.jsxs)(n.p,{children:["Today, we will use the ",(0,r.jsx)(n.a,{href:"https://github.com/Azure-Samples/aks-store-demo",children:"aks store demo\u2014Pets"})," sample microservices app, commonly used for AKS demos, tutorials, and experiments. We will deploy it to our AKS cluster, update it to use a locally hosted mistral model to generate the local product descriptions, and then call the externally hosted Azure OpenAI endpoint."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["This sample demo app consists of containerized microservices that can be easily deployed into an Azure Kubernetes Service (AKS) cluster. It is meant to show a realistic scenario using a polyglot architecture, event-driven design, and common open-source back-end services ",(0,r.jsx)(n.em,{children:"(e.g., RabbitMQ, MongoDB)"}),". The application also leverages OpenAI's GPT-3 models to generate product descriptions. This can be done using either Azure OpenAI or OpenAI."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Demo Pets - Architecture with Azure OpenAI",src:t(609).A+"",width:"3128",height:"2384"})}),"\n",(0,r.jsx)(n.p,{children:"The application has the following services:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Service"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"makeline-service"})}),(0,r.jsx)(n.td,{children:"This service handles processing orders from the queue and completing them (Golang)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"order-service"})}),(0,r.jsx)(n.td,{children:"This service is used for placing orders (Javascript)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"product-service"})}),(0,r.jsx)(n.td,{children:"This service is used to perform CRUD operations on products (Rust)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"store-front"})}),(0,r.jsx)(n.td,{children:"Web app for customers to place orders (Vue.js)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"store-admin"})}),(0,r.jsx)(n.td,{children:"Web app used by store employees to view orders in queue and manage products (Vue.js)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"virtual-customer"})}),(0,r.jsx)(n.td,{children:"Simulates order creation on a scheduled basis (Rust)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"virtual-worker"})}),(0,r.jsx)(n.td,{children:"Simulates order completion on a scheduled basis (Rust)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ai-service"})}),(0,r.jsx)(n.td,{children:"Optional service for adding generative text and graphics creation (Python)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"mongodb"})}),(0,r.jsx)(n.td,{children:"MongoDB instance for persisted data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"rabbitmq"})}),(0,r.jsx)(n.td,{children:"RabbitMQ for an order queue"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"-overview-1",children:"\ud83d\ude80 Overview"}),"\n",(0,r.jsx)(n.h3,{id:"-prerequisites",children:"\ud83d\udce6 Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/",children:"GitHub account"})," ",(0,r.jsx)(n.em,{children:"(we will use a GitHub Codespace to deploy the App, and requirements)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/free/open-source?WT.mc_id=AZ-MVP-5004796",children:"Azure subscription"})," ",(0,r.jsx)(n.em,{children:"(we will need an Azure subscription, to install the cluster the model to)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/azure/quotas/regional-quota-requests?WT.mc_id=AZ-MVP-5004796",children:"Request a quota increase"})," for a GPU compute workload ",(0,r.jsx)(n.em,{children:"(ie Standard NCSv3)"})," in your region. I am based in New Zealand so that I will be deploying to Australia East."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-deploy-aks-cluster",children:"\ud83d\udc1d Deploy AKS cluster"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Let us deploy our Cluster and application using a pre-created Azure Developer CLI infrastructure as code deployment by connecting to the Codespace ",(0,r.jsx)(n.em,{children:"(which already has the dependencies of Azure Developer CLI, Kubectl, Helm, Azure Bicep, and Terraform Azure CLI pre-installed)"})," at ",(0,r.jsx)(n.a,{href:"https://github.com/Azure-Samples/aks-store-demo",children:"aks store demo\u2014Pets"}),".\n",(0,r.jsx)(n.img,{alt:"Open Codespace",src:t(35379).A+"",width:"1835",height:"970"})]}),"\n",(0,r.jsx)(n.li,{children:"Now, we need to prep for the Azure Developer CLI deployment, which will use Terarform (a random pet name module for a unique name) to deploy our AKS cluster in our specified region. So, let's log in to Azure."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# login to Azure CLI\naz login --use-device-code\n\n# login to Azure Developer CLI\nazd auth login\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Codespace Azure Login",src:t(34905).A+"",width:"1856",height:"970"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Once logged in, we can configure a new Azure Developer CLI environment in preparation for our deployment."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"azd env new\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsxs)(n.li,{children:["Enter an environment name ",(0,r.jsx)(n.em,{children:"(i.e., Kaito)"}),"; the Azure Developer CLI uses this to store the environment settings for the deployment and won't reflect any of your deployed resources."]}),"\n",(0,r.jsxs)(n.li,{children:["Now, we need to set a few environment variables for our deployment, which will be used by the Terraform deployment to deploy the Azure Open AI instance that Pets will use ",(0,r.jsx)(n.em,{children:"(and we will replace it with Falcon later on; however, I recommend including it in the deployment to confirm that the OpenAI components are confirmed working first)"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"azd env set DEPLOY_AZURE_OPENAI true\nazd env set DEPLOY_AZURE_WORKLOAD_IDENTITY true\n"})}),"\n",(0,r.jsx)(n.p,{children:"To confirm variables, you can run the following:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"azd env get-values\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Check Azure Developer CLI variables",src:t(31056).A+"",width:"1856",height:"970"})}),"\n",(0,r.jsxs)(n.ol,{start:"6",children:["\n",(0,r.jsxs)(n.li,{children:["Finally, it's time to deploy. Run the following command and select the Subscription you want to deploy the AKS cluster into and the region ",(0,r.jsx)(n.em,{children:"(remember this region is the region for which you have requested the GPU quota increase)"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"azd up\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Azure Developer CLI -Up",src:t(95889).A+"",width:"1856",height:"970"})}),"\n",(0,r.jsxs)(n.ol,{start:"7",children:["\n",(0,r.jsx)(n.li,{children:"Once the deployment is complete, you will see the output (AKS and helm deployments for the PET microservices), including the AKS cluster name, the Azure OpenAI endpoint, and the OIDC issuer URL."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'After you have provided the information, the azd up command will start by registering Azure providers and features and installing Azure CLI extensions. From there, it will invoke the terraform apply command and execute "azd-hook" scripts, which is a neat way for you to "hook" into the deployment process and add any customizations. We will invoke a helm install command in our deployment to apply our Kubernetes manifests.'}),"\n",(0,r.jsx)(n.p,{children:"This will take a few minutes to complete. Once it's done, you will see the terraform apply command output and the created resources. You can also view the resources in the Azure portal."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"AKS Pet store Azure resource deployments",src:t(702).A+"",width:"2736",height:"549"})}),"\n",(0,r.jsxs)(n.ol,{start:"7",children:["\n",(0,r.jsx)(n.li,{children:"Once the deployment is complete, run the following command to load all the AZD environment variables into your shell to be used by the Kaito deployment."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'eval "$(azd env get-values)"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-test-the-pet-store-app",children:"\ud83e\uddea Test the Pet-store app"}),"\n",(0,r.jsx)(n.p,{children:"Now, lets test that its working, by taking a look at the pet store admin public endpoint."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Open your favorite browser and navigate to the ",(0,r.jsx)(n.a,{href:"https://portal.azure.com/#home",children:"Azure Portal"})]}),"\n",(0,r.jsx)(n.li,{children:"Search for Kubernetes services"}),"\n",(0,r.jsx)(n.li,{children:"Find your AKS cluster and click on the resource"}),"\n",(0,r.jsx)(n.li,{children:"Click on Services and Ingresses"}),"\n",(0,r.jsx)(n.li,{children:"Find the store-admin service and click on the public endpoint to open the store-admin page."}),"\n",(0,r.jsxs)(n.li,{children:["You can test the current ",(0,r.jsx)(n.em,{children:"(external to the AKS cluster)"}),' Azure OpenAI endpoint by clicking on Products, Add Product, typing in a Keyword, clicking on the "Ask AI Assistant" button, and seeing the generated text.']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"AKS - Test Pet store admin OpenAI endpoint",src:t(67559).A+"",width:"1856",height:"989"})}),"\n",(0,r.jsx)(n.h3,{id:"-implement-kaito-and-falcon-llm",children:"\ud83d\udc26 Implement Kaito and Falcon LLM"}),"\n",(0,r.jsx)(n.p,{children:"Now that the AKS cluster is deployed and the Pets store app is running, we can implement Kaito and run the Falcon 7b-instruct large language model."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["You can continue to set this up in the Codespace; I am going to switch to an ",(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796",children:"Azure CloudShell"})," to complete the rest of the implementation, mainly to come at the implementation, from the point of view of installing Kaito in an existing cluster, not related to the Pet store example ",(0,r.jsx)(n.em,{children:"(i.e., make sure that there are no variables, etc. that we might be reliant on, without knowing why)"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{type:"info",children:[(0,r.jsxs)(n.p,{children:["Researchers from Technology Innovation Institute, Abu Dhabi, introduced the ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2311.16867.pdf",children:"Falcon series"}),", which includes models with 7 billion, 40 billion, and 180 billion parameters. These models, intended to be causal decoder-only models, were trained on a high-quality, varied corpus mostly obtained from online data. Falcon-180B, the largest model in the series, is the only publicly available pretraining run ever, having been trained on a dataset of more than 3.5 trillion text tokens."]}),(0,r.jsx)(n.p,{children:"The researchers discovered that Falcon-180B shows great advancements over other models, including PaLM or Chinchilla. It outperforms models being developed concurrently, such as LLaMA 2 or Inflection-1. Falcon-180B achieves performance close to PaLM-2-Large, which is noteworthy given its lower pretraining and inference costs. With this ranking, Falcon-180B joins GPT-4 and PaLM-2-Large as the leading language models in the world. For more information, see the following resources:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2311.16867.pdf",children:"The Falcon Series of Open Language Models"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/tiiuae/falcon-40b-instruct",children:"Falcon-40B-Instruct"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/tiiuae/falcon-180B",children:"Falcon-180B"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/tiiuae/falcon-7b",children:"Falcon-7B"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/Azure-Samples/aks-kaito-terraform/blob/main/alcon-7B-Instruct",children:"Falcon-7B-Instruct"})}),"\n"]}),(0,r.jsxs)(n.p,{children:["Reference: ",(0,r.jsx)(n.a,{href:"https://techcommunity.microsoft.com/t5/azure-for-isv-and-startups/deploy-kaito-on-aks-using-terraform/ba-p/4108930?WT.mc_id=AZ-MVP-5004796",children:"Deploy Kaito on AKS using Terraform"})]})]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"To do this, we need to make sure that we have the preview version of the AKS CLI extension installed. You can check this by running the following command:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"az extension list --query \"[?name=='aks-preview']\"\n"})}),"\n",(0,r.jsx)(n.p,{children:"If you don't have it, you can install it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"az extension add --name aks-preview --allow-preview true\n"})}),"\n",(0,r.jsx)(n.p,{children:"And if needed, update it:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"az extension update --name aks-preview --allow-preview true\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsxs)(n.li,{children:["Re-login to Azure, if needed ",(0,r.jsx)(n.em,{children:"(you don't need to login to the Azure Developer CLI; it was only required for previous steps)"}),". Next, we need to register the AIToolchainProvider resource provider by running the following command:"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'az feature register --namespace "Microsoft.ContainerService" --name "AIToolchainOperatorPreview"\n'})}),"\n",(0,r.jsx)(n.p,{children:"This will allow the Kaito (AI Toolchain Operator) APIs on your Azure subscription to be deployed into your AKS cluster. It could take a few minutes to register."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Register AI Tool Chain Operator",src:t(74505).A+"",width:"1109",height:"372"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Once the registration is complete, we can prepare to deploy the workload identity, workspace, and GPU-enabled node pool for Kaito; as part of this, we need to define the environment variables (if you are still using the Codespace, these should be carried over from the previous deployment; if not, we can set them)*. Adding the variables helps us avoid having to enter the values each time we run a command."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'export AZURE_SUBSCRIPTION_ID="9b06e177-a5bb-468b-9fd6-fc58e5b67239"\nexport AZURE_RESOURCE_GROUP="rg-funnyrhino17"\nexport AZURE_LOCATION="australiaeast"\nexport CLUSTER_NAME="aks-funnyrhino17"\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Set Kaito environment variables",src:t(47516).A+"",width:"1109",height:"372"})}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsxs)(n.li,{children:["We are now going to update our AKS cluster to enable the Kaito operator and OpenID Connect issuer ",(0,r.jsx)(n.em,{children:"(required to enable connectivity to a Managed identity to create the Node pools)"})," by running the following command:"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"az aks update --name ${CLUSTER_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --enable-oidc-issuer --enable-ai-toolchain-operator\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Update AI Toolchain Operator",src:t(91518).A+"",width:"1109",height:"372"})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsx)(n.li,{children:"Once updated, we can then connect directly to the cluster using the Kubernetes command line tool, Kubectl, by running the following command:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"az aks get-credentials --resource-group ${AZURE_RESOURCE_GROUP} --name ${CLUSTER_NAME}\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Connect to AKS cluster with Kubectl",src:t(14980).A+"",width:"1109",height:"381"})}),"\n",(0,r.jsxs)(n.ol,{start:"6",children:["\n",(0,r.jsx)(n.li,{children:"Then verify the connection:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Kubectl get nodes",src:t(84135).A+"",width:"1109",height:"381"})}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"kubectl"})," is a command line tool for interacting with Kubernetes clusters. It's part of the Kubernetes project and is used to deploy and manage applications on Kubernetes. It can also be used to inspect and debug cluster and application resources."]}),(0,r.jsx)(n.p,{children:"Kubectl is the Kubernetes command-line tool that allows you to run commands against Kubernetes clusters. You can use Kubectl to deploy applications, inspect and manage cluster resources, and view logs. It's a crucial component for interacting with Kubernetes and is used in various operations, from creating, updating, and deleting Kubernetes resources to debugging running applications."})]}),"\n",(0,r.jsxs)(n.ol,{start:"8",children:["\n",(0,r.jsx)(n.li,{children:"Lets assign the managed identity, Contributor rights over the Resource Group, that contains the Cluster by running the following command:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'export AKS_OIDC_ISSUER=$(az aks show --name "${CLUSTER_NAME}" --resource-group "${AZURE_RESOURCE_GROUP}" --query "oidcIssuerProfile.issuerUrl" -o tsv)\nexport MC_RESOURCE_GROUP=$(az aks show --resource-group ${AZURE_RESOURCE_GROUP} --name ${CLUSTER_NAME} --query nodeResourceGroup -o tsv)\nKAITO_IDENTITY_PRINCIPAL_ID=$(az identity show --name ai-toolchain-operator-${CLUSTER_NAME} --resource-group ${MC_RESOURCE_GROUP} --query principalId --output tsv)\nKAITO_IDENTITY_CLIENT_ID=$(az identity show  --name ai-toolchain-operator-${CLUSTER_NAME} --resource-group ${MC_RESOURCE_GROUP} --query clientId --output tsv)\naz role assignment create --assignee $KAITO_IDENTITY_PRINCIPAL_ID --scope "/subscriptions/${AZURE_SUBSCRIPTION_ID}/resourcegroups/${AZURE_RESOURCE_GROUP}" --role Contributor\naz identity federated-credential create --name "Kaito-federated-identity" --identity-name ai-toolchain-operator-${CLUSTER_NAME} --resource-group ${MC_RESOURCE_GROUP} --issuer ${AKS_OIDC_ISSUER} --subject system:serviceaccount:kube-system:kaito-gpu-provisioner --audience api://AzureADTokenExchange\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Assign Kaito Managed Identity federated credentials",src:t(57211).A+"",width:"1109",height:"381"})}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["If you get 'Failed to connect to MSI. Please make sure MSI is configured correctly.' Make sure you relogin to Azure ",(0,r.jsx)(n.em,{children:"(ie az login --use-device-code)"}),", you may need to assign Contributor rights manually as well. Reference: ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/azure-cli/issues/17695",children:"Error from CloudShell - Failed to connect to MSI. Please make sure MSI is configured correctly. #17695"})]})}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsx)(n.p,{children:"You can check what entries are in the environment variables by running using echo to output the variable:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"echo $KAITO_IDENTITY_CLIENT_ID\n"})}),(0,r.jsx)(n.p,{children:"If the gpu-provisioner is not running, you can check the logs by running the following command:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --all-namespaces -l app=ai-toolchain-operator\n"})}),(0,r.jsx)(n.p,{children:"Find the name of the gpu-provisioner pod, and then run the following command to get the logs:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl logs -n kube-system <gpu-provisioner-pod-name>\n"})}),(0,r.jsx)(n.p,{children:"I had issues where the gpu-provisioner pod was not running and constantly restarted. The logs showed that the identity was not found, so I had to re-run the command to assign the correct managed identity and use the echo command to reveal the managed identity, highlighting the wrong one was selected."})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"kaito-gpu-provisioner | Azure Portal overview",src:t(73258).A+"",width:"1471",height:"784"})}),"\n",(0,r.jsxs)(n.ol,{start:"9",children:["\n",(0,r.jsx)(n.li,{children:"Let's now deploy the Falcon 7b instruct model to our AKS cluster."}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsx)(n.p,{children:"As soon as the models are grabbed from the remote registry, the GPU nodes will spin up, and you will start paying for their use, so just be aware."})}),"\n",(0,r.jsxs)(n.p,{children:["We are going to use one of the ",(0,r.jsxs)(n.strong,{children:["supported models to create our interference endpoint in AKS, the Falcon 7b instruct model, as per their examples in the ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/kaito/tree/main/examples/inference",children:"Kaito GitHub repository"})]}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f https://raw.githubusercontent.com/Azure/kaito/main/examples/inference/kaito_workspace_falcon_7b-instruct.yaml\n"})}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsx)(n.p,{children:"You can monitor the progress of the workspaces and model deployment by running the following command:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get workspace workspace-falcon-7b-instruct -w\n"})}),(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["The deployment can take a few minutes ",(0,r.jsx)(n.em,{children:"(ie 20-50 minutes, the Resources because ready, before the workspace is ready)"}),", so be patient. It needs to pull the Falcon model from the Microsoft public container registry ",(0,r.jsx)(n.em,{children:'(ie, Pulling image "mcr.microsoft.com/aks/kaito/kaito-falcon-7b-instruct:0.0.4")'}),".  It took me 45 minutes when I tested with the Mistral 7b model"]}),"\n"]}),(0,r.jsx)(n.p,{children:"Wait for the Workspace to be ready before continuing."}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Deploy workspace-falcon-7b-instruct",src:t(90780).A+"",width:"1105",height:"415"})})]}),"\n",(0,r.jsx)(n.p,{children:"Once the workspaces and pods/nodes have been provisioned, let us test the deployment by running the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl get svc workspace-falcon-7b-instruct -o jsonpath=\'{.spec.clusterIP}\'\nexport SERVICE_IP=$(kubectl get svc workspace-falcon-7b-instruct -o jsonpath=\'{.spec.clusterIP}\')\nkubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/chat -H "accept: application/json" -H "Content-Type: application/json" -d "{\\"prompt\\":\\"What is your favorite ice cream flavor?\\"}"\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Test Falcon 7b instruct model",src:t(27869).A+"",width:"1105",height:"415"})}),"\n",(0,r.jsx)(n.h3,{id:"-update-the-pet-store-app",children:"\ud83d\ude80 Update the Pet-store app"}),"\n",(0,r.jsx)(n.p,{children:"Almost there! Now that we know that our local chat endpoint is working with the Falcon 7b instruct model, we can update the Pet Store app to use the local Falcon 7b model and remove the Azure OpenAI to generate the product descriptions."}),"\n",(0,r.jsx)(n.p,{children:"We will continue in our Terminal to do that, but like the previous steps, it can be done in the Codespace. The Pet Store application already has some great examples of doing this, so we will create a new manifest and repoint the application to use the local Falcon workspace instead of the public Azure Open AI endpoint that was created initially:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"First, we need to create a new config map to deploy the Pet Store app with the Falcon 7b instruct model by running the following command:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl apply -n pets -f - <<EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ai-service-configmap\ndata:\n  USE_LOCAL_LLM: "True"\n  AI_ENDPOINT: "http://workspace-falcon-7b-instruct/chat"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ai-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ai-service\n  template:\n    metadata:\n      labels:\n        app: ai-service\n    spec:\n      nodeSelector:\n        "kubernetes.io/os": linux\n      containers:\n      - name: order-service\n        image: ghcr.io/azure-samples/aks-store-demo/ai-service:latest\n        ports:\n        - containerPort: 5001\n        envFrom:\n        - configMapRef:\n            name: ai-service-configmap\n        resources:\n          requests:\n            cpu: 20m\n            memory: 50Mi\n          limits:\n            cpu: 30m\n            memory: 85Mi\n        startupProbe:\n          httpGet:\n            path: /health\n            port: 5001\n          initialDelaySeconds: 60\n          failureThreshold: 3\n          timeoutSeconds: 3\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 5001\n          initialDelaySeconds: 3\n          failureThreshold: 3\n          timeoutSeconds: 3\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 5001\n          failureThreshold: 3\n          initialDelaySeconds: 3\n          timeoutSeconds: 3\n          periodSeconds: 3\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ai-service\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 5001\n    targetPort: 5001\n  selector:\n    app: ai-service\nEOF\n'})}),"\n",(0,r.jsx)(n.p,{children:"Now, let's test it! We will remove the Azure OpenAI instance first and then test it!"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Test Pet Store Local LLM",src:t(2156).A+"",width:"1864",height:"737"})}),"\n",(0,r.jsx)(n.p,{children:"We can confirm within our AI Service ConfigMap now that the local endpoint is being used:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Falcon 7b Kaito Local LLM",src:t(90155).A+"",width:"1147",height:"932"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},90155:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Falconb_Kaito_LocalLLM_ConfigMap-8aa381e07c3293d592fe1966440d7bf1.png"},702:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS-rg-funnyrhino17-25db477192ccdfdc467492a98b8ec56f.png"},57211:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKSAssignKaitoMI-3a562a1f7502ebb82363ef0ddcdfdb37.gif"},84135:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKSKubectlgetnodes-de6170c0d6a6ed4c0db0751aaec04808.gif"},95889:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_Codespace_AZDUp-1d3dd6a586b65003f0daf98c5a09c338.gif"},34905:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_Codespace_AzLogin-b46496e3e30486ac1168c73ee5f70f46.gif"},31056:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_Codespace_CheckAzdEnvVariables-9f4a2eb2daff2b3020329ed6e4fa06b6.gif"},35379:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_Codespace_Open-64c209ec93b7faae9d8375eecb3509e3.gif"},14980:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_ConnectAKSClusterKubectl-95fb6925a5173eeba1388abe1c93d9d1.gif"},91518:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_EnableAIToolChainOperatorExistingCluster-9e60cfca925df9a624cc9b8331b6b1db.gif"},67559:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_PetStoreAdminAzOpenAITest-5b4d00b101aaecbff857daf1ef47feb9.gif"},74505:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_RegisterAIToolChainOperator-99bad2d117c9bb9ea81349d2a47e1259.gif"},47516:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-AKS_Variables-d4784967f1d65f9fcec1ca1de26373fa.gif"},90780:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-DeployFalcon7bInstruct-17fb3c57bf9f6cfb157a44c4c0798c05.gif"},2156:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-TestFalcon7bInstruct-PetStore-78688fa2c54260059b35c4f63db7723a.gif"},27869:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/Kaito-TestFalcon7bInstruct-8b84bb46ea062f23695e4ed1a485d8a8.gif"},609:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/demo-arch-with-openai-5682b589c10eb05f66a5f942e20b0526.png"},10700:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/kaito-arch-2e8dc7b883bd663ee34c62f7781cd570.png"},73258:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/kaito-gpu-provisoner-deployment-azportal-cfdf9a59bb4b05ea42d1fa4f9c2b4c2b.png"},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(96540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}},46452:e=>{e.exports=JSON.parse('{"permalink":"/azure/run-local-llm-aks","source":"@site/blog/2024-05-09-run-local-slm-aks/index.mdx","title":"Deploying Large Language Models on AKS with Kaito","description":"Deploy large language models on AKS using Kaito, an operator that simplifies the deployment of AI/ML inference models in a Kubernetes cluster.","date":"2024-05-08T21:49:34.752Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":15.855,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deploying Large Language Models on AKS with Kaito","metaDescription":"Learn how to deploy large language models (or Small Language Models) on Azure Kubernetes Service (AKS) using Kaito, an operator that automates the deployment of AI/ML inference models in a Kubernetes cluster.","description":"Deploy large language models on AKS using Kaito, an operator that simplifies the deployment of AI/ML inference models in a Kubernetes cluster.","date":"2024-05-08T21:49:34.752Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":"kaito-arch.png","slug":"azure/run-local-llm-aks","keywords":["azure","large language models","AKS","Kaito","Kubernetes","operator","AI/ML inference models","GPU-enabled nodes","deployment","model provisioning","model configuration","model tuning","model deployment","model management","open-source LLMs","cost reduction","data security","AI Shared responsibility model","AKS cluster","quota","GPU compute nodes","Azure CLI","GitHub Codespace","infrastructure as code","Terraform","Azure Developer CLI","deployment","AKS cluster deployment","region","environment configuration","deployment completion","microservices","Pets app","AKS demos","experiments","polyglot architecture","event-driven design","open-source back-end services","RabbitMQ","MongoDB","OpenAI\'s GPT-3 models","generative text","graphics creation","MongoDB instance","RabbitMQ","prerequisites","Azure subscription","GPU compute workload","Standard NCSv3","Azure Developer CLI","Terraform","AKS cluster deployment"]},"unlisted":false,"prevItem":{"title":"Export Azure DevOps Repositories to Azure Storage Account","permalink":"/azure/export-azure-devops-repos-azure-storage-account"},"nextItem":{"title":"Authorization Permission Mismatch error with Azure Storage","permalink":"/azure/authorization-permission-mismatch-error-azure-storage"}}')}}]);