"use strict";(self.webpackChunklukemurraynz=self.webpackChunklukemurraynz||[]).push([[72334],{81894:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>l,default:()=>h,frontMatter:()=>c,metadata:()=>o,toc:()=>p});var o=n(34361),r=n(74848),i=n(28453),a=n(11470),s=n(19365);const c={title:"Export Azure DevOps Repositories to Azure Storage Account",metaDescription:"Learn how to export Azure DevOps repositories to an Azure Storage Account for backup and disaster recovery.",description:"This is a step-by-step guide on exporting Azure DevOps repositories to an Azure Storage Account for backup and recovery.",date:new Date("2024-05-13T08:21:56.906Z"),tags:["Azure"],categories:["Azure"],authors:["Luke"],slug:"azure/export-azure-devops-repos-azure-storage-account",keywords:["Azure","Azure DevOps","Repositories","Export","Azure Storage Account","Backup","Disaster Recovery"]},l=void 0,u={authorsImageUrls:[void 0]},p=[{value:"\ud83d\udcda Overview",id:"-overview",level:2},{value:"\ud83d\ude80 Deployment",id:"-deployment",level:2},{value:"\u2601\ufe0f Azure Storage Account",id:"\ufe0f-azure-storage-account",level:3},{value:"\ud83d\udee0\ufe0f Azure DevOps Pipeline",id:"\ufe0f-azure-devops-pipeline",level:3},{value:"\ud83c\udfc3\u200d\u2642\ufe0f Run",id:"\ufe0f-run",level:2}];function d(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:"When recently deleted, Azure DevOps code repositories go into a soft-delete state, any repositories deleted are retained for 30 days before they are permanently deleted."}),"\n",(0,r.jsx)(t.p,{children:"For Disaster Recovery scenarios or scenarios where you may not realize something has been deleted until after Microsoft retains the backup, you may want to export the repositories at a single point in time to an Azure storage account."}),"\n",(0,r.jsx)(t.p,{children:"The Azure DevOps service is highly redundant and built on core Azure Platform infrastructure components, such as Availability Zones and is built using Azure Cloud native Storage and Azure SQL services, which are highly redundant and backed-up."}),"\n",(0,r.jsx)(t.p,{children:"However, mistakes can happen, and there may be organizational requirements to retain backups of the repositories."}),"\n",(0,r.jsx)(t.p,{children:"Today, we will examine exporting your repositories to an Azure Storage Account using a DevOps pipeline. This pipeline will capture the repositories at their current state and run nightly."}),"\n",(0,r.jsx)(t.h2,{id:"-overview",children:"\ud83d\udcda Overview"}),"\n",(0,r.jsxs)(t.admonition,{type:"info",children:[(0,r.jsx)(t.p,{children:'"You can recover deleted organizations or projects within the 28-day window following deletion. But, once this period elapses, these entities are permanently deleted and can\'t be restored. While these backups serve as a crucial component for disaster recovery, customers need to practice appropriate data management and backup strategies to ensure comprehensive data protection."'}),(0,r.jsx)(t.p,{children:"\"Accidental deletion here refers to scenarios that arise as a result of an incident on our services. It doesn't include customers' accidental deletion of assets (for example, repositories, work items, attachments, or artifacts)."}),(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"We don't support restoring assets that customers accidentally delete. These backups are meant only for business continuity and to aid recovery from outages or disaster scenarios."}),'"']}),(0,r.jsxs)(t.p,{children:["Reference: ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/en-us/azure/devops/organizations/security/data-protection?view=azure-devops&WT.mc_id=AZ-MVP-5004796#mistakes-happen",children:"Data protection - Mistakes Happen"})]})]}),"\n",(0,r.jsx)(t.admonition,{type:"warning",children:(0,r.jsx)(t.p,{children:"Please remember that this backup will only export the repositories and pipelines, not the work items, pipeline variables, or other artifacts. This will also backup any code stored in Azure DevOps, so if secrets or other sensitive information are stored in the code, please ensure that these are not stored in the code or are encrypted, or they may be readable by someone with access to the storage account."})}),"\n",(0,r.jsx)(t.p,{children:"Today, we will run a PowerShell script with an Azure DevOps pipeline. The script will connect to the Azure DevOps API and grab the zip file of each Repository in all Projects into an artifact, which then gets exported and copied to an Azure Storage Account."}),"\n",(0,r.jsx)(t.mermaid,{value:"graph TD\n  A[Trigger Pipeline] --\x3e B[Checkout Code]\n  B --\x3e C[Read Azure DevOps Repositories]\n  C --\x3e G[Export Azure DevOps Repositories]\n  G --\x3e D[Allow Public Access to Azure DevOps Export Storage Account for upload]\n  D --\x3e E[Check Storage Network Access]\n  E --\x3e F[Download Build Artifacts]\n  F --\x3e H[Upload Repositories to Azure Storage Account]"}),"\n",(0,r.jsx)(t.h2,{id:"-deployment",children:"\ud83d\ude80 Deployment"}),"\n",(0,r.jsx)(t.h3,{id:"\ufe0f-azure-storage-account",children:"\u2601\ufe0f Azure Storage Account"}),"\n",(0,r.jsx)(t.p,{children:"We will need to create an Azure Storage Account to store the exported repositories. We can point towards an already existing Storage Account and Container."}),"\n",(0,r.jsxs)(t.p,{children:["We will deploy the Storage account fresh using ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796",children:"Azure Bicep"})," to our existing Resource Group for this article."]}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["I will use the ",(0,r.jsx)(t.a,{href:"https://luke.geek.nz/azure/Azure-Bicep-Deploy-Pane/",children:"Deployment Pane"})," to deploy the Storage Account straight from Visual Studio Code, running in a ",(0,r.jsx)(t.a,{href:"https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/",children:"Codespace"}),"."]})}),"\n",(0,r.jsxs)(t.p,{children:["We will use ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/azure/storage/blobs/lifecycle-management-policy-configure?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796",children:"Lifecycle Management policies"})," to help control the retention of the exported repositories. We will move the repositories to Cool Storage after 30 days, Archive Storage after 90 days, and delete them after 120 days."]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Export ADO - Create Storage Account",src:n(32767).A+"",width:"1864",height:"974"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bicep",children:"// Parameters for the script\nparam storageAccountName string // Name of the storage account\nparam location string = resourceGroup().location // Location of the storage account, default to the location of the resource group\nparam containerName string = 'adoexport' // Name of the container\n\n// Resource definition for the storage account\nresource storageAccount 'Microsoft.Storage/storageAccounts@2023-01-01' = {\n  name: storageAccountName // Set the name of the storage account\n  location: location // Set the location of the storage account\n  sku: {\n    name: 'Standard_LRS' // Set the SKU to Standard_LRS\n  }\n  kind: 'StorageV2' // Set the kind to StorageV2\n  properties: {\n    accessTier: 'Hot' // Set the access tier to Hot\n    allowBlobPublicAccess: false // Disable public access to blobs\n    publicNetworkAccess: 'Disabled' // Disable public network access\n  }\n}\n\n// Resource definition for the blob service\nresource blobService 'Microsoft.Storage/storageAccounts/blobServices@2023-01-01' = {\n  name: 'default' // Set the name of the blob service to default\n  parent: storageAccount // Set the parent to the storage account\n  properties: {\n    lastAccessTimeTrackingPolicy: {\n      enable: true // Enable last access time tracking\n    }\n  }\n}\n\n// Resource definition for the storage container\nresource storageContainer 'Microsoft.Storage/storageAccounts/blobServices/containers@2023-01-01' = {\n  name: containerName // Set the name of the container\n  parent: blobService // Set the parent to the blob service\n  properties: {\n    publicAccess: 'None' // Set public access to None\n  }\n}\n\n// Define a new resource of type 'Microsoft.Storage/storageAccounts/managementPolicies'\nresource lifecyclePolicy 'Microsoft.Storage/storageAccounts/managementPolicies@2023-01-01' = {\n  // The name of the management policy. 'default' is a reserved name for the policy.\n  name: 'default'\n  \n  // The parent resource that this policy is associated with, which is the storage account.\n  parent: storageAccount\n  \n  properties: {\n    policy: {\n      rules: [\n        {\n          // This rule is enabled\n          enabled: true\n          // The name of the rule\n          name: 'MoveToCoolStorageAfter30Days'\n          // The type of rule, in this case, it's a lifecycle rule\n          type: 'Lifecycle'\n          definition: {\n            actions: {\n              // The actions to take on base blobs (i.e., non-versioned blobs)\n              baseBlob: {\n                // Move the blob to cool storage after it hasn't been modified for 30 days\n                tierToCool: {\n                  daysAfterModificationGreaterThan: 30\n                }\n              }\n            }\n            // The filters that determine which blobs the rule applies to\n            filters: {\n              // The types of blobs that the rule applies to. In this case, the rule applies to block blobs\n              blobTypes: [\n                'blockBlob'\n              ]\n            }\n          }\n        }\n        {\n          enabled: true\n          name: 'MoveToArchiveStorageAfter90Days'\n          type: 'Lifecycle'\n          definition: {\n            actions: {\n              baseBlob: {\n                // Move the blob to archive storage after it hasn't been modified for 90 days\n                tierToArchive: {\n                  daysAfterModificationGreaterThan: 90\n                }\n              }\n            }\n            filters: {\n              blobTypes: [\n                'blockBlob'\n              ]\n            }\n          }\n        }\n        {\n          enabled: true\n          name: 'DeleteBlobsAfter120Days'\n          type: 'Lifecycle'\n          definition: {\n            actions: {\n              baseBlob: {\n                // Delete the blob after it hasn't been modified for 120 days\n                delete: {\n                  daysAfterModificationGreaterThan: 120\n                }\n              }\n            }\n            filters: {\n              blobTypes: [\n                'blockBlob'\n              ]\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\noutput storageAccountName string = storageAccount.name // Output the name of the storage account\noutput containerName string = storageContainer.name // Output the name of the storage account container\n\n"})}),"\n",(0,r.jsx)(t.p,{children:"Please note the Storage Account name and Container Name; we will need them for Azure DevOps pipeline variables."}),"\n",(0,r.jsx)(t.h3,{id:"\ufe0f-azure-devops-pipeline",children:"\ud83d\udee0\ufe0f Azure DevOps Pipeline"}),"\n",(0,r.jsxs)(t.p,{children:["Now, it's time to set up your project and repository. If you don't know how to do that, you can follow the official Microsoft Learn documentation here: ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/en-us/azure/devops/repos/git/create-new-repo?view=azure-devops&WT.mc_id=AZ-MVP-5004796",children:"Create a new Git repo in your project"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"Once your repository has been set up and initialized, we need to add the following files."}),"\n",(0,r.jsx)(t.h1,{id:"-powershell-script",children:"\ud83d\udcbb PowerShell Script"}),"\n",(0,r.jsxs)(t.p,{children:["The PowerShell script, ",(0,r.jsx)(t.code,{children:"Export-AzDevOpsRepos.ps1"}),", is responsible for cloning all repositories in a given Azure DevOps organization and downloading them as ZIP files. It uses the Azure DevOps REST API to fetch all projects and their respective repositories. For each repository, it generates a URL to download the repository as a ZIP file and saves it to a local directory. If the script is run in an Azure Pipelines environment, it also uploads the directory containing the downloaded repositories as a pipeline artifact."]}),"\n",(0,r.jsxs)(t.admonition,{type:"tip",children:[(0,r.jsx)(t.p,{children:"The PowerShell script can also be run locally. If it detects an Azure DevOps agent, it will automatically collect the variables from the pipeline variables; otherwise, those variables can be added manually to the script to run it locally."}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-powershell",children:"if ($env:AGENT_ID) {\n    # Running in Azure DevOps\n    $personalAccessToken = \"$env:pat\" # Assuming PAT is stored as a secret variable in the pipeline\n    $organization = \"$env:AzDevOpsOrg\"\n\n}\nelse {\n    # Running on a local PC\n    $personalAccessToken = ''\n    $organization = ''\n}\n"})})]}),"\n",(0,r.jsx)(t.h1,{id:"-azure-pipelines",children:"\ud83d\ude80 Azure Pipelines"}),"\n",(0,r.jsxs)(t.p,{children:["The Azure Pipelines configuration file, ",(0,r.jsx)(t.code,{children:".azure-pipelines/pipeline.ci.adoexport.yml"})," sets up a CI pipeline that runs the PowerShell script on a schedule. The pipeline is configured to run on the latest Windows agent. It uses the Azure CLI task to run the PowerShell script and to manage access to an Azure storage account. After the script has run, the pipeline downloads any ZIP files produced as artifacts and copies them to the Azure storage account. The pipeline is scheduled to run daily at midnight."]}),"\n",(0,r.jsxs)(a.A,{children:[(0,r.jsx)(s.A,{value:"pipeline",label:"pipeline.ci.adoexport.yml",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-yaml",children:'name: "Export Azure DevOps repositories" # Name of the pipeline\n\ntrigger: none # Pipeline is not triggered automatically\n\nschedules: # Define schedules for pipeline runs\n  - cron: "0 12 * * *" # Run daily at midnight\n    displayName: Daily midnight run # Display name for the schedule\n    branches:\n      include:\n        - main # Only run on the \'main\' branch\n    always: true # Run even if there are no code changes\n\npool:\n  vmImage: "windows-latest" # Use the latest Windows agent\n\nsteps:\n  - checkout: self # Checkout the source code from the repository\n    persistCredentials: true # Persist credentials for subsequent steps\n    displayName: "Checkout code" # Display name for the step\n\n  - task: AzureCLI@2 # Use Azure CLI task\n    displayName: "Clone Azure DevOps Repositories" # Display name for the step\n    inputs:\n      azureSubscription: "$(azServiceConnection)" # Azure subscription to use\n      scriptType: "ps" # Use PowerShell script\n      scriptLocation: "scriptPath" # Script location is a file path\n      scriptPath: "$(Build.Repository.LocalPath)/Export-AzDevOpsRepos.ps1" # Path to the PowerShell script\n    env:\n      PAT: $(PAT) # Personal Access Token for authentication\n      AzDevOpsOrg: $(AzDevOpsOrg) # Azure DevOps organization\n\n  - task: AzureCLI@2 # Use Azure CLI task\n    displayName: Allow Public Access to Azure DevOps Export Storage Account for upload # Display name for the step\n    condition: succeeded() # Run only if the previous step succeeded\n    inputs:\n      azureSubscription: "$(azServiceConnection)" # Azure subscription to use\n      scriptType: bash # Use Bash script\n      scriptLocation: inlineScript # Script location is inline\n      inlineScript: |\n        az storage account update --name "${storageAccount}"  --resource-group "${storageAccountRG}" --default-action Allow\n    env:\n      storageAccount: $(storageAccount) # Personal Access Token for authentication\n      storageAccountRG: $(storageAccountRG) # Azure DevOps organization\n\n  - task: AzureCLI@2 # Use Azure CLI task\n    displayName: "Check Storage Network Access" # Display name for the step\n    condition: succeeded() # Run only if the previous step succeeded\n    timeoutInMinutes: 10 # Timeout after 10 minutes\n    continueOnError: true # Continue even if the step fails\n    name: check_storage_access # Name of the step\n    inputs:\n      azureSubscription: "$(azServiceConnection)" # Azure subscription to use\n      scriptType: bash # Use Bash script\n      scriptLocation: inlineScript # Script location is inline\n      inlineScript: |\n        set -x\n        echo -e "Setting up authentication..."\n        AZURE_STORAGE_ACCOUNT=${storageAccount}\n        AZURE_STORAGE_KEY=$(az storage account keys list --account-name ${storageAccount} --query \'[0].value\' --output tsv)\n        echo -e "Checking storage account access every 60 seconds..."\n        sleep 10\n        for i in {1..60}; do\n          if az storage container list --output none; then\n            echo "Access granted"\n            break\n          else\n            echo "Access denied, retrying in 60 seconds..."\n            sleep 60\n          fi\n        done\n\n  - task: DownloadPipelineArtifact@2 # Download pipeline artifacts\n    displayName: "Download Build Artifacts" # Display name for the step\n    inputs:\n      patterns: "**/*.zip" # Include all ZIP files\n      path: "$(Build.ArtifactStagingDirectory)" # Download artifacts to the staging directory\n\n  #Storage account needs the SPN to have a Storage Blob Data Contributor role to allow blob upload.\n\n  - task: AzureFileCopy@6 # Use Azure File Copy task\n    displayName: "Copy artifacts to $(storageAccount)" # Display name for the step\n    inputs:\n      azureSubscription: "$(azServiceConnection)" # Azure subscription to use\n      blobPrefix: "$(Build.DefinitionName)/$(Build.BuildId)" # Prefix for the blob names\n      containerName: $(stgAccContainer) # Name of the storage container\n      destination: "AzureBlob" # Copy to Azure Blob storage\n      sourcePath: "$(Build.ArtifactStagingDirectory)/*" # Source path for the artifacts\n      storage: $(storageAccount) # Storage account to copy the artifacts to\n    # Log the status of artifact download and storage account operations\n\n\n  - task: AzureCLI@2 # Use Azure CLI task\n    displayName: Remove Public Access from Azure DevOps Export Storage Account # Display name for the step\n    condition: succeededOrFailed() # Run whether the previous step succeeded or failed\n    inputs:\n      azureSubscription: "$(azServiceConnection)" # Azure subscription to use\n      scriptType: bash # Use Bash script\n      scriptLocation: inlineScript # Script location is inline\n      inlineScript: |\n        az storage account update --name "${storageAccount}" --resource-group "${storageAccountRG}" --default-action Deny\n    env:\n      storageAccount: $(storageAccount) # Personal Access Token for authentication\n      storageAccountRG: $(storageAccountRG) # Azure DevOps organization\n\n'})})}),(0,r.jsx)(s.A,{value:"script",label:"Export-AzDevOpsRepos.ps1",children:(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-powershell",children:'$apiVersion = \'7.1\' # Update API version to 6.0\n\nif ($env:AGENT_ID) {\n    # Running in Azure DevOps\n    $personalAccessToken = "$env:pat" # Assuming PAT is stored as a secret variable in the pipeline\n    $organization = "$env:AzDevOpsOrg"\n\n}\nelse {\n    # Running on a local PC\n    $personalAccessToken = \'\'\n    $organization = \'\'\n}\n\n$base64AuthInfo = [System.Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes(":$($personalAccessToken)"))\n$headers = @{Authorization = ("Basic {0}" -f $base64AuthInfo) }\n\n# Get all projects\n$projects = Invoke-RestMethod -Uri "https://dev.azure.com/$organization/_apis/projects?api-version=$apiVersion" -Method Get -Headers $headers -Verbose\n\n# Output the count and names of the projects\nWrite-Host "Number of projects: $($projects.value.Count)"\nWrite-Host "Project names: $($projects.value | ForEach-Object { $_.name })"\n\n# For each project, get all repositories and download them as zip\n\n# Ensure the repositories directory exists before starting the loop\n$repositoriesPath = "$env:SYSTEM_DEFAULTWORKINGDIRECTORY/repositories"\nif (-not (Test-Path -Path $repositoriesPath)) {\n    Write-Host "Creating repositories directory: $repositoriesPath"\n    New-Item -ItemType Directory -Path $repositoriesPath | Out-Null\n}\n\n$projects.value | ForEach-Object {\n    $projectName = $_.name\n\n    if (-not [string]::IsNullOrWhiteSpace($projectName)) {\n        $projectName = $projectName.Replace(\' \', \'%20\')\n        $result = Invoke-RestMethod -Uri "https://dev.azure.com/$organization/$projectName/_apis/git/repositories?api-version=$apiVersion" -Method Get -Headers $headers -Verbose\n\n        $result.value | ForEach-Object {\n            $repoName = $_.name\n            Write-Host "Attempting to clone the repository: $repoName"\n\n            if (-not [string]::IsNullOrWhiteSpace($repoName)) {\n                $repoId = $_.id\n                $zipUrl = "https://dev.azure.com/$organization/$projectName/_apis/git/repositories/$repoId/items?scopePath=/&recursionLevel=Full&api-version=$apiVersion&`$format=zip"\n                $outputPath = "repositories/$repoName.zip"\n                Write-Host "Output path: $outputPath"\n                # Ensure the directory exists before trying to download the file\n                $directoryPath = Split-Path -Path $outputPath -Parent\n                if (-not (Test-Path -Path $directoryPath)) {\n                    Write-Host "Creating directory: $directoryPath"\n                    New-Item -ItemType Directory -Path $directoryPath | Out-Null\n                }\n                try {\n                    Write-Host "Starting download for $repoName from $zipUrl"\n                    Invoke-WebRequest -Uri $zipUrl -OutFile $outputPath -Headers $headers\n                    Write-Host "Download completed for $repoName"\n                }\n                catch {\n                    Write-Host "Failed to download $repoName from $zipUrl"\n                    Write-Host $_.Exception.Message\n                }\n            }\n        }\n    }\n}\n\n# Check if the repositories directory exists before trying to upload it\nif (Test-Path -Path "$env:SYSTEM_DEFAULTWORKINGDIRECTORY/repositories") {\n    Write-Host "Repositories directory exists, uploading as an artifact."\n    Write-Host "##vso[artifact.upload containerfolder=repositories;artifactname=AzureDevOpsExportedRepositories;]$env:SYSTEM_DEFAULTWORKINGDIRECTORY/repositories"\n}\nelse {\n    Write-Host "The repositories directory does not exist, so you cannot upload it as an artifact."\n}\n\n'})})})]}),"\n",(0,r.jsxs)(t.p,{children:["You can also find the latest files in the ",(0,r.jsx)(t.a,{href:"https://github.com/lukemurraynz/ADO_Export",children:"lukemurraynz/ADO_Export"})," GitHub repository. Feel free to open a Pull Request, Suggest changes, or improve."]}),"\n",(0,r.jsx)(t.p,{children:"I am going to be lazy and just drag the files into the repo and commit them."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Export_ADO_CommitFiles",src:n(25346).A+"",width:"1903",height:"974"})}),"\n",(0,r.jsx)(t.p,{children:"Once committed, it is time to set up our pipeline."}),"\n",(0,r.jsxs)(t.p,{children:["But first, we need to setup our ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&tabs=Windows&WT.mc_id=AZ-MVP-5004796",children:"PAT (Personal Access Token)"})," and ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796",children:"Service Connection"}),". Both I have completed."]}),"\n",(0,r.jsx)(t.p,{children:"For your PAT token, you will need the following permissions:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Code (Read)"}),"\n",(0,r.jsx)(t.li,{children:"Project and Team (Read)"}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"For your Service Connection, you will need the following permissions:"}),"\n",(0,r.jsxs)(t.p,{children:["Reader on the Subscription that Contains your Storage Account ",(0,r.jsx)(t.em,{children:"(the subscription scoping is required for the Azure CLI commanders to check and access your Storage account; if you are using a Self-Managed DevOps agent, you may be able to avoid this)"}),", and Storage Blob Data Contributor on the Storage Account."]}),"\n",(0,r.jsx)(t.p,{children:"Then, we need to add to our pipeline the following:"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Variable"}),(0,r.jsx)(t.th,{children:"Example"}),(0,r.jsx)(t.th,{children:"Notes"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"azDevOpsOrg"}),(0,r.jsx)(t.td,{children:"Contoso"}),(0,r.jsxs)(t.td,{children:["Your Azure DevOps Org - matching your URL ie (",(0,r.jsx)(t.a,{href:"https://dev.azure.com/contoso",children:"https://dev.azure.com/contoso"}),")"]})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"azServiceConnection"}),(0,r.jsx)(t.td,{children:"AzureServiceConnection"}),(0,r.jsx)(t.td,{children:"Your Service Connection name for Azure Resource Provider access to your Azure Resource Group"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"pat"}),(0,r.jsx)(t.td,{children:"2xj2jv4yf2h6xvqjgk7y2n4d2iucxltlzyxuhcdjxibnrf"}),(0,r.jsx)(t.td,{children:"Your PAT (Personal Access Token), used to authenticate to Azure DevOps. Set this as a secure string."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"stgAccContainer"}),(0,r.jsx)(t.td,{children:"adoexport"}),(0,r.jsx)(t.td,{children:"The Container that will contain your ADO Exports inside of the Storage Account"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"storageAccount"}),(0,r.jsx)(t.td,{children:"adoexport23"}),(0,r.jsx)(t.td,{children:"Storage account that your ADO exports will be going into."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"storageAccountRG"}),(0,r.jsx)(t.td,{children:"adoexport-rg"}),(0,r.jsx)(t.td,{children:"Resource Group, containing your Azure Storage Account"})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"So, let's import our pipeline and configure our variables!"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:'To import the Pipeline, navigate to your Azure DevOps Project, click on Pipelines, and then click on the "New Pipeline" button.'}),"\n",(0,r.jsx)(t.li,{children:"Click Azure Repos Git and select your repository."}),"\n",(0,r.jsx)(t.li,{children:"Select Existing Azure Pipelines YAML file"}),"\n",(0,r.jsx)(t.li,{children:"Select your Pipeline and click Continue"}),"\n",(0,r.jsx)(t.li,{children:"Now, we can add our Variables by clicking Variables and then Add. If you have already imported the Pipeline, you can navigate to the Pipeline, click Edit, and then Variables."}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Import Azure DevOps Pipeline &amp; Configure Variables",src:n(95138).A+"",width:"1903",height:"974"})}),"\n",(0,r.jsxs)(t.admonition,{type:"info",children:[(0,r.jsxs)(t.p,{children:["By default, the pipeline has a cronjob configured to set it for midnight, but you can change this to any time you like by editing the pipeline file and changing the ",(0,r.jsx)(t.a,{href:"https://learn.microsoft.com/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796#cron-syntax",children:"cron syntax"}),"."]}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-yaml",children:"schedules: # Define schedules for pipeline runs\n  - cron: \"0 12 * * *\" # Run daily at midnight\n    displayName: Daily midnight run # Display name for the schedule\n    branches:\n      include:\n        - main # Only run on the 'main' branch\n    always: true # Run even if there are no code changes\n"})})]}),"\n",(0,r.jsx)(t.h2,{id:"\ufe0f-run",children:"\ud83c\udfc3\u200d\u2642\ufe0f Run"}),"\n",(0,r.jsx)(t.p,{children:"Now that you have configured your Storage Account and Pipeline, it's time to run the pipeline manually to ensure there are no problems before your scheduled time runs."}),"\n",(0,r.jsxs)(t.admonition,{type:"warning",children:[(0,r.jsx)(t.p,{children:"Before you run:"}),(0,r.jsx)(t.p,{children:"Double-check the permissions you have granted the Service Principal and your variable naming and values; else you may get an error similar to the following when attempting to copy the artifacts to the Storage Account:"}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-plaintext",children:"ERROR: (InvalidApiVersionParameter) The api-version '2023-01-01' is invalid. The supported versions are '2024-03-01,2023-07-01,2023-07-01-preview,2023-03-01-preview,2022-12-01,2022-11-01-preview,2022-09-01,2022-06-01,2022-05-01,2022-03-01-preview,2022-01-01,2021-04-01,2021-01-01,2020-10-01,2020-09-01,2020-08-01,2020-07-01,2020-06-01,2020-05-01,2020-01-01,2019-11-01,2019-10-01,2019-09-01,2019-08-01,2019-07-01,2019-06-01,2019-05-10,2019-05-01,2019-03-01,2018-11-01,2018-09-01,2018-08-01,2018-07-01,2018-06-01,2018-05-01,2018-02-01,2018-01-01,2017-12-01,2017-08-01,2017-06-01,2017-05-10,2017-05-01,2017-03-01,2016-09-01,2016-07-01,2016-06-01,2016-02-01,2015-11-01,2015-01-01,2014-04-01-preview,2014-04-01,2014-01-01,2013-03-01,2014-02-26,2014-04'.\nCode: InvalidApiVersionParameter\n"})}),(0,r.jsx)(t.p,{children:"It means, that your variables are not parsing correctly, to the Azure CLI, make sure they are correct."})]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.em,{children:'You may need to permit your Pipeline to use your Service Connection by clicking on the "Allow" button when the pipeline runs.'})}),"\n",(0,r.jsxs)(t.p,{children:["Your pipeline may take a while to run, depending on how many repositories you have and how large they are. You can monitor the progress of the pipeline by clicking on the pipeline, then clicking on the Job, and then the Task to see the output of the PowerShell script. This is why I would recommend running this on a schedule during a time that does not impact users, as it can hold up an agent while it runs; also be aware that depending on your repositories, running this every night can quickly use up any ",(0,r.jsx)(t.a,{href:"https://azure.microsoft.com/pricing/details/devops/azure-devops-services/?WT.mc_id=AZ-MVP-5004796",children:"build minutes you may have, if running from a Microsoft hosted agent"}),"."]}),"\n",(0,r.jsx)(t.admonition,{type:"info",children:(0,r.jsxs)(t.p,{children:["If a Repository is only used for Azure Boards and not for code, it will not be exported, as the API will not return it ",(0,r.jsx)(t.em,{children:", i.e., you will use a (404) Not Found"}),"."]})}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Azure DevOps - Artifact_Export",src:n(65690).A+"",width:"1294",height:"826"})}),"\n",(0,r.jsx)(t.p,{children:"Once that's completed - you will be able to see your exported repositories in your storage account!"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Azure DevOps - Storage Account",src:n(98606).A+"",width:"1903",height:"974"})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},19365:(e,t,n)=>{n.d(t,{A:()=>a});n(96540);var o=n(34164);const r={tabItem:"tabItem_Ymn6"};var i=n(74848);function a({children:e,hidden:t,className:n}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,o.A)(r.tabItem,n),hidden:t,children:e})}},11470:(e,t,n)=>{n.d(t,{A:()=>z});var o=n(96540),r=n(34164),i=n(23104),a=n(56347),s=n(205),c=n(57485),l=n(31682),u=n(70679);function p(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:t,children:n}=e;return(0,o.useMemo)((()=>{const e=t??function(e){return p(e).map((({props:{value:e,label:t,attributes:n,default:o}})=>({value:e,label:t,attributes:n,default:o})))}(n);return function(e){const t=(0,l.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function h({value:e,tabValues:t}){return t.some((t=>t.value===e))}function g({queryString:e=!1,groupId:t}){const n=(0,a.W6)(),r=function({queryString:e=!1,groupId:t}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:e,groupId:t});return[(0,c.aZ)(r),(0,o.useCallback)((e=>{if(!r)return;const t=new URLSearchParams(n.location.search);t.set(r,e),n.replace({...n.location,search:t.toString()})}),[r,n])]}function m(e){const{defaultValue:t,queryString:n=!1,groupId:r}=e,i=d(e),[a,c]=(0,o.useState)((()=>function({defaultValue:e,tabValues:t}){if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=t.find((e=>e.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:i}))),[l,p]=g({queryString:n,groupId:r}),[m,f]=function({groupId:e}){const t=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,r]=(0,u.Dv)(t);return[n,(0,o.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:r}),A=(()=>{const e=l??m;return h({value:e,tabValues:i})?e:null})();(0,s.A)((()=>{A&&c(A)}),[A]);return{selectedValue:a,selectValue:(0,o.useCallback)((e=>{if(!h({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);c(e),p(e),f(e)}),[p,f,i]),tabValues:i}}var f=n(92303);const A={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=n(74848);function b({className:e,block:t,selectedValue:n,selectValue:o,tabValues:a}){const s=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.a_)(),l=e=>{const t=e.currentTarget,r=s.indexOf(t),i=a[r].value;i!==n&&(c(t),o(i))},u=e=>{let t=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const n=s.indexOf(e.currentTarget)+1;t=s[n]??s[0];break}case"ArrowLeft":{const n=s.indexOf(e.currentTarget)-1;t=s[n]??s[s.length-1];break}}t?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},e),children:a.map((({value:e,label:t,attributes:o})=>(0,y.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{s.push(e)},onKeyDown:u,onClick:l,...o,className:(0,r.A)("tabs__item",A.tabItem,o?.className,{"tabs__item--active":n===e}),children:t??e},e)))})}function v({lazy:e,children:t,selectedValue:n}){const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(e){const e=i.find((e=>e.props.value===n));return e?(0,o.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:i.map(((e,t)=>(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==n})))})}function x(e){const t=m(e);return(0,y.jsxs)("div",{className:(0,r.A)("tabs-container",A.tabList),children:[(0,y.jsx)(b,{...t,...e}),(0,y.jsx)(v,{...t,...e})]})}function z(e){const t=(0,f.A)();return(0,y.jsx)(x,{...e,children:p(e.children)},String(t))}},65690:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/Artifact_Export-3ce83535131fbf52988deb4b2d59f7f3.png"},98606:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/Export_ADO_CheckAzureDevOpsExportStfAcc-1067ea82c3f3b98a9ae13131bfb85035.gif"},25346:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/Export_ADO_CommitFiles-b8ad417e67074d3ae587901f68555ea7.gif"},32767:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/Export_ADO_CreateStgAccount-169b0507885b17bde9569ed0ccccb95d.gif"},95138:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/Export_ADO_ImportPipelineVariables-7e89a1a95ca45e77a5c6ad1febab6765.gif"},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>s});var o=n(96540);const r={},i=o.createContext(r);function a(e){const t=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(i.Provider,{value:t},e.children)}},34361:e=>{e.exports=JSON.parse('{"permalink":"/azure/export-azure-devops-repos-azure-storage-account","source":"@site/blog/2024-05-13-ado-export-stgaccount/index.mdx","title":"Export Azure DevOps Repositories to Azure Storage Account","description":"This is a step-by-step guide on exporting Azure DevOps repositories to an Azure Storage Account for backup and recovery.","date":"2024-05-13T08:21:56.906Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":15.82,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Export Azure DevOps Repositories to Azure Storage Account","metaDescription":"Learn how to export Azure DevOps repositories to an Azure Storage Account for backup and disaster recovery.","description":"This is a step-by-step guide on exporting Azure DevOps repositories to an Azure Storage Account for backup and recovery.","date":"2024-05-13T08:21:56.906Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/export-azure-devops-repos-azure-storage-account","keywords":["Azure","Azure DevOps","Repositories","Export","Azure Storage Account","Backup","Disaster Recovery"]},"unlisted":false,"prevItem":{"title":"Cloud Design Patterns","permalink":"/azure/cloud-design-patterns"},"nextItem":{"title":"Deploying Large Language Models on AKS with Kaito","permalink":"/azure/run-local-llm-aks"}}')}}]);