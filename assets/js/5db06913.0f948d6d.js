"use strict";(self.webpackChunklukemurraynz=self.webpackChunklukemurraynz||[]).push([[12013],{14888:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"azure","metadata":{"permalink":"/azure","source":"@site/blog/2025-04-25-acrcontinouspatching/index.mdx","title":"Azure Container Registry Continuous Patching for Security","description":"Learn how to use Azure Container Registry\'s Continuous Patching to detect and fix vulnerabilities in container images with Trivy and Copa.","date":"2025-04-25T05:16:22.619Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":9.355,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Container Registry Continuous Patching for Security","metaDescription":"Learn how to use Azure Container Registry\'s Continuous Patching to detect and fix vulnerabilities in container images with Trivy and Copa.","date":"2025-04-25T05:16:22.619Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure","keywords":["Azure","Container Registry","ACR","Continuous Patching","Container Security","Trivy","Copa","Vulnerability Scanning","DevSecOps"],"description":"Learn how to use Azure Container Registry\'s Continuous Patching to detect and fix vulnerabilities in container images with Trivy and Copa."},"unlisted":false,"nextItem":{"title":"Processing SFTP Events with Azure Function and Event Hub","permalink":"/azure/sftp-event-hub-function"}},"content":"Last year, I blogged about [Container Patching with Azure DevOps, Trivy and Copacetic](https://luke.geek.nz/azure/automate-container-patching-with-trivy-copacetic-azure-devops/), and how to use Azure DevOps to automate the patching of your container images using Trivy and Copacetic. This was a great solution, but it required a lot of manual work to set up and maintain. Today, I am going to take a look at Continuous Patching with Azure Container Registry (ACR) and how to use it to automate the patching of your container images.\\n\\n{/* truncate */}\\n\\n## \ud83d\udee1\ufe0f What is ACR Continuous Patching?\\n\\n:::warning\\nAt the time of writing, this is a **Preview feature**, so the experience we run through today may change in the future or the feature may be removed entirely.\\n\\nThe following limitations apply:\\n\\n* Windows-based container images aren\'t supported.\\n* Only \\"OS-level\\" vulnerabilities that originate from system packages will be patched. This includes system packages in the container image managed by an OS package manager such as \\"apt\u201d and \\"yum\u201d. Vulnerabilities that originate from application packages, such as packages used by programming languages like Go, Python, and NodeJS, cannot be patched.\\n* End of Service Life (EOSL) images are not supported by Continuous Patching. EOSL images refer to images where updates, security patches, or technical support are no longer available for the underlying operating system. Examples include images based on older operating system versions, such as Debian 8 and Fedora 28. EOSL images will be skipped from the patch, despite having vulnerabilities. The recommended approach is to upgrade the underlying operating system of your image to a supported version.\\n:::\\n\\n:::info\\n[Azure Container Registry](https://learn.microsoft.com/azure/container-registry/container-registry-intro?WT.mc_id=AZ-MVP-5004796)\'s Continuous Patching feature automates the detection and remediation of operating system(OS) level vulnerabilities in container images. By scheduling regular scans with [Trivy](https://trivy.dev/) and applying security fixes using [Copa](https://project-copacetic.github.io/copacetic/website/), you can maintain secure, up-to-date images in your registry, without requiring access to source code or build pipelines. Simply customize the schedule and target images to keep your Azure Container Registry(ACR) environment safe and compliant.\\n\\nHere are a few scenarios to use Continuous Patching:\\n\\n* Enforcing container security and hygiene: Continuous Patching enables users to quickly fix OS container CVEs without the need to rebuild from upstream fully.\\n* Speed of Use: Continuous Patching eliminates the dependency on upstream updates for specific images by automatically updating packages. Vulnerabilities can appear every day, while popular image publishers may only release new content once a month. With Continuous Patching, you can ensure that container images within your registry are patched as soon as the latest set of OS vulnerabilities is detected.\\n:::\\n\\n## \ud83e\uddea Test Environment Setup\\n\\nIn my testing, I am going to use the [api-firewall](https://hub.docker.com/_/api-firewall) image, version [0.6.16](https://hub.docker.com/layers/library/api-firewall/0.6.16/images/sha256-d6538706176b7c481355abb46dbeb25afcb272647e37dc2521e2a40db55a046c) that I have pushed to my Azure Container Registry.\\n\\n![Image - Azure Container Registry](images/ContinuousPatching_ACR_APIFirewallImageBefore.jpg)\\n\\n## \u2699\ufe0f How Continuous Patching Works\\n\\nLet\'s take a look at how this will work.\\n\\nContinuous Patching in ACR creates a new image per patch. ACR relies on a tag convention to version and identify patched images. The two main approaches are incremental and floating.\\n\\n| Feature | Incremental Tagging | Floating Tagging |\\n| ------- | ------------------- | ---------------- |\\n| **How It Works** | Adds numerical suffix (-1, -2, etc.) to original tag | Uses single mutable tag \\"-patched\\" that always points to latest version |\\n| **Example** | If base is python:3.11:- First patch: python:3.11-1- Second patch: python:3.11-2 | If base is python:3.11:- All patches use: python:3.11-patched |\\n| **Special Rules** | - Tags -1 to -999 are considered patch tags- Tags with -x where x > 999 are treated as original tags- Avoid pushing your tags ending with -1 to --999- Errors if -999 versions are reached | Tag automatically updates with each new patch |\\n| **Version History** | Preserved (each patch gets unique tag) | Not preserved (single tag is updated) |\\n\\n![Image - Azure Container Registry](images/patching_timeline_example1.png)\\n\\nIncremental (default) is ideal for environments where auditability and rollbacks are crucial, as a unique tag identifies each new patch.\\n\\nFloating is ideal if you prefer a single pointer to the latest patch for your CI/CD pipelines. Reduces complexity by eliminating the need to update references in downstream applications with each patch, but sacrifices strict versioning, making it challenging to roll back.\\n\\n## \ud83d\ude80 Getting Started with Continuous Patching\\n\\nSo let us get started. To begin, I will utilize my [Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding) Codespace configuration, which already meets the prerequisites I need, specifically [Azure CLI](https://learn.microsoft.com/cli/azure/what-is-azure-cli?WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\udce6 Installing the Required Extension\\n\\nTo start, we need to install the CLI extension for ACR Continuous Patching:\\n\\n```bash\\naz extension add --source https://acrcssc.z5.web.core.windows.net/debug/acrcssc-1.0.0b2-py3-none-any.whl\\n```\\n\\n![Install Custom extension](images/ACR_ContinuousPatching_InstallExtension.gif)\\n\\nThen we need to log in to our Azure Container Registry:\\n\\n```bash\\naz login\\naz acr login -n <myRegistry>\\n```\\n![Login to ACR](images/ACR_ContinuousPatching_LoginAzureACR.gif)\\n\\n### \ud83d\udcdd Configuring the Continuous Patching Schema\\n\\nNow we need to prepare the JSON schema that we will use to configure our Continuous Patching.\\n\\nThis schema defines which repositories and tags to patch, when to patch them, and how to tag the patched images.\\n\\nThe schema includes these key components:\\n\\n* `version` - Used by the ACR team to track schema versions. Don\'t modify this unless instructed.\\n* `tag-convention` - Optional field that specifies the tagging method. Values can be \\"incremental\\" (default) or \\"floating\\".\\n* `repositories` - An array of objects containing:\\n  * `repository` - The name of the repository to patch\\n  * `tags` - Array of specific tags to patch (use wildcard `*` to include all tags)\\n  * `enabled` - Boolean (true/false) to enable or disable patching for this repository\\n\\nFor example:\\n\\n```json\\n{\\n  \\"version\\": \\"v1\\",\\n  \\"tag-convention\\": \\"incremental\\",\\n  \\"repositories\\": [\\n    {\\n      \\"repository\\": \\"api-firewall\\",\\n      \\"tags\\": [\\"0.6.7\\", \\"latest\\"],\\n      \\"enabled\\": true\\n    }\\n  ]\\n}\\n```\\n\\nSo let us create the file as below:\\n\\n```bash\\ncat <<EOF > continuouspatching.json\\n{\\n  \\"version\\": \\"v1\\",\\n  \\"tag-convention\\": \\"incremental\\",\\n  \\"repositories\\": [\\n    {\\n      \\"repository\\": \\"api-firewall\\",\\n      \\"tags\\": [\\"0.6.7\\", \\"latest\\"],\\n      \\"enabled\\": true\\n    }\\n  ]\\n}\\nEOF\\n```\\n\\n### \ud83e\uddd0 Running a Dry Run\\n\\nNext, we can run a dry run of the supply-chain workflow to validate that our repository and tags are correct:\\n\\n```bash\\naz acr supply-chain workflow create -r myRegistry -g myResourceGroup -t continuouspatchv1 --config ./continuouspatching.json --schedule 1d --dry-run   \\n```\\n\\n![Image - ACR Continuous Patching](images/ACR_ContinuousPatching_DryRun_SupplyChain.gif)\\n\\nYou can also view the run in the Azure Portal, and see it pull the CSSC _(Microsoft\'s Containers Secure Supply Chain (CSSC) framework)_ image to run the workflow.\\n\\n![Image - ACR Continuous Patching](images/ContinuousPatching_ACR_DryRun_AzurePortal.jpg)\\n\\n:::tip\\nYou can use the `az acr supply-chain workflow update` command to update the workflow configuration, if needed, at a later stage.\\n:::\\n\\n## \ud83d\udcc5 Understanding the Scheduling System\\n\\nBefore we do, let\'s understand how scheduling works in ACR Continuous Patching.\\n\\n> The scheduling system works like a calendar with fixed dates each month, not like a countdown timer!\\n\\nThe `--schedule` parameter sets how many days between patch runs, but these always align to fixed days counting from the 1st of each month. Think of it like marking specific dates on a calendar.\\n\\nFor example, if you choose `--schedule 7d`, patching will run on the 1st, 8th, 15th, 22nd, and 29th of each month (every 7 days from the 1st).\\n\\nHere\'s what happens with different schedule values:\\n\\n| Schedule | Runs on these days each month | Example |\\n|----------|-------------------------------|---------|\\n| `1d` | Every day | Patches run daily |\\n| `3d` | 1st, 4th, 7th, 10th, 13th, 16th, 19th, 22nd, 25th, 28th, 31st | If today is the 5th, next run is on the 7th |\\n| `7d` | 1st, 8th, 15th, 22nd, 29th | If today is the 10th, next run is on the 15th |\\n| `14d` | 1st, 15th, 29th | If today is the 20th, next run is on the 29th |\\n| `30d` | 1st, 31st (if month has 31 days) | Runs at the beginning and end of the month |\\n\\nWhen you add the `--run-immediately` flag, a patch happens right away, and then the next one follows the regular schedule.\\n\\nRemember: The schedule always resets at the beginning of each month. So if your last patch in January was on the 29th with a `7d` schedule, the next one will be on February 1st, not February 5th.\\n\\n### \u25b6\ufe0f Executing the Workflow\\n\\nNow let\'s run the workflow:\\n\\n```bash\\naz acr supply-chain workflow create -r myRegistry -g myResourceGroup -t continuouspatchv1 --config ./continuouspatching.json --schedule 14d --run-immediately --verbose\\n```\\n\\n:::warning\\nIf you get the following warning: \\n\\n`Failed to validate and deploy template: (DeploymentFailed) At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-deployment-operations for usage details.\\nCode: DeploymentFailed\\nMessage: At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-deployment-operations for usage details.\\nTarget: /subscriptions/11b74992-d520-46e1-a9e9-b55c57d2e890/resourceGroups/acrcontinuouspatchingtest/providers/Microsoft.Resources/deployments/continuouspatchingdeployment\\nException Details:      (RoleAssignmentUpdateNotPermitted) Tenant ID, application ID, principal ID, and scope are not allowed to be updated.\\n        Code: RoleAssignmentUpdateNotPermitted\\n        Message: Tenant ID, application ID, principal ID, and scope are not allowed to be updated.      (RoleAssignmentUpdateNotPermitted) Tenant ID, application ID, principal ID, and scope are not allowed to be updated.`\\n\\nIt may mean that the Tasks have existed before, but if the Tasks were deleted in the portal, then it won\'t have removed the permission assignments.\\n\\nYou can check the permissions in the Access control (IAM) portal, and if they exist, you can delete them manually from the portal. \\n\\nIf you run the following workflow delete command _(preferred)_, it will delete the assigned RBAC permissions as well:\\n\\n```bash\\naz acr supply-chain workflow delete -r myRegistry -g myResourceGroup -t continuouspatchv1 --yes\\n```\\n:::\\n\\n:::tip\\nYou can add the `--verbose` flag to the command to get more detailed output during the workflow creation process and the `--debug` flag to get even more detailed output, including the HTTP requests and responses sent to Azure.\\n:::\\n\\n## \ud83d\udd27 Reviewing the Created Tasks\\n\\nNow that the Tasks have been created, you can see them in the Azure Portal.\\n\\n![Image - ACR Continuous Patching](images/ContinuousPatching_ACR_Run_TasksAzurePortal.jpg)\\n\\nYou can also run the following command to see the state of the workflows:\\n\\n```bash\\naz acr supply-chain workflow show -r myRegistry -g myResourceGroup -t Continuouspatchv1  -o table\\n```\\n\\nThe workflow creates three tasks in your Azure Container Registry:\\n\\n| Task Name | Description | Purpose |\\n|-----------|-------------|---------|\\n| **cssc-trigger-workflow** | Triggers the continuous patching workflow based on your configured schedule | This is the main scheduler that runs on your defined schedule (every 14 days in our example). It checks which repositories match your configuration and initiates the scanning process. |\\n| **cssc-scan-image** | Performs vulnerability scanning using Trivy | This task scans your container images for OS-level vulnerabilities. If vulnerabilities are found, it automatically triggers the patching task. |\\n| **cssc-patch-image** | Applies security patches using Copacetic | This task does the actual patching work, fixing OS vulnerabilities in your container images without needing to rebuild them from source. |\\n\\nThese three tasks work together to form a complete patching pipeline: the trigger starts on schedule, the scanner identifies vulnerabilities, and the patcher automatically fixes them.\\n\\n### \ud83d\udcca Monitoring Task Execution\\n\\nYou can check the logs of the run by looking up the recent Runs:\\n\\n```bash\\naz acr task logs --registry myRegistry\\n``````\\n\\n![Image - ACR Continuous Patching](images/ContinuousPatching_ACR_Logs.jpg)\\n\\n## \ud83c\udfaf Viewing the Results\\n\\nNow, if we go into the repositories in the Azure Portal, we can see that the image has been patched with the incremental tag.\\n\\n![Image - Azure Container Registry](images/ContinuousPatching_ACR_Patched.jpg)"},{"id":"azure/sftp-event-hub-function","metadata":{"permalink":"/azure/sftp-event-hub-function","source":"@site/blog/2025-04-18-sftpeventhubfunction/index.mdx","title":"Processing SFTP Events with Azure Function and Event Hub","description":"A practical guide to setting up Azure Storage SFTP with Event Hub and Azure Functions to monitor and process file uploads","date":"2025-04-18T02:13:25.749Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.025,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Processing SFTP Events with Azure Function and Event Hub","metaDescription":"Learn how to use Azure Storage SFTP functionality with Event Hub to trigger Azure Functions for processing file uploads","date":"2025-04-18T02:13:25.749Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/sftp-event-hub-function","keywords":["Azure","SFTP","Event Hub","Azure Functions","PowerShell","Serverless","File Transfer","Event-driven","Storage Account","Integration","Monitoring","Data Processing","Blob Storage","Cloud Functions"],"description":"A practical guide to setting up Azure Storage SFTP with Event Hub and Azure Functions to monitor and process file uploads"},"unlisted":false,"prevItem":{"title":"Azure Container Registry Continuous Patching for Security","permalink":"/azure"},"nextItem":{"title":"Sending Emails with MCP and Azure Communication Services","permalink":"/azure/mcp-acs-email-integration"}},"content":"Today, we are going to use the [Azure Storage SFTP functionality](https://learn.microsoft.com/azure/storage/blobs/secure-file-transfer-protocol-support?WT.mc_id=AZ-MVP-5004796) and an [Event Hub](https://learn.microsoft.com/azure/event-hubs/event-hubs-about?WT.mc_id=AZ-MVP-5004796) to trigger an [Azure Function](https://learn.microsoft.com/azure/azure-functions/functions-overview?WT.mc_id=AZ-MVP-5004796). The Azure Function will then process the Write Logs, and output the File name, SFTP Local User name, Agent header and the SFTP Client IP address to the host, from there you can do whatever you want with the data.\\n\\n{/* truncate */}\\n\\n# \ud83d\ude80 Processing SFTP Events with Azure Functions and Event Hub\\n\\n## \ud83c\udfd7\ufe0f Environment Overview\\n\\nSo for my environment, I have a:\\n\\n- A SFTP Hierarchical namespace-enabled storage account\\n- A Flex Consumption Azure Function App\\n- A Standard Event Hub Namespace with an event hub instance with 1 partition and 1 consumer group\\n\\nThese resources are hosted in the New Zealand North Azure region.\\n\\n```mermaid\\nflowchart LR\\n    User[SFTP Client]--\x3e|Uploads File|Storage[(Azure Storage SFTP)]\\n    Storage--\x3e|Sends Event|EventHubNS[Event Hub Namespace]\\n    subgraph EventHubNS[Event Hub Namespace]\\n        EventHub[Event Hub]\\n        ConsumerGroup[Consumer Group]\\n    end\\n    EventHub--\x3eConsumerGroup\\n    ConsumerGroup--\x3e|Triggers|Function[Azure Function]\\n    Function--\x3e|Processes|Output[File Details Output]\\n\\n    classDef azure fill:#0072C6,stroke:#0072C6,color:white;\\n    classDef client fill:#5C2D91,stroke:#5C2D91,color:white;\\n\\n    class Storage,EventHubNS,EventHub,EventHub,ConsumerGroup,Function azure;\\n    class User client;\\n```\\n\\nSo I\'ve already pre-created these resources, but is there some more information about the resource configuration that you can follow along with?\\n\\n## \ud83d\udce6 Storage Account Configuration\\n\\nFor the Azure Storage account with SFTP enabled, I\'m using the following configuration:\\n\\n- **Name**: sftptestluketest\\n- **Location**: New Zealand North\\n- **Performance**: Standard\\n- **Redundancy**: Locally-redundant storage (LRS)\\n- **Account kind**: StorageV2 (general purpose v2)\\n- **Access tier**: Hot\\n- **Key features enabled**:\\n  - Hierarchical namespace (HNS) \u2705\\n  - SFTP support \u2705\\n  - Local user authentication \u2705\\n- **Security settings**:\\n  - Minimum TLS version: 1.2\\n  - Public blob access: Disabled\\n  - Network access: Public (default action: Allow)\\n  - HTTPS only: Enabled\\n  - Azure services bypass: Enabled\\n  - Shared key access: Enabled\\n- **Diagnostic settings**:\\n\\n  - StorageWrite logs sent to Event Hub Namespace\\n  - Configured to capture StorageWrite logs for all blob operations\\n\\n  ![Azure Storage SFTP Blob Diagnostic](images/AzureSFTPBlogStorageDiagEH.jpg)\\n\\nThe hierarchical namespace (HNS) is a prerequisite for SFTP support in Azure Storage. This enables the directory and subdirectory structure that SFTP clients expect when connecting.\\n\\nWith SFTP and local users enabled, you can create local SFTP users that can authenticate with password and/or SSH key authentication, and assign them permissions to specific containers and directories within your storage account.\\n\\n## \ud83d\udce1 Event Hub Namespace Configuration\\n\\nFor receiving and processing the Azure Storage SFTP events, I\'ve set up an Event Hub Namespace with the following configuration:\\n\\n- **Name**: sftpeventhub\\n- **Location**: New Zealand North\\n- **Pricing tier**: Standard\\n- **Throughput capacity**: 1 throughput unit (base capacity)\\n- **Key features**:\\n  - Auto-inflate enabled \u2705\\n  - Maximum throughput units: 5 (scales automatically as needed)\\n  - Zone redundant \u2705\\n  - Kafka support enabled \u2705\\n- **Security settings**:\\n  - Minimum TLS version: 1.2\\n  - Network access: Public\\n\\nWithin this namespace, I\'ve created an Event Hub instance with a single partition and a dedicated consumer group that will be used by the Azure Function to process SFTP events.\\n\\n![Event Hub Namespace Configuration](images/EventHubNSConfig.jpg)\\n\\n## \u2699\ufe0f Function App Configuration\\n\\nFor processing the SFTP events from the Event Hub, I\'ve set up an Azure Function App with the following configuration:\\n\\n- **Name**: sftptest\\n- **Location**: Australia East (Different from Storage and Event Hub to demonstrate cross-region capability)\\n- **Hosting plan**: FlexConsumption (serverless)\\n- **Runtime stack**: PowerShell 7.4\\n- **Operating system**: Linux\\n- **Key features**:\\n  - Application Insights enabled \u2705\\n  - HTTPS only \u2705\\n  - System-assigned managed identity \u2705\\n- **Scaling configuration**:\\n  - Maximum instance count: 100\\n  - Instance memory: 2048 MB\\n- **Security settings**:\\n  - Public network access: Enabled\\n  - Client certificate mode: Required\\n\\n## \ud83d\udd12 Authentication Setup\\n\\nThe Function App connects to the Event Hub namespace using the System Managed Identity of the Function App, which is granted the `Azure Event Hubs Data Receiver` role on the Event Hub namespace.\\n\\n![Function App Configuration](images/Function_SMI_EventHubDataReceiver.jpg)\\n\\n## \ud83d\udee0\ufe0f Function App Settings\\n\\nTo use the Event Hub trigger in the Azure Function with Managed Identity authentication, you need to add the following settings to the Function App settings :\\n\\n```json\\n{\\n  \\"EventHubConnection__credential\\": managedIdentity\\n  \\"EventHubConnection__fullyQualifiedNamespace\\": \\"<Your Event Hub Namespace>.servicebus.windows.net\\"\\n}\\n```\\n\\n![Event Hub App setting](images/Function_AppSetting.jpg)\\n\\n## \ud83d\udccb Function Configuration File\\n\\nThe namespace, Consumer Group, and identity type will need to be updated in the function.json file.\\n\\n```json\\n{\\n  \\"bindings\\": [\\n    {\\n      \\"type\\": \\"eventHubTrigger\\",\\n      \\"name\\": \\"eventHubMessages\\",\\n      \\"direction\\": \\"in\\",\\n      \\"eventHubName\\": \\"sftp\\",\\n      \\"connection\\": \\"EventHubConnection\\",\\n      \\"cardinality\\": \\"many\\",\\n      \\"consumerGroup\\": \\"$Default\\",\\n      \\"identity\\": \\"SystemAssigned\\",\\n      \\"fullyQualifiedNamespace\\": \\"sftpeventhub.servicebus.windows.net\\"\\n    }\\n  ]\\n}\\n```\\n\\n## \ud83d\udcbb PowerShell Code\\n\\nHere is my run.ps1 file for the Azure Function:\\n\\n```powershell\\nparam($eventHubMessages, $TriggerMetadata)\\n\\nWrite-Host \\"PowerShell SFTP event hub trigger function called\\"\\n\\n# Process each message\\n$eventHubMessages | ForEach-Object {\\n    # Get the records array from the message\\n    if ($_ -is [System.Management.Automation.OrderedHashtable] -or $_ -is [hashtable]) {\\n        if ($_.ContainsKey(\'records\')) {\\n            Write-Host \\"Processing SFTP Storage events...\\"\\n\\n            # Filter for StorageWrite operations with status code 200\\n            $filteredRecords = $_.records | Where-Object {\\n                $_.category -eq \\"StorageWrite\\" -and $_.statusCode -eq 200 -and $_.operationName -eq \\"SftpCreate\\"\\n            }\\n\\n            Write-Host \\"Found $($filteredRecords.Count) StorageWrite 200 operations\\"\\n\\n            # Display only the requested information\\n            foreach ($record in $filteredRecords) {\\n                $output = [ordered]@{\\n                    \'Operation\'     = $record.operationName\\n                    \'FileName\'      = if ($record.properties.objectKey -match \'^(?:[^/]*/){3}(.*)$\') { $matches[1] } else { $record.properties.objectKey }\\n                    \'UserID\'        = $record.identity.requester.objectId\\n                    \'UserIPAddress\' = $record.callerIpAddress\\n                    \'UserAgent\'     = $record.properties.userAgentHeader\\n                    \'Time\'          = $record.time\\n                }\\n\\n                # Output in a clean table format\\n                Write-Host \\"----------------------------------------\\"\\n                foreach ($key in $output.Keys) {\\n                    Write-Host \\"$($key.PadRight(12)): $($output[$key])\\"\\n                }\\n            }\\n\\n            if ($filteredRecords.Count -gt 0) {\\n                Write-Host \\"----------------------------------------\\"\\n            }\\n        }\\n    }\\n    else {\\n        Write-Host \\"Message wasn\'t in the expected format\\"\\n    }\\n}\\n```\\n\\n:::info\\nYou can find the Function App code in the following GitHub repository [lukemurraynz/SFTPEventHubFunction](https://github.com/lukemurraynz/SFTPEventHubFunction) for this blog post.\\n:::\\n\\n## \ud83d\udd0d See it in Action!\\n\\nSo let us take a look at it in action!\\n\\n![Azure SFTP Blob Write Run](images/SFTPEventHubFunctionRun.gif)\\n\\n## \u23f1\ufe0f Performance Considerations\\n\\n_(On average, I have seen this take about 2 minutes, from the initial file upload, to the Function execution)_.\\n\\n![SFTP Function execution](images/SFTPFuncExecuteTime.jpg)\\n\\n## \ud83d\ude80 Next Steps and Extensions\\n\\nHopefully, that\'s given you the base to work from; you could potentially add logic around if a file from x user, do this, or notify the user of receipt based on a map lookup, etc."},{"id":"azure/mcp-acs-email-integration","metadata":{"permalink":"/azure/mcp-acs-email-integration","source":"@site/blog/2025-04-07-mcpacsemailserver/index.mdx","title":"Sending Emails with MCP and Azure Communication Services","description":"Learn how to use the Model Context Protocol (MCP) to connect GitHub Copilot to Azure Communication Services for sending emails directly from AI interactions.","date":"2025-04-07T09:05:54.303Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":10.25,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Sending Emails with MCP and Azure Communication Services","metaDescription":"Learn how to use the Model Context Protocol (MCP) to connect GitHub Copilot to Azure Communication Services for sending emails directly from AI interactions.","date":"2025-04-07T09:05:54.303Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/mcp-acs-email-integration","keywords":["Azure","Model Context Protocol","MCP","Azure Communication Services","Email","GitHub Copilot","Python","AI Integration","MCP Server","MCP Client"],"description":"Learn how to use the Model Context Protocol (MCP) to connect GitHub Copilot to Azure Communication Services for sending emails directly from AI interactions."},"unlisted":false,"prevItem":{"title":"Processing SFTP Events with Azure Function and Event Hub","permalink":"/azure/sftp-event-hub-function"},"nextItem":{"title":"Understanding Workload Criticality in the Cloud","permalink":"/azure/business-critical-workloads"}},"content":"You\'ve probably heard the acronym \'MCP\' before or [Model Context Protocol _(MCP)_](https://modelcontextprotocol.io/). Open-sourced by Anthropic in November 2024, it has quickly become the go-to standard for connecting AI assistants to systems of various types.\\n\\n> The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: Developers can expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.\\n\\nToday, we are going to use the MCP protocol to connect to [Azure Communication Services](https://learn.microsoft.com/azure/communication-services/overview?WT.mc_id=AZ-MVP-5004796) and send an email from an MCP client and server running on a GitHub Codespace from GitHub Copilot.\\n\\n{/* truncate */}\\n\\nThe MCP protocol has two different transport layers:\\n\\n* Stdio transport: This is the default transport layer. It uses standard input and output to communicate between the MCP client and server. This is useful for local development and testing.\\n* HTTP transport: This transport layer uses HTTP to communicate between the MCP client and server. This is useful for production environments where the MCP client and server run on different machines.\\n\\nIn our use case, we will use the Stdio transport layer because we are running the MCP client and server on the same machine (GitHub Codespace). \\n\\nThere are also different types of primitives that an MCP server can expose:\\n\\n* Resources are a core primitive in the Model Context Protocol (MCP) that allows servers to expose data and content that clients can read and use as context for LLM interactions.\\n* Prompts enable servers to define reusable prompt templates and workflows that clients can quickly surface to users and LLMs. They provide a powerful way to standardize and share common LLM interactions.\\n* Tools are a powerful primitive in the Model Context Protocol (MCP) that enable servers to expose executable functionality to clients. Through tools, LLMs can interact with external systems, perform computations, and take actions in the real world.\\n\\n![Model Context Protocol (MCP) Explained](images/MCPClearlyExplained.gif)\\n_(Image credit: [blog.dailydoseofds.com](https://blog.dailydoseofds.com/p/visual-guide-to-model-context-protocol))_\\n\\nWe will use the `Tool` primitive to expose a tool for sending emails using Azure Communication Services. The MCP server will expose the tool, which the MCP client will use to send the email when prompted in the GitHub Copilot chat.\\n\\nThe MCP server will run on a GitHub Codespace and send an email using Azure Communication Services. The MCP client will run on the same Codespace and send the email using GPT-4o.\\n \\n Using Python, we will use the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk) to do a lot of the heavy lifting for us. The SDK is a wrapper around the MCP protocol and allows us to create an MCP server quickly, and thanks to a recent update by [GitHub Model Context Protocol (MCP) is now avaliable in Public preview in GitHub Copilot](https://github.blog/news-insights/product-news/github-copilot-agent-mode-activated/#model-context-protocol-mcp-is-now-available-in-public-preview), which allows us us to use the MCP protocol straight from GitHub Copilot chat without needing Claude Desktop for example, installed.\\n\\nWe are going to create three tools:\\n\\n1. send_email\\nPurpose: Send a simple email to a single recipient.\\nParameters:\\n* recipient: The email address of the recipient.\\n* subject: The subject line of the email.\\n* content: The HTML or plain text content of the email.\\nUsage: This tool is used for sending basic emails without attachments.\\n\\n2. send_email_with_attachments\\nPurpose: Send an email with file attachments to a single recipient.\\nParameters:\\n* recipient: The email address of the recipient.\\n* subject: The subject line of the email.\\n* content: The HTML or plain text content of the email.\\n* attachments: A list of file paths to attach to the email.\\nUsage: This tool is used when you need to send documents, images, or other files along with the email.\\n\\n3. send_bulk_email\\nPurpose: Send the same email to multiple recipients in a single operation.\\nParameters:\\n* recipients: A list of email addresses to send the email to.\\n* subject: The subject line of the email.\\n* content: The HTML or plain text content of the email.\\nUsage: This tool is ideal for sending announcements, updates, or newsletters to a group of recipients.\\n\\nEach tool will be implemented as a function in Python, and we will use the MCP Python SDK to expose these functions as tools in the MCP server. The MCP client will then be able to call these tools to send emails using Azure Communication Services based on the prompts we provide in GitHub Copilot chat.\\n\\n\\n:::info\\nYou can find the ACS Email MCP server code on my GitHub repo here: [lukemurraynz/mcp-server-acsemail](https://github.com/lukemurraynz/mcp-server-acsemail). Feel free to clone it, run it, and open any Pull Requests if you find any issues or have suggestions for improvements.\\n:::\\n\\nDue to the Python dependencies, we will be using a devcontainer, using the [python:3.12-bullseye](https://hub.docker.com/r/microsoft/devcontainers-python) image as our base image. This will allow us to use the latest version of Python, and the postCreateCommand we will install the python depedencies using pip.\\n\\n```txt\\nmcp\\nazure-communication-email\\npython-dotenv\\n```\\nAssuming you have the Azure Communication Services Email resource created, and the connection string saved in a `.env` file in the root of your project. The connection string should look something like this:\\n\\n```txt\\n# Azure Communication Services credentials\\nACS_CONNECTION_STRING=endpoint=\'https://azcommunicationservicestest.australia.communication.azure.com/;accesskey=FPipiKEH95KCHYQnJOxqMV0P7ZpP3qcKWZHaAhulbKCWI3fPyQZQJQQJ99BDACULyCphGPgtAAAAAZCS58g3\'\\nACS_SENDER_ADDRESS=\'DoNotReply@58b87ce7-7f73-444a-ba0b-d754677503cf.azurecomm.net\'\\n```\\nThe `ACS_SENDER_ADDRESS` is the email address that will be used to send the email. This is the email address that will appear in the \\"From\\" field of the email.\\n\\nThe Python code for the MCP server is as follows:\\n\\n```python\\n# basic import \\nfrom mcp.server.fastmcp import FastMCP\\nimport logging\\nfrom azure.communication.email import EmailClient\\nimport os\\nfrom dotenv import load_dotenv\\nfrom typing import Dict, Any, Optional\\n\\n# Configure logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\\n)\\nlogger = logging.getLogger(\\"acs-email-sender\\")\\n\\n# Load environment variables\\nload_dotenv()\\n\\n# Azure Communication Services configuration\\nACS_CONNECTION_STRING = os.getenv(\\"ACS_CONNECTION_STRING\\")\\nACS_SENDER_ADDRESS = os.getenv(\\"ACS_SENDER_ADDRESS\\")\\n\\n# Validation at startup\\ndef validate_config() -> bool:\\n    \\"\\"\\"Validate the required configuration is present\\"\\"\\"\\n    if not ACS_CONNECTION_STRING or ACS_CONNECTION_STRING == \\"your_acs_connection_string_here\\":\\n        logger.error(f\\"ACS_CONNECTION_STRING is not configured properly\\")\\n        return False\\n    \\n    if not ACS_SENDER_ADDRESS or ACS_SENDER_ADDRESS == \\"DoNotReply@your-domain.azurecomm.net\\":\\n        logger.error(f\\"ACS_SENDER_ADDRESS is not configured properly\\")\\n        return False\\n    \\n    logger.info(f\\"Email configuration validated. Sender: {ACS_SENDER_ADDRESS}\\")\\n    return True\\n\\n# Cache the email client to avoid recreating it for each request\\n_email_client: Optional[EmailClient] = None\\n\\ndef get_email_client() -> Optional[EmailClient]:\\n    \\"\\"\\"Get or create an EmailClient instance\\"\\"\\"\\n    global _email_client\\n    if _email_client is None:\\n        try:\\n            _email_client = EmailClient.from_connection_string(ACS_CONNECTION_STRING)\\n            logger.info(\\"Email Client initialized successfully\\")\\n        except Exception as e:\\n            logger.error(f\\"Failed to initialize Email Client: {str(e)}\\")\\n            return None\\n    return _email_client\\n\\n# instantiate an MCP server client\\nmcp = FastMCP(\\"Email Sender\\")\\n\\n# DEFINE TOOLS\\n\\n@mcp.tool(name=\\"send_email\\")  \\ndef send_email(recipient: str, subject: str, content: str) -> str:\\n    \\"\\"\\"Send all emails using Azure Communication Services\\n    \\n    Args:\\n        recipient: Email address of the recipient\\n        subject: Subject line of the email\\n        content: HTML content of the email\\n        \\n    Returns:\\n        A message indicating the status of the email sending operation\\n    \\"\\"\\"\\n    logger.info(f\\"Starting email sending process to: {recipient}\\")\\n    \\n    # Validate configuration\\n    if not validate_config():\\n        return \\"Error: Email service is not configured properly. Check your .env file.\\"\\n    \\n    # Get email client\\n    email_client = get_email_client()\\n    if not email_client:\\n        return \\"Error: Unable to initialize email client\\"\\n    \\n    try:\\n        # Create the email message\\n        logger.info(f\\"Creating email message with subject: {subject}\\")\\n        message = create_email_message(recipient, subject, content)\\n        \\n        # Send the email\\n        logger.info(f\\"Beginning email send operation\\")\\n        poller = email_client.begin_send(message)\\n        logger.info(f\\"Email send operation initiated, waiting for result\\")\\n        result = poller.result()\\n        \\n        return process_email_result(result, recipient)\\n    except Exception as e:\\n        logger.error(f\\"Failed to send email: {str(e)}\\", exc_info=True)\\n        return f\\"Failed to send email: {str(e)}\\"\\n\\ndef create_email_message(recipient: str, subject: str, content: str) -> Dict[str, Any]:\\n    \\"\\"\\"Create an email message for Azure Communication Services\\"\\"\\"\\n    return {\\n        \\"content\\": {\\n            \\"subject\\": subject,\\n            \\"plainText\\": content,\\n            \\"html\\": content\\n        },\\n        \\"recipients\\": {\\n            \\"to\\": [\\n                {\\n                    \\"address\\": recipient,\\n                    \\"displayName\\": \\"Email Recipient\\"\\n                }\\n            ]\\n        },\\n        \\"senderAddress\\": ACS_SENDER_ADDRESS\\n    }\\n\\ndef process_email_result(result: Any, recipient: str) -> str:\\n    \\"\\"\\"Process the result from an email send operation\\"\\"\\"\\n    if isinstance(result, dict) and \'id\' in result:\\n        message_id = result.get(\'id\', \'unknown\')\\n        logger.info(f\\"Email sent successfully to {recipient}. Message ID: {message_id}\\")\\n        return f\\"Email sent to {recipient} successfully! Message ID: {message_id}\\"\\n    elif hasattr(result, \'message_id\'):\\n        logger.info(f\\"Email sent successfully to {recipient}. Message ID: {result.message_id}\\")\\n        return f\\"Email sent to {recipient} successfully! Message ID: {result.message_id}\\"\\n    else:\\n        logger.info(f\\"Email sent successfully to {recipient}.\\")\\n        return f\\"Email sent to {recipient} successfully!\\"\\n\\n@mcp.tool(name=\\"send_email_with_attachments\\")\\ndef send_email_with_attachments(recipient: str, subject: str, content: str, attachments: list) -> str:\\n    \\"\\"\\"Send email with file attachments using Azure Communication Services\\n    \\n    Args:\\n        recipient: Email address of the recipient\\n        subject: Subject line of the email\\n        content: HTML content of the email\\n        attachments: List of file paths to attach\\n        \\n    Returns:\\n        A message indicating the status of the email sending operation\\n    \\"\\"\\"\\n    logger.info(f\\"Starting email sending process with attachments to: {recipient}\\")\\n    \\n    # Validate configuration\\n    if not validate_config():\\n        return \\"Error: Email service is not configured properly. Check your .env file.\\"\\n    \\n    # Get email client\\n    email_client = get_email_client()\\n    if not email_client:\\n        return \\"Error: Unable to initialize email client\\"\\n    \\n    try:\\n        # Create the email message with attachments\\n        logger.info(f\\"Creating email message with subject: {subject} and {len(attachments)} attachments\\")\\n        message = create_email_message_with_attachments(recipient, subject, content, attachments)\\n        \\n        # Send the email\\n        logger.info(f\\"Beginning email send operation with attachments\\")\\n        poller = email_client.begin_send(message)\\n        logger.info(f\\"Email send operation initiated, waiting for result\\")\\n        result = poller.result()\\n        \\n        return process_email_result(result, recipient)\\n    except Exception as e:\\n        logger.error(f\\"Failed to send email with attachments: {str(e)}\\", exc_info=True)\\n        return f\\"Failed to send email with attachments: {str(e)}\\"\\n\\ndef create_email_message_with_attachments(recipient: str, subject: str, content: str, attachments: list) -> Dict[str, Any]:\\n    \\"\\"\\"Create an email message with attachments for Azure Communication Services\\"\\"\\"\\n    import base64\\n    \\n    message = {\\n        \\"content\\": {\\n            \\"subject\\": subject,\\n            \\"plainText\\": content,\\n            \\"html\\": content\\n        },\\n        \\"recipients\\": {\\n            \\"to\\": [\\n                {\\n                    \\"address\\": recipient,\\n                    \\"displayName\\": \\"Email Recipient\\"\\n                }\\n            ]\\n        },\\n        \\"senderAddress\\": ACS_SENDER_ADDRESS,\\n        \\"attachments\\": []\\n    }\\n    \\n    for file_path in attachments:\\n        try:\\n            with open(file_path, \\"rb\\") as file:\\n                file_content = file.read()\\n                filename = os.path.basename(file_path)\\n                \\n                # Base64 encode the file content\\n                encoded_content = base64.b64encode(file_content).decode(\'utf-8\')\\n                \\n                # Add attachment to message\\n                message[\\"attachments\\"].append({\\n                    \\"name\\": filename,\\n                    \\"contentType\\": \\"application/octet-stream\\",\\n                    \\"contentInBase64\\": encoded_content\\n                })\\n                \\n                logger.info(f\\"Added attachment: {filename}\\")\\n        except Exception as e:\\n            logger.warning(f\\"Failed to add attachment {file_path}: {str(e)}\\")\\n    \\n    return message\\n\\n@mcp.tool(name=\\"send_bulk_email\\")\\ndef send_bulk_email(recipients: list, subject: str, content: str) -> str:\\n    \\"\\"\\"Send email to multiple recipients using Azure Communication Services\\n    \\n    Args:\\n        recipients: List of email addresses to send to\\n        subject: Subject line of the email\\n        content: HTML content of the email\\n        \\n    Returns:\\n        A message indicating the status of the email sending operation\\n    \\"\\"\\"\\n    logger.info(f\\"Starting bulk email sending process to {len(recipients)} recipients\\")\\n    \\n    # Validate configuration\\n    if not validate_config():\\n        return \\"Error: Email service is not configured properly. Check your .env file.\\"\\n    \\n    # Get email client\\n    email_client = get_email_client()\\n    if not email_client:\\n        return \\"Error: Unable to initialize email client\\"\\n    \\n    success_count = 0\\n    failed_recipients = []\\n    \\n    try:\\n        # Create the email message for multiple recipients\\n        logger.info(f\\"Creating bulk email message with subject: {subject}\\")\\n        message = create_bulk_email_message(recipients, subject, content)\\n        \\n        # Send the email\\n        logger.info(f\\"Beginning bulk email send operation\\")\\n        poller = email_client.begin_send(message)\\n        logger.info(f\\"Bulk email send operation initiated, waiting for result\\")\\n        result = poller.result()\\n        \\n        success_count = len(recipients)\\n        return f\\"Bulk email sent successfully to {success_count} recipients!\\"\\n    except Exception as e:\\n        logger.error(f\\"Failed to send bulk email: {str(e)}\\", exc_info=True)\\n        return f\\"Failed to send bulk email: {str(e)}\\"\\n\\ndef create_bulk_email_message(recipients: list, subject: str, content: str) -> Dict[str, Any]:\\n    \\"\\"\\"Create an email message for multiple recipients using Azure Communication Services\\"\\"\\"\\n    to_list = []\\n    for recipient in recipients:\\n        to_list.append({\\n            \\"address\\": recipient,\\n            \\"displayName\\": \\"Email Recipient\\"\\n        })\\n    \\n    return {\\n        \\"content\\": {\\n            \\"subject\\": subject,\\n            \\"plainText\\": content,\\n            \\"html\\": content\\n        },\\n        \\"recipients\\": {\\n            \\"to\\": to_list\\n        },\\n        \\"senderAddress\\": ACS_SENDER_ADDRESS\\n    }\\n\\n\\n# execute and return the stdio output\\nif __name__ == \\"__main__\\":\\n    # Validate configuration at startup\\n    if not validate_config():\\n        logger.warning(\\"Starting with invalid configuration - email sending will fail\\")\\n    \\n    logger.info(\\"Starting MCP server for ACS Email Sender\\")\\n    mcp.run(transport=\\"stdio\\")\\n```\\n\\nTo allow GitHub Copilot to recognize and run the server, a `.vscode/mcp.json` file is created to run the server.\\n\\n```json\\n{\\n    \\"servers\\": {\\n        \\"email\\": {\\n            \\"command\\": \\"python\\",\\n            \\"args\\": [\\n                \\"/workspaces/mcp-server-acsemail/src/server.py\\",\\n            ]\\n        }\\n    }\\n}\\n```\\n\\nSo lets test this, first we will test an email of the selection using the `send_email` tool. \\n\\n![MCP Email Test](images/MCP-ACS_SendEmail.gif)\\n\\nNext, let\'s test the same, but to multiple recipients using the `send_bulk_email` tool.\\n\\n![MCP Email Test](images/MCP-ACS_BulkSendEmail.gif)\\n\\nLast, let\'s try sending the README.md as an attachment using the `send_email_with_attachments` tool.\\n\\n![MCP Email Test](images/MCP-ACS_SendEmailWithAttachment.gif)\\n\\nHopefully, that gives you a glimpse of how to use the MCP protocol to connect to [Azure Communication Services](https://learn.microsoft.com/azure/communication-services/overview?WT.mc_id=AZ-MVP-5004796) and send emails using GitHub Copilot chat. The MCP protocol is a powerful tool that allows you to connect AI assistants to systems of various types, and with the recent updates to GitHub Copilot, it is now easier than ever to use! This was just my first test usecase, can\'t wait to see what else we can do with it!"},{"id":"azure/business-critical-workloads","metadata":{"permalink":"/azure/business-critical-workloads","source":"@site/blog/2025-04-05-businesscriticalitycloudworkloads/index.mdx","title":"Understanding Workload Criticality in the Cloud","description":"Learn to assess workload criticality in Azure, understand SLAs, and make informed decisions based on business impact and risks.","date":"2025-04-05T05:41:20.421Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Service Management","permalink":"/tags/service-management"},{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":13.205,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Understanding Workload Criticality in the Cloud","metaDescription":"Learn to assess workload criticality in Azure, understand SLAs, and make informed decisions based on business impact and risks.","date":"2025-04-05T05:41:20.421Z","tags":["Azure","Service Management","Misc"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/business-critical-workloads","keywords":["Azure","Workload criticality","Service Level Agreement","SLA","Business critical workloads","Mission-critical applications","Cloud reliability","Availability zones","Risk assessment","Azure Well-Architected Framework","Cloud architecture"],"description":"Learn to assess workload criticality in Azure, understand SLAs, and make informed decisions based on business impact and risks."},"unlisted":false,"prevItem":{"title":"Sending Emails with MCP and Azure Communication Services","permalink":"/azure/mcp-acs-email-integration"},"nextItem":{"title":"How to Use GitHub Codespaces with Azure DevOps Repositories","permalink":"/azure/github-codespaces-azure-devops-repo"}},"content":"Today, we are going to take a look at something a little different. We are going to talk about workload criticality, methods to assess how critical your workload is, and the ramifications of that criticality in terms of the Cloud.\\n\\n![Workload Criticality](images/workload-criticality.jpg)\\n\\n{/* truncate */}\\n\\n## \ud83d\udce6 Defining a Workload\\n\\nFirst, let\'s define Workload.\\n\\n:::info\\nA workload is a collection of resources and code \u2013 Services/VMs, applications, data, or appliances \u2013 that delivers business value, such as a customer-facing application.\\n:::\\n\\n![Workload](images/workload-definition.jpg)\\n\\nAll these resources in the Microsoft Azure Cloud can be designed to work together to deliver a service or application. The resources can be anything from a simple web app to a complex multi-tier application with multiple services and databases.\\n\\nIn the Microsoft Azure Cloud, these resources can be deployed in different regions, availability zones, and resource groups. This allows for high availability and disaster recovery options to be built into the application architecture.\\n\\n## \ud83e\udd1d Shared Responsibility in the Cloud\\n\\nLet us delve into the [Reliability](https://learn.microsoft.com/azure/well-architected/reliability/?WT.mc_id=AZ-MVP-5004796) and Availability of these workloads by looking at the [Shared responsibility model](https://learn.microsoft.com/azure/reliability/concept-shared-responsibility?WT.mc_id=AZ-MVP-5004796).\\n\\nThere is some overlap in the responsibility of your workloads in the Cloud, and this is where the criticality of your workload comes into play. The more critical the workload, the more responsibility you have regarding availability and reliability, and the more you need to consider the implications of that criticality in your architecture and design. Before I jump ahead, let\'s assume Microsoft\'s responsibility as the customer and us as the cloud provider.\\n\\n![Shared responsibility model](images/shared-responsibility-model.jpg)\\n\\n:::info\\nMicrosoft is solely responsible for [core platform reliability](https://learn.microsoft.com/en-us/azure/reliability/concept-shared-responsibility?WT.mc_id=AZ-MVP-5004796#core-platform-reliability). Microsoft is also responsible for providing [resilience-enhancing capabilities](https://learn.microsoft.com/en-us/azure/reliability/concept-shared-responsibility?WT.mc_id=AZ-MVP-5004796#resilience-enhancing-capabilities) that you can use. You\'re responsible for selecting and using the appropriate components.\\n:::\\n\\nThe configuration of each of the resources that make up your workload determines how reliable and available it is. The more critical the workload, the more you need to consider its implications for your architecture and design.\\n\\n## \ud83d\udcdd Service Level Agreements (SLAs)\\n\\n> Understanding the service level agreements (SLAs) for each Azure service is important. SLAs provide essential information on the expected uptime of the service and any conditions you need to meet to be eligible for the SLA. For SLAs for each service, see [Service Level Agreements (SLA) for Online Services](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services).\\n\\nSo, let\'s delve a little deeper into SLAs and how they relate to the criticality of your workload.\\n\\nAn SLA _(Service Level Agreement)_ is a contract between you and the service provider that defines the level of service you can expect from the provider. It typically includes information on the availability of the service, the response time for support requests, and any penalties for not meeting the SLA. In the context of Azure resources, an SLA guarantees that the service will be available for a certain percentage of time over a given period. For example, an SLA of 99.9% means that the service is guaranteed to be available for 99.9% over a given period.\\n\\n![Workload SLA](images/workload-sla.JPG)\\n\\nThis means that the service can be down for a maximum of 43.2 minutes per month, or 8.76 hours per year. If the service is down for longer than this, you may be eligible for a credit on your bill. \\n\\nThere are some considerations however, and these are outlined in the SLA for each service. For example, if you are using a service that the SLA does not cover, or if you are not meeting the conditions of the SLA, you may not be eligible for a credit, and by meeting conditions, I mean the configuration of the resources that make up your workload - for example, if you are using a single instance of a service, and not Zone redundant, you may not be eligable for a credit, as the SLA may not match. There is a difference between an outage of one availability zone vs an entire region. Virtual Machines come to mind where the Disk type selected can impact the SLA of your resource.\\n\\n## \ud83c\udfaf SLO and SLI: Beyond SLAs\\n\\nWe won\'t go into detail about SLO (Service Level Objective) or SLI (Service Level Indicator) in this post, but they are important to understand when assessing the SLA of your workload. \\n\\nAn SLO is a target for the level of service you want to achieve, and an SLI measures the level of service you are achieving. The SLO and SLI can be used to measure the performance of your workload and help you identify areas for improvement, for Microsoft Azure cloud services, the SLO and SLIs are managed within Microsoft, however if you are looking at offering SLAs to your customers based on the services you are using, you will need to consider the SLO and SLI of the services you are using. How they relate to the SLA of your workload, the same is true in the context of OLAs _(Organisation Level Agreements)_.\\n\\n![Workload SLO](images/workload-slaslo.JPG)\\n\\n\\n\\n## \ud83e\udde9 Composite SLAs\\n\\nWhat we will touch on is [Composite SLAs](https://learn.microsoft.com/en-us/azure/well-architected/reliability/metrics?WT.mc_id=AZ-MVP-5004796#define-composite-slo-targets), which are a combination of the SLAs of the individual resources that make up your workload. For example, if you have a web app that is using an SQL database, the SLA for the web app and the SLA for the SQL database will determine the overall SLA for your workload. The composite SLA is calculated by multiplying the individual SLAs together. For example, if the web app has an SLA of 99.95% and the SQL database has an SLA of 99.99%, the composite SLA for the workload would be 99.94% _(Maximum acceptable downtime /Year: 315m 33s)_.\\nThat\'s lower than the individual SLAs, which isn\'t surprising because an application that relies on multiple services has more potential failure points.\\n\\nA few resources that are useful for working out Composite SLA\'s are:\\n\\n- [Azure Composite SLA Estimator](https://slaestimator.aztoso.com/)\\n- [SLA Calculator](https://wiki.unosd.com/slacalculator/)\\n\\n![CLSA](images/csla.jpg)\\n\\nFor more Active/Active workloads, you can have a parallel SLA _(Parallel SLA = 100% - (ServiceA unavailability * ServiceB unavailability))_.\\n\\n## \ud83c\udf10 Regions and Availability Zones\\n\\nI\'ve alluded to it but not touched on it - let us discuss [Regions](https://learn.microsoft.com/azure/reliability/regions-overview?WT.mc_id=AZ-MVP-5004796) and [Avaliability Zones](https://learn.microsoft.com/azure/reliability/availability-zones-overview?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796), which are key considerations when looking at how your Azure Cloud workloads are architected.\\n\\n![Azure Region](images/workload-azregion.JPG)\\n\\nAn Azure Region - such as [New Zealand North](https://datacenters.microsoft.com/globe/explore?info=region_newzealandnorth) - is a set of datacenters _(3 or more)_ deployed within a latency-defined perimeter and connected through a low-latency network, datacenters is a term I try to avoid when talking about the Microsoft Azure Cloud, as it is a little misleading, as the datacenters are not just a single building, but a collection of buildings and resources that make up the region, each called an Avaliability Zone, and each one designed to run independently of the others. This means that if one zone goes down, the other zones are still available, and your workload can continue to run. This is a key consideration when looking at the criticality of your workload, as it allows for a level of high availability and disaster recovery options to be built into the application architecture _(think Tier 4, from a datacenter tier/classification perspective)_. \\n\\n:::tip\\nThere isn\'t a 1:1 mapping of physical and logical _(ie, what you see in the Portal)_ mapping to your availability zones; the mapping is generated at the time that the subscription gets created, so Availability Zone 1 in one subscription may not be Availability Zone 1 in another. Refer to a previous article of mine for more information: [Azure Availability Zone Peering](https://luke.geek.nz/azure/azure-availability-zone-peering/).\\n:::\\n\\nWhether you use Availability Zones or cross-regions can depend a lot on the criticality of your workload and the SLA of your workload with your customers. A lot of redundancy is built into multiple layers, from the selection of the location for the data centers to be built through the hardware in the racks to the Azure fabric itself. However, issues do happen, and you need to consider your risk profile.\\n\\n:::tip\\nI suggest reading the Azure architecture center [Cloud design patterns](https://learn.microsoft.com/azure/architecture/patterns/?WT.mc_id=AZ-MVP-5004796) for recommendations on how to design your workloads, with patterns like retry, sharding, and bulkhead. I also touched on some design patterns in a previous article [Cloud Design Patterns](https://luke.geek.nz/azure/cloud-design-patterns/) and video [Cloud Design Patterns](https://youtu.be/nnuo_mxPcNw).\\n:::\\n\\n## \u26a0\ufe0f Risk Assessment\\n\\n![Workload Disaster Scope](images/workload-disasterscope.JPG)\\n\\nWhen considering the business criticality and plans, you need to consider the likelihood of an outage, and the impact and risk of that outage _(RISK = IMPACT x\\tPROBABILITY)_:\\n\\n| Risk               | Example                                 | Likelihood      |\\n|--------------------|-----------------------------------------|----------------|\\n| **Hardware outage** | Host reboot, node failure             | \ud83d\udfe0\ud83d\udfe0\ud83d\udfe0\ud83d\udfe0\ud83d\udfe0 Very likely |\\n| **Datacentre outage** | Power, cooling or network failure  | \ud83d\udfe0\ud83d\udfe0 Unlikely  |\\n| **Region outage** | Major natural disaster affecting wide area | \ud83d\udfe0 Very unlikely |\\n\\nAnd then, compare those risks to the design and SLA of your workload in accordance with the [Azure Well-Architected Framework](https://learn.microsoft.com/azure/well-architected/?WT.mc_id=AZ-MVP-5004796) pillars for guidance, as there are tradeoffs between the pillars and the criticality of your workload:\\n\\n|                          | Locally redundant | Zonal (pinned) | Zone-redundant | Multi-region |\\n|--------------------------|------------------|---------------|----------------|--------------|\\n| **Reliability**         | \ud83d\udd34 Low           | \ud83d\udd35 Depends    | \ud83d\udfe2 High        | \ud83d\udfe2 High      |\\n| **Cost Optimization**   | \ud83d\udfe2 Low           | \ud83d\udd35 Depends    | \ud83d\udfe1 Moderate    | \ud83d\udd34 High      |\\n| **Performance Efficiency** (for most workloads) | \ud83d\udfe2 Acceptable  | \ud83d\udfe2 Good       | \ud83d\udfe2 Acceptable  | \ud83d\udd35 Depends    |\\n| **Operational Excellence** | \ud83d\udfe2 Easy          | \ud83d\udd34 Complex    | \ud83d\udfe2 Easy        | \ud83d\udd34 Complex   |\\n| **Data Residency**      | \ud83d\udfe2 Strong        | \ud83d\udfe2 Strong     | \ud83d\udfe2 Strong      | \ud83d\udd35 Depends   |\\n\\n## \u2753 How to Determine Workload Criticality\\n\\nSo, we\'ve discussed SLA, some of the technical considerations for your workloads - lets take a look at how do you know - that your workload is critical?\\n\\nThis is always a hard question to answer, and it is up to the business to determine\u2014it\'s a risk/benefit analysis. For example, in Health, is there a Clinical Risk to patients? In finance, is there a financial risk to the business? In retail, is there a reputational risk to the business? The criticality of your workload can be determined by looking at the impact of an outage on the business and the cost of that outage.\\n\\n## \ud83c\udfc6 Criticality Classification Framework\\n\\nHowever, for those starting out or don\'t know where to start with classifying your workload criticality, we can use the [Cloud Adoption Framework workload priority](https://learn.microsoft.com/azure/cloud-adoption-framework/manage/protect?WT.mc_id=AZ-MVP-5004796#manage-reliability) as a base.\\n\\n> Enterprise organizations typically have an extensive application portfolio, but not all applications are of equal importance. Applications can be classified based on a criticality scale. For example, business-critical applications are designed to prevent financial losses, and safety-critical applications are focused on costs associated with the loss of human life. Mission-critical applications cover both aspects that can be impacted by unavailability or underperformance. Criticality should be identified and classified to direct investment of business continuity, monitoring, support, and other resources appropriately. It should be noted that certain business functions within applications may also be more critical than others.\\n\\n| **Tier**   | **Criticality**                       | **Business View**                                                                                                                                   | **Financial** | **Brand Reputation** | **Customer Trust** | **Customer Experience** | **Injury / Loss of Life** | **Employee Productivity** |  \\n|------------|---------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-----------------------|--------------------|--------------------------|---------------------------|---------------------------|  \\n| **Tier 1** | Mission Critical       | Affects the company\'s mission and might noticeably affect corporate profit-and-loss statements.                                                   | n/a           | Yes                   | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 1** | Business Critical      | Can lead to financial losses for the organization.                                                                                                 | > $250k       | Yes                   | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 1** | Compliance Critical    | In heavily regulated industries, some applications might be critical as part of an effort to maintain compliance requirements.                     | n/a           | Yes                   | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 1** | Safety Critical        | When the lives or physical safety of employees and customers is at risk during an outage, it can be wise to classify applications as safety-critical. | n/a           | Yes                   | Yes                | Yes                      | Yes                       | Yes                       |  \\n| **Tier 1** | Security Critical      | Some applications might not be mission critical, but outages could result in loss of data or unintended access to protected information.           | n/a           | Yes                   | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 1** | Unit Critical          | Affects the mission of a specific business unit and its profit-and-loss statements.                                                                | > $250k       | Yes                   | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 2** | High                | Might not hinder the mission, but affects high-importance processes. Measurable losses can be quantified in the case of outages.                   | < $250k       | Yes                   | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 3** | Medium               | Impact on processes is likely. Losses are low or immeasurable, but brand damage or upstream losses are likely.                                     | < $100k       | No                    | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 4** | Low                   | Impact on business processes isn\'t measurable. Neither brand damage nor upstream losses are likely. A localized impact on a single team is likely.   | < $50k        | No                    | Yes                | Yes                      | No                        | Yes                       |  \\n| **Tier 5** | Unsupported           | No business owner, team, or process that\'s associated with this application can justify any investment in the ongoing management of the application. | $0            | No                    | No                 | No                       | No                        | No                        |  \\n\\nThe most successfully organisations I have seen implement criticality assessments, have a clear understanding of the business impact of an outage - they also have a review process that takes into effect an unbiased view of the workload, and the impact of the workload on the business, and not just the technical impact of the workload. \\n\\n:::tip\\nIdeally, the criticality of your workload should be assessed at the time of the design of the workload and not AFTER the system is in production or in use. The criticality SHOULD determine how the workload is designed and built, and not the other way around. However, in my career, I have found that this is not always the case. The criticality of the workload is assessed after the fact, and this can lead to issues down the line, as the workload may not be designed to be critical (or not), and this can lead to issues with availability and reliability and cost expenditure on the wrong workloads.\\n:::\\n\\n## \ud83d\udccb Key Criticality Assessment Considerations\\n\\nAreas to consider when looking at the criticality of your workload:\\n\\n* Reputational damage\\n* Industry regulations\\n* Legal and compliance\\n* Security\\n* Customer satisfaction\\n* Financial\\n* Loss of life or injury.\\n* Employee morale \\n* Perceived outage acceptance _(ie, how long is acceptable for the workload to be down, are there any workarounds or manual processes)_\\n* Support hours and costs _(ie if the system is only a 9-5 system, then the support costs may be lower than a 24/7 system, and as such its a lower criticality if it goes down over the weekend)_\\n\\n:::tip\\nFor more questions and considerations for architecting solutions on Azure\u2014including business context questions that can help you with criticality\u2014consider the Solution Requirement Consideration Checklist I made a few years ago! It\'s open for Pull Requests, and you can read more here: [Azure Architecture - Solution Requirement Consideration Checklist](https://luke.geek.nz/azure/azure-architecture-solution-requirement-consideration-checklist/).\\n:::\\n\\n## \ud83d\udca1 Conclusion\\n\\nHopefully, that has given you a glimpse into the world of workload criticality and how it can impact your workloads in the Microsoft Azure Cloud. There is a lot to consider when looking at the criticality of your workload, and it is important to understand the implications of that criticality in terms of your architecture and design. The more critical the workload, the more you need to consider the implications of that criticality in terms of your architecture and design, cost, and complexity. \\n\\n**The question I leave you is: Do you really need that second region?**"},{"id":"azure/github-codespaces-azure-devops-repo","metadata":{"permalink":"/azure/github-codespaces-azure-devops-repo","source":"@site/blog/2025-03-07-CodespaceADORepo/index.mdx","title":"How to Use GitHub Codespaces with Azure DevOps Repositories","description":"Learn how to connect GitHub Codespaces to external repositories in Azure DevOps using the external-repository feature and devcontainer configuration.","date":"2025-03-07T07:40:37.780Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.89,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to Use GitHub Codespaces with Azure DevOps Repositories","metaDescription":"Learn how to connect GitHub Codespaces to external repositories in Azure DevOps using the external-repository feature and devcontainer configuration.","date":"2025-03-07T07:40:37.780Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/github-codespaces-azure-devops-repo","keywords":["Azure DevOps","devcontainer","Development Environment","External Repository","GitHub Codespaces"],"description":"Learn how to connect GitHub Codespaces to external repositories in Azure DevOps using the external-repository feature and devcontainer configuration."},"unlisted":false,"prevItem":{"title":"Understanding Workload Criticality in the Cloud","permalink":"/azure/business-critical-workloads"},"nextItem":{"title":"Deploying Azure Landing Zones with the Terraform Accelerator","permalink":"/azure/azure-landing-zone-accelerator"}},"content":"I\'ve talked about [GitHub Codespaces](https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/) before, but what I haven\'t covered - is using a GitHub Codespace to connect to a repository in Azure DevOps. It\'s classed as \'Remote Repository\' in the Codespace, however, it\'s something that can be done.\\n\\n{/* truncate */}\\n\\nTo start with, we have a repository in Azure DevOps, and we want to connect to it using a GitHub Codespace. I\'ve created a repository in Azure DevOps called \'CodespaceADORepo\' for demo purposes. This repo contains two files _(README.md and a main.bicep file)_ although it doesn\'t matter what the repository has.\\n\\n![Azure DevOps Repo](images/Codespace_ADORepo.jpg)\\n\\nTo make this work, we need to add the following feature to our Codespace: [codespace-features/external-repository](https://github.com/microsoft/codespace-features/pkgs/container/codespace-features%2Fexternal-repository). This feature allows you to connect to a repository that is not hosted in GitHub.\\n\\nTo add this feature, you need to create a `.devcontainer` folder in the root of your repository. Inside this folder, you need to create a `devcontainer.json` file. This file is used to configure the Codespace, and in this case, we are adding the feature to connect to an external repository.\\n\\nFirst we need to grab the clone URL.\\n\\n![Azure DevOps Clone URL](images/Codespace_ADO_CloneURL.gif)\\n\\nThen we need to add the URL to the devcontainer.json\\n\\n```json\\n\\"features\\": {\\n\\t\\t\\"ghcr.io/microsoft/codespace-features/external-repository:latest\\": {\\n\\t\\t\\t\\"cloneUrl\\": \\"https://luke0153@dev.azure.com/luke0153/CodespaceDemo/_git/CodespaceADORepo\\",\\n\\t\\t\\t\\"folder\\": \\"/workspaces/ado-repos\\"\\n\\t\\t}\\n}\\n```\\n:::info\\nYou can add a PAT token to authenticate by adding: \\"cloneSecret\\": \\"ADO_PAT\\", to the feature settings, but storing your PAT token in plain text in a repo is not recommended.\\n:::\\n\\nAnd then add in: \\n\\n```json\\n\\"initializeCommand\\": \\"mkdir -p ${localWorkspaceFolder}/../ado-repos\\",\\n  \\"postStartCommand\\": \\"external-git clone\\",\\n  \\"postAttachCommand\\": \\"external-git config\\",\\n```\\n\\n![Azure DevOps Clone URL](images/Codespace_ADO_AddExternalGitClone.gif)\\n\\nOnce done, it\'s time to run your Codespace, as normal. It will ignore the GitHub repo that you are running it in, then clone the ADO repo, once authenticated - in this case, because I haven\'t specified a PAT token, it will prompt for user authentication, then mount the repo.\\n\\n![Azure DevOps Clone URL](images/Codespace_ADO_RunExternalGit.gif)\\n\\nAnd the original git repo is ignored.\\n\\nFor full reference to the devcontainer.json I am using:\\n\\n```json\\n\\n{\\n\\t\\"name\\": \\"Default Linux Universal\\",\\n\\t\\"image\\": \\"mcr.microsoft.com/devcontainers/universal:2-linux\\",\\n\\t\\"features\\": {\\n\\t\\t\\"ghcr.io/microsoft/codespace-features/external-repository:latest\\": {\\n\\t\\t\\t\\"cloneUrl\\": \\"https://contoso@dev.azure.com/contosoOORG/CodespaceDemo/_git/CodespaceADORepo\\",\\n\\t\\t\\t\\"folder\\": \\"/workspaces/ado-repos\\"\\n\\t\\t},\\n\\t\\t  \\"ghcr.io/prulloac/devcontainer-features/pre-commit:1.0.3\\": {},\\t\\n\\t},\\n\\t\\"workspaceFolder\\": \\"/workspaces/ado-repos\\",\\n\\t\\"initializeCommand\\": \\"mkdir -p ${localWorkspaceFolder}/../ado-repos\\",\\n  \\"postStartCommand\\": \\"external-git clone\\",\\n  \\"postAttachCommand\\": \\"external-git config\\",\\n\\n\\t\\"customizations\\": {\\n\\t\\t\\"vscode\\": {\\n\\t\\t\\t\\"extensions\\": [\\n\\t\\t\\t\\t\\"GitHub.copilot\\",\\n\\t\\t\\t\\t\\"GitHub.copilot-chat\\",\\n\\t\\t\\t]\\n\\t\\t}\\n\\t}\\n}\\n```"},{"id":"azure/azure-landing-zone-accelerator","metadata":{"permalink":"/azure/azure-landing-zone-accelerator","source":"@site/blog/2025-03-01-TerraformAzureLandingZoneAccelerator/index.mdx","title":"Deploying Azure Landing Zones with the Terraform Accelerator","description":"Learn how to deploy Azure Platform Landing Zones using the Terraform Accelerator and Azure DevOps.","date":"2025-03-01T07:41:23.466Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":18.785,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deploying Azure Landing Zones with the Terraform Accelerator","metaDescription":"Learn how to deploy Azure Platform Landing Zones using the Terraform Accelerator and Azure DevOps.","date":"2025-03-01T07:41:23.466Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/azure-landing-zone-accelerator","keywords":["Azure","Terraform","Landing Zones","ALZ","Azure DevOps","IaC","Cloud Adoption Framework","Platform Landing Zone","Azure Infrastructure","Hub and Spoke"],"description":"Learn how to deploy Azure Platform Landing Zones using the Terraform Accelerator and Azure DevOps."},"unlisted":false,"prevItem":{"title":"How to Use GitHub Codespaces with Azure DevOps Repositories","permalink":"/azure/github-codespaces-azure-devops-repo"},"nextItem":{"title":"Getting Started with Azure Developer CLI (azd)","permalink":"/azure/azure-developer-cli"}},"content":"As part of the [Azure Spring Clean 2025](https://www.azurespringclean.com/) event for 2025, we are here to take a look at [Azure Landing Zones](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796), specifically [Platform Landing Zones](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796#platform-landing-zones-vs-application-landing-zones), and the deployment of the Platform Landing Zone, using an [accelerator](https://azure.github.io/Azure-Landing-Zones/accelerator/) and Azure DevOps for the CI/CD.\\n\\n[![Azure Spring Clean 2025](images/AzureSpringClean25_Logo.png)](https://www.azurespringclean.com/)\\n\\n{/* truncate */}\\n\\n## \ud83c\udfd7\ufe0f Introduction to Azure Landing Zones\\n\\n[Azure Landing Zones](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796) provide a structured approach for designing and implementing cloud environments in Azure. They are essential for organizations looking to migrate, modernize, and innovate their applications at scale, mainly because of the considerations you make in designing and implementing them, considerations such as how your workloads are going to connect to the internet, how each service connects, and importantly, how your organisation will use Cloud - I saw a comment from someone the other day _(Cloud is not WHERE you work, its HOW you work)_. \\n\\n:::info\\nAn Azure landing zone is an environment that follows key design principles across eight design areas. These design principles accommodate all application portfolios and enable application migration, modernization, and innovation at scale. An Azure landing zone uses subscriptions to isolate and scale application and platform resources. Subscriptions for application resources are called application landing zones, and subscriptions for platform resources are called platform landing zones.\\n\\nAn Azure landing zone architecture is scalable and modular to meet various deployment needs. The repeatable infrastructure allows you to consistently apply configurations and controls to every subscription. Modules make deploying and modifying specific Azure landing zone architecture components easy as your requirements evolve.\\n\\n![Azure Landing Zones](images/azure-landing-zone-architecture-diagram-hub-spoke.png)\\n:::\\n\\nMore information on Azure Landing Zones, can be found in the [Microsoft Cloud Adoption Framework for Azure](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796), Landing Zones specifically sits under the Ready phase.\\n\\n## \ud83d\ude80 Understanding Azure Landing Zone Accelerators\\n\\n[Azure Landing Zone Accelerators](https://azure.github.io/Azure-Landing-Zones/accelerator/) are automation frameworks designed to expedite the deployment of Azure Landing Zone architecture. They come in three flavors:\\n\\n* Bicep Accelerator\\n* Terraform Accelerator\\n* Portal Accelerator\\n\\nThese Accelerators, adopt a best-practice approach _(but also opinionated)_ to deploy an Azure Landing Zone, from scratch, whether Brownfield _(already existing)_ or Greenfield _(new)_, and they have been going through a transformation, from previous accelerators _(marked as vNext)_ to adopt a more modular approach with both Bicep and Terraform deployments aligned to [Azure Verified Modules](https://azure.github.io/Azure-Verified-Modules/). These Accelerators are what Microsoft would run in a VBD workshop _(I believe they would use the Portal accelerator)_.\\n\\nI want to make clear, these just like the Cloud Adoption Framework, are best practices but also generalized to support multiple types of organisations, so make sure you adjust them to suit your needs and organisations, or even mark it as a northstar to aim towards, but not necessary what you need in that moment, but also consider strategically where you want to go, ie platform engineering, de-centralized or centralized operations, all these decisions will determine how your Landing Zone structure is based and used _(ie if you don\'t like the word Online, change it to Public, your business needs to understand where to put workloads and how they will work _(not spending the time fighing over what something is named)_)_.\\n\\nWith Landing Zones, there\'s the Platform Landing Zone and the Application Landing Zone, the Platform Landing Zone is the shared services like identity, connectivity, and management, whereas the Application Landing Zone is specific environments for particular applications or workloads _(consider Arc, maybe APIM, anything generalized - this is really where your business logic and IP sits)_.\\n\\n![Azure Landing Zones - Customer Journey](images/ALZAccelerator_CustomerJourney.JPG)\\n\\nFor this article, we will be discussing the:\\n\\n* Bootstrap of our environment (into Azure DevOps)\\n* Deployment of Azure platform Landing Zone components using Terraform\\n\\n![Azure Landing Zones - Customer Journey](images/ALZAccelerator_SampleLandingZoneDeployment.JPG)\\n\\n## \ud83d\udd04 Deploying Azure Landing Zones with Terraform\\n\\n### \ud83d\udee0\ufe0f Bootstrap\\n\\nIt begins with a Bootstrap process, which is the initial setup of the Azure DevOps environment, including creating a new project, repository, and service connection. This process is automated using a mix of PowerShell and Terraform.\\n\\n![Azure Landing Zones - Customer Journey](images/ALZAccelerator_Components.JPG)\\n\\nTo bootstrap, we will make use of the:[ALZ-PowerShell-Module](https://github.com/Azure/ALZ-PowerShell-Module), this script will help bring together the different dependencies and Terraform bootstrapping to setup our Azure DevOps environment, Terraform state storage account and bring in the necessary Platform Landing Zone library files for our deployment.\\n\\n![Azure Landing Zones - Customer Journey](images/ALZAccelerator_CICDComponents.JPG)\\n\\nThe bootstrap for Azure DevOps includes [self-hosted agents](https://learn.microsoft.com/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser&WT.mc_id=AZ-MVP-5004796#self-hosted-agents) _([Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) or [Container Instances](https://learn.microsoft.com/azure/container-instances/container-instances-overview?WT.mc_id=AZ-MVP-5004796))_ as optional, and preconfigured pipelines, including validation and branch policies with approval steps, so a foundational platform to work towards.\\n\\n| Component | Description | Notes |\\n|-----------|-------------|-------|\\n| **Azure Resources** |||\\n| Resource Group for State | Container for Terraform state storage resources | Terraform only |\\n| Storage Account and Container for State | Stores Terraform state files securely | Terraform only |\\n| Resource Group for Identity | Container for managed identity resources | |\\n| User Assigned Managed Identities (UAMI) with Federated Credentials | Identities for secure pipeline execution | For Plan and Apply |\\n| Permissions for the UAMI | RBAC assignments needed for deployment | On state storage container, subscriptions, and management groups |\\n| [Optional] Container Registry for Azure DevOps Agent Image | Stores custom agent container images | For self-hosted agents |\\n| [Optional] Container Instances hosting Azure DevOps Agents | Runs pipeline jobs in your Azure environment | For self-hosted agents |\\n| [Optional] Virtual network, subnets, private DNS zone, and private endpoint | Network resources for private connectivity | For enhanced security |\\n| **Azure DevOps Resources** |||\\n| Project (can be supplied or created) | Azure DevOps project for Landing Zone resources | Can use existing or create new |\\n| Repository for the Module | Stores Landing Zone Terraform code | Main implementation repository |\\n| Repository for the Pipeline Templates | Stores CI/CD pipeline definitions | Enables template reuse |\\n| Starter Terraform module with tfvars | Pre-configured Terraform configuration | Customizable baseline |\\n| Branch policy | Ensures code quality and reviews | Prevents direct main branch changes |\\n| Pipeline for Continuous Integration | Validates and plans Terraform changes | Non-destructive verification |\\n| Pipeline for Continuous Delivery | Implements Terraform changes in Azure | Creates/updates resources |\\n| Environment for Plan | Isolated context for planning stage | Separates planning permissions |\\n| Environment for Apply | Isolated context for apply stage | Separates deployment permissions |\\n| Variable Group for Backend | Stores Terraform backend configuration | Used across pipelines |\\n| Service Connections with Workload identity federation for Plan and Apply | Securely connects Azure DevOps to Azure | Uses OIDC for enhanced security |\\n| Service Connection Approvals, Template Validation, and Concurrency Control | Provides governance for deployments | Ensures controlled changes |\\n| Group and Members for Apply Approval | Team responsible for approving changes | Change control mechanism |\\n| [Optional] Agent Pool | Collection of build/release agents | For self-hosted agents |\\n\\n:::tip\\nMake sure you check out the following resources for more information on the Azure Landing Zone Accelerator and Azure Landing Zone Library, these are your sources of truth:\\n\\n* [Azure Landing Zones Documentation](https://aka.ms/alz/acc)\\n* [Azure Landing Zones Library](https://azure.github.io/Azure-Landing-Zones-Library/)\\n\\nIf you have issues, you can also raise them on the [ALZ-PowerShell-Module](https://github.com/Azure/ALZ-PowerShell-Module/issues) GitHub repository.\\n:::\\n\\nBefore we get started, we need some pre-requisites \\n\\n* 3 Azure subscriptions _(1 for Connectivity, 1 for Identity, and 1 for Management) - (for this demo, I will be using a single subscription)_ we will also need the bootstrap and Terraform deployment IDs.\\nPermissions Required for Management Group and Subscriptions:\\n\\n* Owner on your chosen parent management group:\\n\\nThe owner account will grant permissions to the identities running the management group deployment. Owner on each of your 3 Azure landing zone subscriptions.\\n\\n* A PAT token for the creation of a Service Connection in Azure DevOps _(Agent Pools (Read & manage)_, Build _(Read & execute)_, Code _(Full)_, Environment _(Read & manage)_, Graph _(Read & manage)_, Pipeline Resources _(Use & manage)_, Project and Team _(Read, write & manage)_, Service Connections _(Read, query & manage)_, and Variable Groups _(Read, create & manage)_.)_  This token is only needed for the duration required to bootstrap the environment and can be revoked after.\\n\\nOnce we have those pre-requisites, we can proceed with the bootstrapping process. \\n\\n:::tip\\nI recommend starting with the [Azure Landing Zone Accelerator Checklist](https://azure.github.io/Azure-Landing-Zones/examples/tf/accelerator/config/checklist.xlsx), this will help you understand the requirements and dependencies for the deployment, and ensure you have everything you need before you start the process.\\n\\n![Azure Landing Zones - Customer Journey](images/ALZ_Bootstrap_Checklist.jpg)\\n:::\\n\\nWe will bootstrap the environment, to create our Azure DevOps environment, and then deploy the base components for a [Platform Landing Zone](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796#platform-landing-zones-vs-application-landing-zones) and the associated [archetypes](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/tailoring-alz?WT.mc_id=AZ-MVP-5004796#what-is-a-landing-zone-archetype-in-azure-landing-zones) for the fictional company of Contoso. To do this, we will be using the Terraform - Azure Verified Modules for [Platform Landing Zone (ALZ) starter modules](https://azure.github.io/Azure-Landing-Zones/accelerator/startermodules/terraform-platform-landing-zone/) and configuration in the Australia East Azure region.\\n\\n:::info\\nBefore you start, make sure you request a parallelism grant for Azure DevOps, by default, Azure DevOps has a limit of 1 concurrent job, to request a free parallelism grant, please fill out the following form https://aka.ms/azpipelines-parallelism-request\\n:::\\n\\n### \ud83d\udccb Bootstrap Process\\n\\n1. First login to Azure \'az login\' and then run the following commands:\\n2. Next we need the ALZ PowerShell Module, so we will install that:\\n\\n```powershell\\n# Check if the ALZ module is installed\\n$module = Get-InstalledModule -Name ALZ -ErrorAction SilentlyContinue\\n\\nif ($null -eq $module) {\\n    # If the module is not installed, install it\\n    Install-Module -Name ALZ -Force\\n    Write-Output \\"ALZ module installed successfully.\\"\\n} else {\\n    # If the module is installed, update it\\n    Update-Module -Name ALZ\\n    Write-Output \\"ALZ module updated successfully.\\"\\n}\\n```\\n\\n![Azure Landing Zones - Customer Journey](images/ALZ_InstallPshModule.gif)\\n\\n:::info\\nThree sets of configuration can be supplied to the accelerator to pre-configure it.\\n\\nThe available configuration inputs are:\\n\\n* [Bootstrap Configuration File](https://azure.github.io/Azure-Landing-Zones/accelerator/userguide/2_start/#bootstrap-configuration-file)\\n* [Platform Landing Zone Configuration File](https://azure.github.io/Azure-Landing-Zones/accelerator/userguide/2_start/#platform-landing-zone-configuration-file)\\n* [Platform Landing Zone Library (lib) Folder](https://azure.github.io/Azure-Landing-Zones/accelerator/userguide/2_start/#platform-landing-zone-library-lib-folder)\\n:::\\n\\nWe will start with th bootstrap configuration file, this will be used to create the Azure DevOps environment, and the Terraform state storage account and container, then we adjust the Platform Landing Zone configuration file to control what Azure resources we will need to deploy _(ie Hub and Spoke, or Virtual WAN, Bastion etc)_ and the Platform Landing Zone library folder, which will contain the Archetype definitions and policy assignments for greater flexibility.\\n\\n3. Let\'s create the local configuration files for the bootstrap and Platform Landing Zone deployment:\\n\\n```powershell\\nNew-Item -ItemType \\"file\\" c:\\\\Code\\\\accelerator\\\\config\\\\inputs.yaml -Force\\nNew-Item -ItemType \\"file\\" c:\\\\Code\\\\accelerator\\\\config\\\\platform-landing-zone.tfvars -Force  # Exclude this line if using FSI or SLZ starter modules\\nNew-Item -ItemType \\"directory\\" c:\\\\Code\\\\accelerator\\\\config\\\\lib \\nNew-Item -ItemType \\"directory\\" c:\\\\Code\\\\accelerator\\\\output\\n```\\n\\n4. Now we need to open the inputs.yaml file and add the following configuration found here: [inputs-azure-devops.yaml](https://raw.githubusercontent.com/Azure/alz-terraform-accelerator/refs/heads/main/templates/platform_landing_zone/examples/bootstrap/inputs-azure-devops.yaml).\\n\\nIt should look something like below:\\n\\n```yaml\\n# For detailed instructions on using this file, visit:\\n# https://aka.ms/alz/accelerator/docs\\n\\n# Basic Inputs\\niac_type: \\"terraform\\"\\nbootstrap_module_name: \\"alz_azuredevops\\"\\nstarter_module_name: \\"platform_landing_zone\\"\\n\\n# Shared Interface Inputs\\nbootstrap_location: \\"australiaeast\\"\\nstarter_locations: [\\"australiaeast\\"]\\nroot_parent_management_group_id: \\"Contoso\\"\\nsubscription_id_management: \\"00000000-0000-4000-8000-000000000001\\"\\nsubscription_id_identity: \\"00000000-0000-4000-8000-000000000002\\"\\nsubscription_id_connectivity: \\"00000000-0000-4000-8000-000000000003\\"\\n\\n# Bootstrap Inputs\\nazure_devops_personal_access_token: \\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\"\\n# azure_devops_agents_personal_access_token: \\"<token-2>\\"\\nazure_devops_organization_name: \\"Contoso\\"\\nuse_separate_repository_for_templates: true\\nbootstrap_subscription_id: \\"00000000-0000-4000-8000-000000000004\\"\\nservice_name: \\"plz\\"\\nenvironment_name: \\"mgmt\\"\\npostfix_number: 1\\nazure_devops_use_organisation_legacy_url: false\\nazure_devops_create_project: true\\nazure_devops_project_name: \\"ADO-PALZ\\"\\nuse_self_hosted_agents: false\\nuse_private_networking: true\\nallow_storage_access_from_my_ip: false\\napply_approvers: [\\"luke@contoso.com\\"]\\ncreate_branch_policies: true\\n\\n# Advanced Inputs\\nbootstrap_module_version: \\"latest\\"\\nstarter_module_version: \\"latest\\"\\noutput_folder_path: \\"c:/Code/accelerator/output\\"\\n```\\n\\n:::tip\\nFor further customisation, make sure you take a look at the [variables.tf](https://github.com/Azure/accelerator-bootstrap-modules/blob/main/alz/local/variables.tf) file directly in the accelerator bootstrap modules GitHub, it can show further options that you can add to the bootstrap, such as the storage account replication type for your Terraform state files, or additional custom roles and resource naming.\\n:::\\n\\nNow that we have configured the variables required for our bootstrap, let us define the Terraform variables for the Platform Landing Zone deployment, this will be used to determine the Azure resources we will deploy, such as the Hub and Spoke, Virtual WAN, Bastion, etc, and this is the platform-landing-zone.tfvars file. I recommend copying one from one of the [already pre-existing Scenarios](https://azure.github.io/Azure-Landing-Zones/accelerator/startermodules/terraform-platform-landing-zone/scenarios/) and modifying to suit your needs. For my purposes, I will be going with a Single region hub and spoke VNET with [Azure Firewall](https://azure.github.io/Azure-Landing-Zones/accelerator/startermodules/terraform-platform-landing-zone/scenarios/single-region-hub-and-spoke-vnet-with-azure-firewall/).\\n\\n5. Open the platform-landing-zone.tfvars file and add the configuration and modify for your needs, for my purposes, I will be using the following configuration: [platform_landing_zone/examples/full-single-region/hub-and-spoke-vnet.tfvars](https://raw.githubusercontent.com/Azure/alz-terraform-accelerator/refs/heads/main/templates/platform_landing_zone/examples/full-single-region/hub-and-spoke-vnet.tfvars).\\n\\n:::info\\nThe `platform-landing-zone.tfvars` file is a crucial configuration for deploying an Azure Landing Zone using the Azure Landing Zones Accelerator with Terraform. This file defines the variables that customize your Azure foundation by your requirements, and this file will be your main configuration file for the deployment of the Platform Landing Zone.\\n\\nKey Components of the File\\n\\n**Built-in Replacements**\\n\\nThe file uses a templating system with `$${variable_name}` syntax to reference predefined values, such as:\\n\\n- Azure locations (e.g., `$${starter_location_01}`)\\n- Subscription IDs (e.g., `$${subscription_id_management}`)\\n- Management group IDs\\n\\nThese built-in replacements help maintain consistency throughout the configuration.\\n\\n**Custom Replacements**\\n\\nThe configuration defines custom naming patterns and references for:\\n\\n1. **Resource Names** - Standardizing naming conventions for:\\n   - Resource groups\\n   - Virtual networks\\n   - Azure Firewall\\n   - Log Analytics workspace\\n   - And many other resources\\n\\n2. **IP Address Spaces** - Defining network address spaces for the hub network:\\n   - Virtual network address space\\n   - Subnet address prefixes\\n   - Firewall subnet\\n\\n**Management Resources**\\n\\nThis section configures core management resources including:\\n\\n- Log Analytics workspace\\n- Automation account\\n- Azure Monitor Agent settings\\n- Data Collection Rules for monitoring\\n\\n**Management Groups and Policy**\\n\\nThe file configures:\\n\\n- Management group hierarchy structure\\n- Policy assignments and overrides, for example Defender for Cloud Settings\\n- Subscription placement within management groups\\n\\n**Connectivity Resources**\\n\\nThe hub-and-spoke network topology is defined with:\\n\\n- DDoS protection plan\\n- Virtual network configuration\\n- Azure Firewall and routing tables\\n- Virtual network gateways (ExpressRoute and VPN)\\n- Private DNS zones and resolver\\n- Azure Bastion\\n\\n**How to Modify This File**\\n\\n1. Customize Basic Information\\n\\n- **Email Contacts**: Update security contact email addresses\\n- **Tags**: Modify organization-specific tags\\n\\n```hcl\\ntags = {\\n  deployed_by = \\"terraform\\"\\n  technical_contact = \\"your-email@company.com\\"\\n  environment = \\"production\\"\\n  business_unit = \\"finance\\"\\n}\\n```\\n\\n2. Adjust Network Addressing\\n\\nModify the IP address ranges to match your organization\'s network plan:\\n\\n```hcl\\nprimary_hub_address_space = \\"10.100.0.0/16\\"\\nprimary_hub_virtual_network_address_space = \\"10.100.0.0/22\\"\\n```\\n\\n3. Change Resource Naming\\n\\nUpdate naming conventions to align with your organization\'s standards:\\n\\n```hcl\\nlog_analytics_workspace_name = \\"law-prod-eastus\\"\\nautomation_account_name = \\"aa-prod-eastus\\"\\n```\\n\\n4. Modify Policy Settings\\n\\nAdjust policy enforcement levels for your compliance requirements:\\n\\n```hcl\\nEnable-DDoS-VNET = {\\n  enforcement_mode = \\"Default\\" # or \\"DoNotEnforce\\"\\n}\\n```\\n\\n5. Add or Remove Resources\\n\\nYou can customize which components are deployed by adding or removing sections:\\n\\n- Remove the VPN gateway if not needed\\n- Add additional hub networks for multi-region deployments\\n- Modify Azure Firewall SKU based on requirements\\n\\nAlso make sure you run a `terraform validate` and `terraform fmt` to validate the configuration and the Terraform formatting is correct on the file.\\n\\n**Make sure to check out the [Options](https://azure.github.io/Azure-Landing-Zones/accelerator/startermodules/terraform-platform-landing-zone/options/) page for the Terraform Platform Landing Zone, as it will give you more information on the common scenarios, such as turning off DDOS protection plans, adding additional IP ranges etc.**\\n:::\\n\\nAnd don\'t worry if you don\'t get this right the first time, you can always adjust and redeploy, this is the beauty of Infrastructure as Code. I work in an iterative approach, so it\'s always good to get something working, then adjust and improve, rather than trying to get it perfect the first time, and the Terraform plan and validate pipelines help with this approach.\\n\\n6. We are close, the last thing we will add is a custom library folder. This isn\'t required for base Platform Landing Zones, however, I recommend adding it in, for additional flexibility, especially as you start to deploy more complex environments and archetypes within your organization. You can refer to the [Azure Landing Zones Library](https://azure.github.io/Azure-Landing-Zones-Library/) for more information on how to create your own custom library and additional information. Still, for now we are going to copy the alz folder from the [Azure Landing Zones Library](https://github.com/Azure/Azure-Landing-Zones-Library/tree/main/platform) and place it in the lib folder in our config directory.\\n\\nBy default, the architecture that will be deployed will be _(you can also refer to the README file in the lib folder for more information)_:\\n\\n```mermaid\\nflowchart TD\\n  alz[\\"Azure Landing Zones\\n(root)\\"]\\n  alz --\x3e decommissioned\\n  decommissioned[\\"Decommissioned\\n(decommissioned)\\"]\\n  alz --\x3e landingzones\\n  landingzones[\\"Landing zones\\n(landing_zones)\\"]\\n  landingzones --\x3e corp\\n  corp[\\"Corp\\n(corp)\\"]\\n  landingzones --\x3e online\\n  online[\\"Online\\n(online)\\"]\\n  alz --\x3e platform\\n  platform[\\"Platform\\n(platform)\\"]\\n  platform --\x3e connectivity\\n  connectivity[\\"Connectivity\\n(connectivity)\\"]\\n  platform --\x3e identity\\n  identity[\\"Identity\\n(identity)\\"]\\n  platform --\x3e management\\n  management[\\"Management\\n(management)\\"]\\n  alz --\x3e sandbox\\n  sandbox[\\"Sandbox\\n(sandbox)\\"]\\n```\\n\\nYou can adujust the Management Group names and parent/child relationships by editing the `alz.alz_architecture_definition.json` file in the lib\\\\architecture_definitions folder, and th architectype definitions allow you to configure the type of policy definitions, role assignments that are deployed to specific Management Groups with a certain archetype, for example you could have workload Landing Zones of \'Workload\' archetype with specific policy definitions and role assignments, that may be different from the root or Platform groups, and Management Groups could share this same archetype, so you can have a consistent policy and role assignment across your environment.\\n\\n:::tip\\nNot required, but make the [alzlibtool](https://github.com/Azure/alzlib) tool, this is a tool that can help you create and manage your own custom library for your Azure Landing Zones, and can be used to check policy and Azure Landing Zone architecture definition structure, and document your own custom library.\\n:::\\n\\n7. Now that the configuration files for the bootstrap, Terraform are in place, and the architectures, it\'s time to start the bootstrap. Run the following command to start the bootstrap process:\\n\\n:::tip\\nIf you need to make any last-minute changes, you can manually change the Terraform code after the Terraform plan and before the application. Check your local output folder. It is not something I would rely on - ideally the platform tfvars and yaml input file should be updated, but in a pinch, you can make manual changes. I had to do this recently, when deploying a Landing Zone into New Zealand North. I had to manually update the location of the Log Analytics workspace, as it has not been supported in that [region yet](https://learn.microsoft.com/azure/reliability/availability-service-by-category?WT.mc_id=AZ-MVP-5004796). However, for Platform resources, you can also make this change once the code is in the repository.\\n:::\\n\\n```powershell\\nDeploy-Accelerator `\\n  -inputs \\"C:\\\\Code\\\\accelerator\\\\config\\\\inputs.yaml\\", \\"C:\\\\Code\\\\accelerator\\\\config\\\\platform-landing-zone.tfvars\\" `\\n  -starterAdditionalFiles \\"C:\\\\Code\\\\accelerator\\\\config\\\\lib\\" `\\n  -output \\"C:\\\\Code\\\\accelerator\\\\output\\"\\n```\\n\\n![Azure Landing Zones - Customer Journey](images/ALZ_DeployBootstrap.gif)\\n\\nOnce completed, in Azure, you should have two resource groups - one for your Managed identities that will be linked to 2 Service Connections in Azure DevOps for Plan and Apply, and one for your Terraform state file.\\n\\n![Azure Landing Zones - Customer Journey](images/ALZ_Bootstrap_Resource_Deployed.png)\\n\\nIn Azure DevOps, you should have a new project, repository, and service connection, with the pipelines and environments set up for the Plan and Apply stages.\\n\\n![AzureDevOps Azure Landin Zone Code](images/ALZ_Bootstrap_ADO_Deployed.png)\\n\\n:::info\\nThe plz acronym I am using, is meant to indicate \'Platform Landing Zone\'.\\n:::\\n\\nAnd Pipelines preconfigured - Continuous Integration _(ie Terraform Validate and Plan)_ and Continuous Delivery _(ie Terraform Apply)_ with the approvers and branch policies in place.\\n\\n![Azure DevOps Azure Landing Zone Pipelines](images/ALZ_Bootstrap_ADOPipelines.jpg)\\n\\n:::tip\\nI recommend renaming the pipelines, in Azure DevOps, so they are more descriptive, ie, \'Terraform Plan\' and \'Terraform Apply\' - pretty simplified, but can make more sense to people consuming the pipelines.\\n\\n![Azure DevOps Azure Landing Zone Pipelines](images/ALZ_DeployBootstrap_RenameADOPipelines.gif)\\n:::\\n\\nNow, let\'s run the Continuous Integration pipeline to see what the Terraform plan looks like.\\n\\n![Azure DevOps Azure Landing Zone Pipelines](images/ALZ_PLZ_ValidatePlan.gif)\\n\\nAs we can see, it validates and will deploy our full hub and spoke _(both expressroute and VPN gateways)_ so that would need tweaking for my environment, but this is the beauty of Infrastructure as Code, you can see what is going to be deployed before it is deployed, and make the necessary adjustments, by opening up a new branch and adjusting the the platform-landing-zone.auto.tfvars, and run validate and plan on that branch until correct.\\n\\n\\nSo, now let\'s deploy. Once you are happy with the plan, you can run the Continuous Delivery pipeline to deploy the Platform Landing Zone. \\n\\n:::warning\\nBe aware that the initial deployment could take a while, especially if deploying all the default policies, and DNS zones. As such, the entire deployment is not shown in the GIF below, but you can see the new Platform Management Groups and core hub resources that have started to be created.\\n:::\\n\\n![Azure DevOps Azure Landing Zone Pipelines](images/ALZ_PLZ_Apply.gif)\\n\\n\\nAnd once completed, you should have a full Platform Landing Zone deployed, with the Management Groups, Policies, and core resources in place.\\n\\n:::info\\nYou can rerun the Continuous Delivery pipeline, and select Destroy if you want to remove all the platform resources and start your deployment from scratch, in a production scenario, I recommend removing that, but not the additional approval step so that you can have a manual check before the Apply stage. The approval is done in Azure DevOps at the Service Connection, under Approval and Checks.\\n:::\\n\\n![Azure DevOps Azure Landing Zone Pipelines](images/ALZAccelerator_DepoloyedLandingZoneDeployment.JPG)\\n![Azure DevOps Azure Landing Zone Pipelines](images/ALZAccelerator_DepoloyedLandingZoneDeploymentResources.JPG)\\n\\n## \u2705 Conclusion\\n\\nThe [Azure Landing Zone Terraform Accelerator](https://techcommunity.microsoft.com/blog/azuretoolsblog/azure-landing-zones-accelerators-for-bicep-and-terraform-announcing-general-avai/4029866?WT.mc_id=AZ-MVP-5004796) offers a robust foundation for organizations looking to implement a well-architected Azure environment. \\n\\nThrough this article, we\'ve explored how to:\\n\\n1. Bootstrap your environment with automation using the [ALZ PowerShell module](https://github.com/Azure/ALZ-PowerShell-Module)\\n2. Configure and deploy [Platform Landing Zone components](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796#platform-landing-zones-vs-application-landing-zones) using Infrastructure as Code _([Terraform](https://www.terraform.io/))_\\n3. Establish governance through management groups, policies, and controlled [Azure DevOps pipelines](https://learn.microsoft.com/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops&WT.mc_id=AZ-MVP-5004796).\\n\\nNext Steps After Deployment\\n\\nOnce your Platform Landing Zone is established, consider these follow-up activities:\\n\\n- Document your environment: Create detailed documentation explaining your Landing Zone design choices and customizations _(make sure to checkout [alzlibtool](https://github.com/Azure/alzlib/tree/main))\\n- Establish operational procedures: Define processes for managing the environment, including approvals and adjustments to the platform\\n- Plan your Application Landing Zones: Design the specific landing zones for your workloads based on your organization\'s needs and investigate [Subscription vending](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/design-area/subscription-vending?WT.mc_id=AZ-MVP-5004796).\\n\\nResources for Continued Learning\\n\\n- [AWESOME Azure Architecture](http://aka.ms/AwesomeAzureArchitecture)\\n- [Azure Landing Zones Library](https://azure.github.io/Azure-Landing-Zones-Library/)\\n- [Azure landing zone design areas and conceptual architecture](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/design-areas?WT.mc_id=AZ-MVP-5004796)\\n- [Azure Management Guide](https://learn.microsoft.com/azure/cloud-adoption-framework/manage/azure-management-guide/?WT.mc_id=AZ-MVP-5004796)\\n- [Azure Verified Modules](https://azure.github.io/Azure-Verified-Modules/)\\n- [Enterprise-scale architecture GitHub repo](https://github.com/Azure/Enterprise-Scale)\\n\\nFor the GitHub repos, remember to view the Issues and Pull Requests, you can learn a lot from what other people have contributed to the initiatives, and the Issues being raised. \\n\\nRemember, the Landing Zone is not a one-time deployment but an evolving foundation that grows with your organization\'s cloud journey. Regular reviews and updates will ensure it continues to support your changing business needs while maintaining security and governance standards, and how the business plans on consuming this - is not to be underestimated in the design and implementation of this."},{"id":"azure/azure-developer-cli","metadata":{"permalink":"/azure/azure-developer-cli","source":"@site/blog/2025-02-19-azd/index.mdx","title":"Getting Started with Azure Developer CLI (azd)","description":"Learn how to use Azure Developer CLI (azd) to accelerate your Azure application development with templates, environment management, and CI/CD integration.","date":"2025-02-19T07:27:28.954Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":15.22,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Getting Started with Azure Developer CLI (azd)","metaDescription":"Learn how to use Azure Developer CLI (azd) to accelerate your Azure application development with templates, environment management, and CI/CD integration.","date":"2025-02-19T07:27:28.954Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/azure-developer-cli","keywords":["Azure","Azure Developer CLI","azd","Infrastructure as Code","DevOps","CI/CD","Azure Container Apps","Azure Functions","Azure Static Web Apps","Azure Application Insights","Azure Key Vault","Development","Cloud Development","Azure Templates"],"description":"Learn how to use Azure Developer CLI (azd) to accelerate your Azure application development with templates, environment management, and CI/CD integration."},"unlisted":false,"prevItem":{"title":"Deploying Azure Landing Zones with the Terraform Accelerator","permalink":"/azure/azure-landing-zone-accelerator"},"nextItem":{"title":"How Microsoft Releases Changes to Azure - Safe Deployment","permalink":"/azure/azure-platform-release-process"}},"content":"Today, we will touch on [Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview?tabs=windows&WT.mc_id=AZ-MVP-5004796).\\n\\nSo, let us take a closer look.\\n\\n![azd](images/AZD.jpg)\\n\\n{/* truncate */}\\n\\n## \ud83c\udf1f Introduction\\n\\n:::info\\nThe [Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview?tabs=windows&WT.mc_id=AZ-MVP-5004796) is an open-source tool that accelerates provisioning and deploying app resources on Azure. azd provides best practice, developer-friendly commands that map to key stages in your development workflow, whether you\'re working in the terminal, an integrated development environment (IDE), or through CI/CD (continuous integration/continuous deployment) pipelines.\\n\\nazd uses [extensible blueprint templates](https://learn.microsoft.com/azure/developer/azure-developer-cli/azd-templates?WT.mc_id=AZ-MVP-5004796) that include everything you need to get an application up and running on Azure. These templates include:\\n\\n* Reusable infrastructure as code assets to provision cloud resources services using Bicep or Terraform.\\n* Proof-of-concept or starter app code that can be customized or replaced with your own app code.\\n* Configuration files are needed to deploy your app to the provisioned resources.\\n* Optionally, pipeline workflow files for GitHub Actions or Azure Pipelines to enable CI/CD integrations.\\n\\nYou can also [create your own template](https://learn.microsoft.com/azure/developer/azure-developer-cli/make-azd-compatible?pivots=azd-create&WT.mc_id=AZ-MVP-5004796) or find one to customize and expand on from the [Awesome AZD](https://learn.microsoft.com/azure/developer/azure-developer-cli/make-azd-compatible?pivots=azd-convert&WT.mc_id=AZ-MVP-5004796) gallery.\\n:::\\n\\n### \ud83e\udd14 What is Azure Developer CLI (azd)?\\n\\nThe [Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview?tabs=windows&WT.mc_id=AZ-MVP-5004796) is a developer-centric command-line interface (CLI) tool for deploying Azure applications, and their infrastructure components. \\n\\nThe goals of the CLI are to:\\n\\n* reduce the time required for a developer to be productive\\n* demonstrate opinionated best practices for Azure development\\n* help developers understand core Azure development constructs\\n\\nTo take full advantage of the CLI, code repositories need to conform to a well defined set of conventions that will be recognized by the tooling. \\n\\nYou can find more information on the [Azure Developer CLI (azd) GitHub repository](https://github.com/Azure/azure-dev), including installation instructions for Windows, Linux amd Mac.\\n\\nA common misconception is that the Azure Developer CLI (azd) is a replacement for the Azure CLI. This is not the case. The Azure Developer CLI (azd) is a tool that is designed to help developers deploy Azure applications, ideal for rapid innovation and learning scenarios, and supports the deployment of infrastructure and application components. \\n\\n:::tip\\nMake sure to check out [**awesome-azd**](https://azure.github.io/awesome-azd/), a curated list of resources for the Azure Developer CLI (azd) community.! I have found this to be a great resource for finding templates and other resources to help me get started with the Azure Developer CLI (azd), and sometimes the exact scenario that you may need. And there is an active Pull Request to update alot of these examples to use [Azure Verified Modules](https://azure.github.io/Azure-Verified-Modules/).\\n\\nFor more AI-orientated workloads, refer to the [**ai-app-template gallery](https://azure.github.io/ai-app-templates/). This collection of templates can be used to deploy AI workloads to Azure, such as an Azure Agents Travel Assistant, Prompty and Semantic Kernel examples.\\n:::\\n\\nIt is not a general-purpose tool for managing Azure resources.\\n\\n### \ud83c\udfaf Common use cases\\n\\nI have found the Azure Developer CLI (azd) to be particularly useful in the following scenarios:\\n\\n* **Learning Azure**: The Azure Developer CLI (azd) is a great tool for learning Azure, as it provides a set of opinionated best practices for Azure development. This can help you understand core Azure development constructs and get up and running quickly with Azure without having to worry about whether a solution is deployed securely or how to architect or create each resource manually. AZD allows you to easily deploy and destroy the resources you need for learning, keeping your Azure subscription clean and tidy.\\n* **Rapid Innovation**: The Azure Developer CLI (azd) is also great for rapid innovation scenarios, where you need to quickly deploy a solution to Azure to test an idea or prototype or deploy in another environment in a once-off scenario.\\n* **CI/CD Pipelines**: The Azure Developer CLI (azd) can create CI/CD pipelines for your Azure applications. This can help you automate the deployment of your applications to Azure, and ensure that your applications are deployed consistently and securely.\\n\\n> I personally would fit Azure Developer CLI into the early lifecycle of a product - it\'s great to get going in your iterative phases [PoT/PoC/MVP](https://luke.geek.nz/misc/product-development-lifecycle/), AZD helps bootstrap and accelerate deployments into Azure from local development environments. \\n\\nFor example, here is an example [copied from the MS Learn](https://learn.microsoft.com/azure/developer/azure-developer-cli/azd-commands?WT.mc_id=AZ-MVP-5004796#compare-azure-developer-cli-commands) website that showcases it nicely:\\n\\n| Tool | Sample Command | Outcome |\\n|------|---------------|----------|\\n| Azure Developer CLI | `azd provision` | Provisions multiple Azure resources required for an app based on project resources and configurations, such as an Azure resource group, an Azure App Service web app, and app service plan, an Azure Storage account, and an Azure Key Vault. |\\n| Azure CLI | `az webapp create --resource-group myResourceGroup --plan myAppServicePlan --name myWebApp` | Provisions a new web app in the specified resource group and app service plan. |\\n| Azure PowerShell | `New-AzWebApp -ResourceGroupName \\"myResourceGroup\\" -Name \\"myWebApp\\" -AppServicePlan \\"myAppServicePlan\\"` | Provisions a new web app in the specified resource group and app service plan. |\\n\\nAt the time of writing, AZD can be used to deploy the infrastructure components and application components for the following services:\\n\\n| Service | Status |\\n|---------|---------|\\n| Azure App Service | GA |\\n| Azure Static Web Apps | GA |\\n| Azure Container Apps | Beta |\\n| Azure Functions | GA |\\n| Azure Kubernetes Service | Beta* |\\n| Azure Spring Apps | Beta |\\n\\n_*KS support limited to projects deployable via kubectl apply -f_\\n\\nThis covers the majority of scenarios, for example, a Static WebApp Frontend, an Azure Functions API backend, or a full-blown AKS cluster with a Spring App.\\n\\n| Category | Supported Technologies |\\n|----------|----------------------|\\n| Programming Languages | Node.js, Python, .NET, Java |\\n| Infrastructure as Code | Bicep, Terraform |\\n\\nMake sure to keep an eye on [Supported languages and environments](https://learn.microsoft.com/azure/developer/azure-developer-cli/supported-languages-environments?WT.mc_id=AZ-MVP-5004796) and [Azure/azure-dev](https://github.com/Azure/azure-dev) for the most up to date information.\\n\\n## \ud83d\ude80 Getting Started\\n\\nLet\'s get started with the Azure Developer CLI (azd) by installing the CLI and creating a new project.\\n\\nYou can install it using tools [such as winget, straight curl sh script install, or homebrew](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd?tabs=choco-windows%2Cbrew-mac%2Cscript-linux&pivots=os-windows&WT.mc_id=AZ-MVP-5004796).\\n\\nFor me, I favor [GitHub Codespaces](https://github.com/features/codespaces), and using a GitHub Codespace I have preconfigured with AZD allows me to get started quickly.\\n\\n:::info\\nYou can find my IaC Coding Codespace template here: [lukemurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding).\\n:::\\n\\nIf you want to add Azure CLI support to your own devcontainer/Codespace, then in the devcontainer.json under the features section, add:\\n\\n```json\\n\\"features\\": {\\n    // Adds Azure Developer CLI (azd) support.\\n    \\"ghcr.io/azure/azure-dev/azd:latest\\": {},\\n     },\\n```\\nYou can also add the Azure Developer CLI [Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.azure-dev) by:\\n\\n```json\\n\\"extensions\\": [\\n    \\"ms-azuretools.azure-dev\\"\\n]\\n``` \\n\\n:::info\\nMy demo environment that I will be using for this article is run from my [lukemurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding) GitHub Codespace template. This contains AZD, Terraform, Bicep, pretty much everything I need for any Infrastructure as Code work. \\n:::\\n\\n### \ud83d\udcdd Project initialization (azd init)\\n\\nTo create a new project using the Azure Developer CLI _(azd)_, you can use the `azd init` command. This command will create a new project in the current directory or will prompt you to select a template to use for the project from the awesome-and templates list.\\n\\nFor our demo, we will select a template already preconfigured.\\n\\nWe will select the [dreamteam](https://github.com/Azure-Samples/dream-team) _(Autogen application, hosted in Azure Container App, with a Streamlit front end with Azure OpenAI)_ template. We can use the up and down arrows and type if we know what template we want to easily search.\\n\\nThis template contains [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/?WT.mc_id=AZ-MVP-5004796), [Azure Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?WT.mc_id=AZ-MVP-5004796), [Key Vault](https://learn.microsoft.com/azure/key-vault/general/basic-concepts?WT.mc_id=AZ-MVP-5004796), [Azure Log Analytics](https://learn.microsoft.com/azure/azure-monitor/logs/log-analytics-overview?WT.mc_id=AZ-MVP-5004796), [Managed identities](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796) and [Azure OpenAI service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n![azd init](images/demo-azdinit-template.gif)\\n\\nWe could have also typed `azd init -t dream-team` to select the dream-team template if we knew what it was.\\n\\n* An infra folder was added that includes Bicep files to create Azure resources (it could easily be Terraform).\\n* An azure.yaml configuration file was added to map the app code in the src directory to the provision of Azure resources.\\n\\n![azd init](images/AZD_Base-FilesFolders.jpg)\\n\\n### \u2b06\ufe0f Deployment (azd up)\\n\\nNow that we have the AZD project initialized, we can deploy the resources to Azure using the `azd up` command.\\n\\nBut first, we need to authenticate to Azure by running `and auth login` and following the instructions to select the appropriate tenancy and subscription.\\n\\n![azd up](images/demo-azdauth.gif)\\n\\n:::warning\\nIf you have tags or compliance reasons why resources need to be named something specific, now is the time to amend the infrastructure as code, in most projects, the name is made up of a resourcetoken generated by a unique token based on the subscription ID, environment name, and location + a resource suffix, for example, if it you were deploying API Management, the name could look something like: apim-nepvrtdddlfbu, but the names can be overwritten in the Bicep files like you would any other Bicep deployment - in fact, you can adjust the IaC as best to suit your environment, ie add in Private Endpoints, or adjust the SKU of a resource.\\n:::\\n\\nNow we can run `azd up` to deploy the resources to Azure.\\n\\n![azd up](images/demo-azddeploy.gif)\\n\\nAnd if we click on the supplied endpoint, we can access our Streamlit application that is running in Azure Container Apps.\\n\\n![azd up](images/demo-azd-deploy-streamlit.jpg)\\n\\n:::tip\\nYou can track the deployment in the Deployment blade of the created Resource Group as well.\\n:::\\n\\nAs you can see, AZD went through a build phase to package and build the docker container from the src directory before proceeding with the infrastructure deployment and application deployment _(this behavior can be controlled by editing the azure.yaml file, but following this pattern is the default, build project, build infrastructure, then deploy project)_. No point in deploying the infrastructure if the build fails, and talking about failure, you may have issues deploying or building your service and not quite know where the problem is - one of the things you can do is enable [debug](https://learn.microsoft.com/azure/developer/azure-developer-cli/troubleshoot?tabs=Browser&WT.mc_id=AZ-MVP-5004796#using-the---debug-switch) to have more verbose output, ie `azd up --debug`.\\n\\n\\n### \ud83c\udf0d Environment management (azd env)\\n\\nNow we have a fully deployed application using `and up` let\'s discuss environment management.\\n\\n`azd env` is a command that allows you to manage your environments. You can create, list, delete, and switch between environments using this command, so you can easily manage different environments, such as development, staging, and production, without affecting each one.\\n\\n:::info\\nManage your application environments. With this command group, you can create a new environment or get, set, and list your application environments.\\n\\n  \u2022 An Application can have multiple environments (ex: dev, test, prod).\\n  \u2022 Each environment may have a different configuration (that is, connectivity information) for accessing Azure resources.\\n  \u2022 You can find all environment configurations under the `.azure/<environment-name>` folder.\\n  \u2022 The environment name is stored as the AZURE_ENV_NAME environment variable in the `.azure/<environment-name>/.env` file.\\n\\n```text\\nUsage\\n  azd env [command]\\n\\nAvailable Commands\\n  get-value     : Get specific environment value.\\n  get-values    : Get all environment values.\\n  list          : List environments.\\n  new           : Create a new environment and set it as the default.\\n  refresh       : Refresh environment settings by using information from a previous infrastructure provision.\\n  select        : Set the default environment.\\n  set           : Manage your environment settings.\\n  set-secret    : Set a <name> as a reference to a Key Vault secret in the environment.\\n\\nGlobal Flags\\n    -C, --cwd string    : Sets the current working directory.\\n        --debug         : Enables debugging and diagnostics logging.\\n        --docs          : Opens the documentation for azd env in your web browser.\\n    -h, --help          : Gets help for env.\\n        --no-prompt     : Accepts the default value instead of prompting, or it fails if there is no default.\\n```\\n:::\\n\\nThe first thing we can do is the environment variables of our environments by running `azd env get-values`.\\n\\nThis will output the environment variables; each solution may be different, but common values are AZURE_ENV_NAME and AZURE_LOCATION.\\n\\n![azd env get-values](images/demo-azdgetenvvalues.gif)\\n\\nThese values are also stored in the `.azure/<environment-name>/.env` file.\\n\\nSo we have an environment named: testdeploy, lets see if we can create a new environment, and switch to it.\\n\\n![azd env new](images/demo-azdcreateenv.gif)\\n\\nNow you can manage multiple environments of your application and switch between them easily, each one treated separately in terms of the values that are set, so dev could be in another region, or you could do something like this:\\n\\n![azd env select](images/demo-azdcreateenvvalues.gif)\\n\\nIt\'s pretty basic, and the TagENV variable, you could just pull out by using the environmentName variable in Bicep _(take a look at the main.parameters.json)_, but it shows you how you can manage different environments, and the values that are set for them.\\n\\nMultiple environments can be useful for testing, for different regions, or for different stages of the development lifecycle, particularly when looking at having multiple environments for a CI/CD pipeline, so dev, test, staging, production, and and up can be used to deploy to these environments. I have run into scenarios where I have deployed something with an old configuration that started to get used by a team while I continued to work on new configuration and settings - having multiple environments allowed me to leave the old one untouched while I worked on a new environment\\n\\n### \ud83d\udcca Monitoring (azd monitor)\\n\\nTo be fair - this isn\'t one I\'ve used much, but it is a useful command to know about.\\n\\n`azd monitor` is a command that allows you to monitor your application. You can view logs, metrics, and other monitoring data using this command, so you can easily monitor the health and performance of your application by utilizing [Application Insights dashboards](https://learn.microsoft.com/en-us/azure/azure-monitor/app/overview-dashboard?WT.mc_id=AZ-MVP-5004796).\\n\\n| Dashboard Type | Command |\\n|---------------|---------|\\n| Main dashboard | `azd monitor --overview` |\\n| Live metrics | `azd monitor --live` |\\n| Logs dashboard | `azd monitor --logs` |\\n\\nThese commands will open the respective dashboards in your default web browser, making it easy to monitor your application\'s performance and health.\\n\\nLet us take a brief look after I switched back to my testdeploy environment:\\n\\n![azd monitor](images/demo-azdmonitor.gif)\\n\\n### \u2b07\ufe0f Destroying resources (azd down)\\n\\nNow that we have deployed and monitored our application, we can destroy the resources using the `azd down` command.\\n\\n:::warning\\nBefore using the azd down command, make sure you have backed up any data or config that you need, as this command will delete all resources deployed with AZD and anything inside of the Resource Group that was deployed by AZD initially.\\n:::\\n\\n:::tip\\nA few Azure resources, such as Azure OpenAI, Azure API Management, and Key Vault, are not entirely deleted when you run `azd down`. You can delete these resources by running `azd down --purge to remove purge them from a soft-delete state altogether; you may want to do this if you are recreating the same resources with the same names consistently and don\'t want to have Bicep reinstate these resources for you.\\n:::\\n\\n![azd down](images/demo-azddown.gif)\\n\\n## \ud83d\udd04 CI/CD Integration\\n\\nNow, you might have a scenario where you want to integrate the Azure Developer CLI _(azd)_ into your CI/CD pipeline so you can automate the deployment of your applications to Azure, especially in a team or collaborative environment.\\n\\nAnd its doable, there are a few ways to do this, and I will cover the out of the box one first, this will work for both Azure DevOps pipelines or GitHub Actions.\\n\\n:::info\\nManage integrating your application with deployment pipelines. (Beta)\\n\\n  \u2022 azd commands (e.g., provision, deploy) can be used within GitHub Actions and Azure Pipelines to test your code against real Azure resources and facilitate deployments.\\n  \u2022 After creating a pipeline definition file, running pipeline config will help configure your deployment pipeline to connect securely to Azure.\\n  \u2022 For more information on how to use azd in your pipeline, go to: https://aka.ms/azure-dev/pipeline.\\n\\n```text\\nUsage\\n  azd pipeline [command]\\n\\nAvailable Commands\\n  config        : Configure your deployment pipeline to connect securely to Azure. (Beta)\\n\\nGlobal Flags\\n    -C, --cwd string    : Sets the current working directory.\\n        --debug         : Enables debugging and diagnostics logging.\\n        --docs          : Opens the documentation for azd pipeline in your web browser.\\n    -h, --help          : Gets help for pipeline.\\n        --no-prompt     : Accepts the default value instead of prompting, or it fails if there is no default.\\n\\nUse azd pipeline [command] --help to view examples and more information about a specific command.\\n\\nExamples\\n  Walk through the steps required to set up your deployment pipeline.\\n    azd pipeline config\\n```\\n:::\\n\\n![azd pipeline config](images/demo-pipelineconfig.gif)\\n\\nForgive the stop; my pre-commit file prevented it from actually committing the code, but hopefully, it was enough to show you how to configure the pipeline and link it directly into GitHub with Azure Developer CLI doing a lot of the heavy lifting for you.\\n\\n:::tip\\nWhen working with multiple environments, it requires some tweaking to the GitHub Actions.\\n\\nI am not going to re-invent the wheel here; there is some great documentation by [Jason Taylor](https://www.linkedin.com/in/jasontaylordev/), Microsoft Azure MVP from Australia, that I have personally used to deploy to multiple environments, and it is a great resource to get started with: [Support multiple environments](https://github.com/jasontaylordev/todo-aspnetcore-csharp-sqlite/blob/main/OPTIONAL_FEATURES.md). Jason also recently did a video to the Microsoft Azure Cloud Commanders Learning Room: [Speedrunning Code to Cloud with the Azure Developer CLI (azd) with Jason Taylor](https://youtu.be/e6K2EuVfgWE). \\n:::\\n\\n## \ud83d\udca1 Tips and Tricks\\n\\nJust finishing off with some tips and tricks:\\n\\n* Make use of the [azuredeveloper cli hooks](https://learn.microsoft.com/azure/developer/azure-developer-cli/azd-extensibility?WT.mc_id=AZ-MVP-5004796#available-hooks), to run scripts and custom actions on post, predeploy. Refer to my article [Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI](https://luke.geek.nz/azure/staticwebapp-azd-environment-variables/) for some guidance on how I have used this.\\n* Use Templates, as mentioned earlier, the [Awesome AZD](https://azure.github.io/awesome-azd/) gallery is a great resource to find templates to get you started and AI orientated workloads, you can also refer to the [ai-app-template gallery](https://azure.github.io/ai-app-templates/).\\n* There is a issue and PR being worked on, to update alot of the templates with [AVM _(Azure Verified Module)_](https://azure.github.io/Azure-Verified-Modules/) templates, but it hasn\'t been completed yet, so feel free to replace or add in your own modules and create your own AZD template! Start from your application code and build it out from there by testing the deployment with AZD to an Azure Function or Container App.\\n* Keep an eye on the Pull Requests and [GitHub repo](https://github.com/Azure/azure-dev/) for the latest updates and changes, and join in on the latest [discussions and community calls](https://github.com/Azure/azure-dev/discussions).\\n\\nHopefully, this has given you enough of a view of some of the great functionality of Azure Developer CLI (azd) and how you can use it to deploy your applications to Azure and innovate and learn quickly."},{"id":"azure/azure-platform-release-process","metadata":{"permalink":"/azure/azure-platform-release-process","source":"@site/blog/2025-01-28-azureplatformrelease/index.mdx","title":"How Microsoft Releases Changes to Azure - Safe Deployment","description":"Learn how Microsoft implements changes daily across Azure using safe deployment practices, quality gates, and automated processes.","date":"2025-01-28T06:32:57.324Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.38,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How Microsoft Releases Changes to Azure - Safe Deployment","metaDescription":"Learn how Microsoft implements changes daily across Azure using safe deployment practices, quality gates, and automated processes.","date":"2025-01-28T06:32:57.324Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/azure-platform-release-process","keywords":["Azure","Azure Platform","Safe Deployment","Azure Quality Gates","Azure Release Process","Azure Well-Architected","Azure Reliability","Change Management","Platform Resiliency","Azure Service Health"],"description":"Learn how Microsoft implements changes daily across Azure using safe deployment practices, quality gates, and automated processes."},"unlisted":false,"prevItem":{"title":"Getting Started with Azure Developer CLI (azd)","permalink":"/azure/azure-developer-cli"},"nextItem":{"title":"Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI","permalink":"/azure/staticwebapp-azd-environment-variables"}},"content":"Today, we will touch on how Microsoft releases changes globally across the Microsoft Azure ecosystem. It always amazes me how Microsoft can release changes daily across Azure without customers noticing _(the majority of the time)_, yet they benefit from new capabilities swiftly. \\n\\nMicrosoft has implemented several quality stages for Azure, and they are continuously enhancing platform automation and processes to ensure Azure service changes have zero impact on customer workloads. Let\'s take a look at how Microsoft is achieving this.\\n\\n> Would you like to know how Azure releases changes to Production while maintaining high Quality and Agility? Azure implements hundreds of changes daily without customers noticing, yet they benefit from new capabilities swiftly. You can learn about the quality stages for Azure and how they are learning to help ensure Microsoft continuously enhances platform automation and processes to ensure Azure service changes have zero impact on customer workloads.\\n\\n{/* truncate */}\\n\\n## \ud83c\udfaf Azure Well-Architected Framework Integration\\n\\nThe Well-Architected Framework has an entire pillar dedicated to [Reliability](https://learn.microsoft.com/azure/well-architected/reliability/?WT.mc_id=AZ-MVP-5004796) and [Operational excellence](https://learn.microsoft.com/azure/well-architected/operational-excellence/?WT.mc_id=AZ-MVP-5004796), so let us take a look at how these pillars are adapted to work with the deployment of core Azure platform and service deployments, along with elements of [safe deployment practices](https://learn.microsoft.com/devops/operate/safe-deployment-practices?WT.mc_id=AZ-MVP-5004796) that are used to deploy Azure services/payloads at a global scale.\\n\\n## \ud83c\udfd7\ufe0f Azure Investment Pillars\\n\\n![Azure Investments](images/Azure_SDPInvestment.PNG)\\n\\nLet\'s take a look at the aspiration that Microsoft has with the Azure ecosystem and how they are working to ensure that they can deliver changes to the platform without impacting customers, with the aspiration of \'0 changes impact production, and they cause 0 outages\' as a north star.\\n\\nTo get there, Microsoft is working on a number of key areas, including the following investment pillars:\\n\\n* Secure platform\\n* Service health\\n* Change management\\n* Incident management\\n* Supportability\\n* Customer resiliency\\n* Platform resiliency\\n* Capacity management\\n\\n![Azure Investments](images/Azure_SDPQualitryPillars.PNG)\\n\\n## \ud83d\udee1\ufe0f Safe Deployment Practices\\n\\nLets take a look at how the [safe deployment practices](https://learn.microsoft.com/devops/operate/safe-deployment-practices?WT.mc_id=AZ-MVP-5004796) work in the context of the Azure platform, and how they are used to ensure that changes are deployed safely and securely, with 3 key areas of focus:\\n\\n* Prevention\\n* Protection\\n* Learn and refine\\n\\n![Azure Investments](images/Azure_SDP.PNG)\\n\\nPrevention is about:\\n\\n* Testing in Production in isolation, with continuous representative workloads and scenarios.\xa0\\n* Having Additional Health gates, pre-deployment reviews, and a Bill of Quality.\\n* Utilising Canary regions for at-scale pre-prod full E2E validations.\\n* Having increased scenario coverage through health integrated synthetics and representative workloads.\\n\\nProtection is about:\\n\\n* Progressive deployments orchestrated by standard tools with adaptable policies and multilayer gating.\\n* Speed of deployment gated (speed, stops) by risk and health signals.\\n* Multilayer health monitoring integrated for real-time stops and rollbacks.\\n* Central correlation and analysis of aggregated health signals.\\n* Short- and long-range anomaly detection that leverages ML models.\\n* Real-time impact monitoring.\\n\\nLearn and refine is about:\\n\\n* Continuous refinement and additions of [health signals/SLIs/SLOs](https://learn.microsoft.com/azure/well-architected/design-guides/health-modeling?WT.mc_id=AZ-MVP-5004796).\\n* Learnings from the smallest outages drive repairs towards minimizing the time for self-detection in pre-prod.\\n* Enrichment of synthetics shared tests. Injection of gating signals from synthetics and customers.\\n* Ongoing ML refinements with increased cross-service signals.\\n\\nThese areas of focus help to define rollout systems and processes that are used to ensure that changes are deployed safely and securely, with the ability to roll back changes if they are not successful, emphasizing quality and ready-to-deploy validation steps.\\n\\n## \ud83c\udfaf Quality Gates and Release Process\\n\\n![Azure Release quality gates](images/Azure_SDPQualityGates.PNG)\\n\\nOnce the quality gates and processes, the rollout of the payload occurs across Canary _(no Production workloads)_, Pilot regions, Medium and large before being deployed further to the rest of the areas, with 24-hour \'bake time\' before each deployment, with the ability to rollback changes if they are not successful, and the processes and quality gates are started again.\\n\\n![Azure Release stages](images/Azure_SDPQualityGatesRelease.PNG)\\n\\n![Azure Release triggers](images/AzureSDP_Triggers.png)\\n\\n## \ud83d\udcda References\\n\\n* [Configuring your release pipelines for safe deployments](https://devblogs.microsoft.com/devops/configuring-your-release-pipelines-for-safe-deployments/?WT.mc_id=AZ-MVP-5004796)\\n* [How Microsoft Azure ensures high-Quality releases with Agility](https://ignite.microsoft.com/en-US/sessions/BRK243)\\n* [Deployment and testing for mission-critical workloads on Azure](https://learn.microsoft.com/azure/architecture/reference-architectures/containers/aks-mission-critical/mission-critical-deploy-test?WT.mc_id=AZ-MVP-5004796)\\n* [Advancing safe deployment practices](https://azure.microsoft.com/blog/advancing-safe-deployment-practices/?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/staticwebapp-azd-environment-variables","metadata":{"permalink":"/azure/staticwebapp-azd-environment-variables","source":"@site/blog/2025-01-22-azd-staticwebapp-variables/index.mdx","title":"Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI","description":"Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI","date":"2025-01-22T05:02:57.200Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.395,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI","metaDescription":"Learn how to configure and manage environment variables in Azure Static Web Apps using Azure Developer CLI (azd) deployment hooks for React applications.","date":"2025-01-22T05:02:57.200Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/staticwebapp-azd-environment-variables","keywords":["Azure","Azure Developer CLI","AZD","Static Web Apps","React","Environment Variables","Runtime Configuration","DevOps","Cloud Development","Web Development"],"description":"Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI"},"unlisted":false,"prevItem":{"title":"How Microsoft Releases Changes to Azure - Safe Deployment","permalink":"/azure/azure-platform-release-process"},"nextItem":{"title":"PowerShell based Terraform Bootstrap Script","permalink":"/azure/powershell-terraform-bootstrap"}},"content":"The [Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview?tabs=windows&WT.mc_id=AZ-MVP-5004796) is an open-source tool that accelerates provisioning and deploying app resources on Azure, it is great for deploying resources quickly and easily - especially in a proof of concept, or rapid innovation scenario.\\n\\nOne of the resources you can deploy _(both Infrastructure and Application)_ is a [Azure Static Web App](https://learn.microsoft.com/azure/static-web-apps/overview?WT.mc_id=AZ-MVP-5004796) recently, I ran into an issue where I was deploying a backend [Azure Function](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-python&WT.mc_id=AZ-MVP-5004796), and I needed a react web application front end hosted on the Azure Static WebApp to pull the function app URL and function key from the runtime environment variables, and quickly realized that React does not support runtime environment variables - only build-time environment variables.\\n\\n{/* truncate */}\\n\\nIt makes a bit of sense when you step back - React is a front-end framework, and the environment variables are injected at build time, so the front end can be built with the correct values, especially for a static web app - that isn\'t dynamically generated at runtime. \\n\\n> [Adding Custom Environment Variables](https://create-react-app.dev/docs/adding-custom-environment-variables/). The environment variables are embedded during the build time. Since Create React App produces a static HTML/CSS/JS bundle, it can\u2019t possibly read them at runtime. To read them at runtime, you would need to load HTML into memory on the server and replace placeholders in runtime, as [described here](https://create-react-app.dev/docs/title-and-meta-tags#injecting-data-from-the-server-into-the-page). Alternatively you can rebuild the app on the server anytime you change them.\\n\\nSo, if that is the case, how can I get the Azure Function URL and Function Key into the React front end? When the name of the function app, and the Static WebApp are dynamically created each time I deploy?\\n\\n:::warning\\nThe method below leaves the endpoint and key in the source code, which is not recommended for production applications. Please look at other forms of authentication to a backend API service, such as the built-in [Azure Static API support](https://learn.microsoft.com/azure/static-web-apps/apis-overview?WT.mc_id=AZ-MVP-5004796).\\n:::\\n\\nTo get around the limitation of the dynamic variables, I made use of the [Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview?tabs=windows&WT.mc_id=AZ-MVP-5004796) postdeploy feature, which allows you to run a script after a deployment has completed.\\n\\nMy setup looks like this:\\n\\n```mermaid\\ngraph LR\\n    %% Main workflow\\n    A[azd up] --\x3e B[provision]\\n    B --\x3e C[deploy]\\n    \\n    %% Services\\n    subgraph Services\\n        D[api service]\\n        E[web service]\\n    end\\n    \\n    %% Hosting\\n    D --\x3e F[Azure Functions]\\n    E --\x3e G[Static Web App]\\n    \\n    %% Hooks\\n    E -.->|preprovision| H[preprovision.sh]\\n    D -.->|postdeploy| I[update_env.sh]\\n    \\n    %% Language/Tech\\n    D --- J[Python]\\n    E --- K[TypeScript]\\n```\\n\\nEssentially, once the API _(Python)_ has been deployed, Azure Developer CLI will run an update_env.sh script, which will update the React `.env.production` file with the correct values for the Azure Function URL and Function Key.\\n\\nSo my azure.yaml file looks like this:\\n\\n```yaml\\nservices:\\n  api:\\n    project: ./src/api\\n    language: python\\n    host: function\\n    hooks:\\n      postdeploy:\\n        posix:\\n          shell: sh\\n          run: ../../hooks/update_env.sh\\n          interactive: true\\n          continueOnError: false\\n```\\n\\nAnd inside my update_env.sh script, I have the following:\\n\\n```bash\\n#!/bin/bash\\nset -e\\n\\n# Fetch the Function App name and Resource Group from azd environment\\nAZD_VALUES=$(azd env get-values --output json)\\necho \\"AZD_VALUES: $AZD_VALUES\\"\\n\\nFUNCTION_APP_NAME=$(echo $AZD_VALUES | jq -r \'.AZURE_FUNCTION_NAME\')\\nRESOURCE_GROUP=$(echo $AZD_VALUES | jq -r \'.AZURE_RESOURCE_GROUP\')\\nFUNCTION_APP_URL=$(echo $AZD_VALUES | jq -r \'.AZURE_FUNCTION_URI\')\\n\\necho \\"Function App Name: $FUNCTION_APP_NAME\\"\\necho \\"Resource Group: $RESOURCE_GROUP\\"\\n\\n# Get the Function App URL\\necho \\"Function App URL: $FUNCTION_APP_URL\\"\\n\\n# Get the Function App key\\nFUNCTION_APP_KEY=$(az functionapp keys list --name $FUNCTION_APP_NAME --resource-group $RESOURCE_GROUP --query functionKeys.default -o tsv)\\necho \\"Function App Key: $FUNCTION_APP_KEY\\"\\n\\n# Update the .env.production file\\nsed -i \\"s|^REACT_APP_API_URL=.*|REACT_APP_API_URL=$FUNCTION_APP_URL/api|\\" ../web/.env.production\\n\\necho \\"Function App Name: $FUNCTION_APP_NAME\\"\\necho \\"Resource Group: $RESOURCE_GROUP\\"\\necho \\"Function App URL: $FUNCTION_APP_URL\\"\\n```\\n\\nTo utilize the script, I need to make sure I am authenticated to Azure _(az login)_ and the script runs as part of my deployment.\\n\\nI also made sure I was outputting the Resource Group, FunctionApp name etc as part of my main.bicep file, so it was able to be picked up by the Azure Developer CLI variables.\\n\\nThen, after the script ran and the `.env.production` file updated, Azure Developer CLI was able to continue on to the second part of my deployment - building the React website and then deploying it to the Azure Static WebApp."},{"id":"azure/powershell-terraform-bootstrap","metadata":{"permalink":"/azure/powershell-terraform-bootstrap","source":"@site/blog/2025-01-13-terraformpwshbootstrap/index.mdx","title":"PowerShell based Terraform Bootstrap Script","description":"Create a PowerShell script to bootstrap Terraform, handle installation, create project directories, and automate deployments across Windows, Mac, and Linux.","date":"2025-01-13T07:44:51.071Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.905,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"PowerShell based Terraform Bootstrap Script","metaDescription":"Create a PowerShell script to bootstrap Terraform, handle installation, create project directories, and automate deployments across Windows, Mac, and Linux.","date":"2025-01-13T07:44:51.071Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/powershell-terraform-bootstrap","keywords":["PowerShell","Terraform","Infrastructure as Code","Automation","DevOps","Bootstrap","Cross-platform","Infrastructure automation","Configuration management","Script development"],"description":"Create a PowerShell script to bootstrap Terraform, handle installation, create project directories, and automate deployments across Windows, Mac, and Linux."},"unlisted":false,"prevItem":{"title":"Managing Environment Variables in Azure Static Web Apps with Azure Developer CLI","permalink":"/azure/staticwebapp-azd-environment-variables"},"nextItem":{"title":"Product Development lifecycle","permalink":"/misc/product-development-lifecycle"}},"content":"Today, we will implement a Terraform bootstrap script that will install Terraform and create directories where we can place our Terraform project, which will then run a plan against and deploy. This script will be written in PowerShell to bootstrap a new Terraform project.\\n\\n> \\"Bootstrapping usually refers to a self-starting process that is supposed to continue or grow without external input. Many analytical techniques are often called bootstrap methods in reference to their self-starting or self-supporting implementation\\" ~ Wikipedia.\\n\\n{/* truncate */}\\n\\n## \ud83d\udcdc Script Overview\\n\\nInspired by the [ALZ Accelerator](https://github.com/Azure/ALZ-PowerShell-Module) bootstrap script, this script will:\\n\\n1. Install-Terraform\\n- Downloads and installs Terraform if not present\\n- Handles version management\\n- Supports Windows/Mac/Linux detection\\n- Adds Terraform to PATH\\n2. Creates required directories (config and output)\\n- Copies *.tf and *.tfvars files from config to output\\n- Validate file contents and paths\\n3. Invoke-Terraform\\n- Initializes Terraform\\n- Creates execution plan\\n- Handles apply/destroy with optional auto-approve\\n- Manages working directory context\\n\\nAnd I\'ve tested it on my Windows 11 machine and a Linux Codespace. In my examples, I am using it to run some base Terraform to deploy a new Resource Group and Storage Account in the New Zealand North Azure region.\\n\\n![Run Terraform Bootstrap](images/RunTerraformBootstrap.gif)\\n\\n:::tip\\nYou could also look at turning it into an executable using something like [PS2EXE](https://github.com/MScholtes/PS2EXE), and then you can run it like any other executable, as below:\\n\\n![Run Terraform Bootstrap Executable](images/TerraformBootstrapExecutable.gif)\\n:::\\n\\n## \ud83d\udccb Prerequisites\\n\\n- [PowerShell](https://learn.microsoft.com/powershell/scripting/install/installing-powershell?view=powershell-7.4&WT.mc_id=AZ-MVP-5004796) 5.1 or PowerShell Core 6.0+\\n- Internet connectivity for downloading [Terraform](https://www.terraform.io/)\\n- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli?WT.mc_id=AZ-MVP-5004796) is installed and logged in (`az login`). It is only required if deploying to [Microsoft Azure](https://learn.microsoft.com/azure/developer/terraform/overview?WT.mc_id=AZ-MVP-5004796).\\n- Write permissions to the directory where you\'ll run the script\\n\\n## \ud83c\udfaf Usage Examples\\n\\nBasic usage with default parameters:\\n\\n```powershell\\n.\\\\Terraform-Bootstrap.ps1\\n```\\n\\nCustom paths and auto-approved apply:\\n\\n```powershell\\n.\\\\Terraform-Bootstrap.ps1 -terraformPath \\"C:\\\\terraform\\" -configPath \\"C:\\\\tf-configs\\" -outputPath \\"C:\\\\tf-output\\" -autoApprove\\n```\\n\\nUsing a specific Terraform version:\\n\\n```powershell\\n.\\\\Terraform-Bootstrap.ps1 -terraformPath \\"C:\\\\terraform\\" -configPath \\"C:\\\\tf-configs\\" -outputPath \\"C:\\\\tf-output\\"\\n```\\n\\n## \ud83d\udcbb PowerShell Script\\n\\n```powershell title=\\"Terraform-Bootstrap.ps1\\"\\n<#\\n    .SYNOPSIS\\n    Bootstraps Terraform environment and runs specified Terraform commands.\\n\\n    .DESCRIPTION\\n    This script ensures Terraform is installed, sets up the Terraform workspace, and runs specified Terraform commands.\\n    It supports downloading the latest version of Terraform, creating necessary directories, and copying configuration files.\\n    The script is intended to be used to bootstrap Terraform environments for testing and development purposes and once-off deployments of any code in the Config directory.\\n\\n    .NOTES\\n    Version:        1.0\\n    Author:         luke.geek.nz\\n    Creation Date:  10/01/25\\n    Purpose/Change: \\n    14/05/17 - Initial script creation\\n\\n    .PARAMETER terraformPath\\n    The path where Terraform will be installed.\\n\\n    .PARAMETER terraformVersion\\n    The version of Terraform to install. Defaults to \\"latest\\".\\n\\n    .PARAMETER configPath\\n    The path to the directory containing Terraform configuration files.\\n\\n    .PARAMETER outputPath\\n    The path to the directory where Terraform will be executed.\\n\\n    .PARAMETER autoApprove\\n    Automatically approve Terraform apply and destroy actions.\\n\\n    .EXAMPLE\\n    .\\\\Terraform-Bootstrap.ps1 -terraformPath \\".\\\\terraform\\" -terraformVersion \\"latest\\" -configPath \\".\\\\config\\" -outputPath \\".\\\\output\\"\\n#>\\n\\n[CmdletBinding()]\\nparam (\\n    [Parameter(Mandatory = $false)]\\n    [string]$terraformPath = \\".\\\\terraform\\",\\n    \\n    [Parameter(Mandatory = $false)]\\n    [string]$terraformVersion = \\"latest\\",\\n    \\n    [Parameter(Mandatory = $false)]\\n    [string]$configPath = \\".\\\\config\\",\\n    \\n    [Parameter(Mandatory = $false)]\\n    [string]$outputPath = \\".\\\\output\\",\\n    \\n    [Parameter(Mandatory = $false)]\\n    [switch]$autoApprove\\n)\\n\\n# Function to ensure Terraform is installed\\nfunction Install-Terraform {\\n    param (\\n        [string]$version,\\n        [string]$path\\n    )\\n    \\n    # Get latest version if not specified\\n    if ($version -eq \\"latest\\") {\\n        $versionResponse = Invoke-WebRequest -Uri \\"https://checkpoint-api.hashicorp.com/v1/check/terraform\\"\\n        $version = ($versionResponse).Content | ConvertFrom-Json | Select-Object -ExpandProperty current_version\\n    }\\n\\n    # Check if Terraform is already installed\\n    $tfCommand = Get-Command -Name terraform -ErrorAction SilentlyContinue\\n    if ($tfCommand) {\\n        Write-Verbose \\"Terraform already installed at $($tfCommand.Path)\\"\\n        return\\n    }\\n\\n    # Create tools directory\\n    if (!(Test-Path $path)) {\\n        New-Item -ItemType Directory -Path $path | Out-Null\\n    }\\n\\n    # Download and extract Terraform\\n    $os = if ($IsWindows) { \\"windows\\" } else { if ($IsMacOS) { \\"darwin\\" } else { \\"linux\\" } }\\n    $arch = if ([System.Environment]::Is64BitOperatingSystem) { \\"amd64\\" } else { \\"386\\" }\\n    \\n    $url = \\"https://releases.hashicorp.com/terraform/$($version)/terraform_$($version)_${os}_${arch}.zip\\"\\n    $zipFile = Join-Path $path \\"terraform.zip\\"\\n    $extractPath = Join-Path $path \\"terraform_$version\\"\\n\\n    Write-Verbose \\"Downloading Terraform from $url\\"\\n    Invoke-WebRequest -Uri $url -OutFile $zipFile\\n    \\n    Write-Verbose \\"Extracting Terraform to $extractPath\\"\\n    Expand-Archive -Path $zipFile -DestinationPath $extractPath -Force\\n    Remove-Item $zipFile\\n\\n    # Add to PATH\\n    $env:PATH = \\"$extractPath;$env:PATH\\"\\n}\\n\\n# Function to run Terraform commands\\nfunction Invoke-Terraform {\\n    param (\\n        [string]$workingDirectory,\\n        [string]$command,\\n        [switch]$autoApprove\\n    )\\n\\n    Push-Location $workingDirectory\\n    try {\\n        # Initialize\\n        Write-Host \\"Initializing Terraform...\\" -ForegroundColor Green\\n        terraform init\\n\\n        # Run specified command\\n        Write-Host \\"Running terraform $command...\\" -ForegroundColor Green\\n        if ($command -eq \\"apply\\" -or $command -eq \\"destroy\\") {\\n            terraform plan -out=tfplan\\n            \\n            if (!$autoApprove) {\\n                $confirmation = Read-Host \\"Do you want to proceed with terraform $command? (y/n)\\"\\n                if ($confirmation -ne \'y\') {\\n                    Write-Host \\"Operation cancelled\\" -ForegroundColor Yellow\\n                    return\\n                }\\n            }\\n            \\n            if ($command -eq \\"apply\\") {\\n                terraform apply -auto-approve tfplan\\n            }\\n            else {\\n                terraform destroy -auto-approve\\n            }\\n        }\\n        else {\\n            terraform $command\\n        }\\n    }\\n    finally {\\n        Pop-Location\\n    }\\n}\\n\\n# Main script\\ntry {\\n    # Create required directories\\n    if (!(Test-Path $configPath)) {\\n        New-Item -ItemType Directory -Path $configPath -Force | Out-Null\\n        Write-Host \\"Config directory created at $configPath. Please place Terraform files into this directory and press any key to continue...\\" -ForegroundColor Yellow\\n        Read-Host\\n    }\\n    if (!(Test-Path $outputPath)) {\\n        New-Item -ItemType Directory -Path $outputPath -Force | Out-Null\\n    }\\n\\n    # Install Terraform\\n    Write-Host \\"Ensuring Terraform is installed...\\" -ForegroundColor Green\\n    Install-Terraform -version $terraformVersion -path $terraformPath\\n\\n    # Copy Terraform files from config to output directory\\n    Write-Host \\"Setting up Terraform workspace...\\" -ForegroundColor Green\\n    \\n    # Convert to absolute paths\\n    $configPathFull = Resolve-Path $configPath -ErrorAction Stop\\n    $outputPathFull = Resolve-Path $outputPath -ErrorAction Stop\\n    \\n    Write-Verbose \\"Config Path: $configPathFull\\"\\n    Write-Verbose \\"Output Path: $outputPathFull\\"\\n    \\n    $configFiles = Get-ChildItem -Path $configPathFull -Recurse -File -Filter \\"*.tf\\" -ErrorAction Stop\\n    $varFiles = Get-ChildItem -Path $configPathFull -Recurse -File -Filter \\"*.tfvars\\" -ErrorAction Stop\\n    \\n    Write-Verbose \\"Found $($configFiles.Count) .tf files\\"\\n    \\n    foreach ($file in $configFiles) {\\n        Write-Verbose \\"Processing file: $($file.FullName)\\"\\n        \\n        # Verify source file\\n        if (!(Test-Path $file.FullName)) {\\n            Write-Error \\"Source file not found: $($file.FullName)\\"\\n            continue\\n        }\\n        \\n        # Check file content\\n        $content = Get-Content $file.FullName -Raw\\n        if ([string]::IsNullOrWhiteSpace($content)) {\\n            Write-Warning \\"File is empty: $($file.FullName)\\"\\n            continue\\n        }\\n        \\n        Write-Host \\"Copying $($file.Name) to $outputPathFull\\" -ForegroundColor Green\\n        Copy-Item -Path $file.FullName -Destination $outputPathFull -Force\\n        \\n        # Verify copy succeeded\\n        $destFile = Join-Path $outputPathFull $file.Name\\n        if (!(Test-Path $destFile)) {\\n            Write-Error \\"Failed to copy file to: $destFile\\"\\n        }\\n    }\\n    \\n    foreach ($file in $varFiles) {\\n        Write-Verbose \\"Processing var file: $($file.FullName)\\"\\n        Write-Host \\"Copying $($file.Name) to $outputPathFull\\" -ForegroundColor Green\\n        Copy-Item -Path $file.FullName -Destination $outputPathFull -Force\\n    }\\n\\n    \\n    # Run Terraform\\n    Write-Host \\"Running Terraform...\\" -ForegroundColor Green\\n    Invoke-Terraform -workingDirectory $outputPath -command \\"apply\\" -autoApprove:$autoApprove\\n\\n}\\ncatch {\\n    Write-Error \\"Error occurred: $_\\"\\n    exit 1\\n}\\n```\\n\\nThis script can also be found on GitHub [here](https://github.com/lukemurraynz/PowerOfTheShell/blob/master/Other/Terraform-Bootstrap.ps1), if you wanted to fork, or open up a Pull Request with changes.\\n\\nHopefully this is useful for you, having a script like this means I can quickly deploy resources that are coded in Terraform _(HCL)_.\\n\\n## \ud83d\udcc2 Example Terraform Files\\n\\nFor those interested, here is the base Terraform code I am using in my example:\\n\\n```hcl title=\\"config/main.tf\\"\\nresource \\"azurerm_resource_group\\" \\"example\\" {\\n  name     = \\"example-stgaccount-rg\\"\\n  location = \\"New Zealand North\\"\\n}\\n\\nresource \\"azurerm_storage_account\\" \\"example\\" {\\n  name                     = \\"stgacctfboot1\\"\\n  resource_group_name      = azurerm_resource_group.example.name\\n  location                 = azurerm_resource_group.example.location\\n  account_tier             = \\"Standard\\"\\n  account_replication_type = \\"LRS\\"\\n\\n  tags = {\\n    environment = \\"staging\\"\\n  }\\n}\\n```\\n\\n```hcl title=\\"config/providers.tf\\"\\nterraform {\\n  required_providers {\\n    azurerm = {\\n      source = \\"hashicorp/azurerm\\"\\n      version = \\"4.15.0\\"\\n    }\\n  }\\n}\\n\\nprovider \\"azurerm\\" {\\n  subscription_id = \\"9dc6cc8c-5b10-403b-9a2f-5192497ca1ed\\"\\n\\n  features {}\\n\\n}\\n```"},{"id":"misc/product-development-lifecycle","metadata":{"permalink":"/misc/product-development-lifecycle","source":"@site/blog/2025-01-10-pdlifecycle/index.mdx","title":"Product Development lifecycle","description":"Learn about the stages of product development PoC, PoT, Prototype, MVP, and Pilot, and how to progress through the lifecycle gates.","date":"2025-01-10T09:54:15.923Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Service Management","permalink":"/tags/service-management"}],"readingTime":13.55,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Product Development lifecycle","metaDescription":"Learn about the stages of Cloud product development PoC, PoT, Prototype, MVP, and Pilot.","date":"2025-01-10T09:54:15.923Z","tags":["Misc","Service Management"],"categories":["Misc"],"authors":["Luke"],"slug":"misc/product-development-lifecycle","keywords":["product development","poc","pot","prototype","mvp","pilot","continuous improvement","product lifecycle","product management","agile development","design thinking"],"description":"Learn about the stages of product development PoC, PoT, Prototype, MVP, and Pilot, and how to progress through the lifecycle gates.","toc_min_heading_level":2,"toc_max_heading_level":5},"unlisted":false,"prevItem":{"title":"PowerShell based Terraform Bootstrap Script","permalink":"/azure/powershell-terraform-bootstrap"},"nextItem":{"title":"Pre-commit Hooks in GitHub Codespaces for Terraform IaC","permalink":"/misc/precommit-hooks-codespaces-terraform-iac"}},"content":"PoC, PoT, Prototype, MVP, Pilot - what do they all mean? What is the difference between them, and when should you use them?\\n\\n# \ud83d\ude80 Introduction\\n\\nToday, we are going to take a look at a technical Cloud Product Development lifecycle - in my eyes, understanding these terminologies is key to determining:\\n\\n* What kind of resources _(i.e., both technological and human)_ do you need to dedicate to a product? \\n* Where should you align lifecycle management with your application or service?\\n* How likely is it that the the product makes a good market fit or solves an issue?\\n* Keep your product lean to avoid wasting the wrong resources at the wrong time\\n* Shift to a product-based mindset\\n* Focus on innovation and early feedback loops\\n\\nAnd, of course - what to expect when people start using these words, often thrown together in a conversation and sometimes even put in writing in Statement of Works and Requests for Proposals.\\n\\n{/* truncate */}\\n\\n# \ud83c\udfa8 Design Thinking and Agile\\n\\nUsing Design Thinking - _(Empathize, Define, Ideate, Prototype, Test, Implement)_ as a base - a Product needs to align with actual customer needs; it\'s an iterative process, as a Product or service itself isn\'t static. We live in a VUCA (Volatile, Uncertain, Complex, Ambiguous) world, where customers expect more from their technology _(as they should)_ to help empower their own needs securely and reliably.\\n\\nAgile is about the speed to adapt, not the velocity to deliver, so if we take in Agile delivery and design thinking methodologies, it is no surprise that we need to break the product lifecycle into specific stages to get faster, more immediate feedback and make sure we are concentrating on the right success metrics for our product at the right time.\\n\\n![Product Development Lifecycle](images/ProductDevelopmentLifecycle.png)\\n\\n# \ud83d\udee0\ufe0f Stages of Product Development Lifecycle\\n\\nIf we break down these stages, we have:\\n\\n| **Stage**             | **Description**                                                                                                           | **Benefits**                                                                                                  | **Acceptance Criteria**                                                                                                                                                       | **Resources Needed**                                                                                   | **Where Used**                                                                                           |\\n|------------------------|---------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\\n| Ideate                | Generate and prioritize potential solutions for the identified problem based on insights and feasibility.                  | Fosters creativity and provides a clear roadmap for experimentation and development.                         | Clear problem definition, prioritized ideas based on feasibility, impact, and alignment with user needs.                                   | Innovation workshops, user interviews, brainstorming tools, and cross-functional collaboration.          | At the beginning of any product development or improvement process.                                      |\\n| PoT (Proof of Technology) | Validate the technical feasibility of a specific technology or approach without focusing on user needs or business value. | Establishes whether the technology is feasible and provides initial direction for further exploration.      | Demonstrates core technical capabilities and resolves major uncertainties regarding technology.                                            | Technical experts, test environments, access to research, and innovation workshops.                     | When exploring unproven or novel technologies before deeper investment in development.                  |\\n| PoC (Proof of Concept) | Proves that the proposed solution can solve a specific problem, aligning technology with business and user needs.         | Validates alignment of the technology with the desired outcomes and de-risks the solution.                   | Demonstrates that the solution addresses the problem effectively under controlled conditions; stakeholder approval is achieved.          | Business analysts, developers, user research teams, and small-scale testing environments.               | Before seeking stakeholder buy-in or funding for further development.                                   |\\n| MVP (Minimum Viable Product) | The product\'s first version with enough functionality to address core user needs and gather feedback.            | Accelerates market entry, reduces time-to-value, and collects feedback to prioritize future improvements.    | Achieves user adoption and gathers user engagement, satisfaction, and performance metrics.                                             | Development teams, product managers, QA testers, analytics tools, and customer success teams.            | To test market fit, validate assumptions, and inform scaling decisions.                                 |\\n| Pilot                  | A limited-scale deployment in a controlled environment to test end-to-end viability and gather comprehensive feedback.   | Identifies risks, operational issues, and unforeseen challenges before scaling to production.                | Demonstrates success in a real-world environment; collects data to refine for production readiness.                                        | End users, operational teams, real-world testing environments, customer support, and analytics tools.    | When preparing for scaling and addressing operational risks.                                             |\\n| Production             | The full-scale product deployment with all necessary features, reliability, and support for end users.           | Delivers the finalized solution to users while ensuring scalability, reliability, and operational efficiency. | Meets key performance indicators (KPIs), user satisfaction benchmarks, and operational SLAs (Service Level Agreements).                   | Development, operations, customer support, monitoring tools, and continuous improvement processes.        | To deliver the product to users at scale with ongoing support and updates.                          |\\n| Continuous Improvement | Ongoing enhancement and optimization of the product based on user feedback, performance metrics, and market needs.       | Ensures the product remains competitive, relevant, and user-friendly over time.                              | Demonstrates improvement in key metrics (e.g., adoption, satisfaction, performance); regular delivery of enhancements with minimal disruptions. | DevOps teams, user research, monitoring tools, and agile sprint cycles for regular updates.              | Post-production phase to address evolving requirements and maintain product excellence.                  |\\n| Sunset                 | The planned decommissioning or replacement of the product when it no longer provides value or becomes obsolete.          | Frees up resources for other priorities while ensuring users transition smoothly to alternatives.             | Completion of decommissioning plan, including user migration, data archiving, and system shutdown with minimal disruption.                 | Project managers, operations teams, user communication channels, and migration tools.                    | When the product reaches end-of-life, either due to replacement or diminished relevance.                 |\\n\\n# \ud83d\udccb Detailed Stage Descriptions\\n\\n* **Ideate**\\nGenerates and prioritizes ideas, ensuring a user-centered approach from the outset. Benefits include a clear focus on feasible and impactful solutions.\\n\\n* **PoT (Proof of Technology)**\\nFocused on assessing the technical feasibility of a solution, ensuring it is viable for solving the identified problem. Benefits include reducing technical risk early in the process and clarifying technological possibilities.\\n\\n* **PoC (Proof of Concept)**\\nAligns technical feasibility with business objectives, showing stakeholders how the solution addresses the problem. Benefits include stakeholder buy-in and a clearer understanding of the project\u2019s potential impact.\\n\\n* **MVP (Minimum Viable Product)**\\nA functional product with core features deployed to early adopters for real-world usage and feedback. Prioritizes speed-to-market and core functionality. Benefits include reduced time-to-market and insights into user needs for prioritizing future features.\\n\\n* **Pilot**\\nA controlled, real-world deployment of the MVP to test the product\u2019s operational viability in a controlled, real-world environment, focusing on end-to-end testing. Benefits include reduced risks and a more straightforward path to production readiness.\\n\\n* **Production**\\nIn the final stage, the product is fully deployed, supported, and scaled for end users. Benefits include delivering business value, achieving scalability, and operational efficiency.\\n\\n* **Continuous Improvement**\\nEnsures the product evolves based on user feedback and performance metrics. Benefits include staying competitive and maintaining user satisfaction through regular updates.\\n\\n* **Sunset**\\nA planned retirement phase that ensures a smooth transition for users while freeing up resources for other priorities. Benefits include minimizing disruptions and maintaining organizational focus.\\n\\n# \u2753 Key Questions for Each Stage\\n\\nOne of the key reasons to break down the Product Lifecycle into these stages is to focus on the right needs of your product or service at the right times; examples of questions you should be asking at each phase are:\\n\\n| **Stage**             | **Key Questions**                                                                                                         |\\n|------------------------|--------------------------------------------------------------------------------------------------------------------------|\\n| PoT (Proof of Technology) | - What unknowns about the technology must be validated?  \\\\n- Are the tools or platforms feasible for this use case?  \\\\n- What are the technical risks, and how can they be mitigated?  \\\\n- Is this the best technology stack for long-term scalability? |\\n| PoC (Proof of Concept) | - Does the solution align with business goals?  \\\\n- Can it solve the problem in a controlled, small-scale test?  \\\\n- What are the critical assumptions being validated?  \\\\n- How will stakeholders measure success?  \\\\n- What constraints or dependencies could limit progress? |\\n| Prototype              | - Does the design meet usability and functionality needs?  \\\\n- What feedback can be gathered on the proposed solution?  \\\\n- Are there any technical barriers to implementing the design?  \\\\n- How well does the prototype align with user personas or scenarios? |\\n| MVP (Minimum Viable Product) | - What core functionality is essential to meet user needs?  \\\\n- Can this be delivered with minimal effort for testing adoption?  \\\\n- How will user feedback be collected and prioritized?  \\\\n- Are the team and infrastructure ready to iterate quickly based on feedback? |\\n| Pilot                  | - What operational or real-world risks must be addressed before scaling?  \\\\n- How will success in the pilot phase be measured?  \\\\n- Are there additional training or support requirements for users?  \\\\n- How will pilot learnings be incorporated into scaling plans? |\\n| Production             | - Is the solution robust and scalable?  \\\\n- Are operational processes in place for monitoring, support, and continuous improvement?  \\\\n- What KPIs or metrics indicate product success?  \\\\n- Are end users satisfied with performance and usability?  \\\\n- How will updates or fixes be managed post-launch? |\\n| Continuous Development | - What feedback from users or metrics indicates areas for improvement?  \\\\n- How can automation enhance development or deployment processes?  \\\\n- Are there new features or updates that align with market trends?  \\\\n- How can emerging technologies be leveraged to improve the product?  \\\\n- Is there a roadmap for ongoing enhancements? |\\n| Sunset                 | - What is the timeline and plan for decommissioning?  \\\\n- How will users transition to alternatives?  \\\\n- How will data and systems be archived or disposed of?  \\\\n- Are there plans to communicate changes effectively to all stakeholders?  \\\\n- What metrics indicate it is the right time to sunset the product? |\\n\\n# \ud83d\udeaa Product Lifecycle Gates\\n\\nSo let us dig a bit deeper into some of the gates to consider when shifting a product through this lifecycle before you reach the next stage _(for example, from MVP to Pilot)_:\\n\\n![Product Development Lifecycle Gates](images/ProductDevelopmentLifecycleGates.png)\\n\\n1. **Problem Definition and Technical Feasibility**\\n(Helps determine if you\'re at the PoT or PoC stage)\\n\\n* What is the primary problem this application/workload/service is trying to solve?\\n* Do we have enough technical understanding to validate whether this can be built?\\n* Are there specific technical risks or uncertainties that need validation before proceeding further?\\n* Is there an existing technology or approach that can achieve the desired outcome, or does this require exploration?\\n* Are stakeholders aligned on the goals of the solution at this stage?\\n\\n2. **User and Business Alignment**\\n(Determines if you\'re ready for PoC, Prototype, or MVP stages)\\n\\n* Who are the target users or audiences for this application/workload/service?\\n* What are the critical business outcomes or objectives tied to this solution?\\n* Do we have user personas or initial feedback to validate assumptions?\\n* Is there clarity on how this solution will fit into existing business processes or workflows?\\n* Are there measurable KPIs or success criteria defined for the next stage?\\n\\n3. **Functionality and Design**\\n(Helps decide if Prototype or MVP is needed)\\n\\n* Do we have clear requirements and design principles for the application?\\n* What specific features or functionality are critical to test user engagement?\\n* Is the goal to explore design and usability (Prototype) or to deliver something functional for user adoption (MVP)?\\n* How much feedback or iteration is needed to validate user needs or business value?\\n* Are there competing priorities (e.g., time-to-market vs. feature completeness) that influence this stage?\\n\\n4. **Real-World Validation**\\n(Clarifies readiness for Pilot stage)\\n\\n* Has the application/workload/service undergone sufficient technical, functional, and usability testing?\\n* Are there operational risks or challenges that require validation in a controlled environment?\\n* Do we have a defined test group or environment to deploy the solution and gather feedback?\\n* What are the success metrics for the pilot phase (e.g., adoption rate, performance, operational readiness)?\\n* What\u2019s the contingency plan if the pilot reveals significant issues?\\n\\n5. **Scalability and Operational Readiness**\\n(Determines readiness for Production)\\n\\n* Has the solution demonstrated scalability under expected workloads?\\n* Are there any technical, security, or compliance gaps preventing full-scale deployment?\\n* Is the team equipped with monitoring, support, and incident response processes?\\n* What are the ongoing maintenance and operational requirements?\\n* Is there a plan for onboarding users or customers at scale?\\n\\n6. **Post-Launch Evolution and Sunset Planning**\\n(Addresses long-term product lifecycle considerations)\\n\\n* What is the plan for iterating and improving the product post-launch?\\n* How will we track performance, gather feedback, and prioritize enhancements?\\n* Is there a decommissioning or migration plan in case of obsolescence or replacement?\\n* What\u2019s the budget and resource allocation for ongoing updates?\\n* What external factors (e.g., market changes and technology updates) could impact the solution\'s longevity?\\n\\n# \ud83d\udcda Scenarios\\n\\nLet\'s look at two scenarios to see how these stages can be applied. Scenario #1 is the development of an Azure Landing Zone _(yes, it doesn\'t have to be specifically a product - this is a way of thinking, more of a guideline/framework than a rule)_ and Scenario #2 is the creation of an eCommerce website.\\n\\nHere is the updated table with the sunset row added at the bottom:\\n\\n| **Gate**                  | **Scenario #1. Azure Landing Zone Implementation**                                                                                   | **Scenario #2. E-Commerce Website**                                                                                       |\\n|---------------------------|------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\\n| Gate 1: **Technical Feasibility Validated** | - All resource types required for the landing zone deployment are provisioned successfully using IaC (Infrastructure as Code).  \\\\n- A network topology supporting hybrid cloud connectivity is deployed and passes end-to-end connectivity tests. | - The website backend can retrieve product data from a database within 50ms under test conditions.  \\\\n- A sandbox environment successfully processes simulated payments through the payment gateway. |\\n| Gate 2: **Stakeholder Approval**             | - Stakeholders review and sign off governance policies for tagging, cost management, and resource compliance.  \\\\n- Cloud adoption teams approve the initial subscription and resource group hierarchy structure.            | - Stakeholders approve a product backlog containing prioritized features for the MVP based on user stories.  \\\\n- Design mockups of key pages (e.g., homepage, product page, checkout page) are reviewed and signed off by business stakeholders. |\\n| Gate 3: **MVP Adoption Metrics Met**         | - The MVP landing zone enables three application teams to deploy workloads without manual intervention.  \\\\n- Monitoring and logging solutions for the MVP environment are active and provide actionable insights within 5 minutes of an incident. | - The MVP site achieves at least 100 daily active users within 2 weeks of launch.  \\\\n- Basic checkout functionality works end-to-end, including order placement and confirmation email delivery. |\\n| Gate 4: **Pilot Success Criteria Achieved**  | - The pilot environment supports the deployment of workloads across all required regions with no SLA violations over 4 weeks.  \\\\n- All critical governance, security, and operational policies pass compliance testing in the pilot.             | - During a limited launch, 1,000 transactions are processed with less than 1% failure rate.  \\\\n- Pilot users provide a Net Promoter Score (NPS) of 7 or higher for their experience on the site. |\\n| Gate 5: **Go-Live Decision**                 | - All key stakeholders approve readiness for production based on agreed KPIs (e.g., uptime, cost optimization, and security).  \\\\n- Documentation for deployment, governance, and user onboarding is complete and validated by end users.         | - The e-commerce platform handles a load test of 20,000 concurrent users with a 99.9% success rate in order processing.  \\\\n- Critical user journeys (search, add to cart, checkout) are tested with zero critical defects reported in production-readiness testing. |\\n| Gate 6:**Sunset**                         | - All resources associated with the landing zone are decommissioned safely and cost-effectively, with no unexpected impact on remaining environments.  \\\\n- Final cost and usage reports for the project are reviewed and shared with stakeholders. | - All user data is successfully migrated or anonymized according to privacy policies.  \\\\n- All legal and financial obligations are met for the e-commerce platform, and any necessary shutdown or scaling activities are completed without affecting the remaining infrastructure. |\\n\\n# \ud83c\udfc1 Conclusion\\n\\nHopefully, this article gives you more clarity about the product design lifecycle, how to apply it to your own projects, and how to use it to your advantage to make sure you are focusing on the right things at the right time and not wasting resources on the wrong things."},{"id":"misc/precommit-hooks-codespaces-terraform-iac","metadata":{"permalink":"/misc/precommit-hooks-codespaces-terraform-iac","source":"@site/blog/2025-01-07-precommit-codespace/index.mdx","title":"Pre-commit Hooks in GitHub Codespaces for Terraform IaC","description":"Learn how to set up pre-commit hooks in GitHub Codespaces for Terraform Infrastructure as Code (IaC) development.","date":"2025-01-07T08:06:54.566Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.265,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Pre-commit Hooks in GitHub Codespaces for Terraform IaC","metaDescription":"Learn how to set up pre-commit hooks in GitHub Codespaces for Terraform Infrastructure as Code (IaC) development.","date":"2025-01-07T08:06:54.566Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"misc/precommit-hooks-codespaces-terraform-iac","keywords":["azure","codespace","github","pre-commit","terraform","infrastructure as code"],"description":"Learn how to set up pre-commit hooks in GitHub Codespaces for Terraform Infrastructure as Code (IaC) development."},"unlisted":false,"prevItem":{"title":"Product Development lifecycle","permalink":"/misc/product-development-lifecycle"},"nextItem":{"title":"Deploying Azure Managed Redis with Bicep","permalink":"/azure/deploying-azure-managed-redis-with-bicep"}},"content":"Pre-commit hooks are automated scripts or checks running before developers commit code to a repository. They are particularly useful in the context of infrastructure, such as code _(IaC)_ development, when working with Terraform. \\n\\nThese hooks serve as an early defense against common issues, ensuring code quality and consistency before changes are committed to version control.\\n\\nToday, we are going to discuss precommit with GitHub Codespaces.\\n\\n{/* truncate */}\\n\\n:::info\\n[Pre-commit hooks](https://pre-commit.com/) offer several advantages in the IaC development lifecycle:\\n\\n* Early Error Detection: They catch syntax errors, formatting issues, and potential security vulnerabilities before code is committed.\\n* Consistency: Hooks enforces coding standards and best practices across the development team.\\n* Time-saving: By automating routine checks, pre-commit hooks reduce the need for manual review and minimize the likelihood of failed CI/CD pipelines.\\n* Security: They can identify potential security risks in infrastructure code early in the development process.\\n\\n![Pre-commit hooks](images/pre-commit-hook-flow.png)\\n\\n_(Image from [AKS DevSecOps Workshop](https://azure.github.io/AKS-DevSecOps-Workshop/))_\\n\\nPre-commits work well in the Inner Loop of the Developer/Cloud Engineer workflow _(for the duration of this article when I use the word developer, I am meaning anyone who writes IaC code)_. They can be lightweight and run quickly, providing immediate feedback to developers as they work on their code. This feedback loop helps developers address issues early, leading to higher-quality code and faster development cycles, before the code hits any peer review or remote code source system (such as DevOps or GitHub repositories)_.\\n\\n![Inner Loop of Developer Workflow](images/devinnerouterloop.png)\\n:::\\n\\n:::warning\\nWhile pre-commit hooks can be valuable tools for maintaining code quality, there are several concerns and potential drawbacks to be aware of when implementing them. The use of GitHub Codespaces can help with some of the drawbacks, especially around consistency and setup, however consider the tradeoffs of what pre-commit hooks you want and how they may impact your development workflow.\\n\\nSome potential drawbacks include:\\n\\n* Pre-commit hooks can significantly slow down the development process, especially if they run extensive checks or tests\\n* Frequent, time-consuming checks can break a developer\'s focus and impede productivity\\n* Overly restrictive pre-commit hooks may be seen as a lack of trust in developers\' abilities and judgment\\n* Strict pre-commit checks might discourage developers from experimenting or pushing work-in-progress code to their branches\\n* Developers might be tempted to bypass hooks using git commit flags, defeating the purpose of the checks\\n\\nThere\'s a delicate balance between comprehensive checks and maintaining a fast, frictionless commit process. Not all checks need to be in pre-commit hooks; some can be deferred to CI/CD pipelines or pre-push hooks, and because of the individual nature of having the pre-commit setup for each developer and the developer able to force the commit _(ignoring the pre-commit checks)_ it is important to have a good culture around the use of pre-commit hooks, and any important checks need to be implemented in a CI/CD pipeline, for consistency.\\n:::\\n\\nSo, let us get started with setting up pre-commit hooks in GitHub Codespaces. The same premise can also be applied outside of a Codespace, but for this article, we will focus on a Codespace.\\n\\nThe guts of this is the [pre-commit framework](https://pre-commit.com/), a Python package that manages pre-commit hooks for you. It is a framework for managing and maintaining multi-language pre-commit hooks. The pre-commit hooks are run against the files that are staged for commit, and if any of the hooks fail, the commit is rejected.\\n\\n> Git hook scripts are useful for identifying simple issues before submission to code review. We run our hooks on every commit to automatically point out issues in code, such as missing semicolons, trailing whitespace, and debug statements. Pointing these issues out before code review allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks.\\n\\nFor the Codespace, we will add the pre-commit framework to the Codespace, then add a script to initialize the pre-commit hooks, then configure the pre-commit hooks to run the checks we want.\\n\\nLet\'s add the pre-commit framework to the Codespace devcontainer.json.\\n\\n```json\\n\\"features\\": {\\n //Add pre-commit\\n    \\"ghcr.io/prulloac/devcontainer-features/pre-commit:1.0.3\\": {},\\n    },\\n```\\n\\nOnce added, we need to add a postCreateCommand to initialize the pre-commit framework, like so:\\n\\n```json\\n\\"postCreateCommand\\": \\"pre-commit install\\",\\n```\\n\\nThis will run after the GitHub Codespace has been created and running.\\n\\nSo, an example for our Terraform IaC would look like this:\\n\\n```json title=\\".devcontainer/devcontainer.json\\"\\n  // \\"features\\" section allows adding and configuring predefined features or tools in the development container.\\n  \\"features\\": {\\n    // Adds and configures the Azure CLI with Bicep and Python installation options.\\n    \\"ghcr.io/devcontainers/features/azure-cli:latest\\": {\\n      \\"installBicep\\": false, //  Bicep installation.\\n      \\"installUsingPython\\": true, // Installs using Python.\\n      \\"version\\": \\"latest\\" // Specifies the version of the Azure CLI to install.\\n    },\\n    // Adds PowerShell to the container.\\n    \\"ghcr.io/devcontainers/features/powershell:latest\\": {\\n      \\"version\\": \\"latest\\" // Specifies the version of PowerShell to install.\\n    },\\n     //Add pre-commit\\n    \\"ghcr.io/prulloac/devcontainer-features/pre-commit:1.0.3\\": {},\\n    // Adds and configures Terraform with specific version, TFLint, and Terragrunt.\\n    \\"ghcr.io/devcontainers/features/terraform:1\\": {\\n      \\"version\\": \\"latest\\", // Specifies the version of Terraform to install.\\n      \\"tflint\\": \\"latest\\", // Specifies the version of TFLint to install.\\n      \\"terragrunt\\": \\"latest\\" // Specifies the version of Terragrunt to install.\\n    }\\n  },\\n    \\"postCreateCommand\\": \\"pre-commit install\\",\\n  ```\\n\\n  :::tip\\n  You can view a version of the devcontainer.json file [here](https://github.com/lukemurraynz/Codespace_IaC_Coding/blob/main/.devcontainer/devcontainer.json). My [lmurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding) repository has a devcontainer.json file that has the pre-commit framework added in. The intention is that you can take this repository as a template and then modify and remove it as you need to.\\n  ::: \\n\\n  Once the Codespace has been created, you can then add a .pre-commit-config.yaml file to the root of your repository. This file will contain the configuration for the pre-commit hooks you want to run.\\n\\n  An example of a .pre-commit-config.yaml file for Terraform would look like this:\\n\\n  ```yaml title=\\".pre-commit-config.yaml\\"\\n  default_stages: [pre-commit]\\nrepos:\\n  - repo: https://github.com/pre-commit/pre-commit-hooks\\n    rev: v5.0.0\\n    hooks:\\n    - id: trailing-whitespace\\n    - id: end-of-file-fixer\\n    - id: check-yaml\\n    - id: check-added-large-files\\n    - id: check-case-conflict\\n    - id: check-merge-conflict\\n  - repo: https://github.com/antonbabenko/pre-commit-terraform\\n    rev: v1.96.3\\n    hooks:\\n      - id: terraform_fmt\\n        args:\\n        - --args=-recursive\\n        exclude: \'.*/\\\\.terragrunt-cache\'\\n      - id: terraform_validate\\n      - id: terraform_tflint\\n      - id: terraform_trivy\\n      - id: terraform_docs\\n        args:\\n         - --hook-config=--add-to-existing-file=true     # Boolean. true or false\\n         - --hook-config=--create-file-if-not-exist=true # Boolean. true or false\\n         - --args=--output-mode=replace\\n      - id: tfupdate\\n        name: Autoupdate Terraform versions\\n        args:\\n         - --args=provider azurerm # Will be pined to latest version\\n  - repo: https://github.com/bridgecrewio/checkov.git\\n    rev: \'3.2.346\' # change to tag or sha\\n    hooks:\\n    - id: checkov\\n      verbose: true\\n      args: [\'--quiet\', \'--compact\', \'--soft-fail\']\\n  ```\\n\\nHere\'s a breakdown of what each part does:\\n\\n1. **`default_stages: [pre-commit]`**: Specifies that the hooks should run at the `pre-commit` stage.\\n\\n2. **`repos`**: Lists repositories containing pre-commit hooks to be used.\\n\\n3. **First repository** (`https://github.com/pre-commit/pre-commit-hooks`):\\n   - **`rev: v5.0.0`**: Specifies the version of the repository.\\n   - **`hooks`**: Lists the hooks to be used from this repository:\\n     - `trailing-whitespace`: Removes trailing whitespace.\\n     - `end-of-file-fixer`: Ensures files end with a newline.\\n     - `check-yaml`: Checks YAML files for syntax errors.\\n     - `check-added-large-files`: Prevents large files from being added.\\n     - `check-case-conflict`: Checks for case conflicts in filenames.\\n     - `check-merge-conflict`: Checks for unresolved merge conflicts.\\n\\n4. **Second repository** (`https://github.com/antonbabenko/pre-commit-terraform`):\\n   - **`rev: v1.96.3`**: Specifies the version of the repository.\\n   - **`hooks`**: Lists the hooks to be used from this repository:\\n     - `terraform_fmt`: Formats Terraform files.\\n       - `args`: Specifies arguments for the hook.\\n       - `exclude`: Excludes certain files from the hook.\\n     - `terraform_validate`: Validates Terraform files.\\n     - `terraform_tflint`: Lints Terraform files.\\n     - `terraform_trivy`: Scans Terraform files for vulnerabilities.\\n     - `terraform_docs`: Generates documentation for Terraform files.\\n       - `args`: Specifies arguments for the hook.\\n     - `tfupdate`: Updates Terraform provider versions.\\n       - `name`: Provides a name for the hook.\\n       - `args`: Specifies arguments for the hook.\\n\\n5. **Third repository** (`https://github.com/bridgecrewio/checkov.git`):\\n   - **`rev: \'3.2.346\'`**: Specifies the version of the repository.\\n   - **`hooks`**: Lists the hooks to be used from this repository:\\n     - `checkov`: Runs Checkov to scan for security issues in infrastructure as code.\\n       - `verbose`: Enables verbose output.\\n       - `args`: Specifies arguments for the hook.\\n\\nThis configuration ensures that various checks and fixes are automatically applied to the codebase before commits are made, helping to maintain code quality and security.\\n\\nSo, let\'s spin up our Codespace and see the pre-commit hooks in action.\\n\\nThe first time you run this, it will take a while as it will download the pre-commit hooks and install them. Be mindful of this if you are running this in a new Codespace environment.\\n\\n![Pre-commit install](images/Precommit_Codespace_Terraform_Run.gif)\\n\\nAs you can see from the pre-commit output, I am missing some dependencies installed, i.e., Terraform docs, but we can see Terraform format ran, the trailing whitespace check ran and fixed the issue, and tflint ran and came back with some issues, such as missing provider versions, and checkov ran a security check, which came back with some failed checks, such as Public access enabled and latest TLS version not set.\\n\\nHopefully, you can see the power of the pre-commit, and as these failed, nothing was committed to the repository, so I can fix these issues before I commit the code.\\n\\n:::info\\nIf needed, we can skip the pre-commit checks and force-push the changes through with the following command:\\n\\ngit push --no-verify\\n\\nThis will bypass the pre-commit checks and force the commit through. There is a [Visual Studio Code extension - Git push(no verify)\\n](https://marketplace.visualstudio.com/items?itemName=DevEscalus.git-push-no-verify) as well, which adds a noverify button to the commit dialog, so you can bypass the pre-commit checks if needed.\\n:::\\n\\nHopefully, this will give you a great starting block for adding pre-commit hooks to your Codespace and workflow and help with secure and reliable IaC development. You can find a list of common prehooks here: [featured hooks\\n](https://pre-commit.com/hooks.html), and if your game - you can create your own and find other hooks people have created - such as this Azure Bicep [Azure4DevOps/check-azure-bicep](https://github.com/Azure4DevOps/check-azure-bicep) one.\\n\\n:::tip\\nYou can run  `pre-commit autoupdate` to update all the pre-commit hook versions. For example, when I ran it, I got:\\n\\n* [https://github.com/bridgecrewio/checkov.git] updating 3.2.346 -> 3.2.350\\n\\nWhich updated my `.pre-commit-config.yaml` file with the latest version without having to go and check each one manually.\\n:::"},{"id":"azure/deploying-azure-managed-redis-with-bicep","metadata":{"permalink":"/azure/deploying-azure-managed-redis-with-bicep","source":"@site/blog/2025-01-02-azuremanagedrediscachebicep/index.mdx","title":"Deploying Azure Managed Redis with Bicep","description":"Learn how to deploy Azure Managed Redis using Bicep with code examples.","date":"2025-01-02T08:45:51.802Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.92,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deploying Azure Managed Redis with Bicep","metaDescription":"Learn how to deploy Azure Managed Redis using Bicep with code examples.","date":"2025-01-02T08:45:51.802Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/deploying-azure-managed-redis-with-bicep","keywords":["azure","Bicep","redis","ManagedRedis","cloud","iac","deployment","tutorial","caching"],"description":"Learn how to deploy Azure Managed Redis using Bicep with code examples."},"unlisted":false,"prevItem":{"title":"Pre-commit Hooks in GitHub Codespaces for Terraform IaC","permalink":"/misc/precommit-hooks-codespaces-terraform-iac"},"nextItem":{"title":"Using Azure Communication Services and UMI PowerShell to Send Emails","permalink":"/azure/using-communication-services-and-umi-powershell-to-send-emails"}},"content":"[Azure Managed Redis](https://learn.microsoft.com/azure/azure-cache-for-redis/managed-redis/managed-redis-overview?WT.mc_id=AZ-MVP-5004796) announced during [Microsoft Ignite 2024](https://techcommunity.microsoft.com/blog/AppsonAzureBlog/introducing-azure-managed-redis-cost-effective-caching-for-your-ai-apps/4299104?WT.mc_id=AZ-MVP-5004796) in Public Preview. \\n\\nThis is a new, fully-managed, first-party Redis offering in Microsoft Azure. Azure Managed Redis is available today in public preview. Microsoft Azure is the first major cloud service provider to offer customers a licensed, multi-tiered Redis service.\\n\\nThis article will show you how to deploy an Azure Managed Redis instance using [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796). Bicep is a Domain Specific Language (DSL) for deploying Azure resources declaratively. It aims to drastically simplify the authoring experience with a cleaner syntax and better support for modularity and code reuse.\\n\\n{/* truncate */}\\n\\n:::info\\nEvery customer that migrates from any Azure Cache for Redis tier to Azure Managed Redis will gain access to years of Redis innovation. This builds on our existing integration with Microsoft Azure on the Enterprise and Enterprise Flash tiers for Azure Cache for Redis. With the introduction of Azure Managed Redis, users on Azure Cache for Redis can now access features that were previously found only on the Azure Cache for Redis Enterprise and Enterprise Flash tiers.\\n\\nAzure Managed Redis delivers on the best scale and SLA in the industry, building on the advancements first introduced by the Azure Cache for the Redis Enterprise tier. Users can now experience up to 99.999% availability when leveraging multi-region Active-Active, the highest availability offered in the market, powered by CRDT. Apps built with Azure Managed Redis can deliver down to sub-millisecond local latency to users globally and simultaneously, no matter which continent or region they\u2019re located on.\\n:::\\n\\n\\nIn this example, I have deployed Azure Managed Redis using Bicep, with bonus code snippets for connecting [API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) to the [Azure Managed Redis](https://learn.microsoft.com/azure/azure-cache-for-redis/managed-redis/managed-redis-overview?WT.mc_id=AZ-MVP-5004796). \\n\\nThe Bicep file creates a new Azure Managed Redis instance.\\n\\nRedis Enterprise Cache Setup\\n\\n* Resource Type: Microsoft.Cache/redisEnterprise\\n* API Version: 2024-09-01-preview\\n* Name: redisEnterprise\\n* Location: Uses a variable location\\n* Tags: Uses a variable tags\\n* SKU: Specifies the SKU as MemoryOptimized_M10\\n* Properties: Sets the minimum TLS version to 1.2\\n\\n```bicep\\n// Redis Enterprise Cache setup\\nresource redisEnterprise \'Microsoft.Cache/redisEnterprise@2024-09-01-preview\' = {\\n  name: \'redisEnterprise\'\\n  location: location\\n  tags: tags\\n  sku: {\\n    name: \'MemoryOptimized_M10\'\\n  }\\n  properties: {\\n    minimumTlsVersion: \'1.2\'\\n  }\\n}\\n\\noutput redisEnterpriseName string = redisEnterprise.name\\n```\\n\\nThis part of the Bicep file sets up a Redis Enterprise Cache instance with specific configurations such as location, tags, SKU, and minimum TLS version.\\n\\nRedis Cache Database Setup\\n\\n* Resource Type: Microsoft.Cache/redisEnterprise/databases\\n* API Version: 2024-09-01-preview\\n* Name: default\\n* Parent: Links to the redisEnterprise resource defined earlier\\nProperties:\\n* Access Keys Authentication: Disabled\\n* Eviction Policy: No eviction\\n* Clustering Policy: Enterprise cluster\\n* Modules: Includes the RediSearch module\\n* Port: Sets the port to 10000\\n\\n```bicep\\nresource redisCache \'Microsoft.Cache/redisEnterprise/databases@2024-09-01-preview\' = {\\n  name: \'default\'\\n  parent: redisEnterprise\\n  properties: {\\n    accessKeysAuthentication: \'Disabled\'\\n    evictionPolicy: \'NoEviction\'\\n    clusteringPolicy: \'EnterpriseCluster\'\\n    modules: [\\n      {\\n        name: \'RediSearch\'\\n      }\\n    ]\\n    port: 10000\\n  }\\n}\\n```\\n\\nThis part sets up a Redis Cache Database within the Redis Enterprise Cache instance. It configures properties such as access key authentication, eviction policy, clustering policy, modules, and port.\\n\\nYou can also link API Management to the Azure Managed Redis resource as a bonus. This is a small snippet of the Connection string for API Management:\\n\\n```bicep\\nredisCacheConnectionString: \'${redisEnterprise.properties.hostName}:${10000},ssl=True,abortConnect=False\'\\n```\\n\\nThis is using Entra ID authentication to connect to the Redis Cache using the following role assignment to the API System Managed Identity:\\n\\n```bicep\\n// Redis Access Policy Assignment\\nresource redisAccessPolicyAssignmentName \'Microsoft.Cache/redisEnterprise/databases/accessPolicyAssignments@2024-09-01-preview\' = {\\n  name: take(\'cachecontributor${uniqueString(resourceGroup().id)}\', 24)\\n  parent: redisCache\\n  properties: {\\n    accessPolicyName: \'default\'\\n    user: {\\n      objectId: apim.outputs.apimPrincipalId\\n    }\\n  }\\n}\\n```"},{"id":"azure/using-communication-services-and-umi-powershell-to-send-emails","metadata":{"permalink":"/azure/using-communication-services-and-umi-powershell-to-send-emails","source":"@site/blog/2024-12-30-azemailcommumi/index.mdx","title":"Using Azure Communication Services and UMI PowerShell to Send Emails","description":"Learn how to use Azure Communication Services and PowerShell to send emails using a User Assigned Managed Identity.","date":"2024-12-29T18:52:25.292Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.61,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Using Azure Communication Services and UMI PowerShell to Send Emails","metaDescription":"Learn how to use Azure Communication Services and PowerShell to send emails using a User Assigned Managed Identity.","date":"2024-12-29T18:52:25.292Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/using-communication-services-and-umi-powershell-to-send-emails","keywords":["azure","communication services","PowerShell","email","script","User Managed Identity","authentication"],"description":"Learn how to use Azure Communication Services and PowerShell to send emails using a User Assigned Managed Identity."},"unlisted":false,"prevItem":{"title":"Deploying Azure Managed Redis with Bicep","permalink":"/azure/deploying-azure-managed-redis-with-bicep"},"nextItem":{"title":"Authenticating Azure OpenAI with Managed Identity","permalink":"/azure/authenticating-azureopenai-python"}},"content":"[Azure Communication Services](https://azure.microsoft.com/products/communication-services?WT.mc_id=AZ-MVP-5004796) brings rich communication APIs to all of your apps across any device on any platform, using the same reliable and secure infrastructure that powers Microsoft Teams.\\n\\nToday, we will explore using Email as part of Azure Communication Services, using the REST API and PowerShell to send an email using a User Assigned Managed Identity.\\n\\n![Azure Communication Services](images/BlogHeading_DeployandTestACS.gif)\\n\\n{/* truncate */}\\n\\n:::info\\nI did a previous article: [Deploying and Testing Azure Email Communication Services](https://luke.geek.nz/azure/using-communication-services-and-powershell-to-send-emails/) on this, however the authentication was using an Azure Service Principal. A reader reached out to me about using a User Assigned Managed identity, so I have tested and updated the script to use a User Assigned Managed Identity.\\n:::\\n\\nIn this example, I will access a token from [Azure Communication Services](https://azure.microsoft.com/products/communication-services?WT.mc_id=AZ-MVP-5004796). I will make a GET request to the identity endpoint of Azure Communication Services, using the oauth identity from the user assigned-managed identity. This will return a token we can use to authenticate against the REST API.\\n\\nYou will need the ClientId of the User Assigned Managed Identity, and that ClientId needs to have Contributor rights to the Azure Communication Service _(not Email or Domain service)_.\\n\\n![User Assigned Managed Identity](images/UIMClientID.png)\\n\\nHere\'s a step-by-step explanation of the script:\\n\\nThe script accepts several parameters and constructs an email message in HTML format. It then authenticates using Azure Managed Identity to obtain an access token, which is used to authorize the email sending request to Azure Communication Services.\\n\\nThe script defines the following parameters:\\n\\n* ClientId: The Client ID for User Assigned Managed Identity (optional).\\n* EmailRecipient: The recipient\'s email address.\\n* SenderAddress: The sender\'s email address.\\n* ResourceID: The resource ID for Azure Communication Services.\\n* CommunicationEndpointUrl: The endpoint URL for Azure Communication Services.\\n\\n1. The script checks if a Client ID is provided. If so, it constructs a URI to obtain an access token using User Assigned Managed Identity. Otherwise, it uses System Assigned Managed Identity.\\n2. The script then makes a GET request to the identity endpoint to retrieve the access token.\\n3. The script constructs the URI for the email sending endpoint and defines the headers, including the access token.\\n4. A function Send-Email is defined to send the email using the Invoke-RestMethod cmdlet.\\n5. The email content and recipient details are defined in a hashtable, which is passed to the Send-Email function.\\n\\n_(This authenticaiton was inititally written to be used in a Azure Automation Runbook, with the User Managed Identity assigned Contributor rights to the Azure Communication Services resource (not the Email Communication Services).)_\\n\\nHere is the PowerShell script to send an email using Azure Communication Services using the [User Managed identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796) of an [Azure Automation account](https://learn.microsoft.com/azure/automation/overview?WT.mc_id=AZ-MVP-5004796):\\n\\n```powershell\\nparam (\\n    [Parameter(Mandatory=$false)]\\n    [string]$ClientId = \'bc012f8f-58d1-43a5-9383-bb4d104ffe27\',\\n    \\n    [Parameter(Mandatory=$false)]\\n    [string]$EmailRecipient = \\"example@example.com\\",\\n    \\n    [Parameter(Mandatory=$false)]\\n    [string]$SenderAddress = \'DoNotReply@af595a23-f54a-4cdc-bffa-fa3ef54eb1c1.azurecomm.net\',\\n    \\n    [Parameter(Mandatory=$false)]\\n    [string]$ResourceID = \'https://communication.azure.com\',\\n    \\n    [Parameter(Mandatory=$false)]\\n    [string]$CommunicationEndpointUrl = \\"commserviceslukeuserassignedtest.australia.communication.azure.com\\"\\n)\\n\\n$emailSubject = \\"Important: Server Maintenance Notification\\"\\n$emailBody = @\\"\\n<html>\\n<body>\\n<p>Dear User,</p>\\n<p>This is to inform you that a <b><i>server maintenance is scheduled for the next week</i></b>.</p>\\n<p>The servers will be down from 10:00 PM to 2:00 AM.</p>\\n<p>Please save your work and log off during this period to avoid any data loss.</p>\\n<p>If you have any questions or concerns, please contact our IT Support team.</p>\\n<p>Thank you for your understanding and cooperation.</p>\\n<p>Best Regards,</p>\\n<p>IT Support Team</p>\\n</body>\\n</html>\\n\\"@\\n\\nif ($emailBody -ne \\"\\") {\\n    Write-Output \\"Email body is not empty. Proceeding with email sending process.\\"\\n\\n    if ($ClientId) {\\n        Write-Output \\"Client ID: $ClientId exists. Using User Assigned Managed Identity...\\"\\n        $Uri = \\"$($env:IDENTITY_ENDPOINT)?api-version=2018-02-01&resource=$ResourceID&client_id=$ClientId\\"\\n    } else {\\n        Write-Output \\"Client ID: $ClientId does not exist. Using System Assigned Managed Identity...\\"\\n        $Uri = \\"$($env:IDENTITY_ENDPOINT)?api-version=2018-02-01&resource=$ResourceID\\"\\n    }\\n\\n    # Function to get access token\\n    try {\\n        # Invoke a GET request to the identity endpoint to get the access token\\n        $AzToken = Invoke-WebRequest -Uri $Uri -Method GET -Headers @{ Metadata = \\"true\\" } -UseBasicParsing | Select-Object -ExpandProperty Content | ConvertFrom-Json | Select-Object -ExpandProperty access_token\\n        # Print the obtained access token\\n        Write-Output \\"Access Token: $AzToken\\"\\n    }\\n    catch {\\n        # If there\'s an error, print the error message and response details\\n        Write-Error \\"Failed to get access token: $_\\"\\n        Write-Output \\"Response Status Code: $($_.Exception.Response.StatusCode.Value__)\\"\\n        Write-Output \\"Response Status Description: $($_.Exception.Response.StatusDescription)\\"\\n        Write-Output \\"Response Content: $($_.Exception.Response.GetResponseStream() | %{ $_.ReadToEnd() })\\"\\n        return\\n    }\\n\\n    # Construct the URI for the email sending endpoint\\n    $uri = \\"https://$CommunicationEndpointUrl/emails:send?api-version=2023-03-31\\"\\n\\n    # Define the headers for the REST API call\\n    # Include the content type and the obtained access token in the Authorization header\\n    $headers = @{\\n        \\"Content-Type\\"  = \\"application/json\\"\\n        \\"Authorization\\" = \\"Bearer $AzToken\\"\\n    }\\n\\n    # Function to send email\\n    function Send-Email {\\n        param (\\n            [string]$Uri,\\n            [hashtable]$Headers,\\n            [hashtable]$Body\\n        )\\n        try {\\n            Write-Output \\"Sending email...\\"\\n            Write-Output \\"URI: $Uri\\"\\n                Write-Output \\"Headers: $(ConvertTo-Json $Headers -Depth 10)\\"\\n            Write-Output \\"Body: $(ConvertTo-Json $Body -Depth 10)\\"\\n            $response = Invoke-RestMethod -Uri $Uri -Method Post -Headers $Headers -Body ($Body | ConvertTo-Json -Depth 10) -UseBasicParsing\\n            Write-Output \\"Email sent successfully. Response: $response\\"\\n            return $response\\n        }\\n        catch {\\n            Write-Error \\"Failed to send email: $_\\"\\n            Write-Output \\"Exception Message: $($_.Exception.Message)\\"\\n            Write-Output \\"Exception StackTrace: $($_.Exception.StackTrace)\\"\\n            throw\\n        }\\n    }\\n\\n    $apiResponse = @{\\n        headers = @{\\n            id = (New-Guid).Guid\\n        }\\n        senderAddress = $SenderAddress\\n        content = @{\\n            subject = $emailSubject\\n            html    = $emailBody\\n        }\\n        recipients = @{\\n            to = @(\\n                @{\\n                    address     = $EmailRecipient\\n                    displayName = $EmailRecipient\\n                }\\n            )\\n        }\\n        replyTo = @(\\n            @{\\n                address     = \\"example@contoso.com\\"\\n                displayName = \\"Contoso\\"\\n            }\\n        )\\n        userEngagementTrackingDisabled = $true\\n    }\\n\\n    Send-Email -Uri $uri -Headers $headers -Body $apiResponse\\n\\n}\\n```\\n\\nYou can run this script in an Azure Automation Runbook _(and theoretically in an [Azure Function](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796) as well)_ to send an email using Azure Communication Services with a User Assigned Managed Identity."},{"id":"azure/authenticating-azureopenai-python","metadata":{"permalink":"/azure/authenticating-azureopenai-python","source":"@site/blog/2024-12-09-azureopenaientraauthpython/index.mdx","title":"Authenticating Azure OpenAI with Managed Identity","description":"Learn how to authenticate with Azure OpenAI using an API key for local development and Managed Identities for production in Python.","date":"2024-12-08T19:31:27.637Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.33,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Authenticating Azure OpenAI with Managed Identity","metaDescription":"Learn how to authenticate with Azure OpenAI using an API key for local development and Managed Identities for production in Python.","date":"2024-12-08T19:31:27.637Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/authenticating-azureopenai-python","keywords":["azure","Python","Authentication","API Key","Managed Identities","OpenAI","ContainerApps"],"description":"Learn how to authenticate with Azure OpenAI using an API key for local development and Managed Identities for production in Python."},"unlisted":false,"prevItem":{"title":"Using Azure Communication Services and UMI PowerShell to Send Emails","permalink":"/azure/using-communication-services-and-umi-powershell-to-send-emails"},"nextItem":{"title":"Azure Region Provider Comparison","permalink":"/azure/azure-region-provider-comparison"}},"content":"When developing a Python application that interacts with [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796), you may want to authenticate with an API key for testing, however in Production, you should use Managed Identities within Microsoft Azure.\\n\\nIn this article, we will look at how to authenticate with [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796) using an API key in Python for local development, then using an environment variable switch to authenticating using [managed Identities](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n\x3c!--truncate--\x3e\\n\\nThe true hero for Azure authentication is the [DefaultAzureCredential](https://learn.microsoft.com/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python&WT.mc_id=AZ-MVP-5004796) class from the Azure Identity library. This class can authenticate with Azure services using various methods, including Managed Identity, Shared Token Cache, and environment variables.\\n\\n![DefaultAzureCredential](images/DefaultAzureCredential.png)\\n\\n:::info\\nThis code snippet demonstrates how to load and validate environment variables needed to connect to Azure OpenAI services. Here\'s a step-by-step breakdown:\\n\\n1. **Load Environment Variables**: The code first attempts to load environment variables from the operating system. If any of the required variables are missing, it then tries to load them from a `.env` file named `azureopenai.env`.\\n\\n2. **Check for Missing Variables**: This function defines a list of required environment variables and checks for missing ones. If any required variables are not found, an error indicates which variables are missing.\\n\\n3. **Authentication Setup**: The code checks if the `AUTHENTICATION` environment variable is set to `AZURE`. If it is, it uses Azure\'s DefaultAzureCredential to get a bearer token for authentication. Otherwise, it uses an API key.\\n\\n4. **Initialize AzureOpenAI Client**: Finally, it initializes the `AzureOpenAI` client with the appropriate authentication method (either bearer token or API key) and the necessary API version and endpoint.\\n:::\\n\\n\\nThe Python code is as follows:\\n\\n```python\\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\\n# Load environment variables from OS and .env file\\nENV = {var: os.getenv(var) for var in [\\"AZURE_OPENAI_ENDPOINT\\", \\"AZURE_OPENAI_KEY\\", \\"AZURE_OPENAI_API_VERSION\\", \\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\\"]}\\nif any(value is None for value in ENV.values()):\\n    ENV.update({k: v for k, v in dotenv.dotenv_values(\\"azureopenai.env\\").items() if k not in ENV or ENV[k] is None})\\n\\nrequired_env_vars = [\\"AZURE_OPENAI_ENDPOINT\\", \\"AZURE_OPENAI_KEY\\", \\"AZURE_OPENAI_API_VERSION\\", \\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\\"]\\nmissing_vars = [var for var in required_env_vars if not ENV.get(var)]\\nif missing_vars:\\n    raise KeyError(f\\"Missing required environment variables: {\', \'.join(missing_vars)}\\")\\n\\n# Check if the Authentication environment variable is set to Azure\\nif os.getenv(\\"AUTHENTICATION\\") == \\"AZURE\\":\\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \\"https://cognitiveservices.azure.com/.default\\")\\n    client = AzureOpenAI(\\n        api_version=ENV[\\"AZURE_OPENAI_API_VERSION\\"],\\n        azure_endpoint=ENV[\\"AZURE_OPENAI_ENDPOINT\\"],\\n        azure_ad_token_provider=token_provider\\n    )\\nelse:\\n    client = AzureOpenAI(\\n        api_version=ENV[\\"AZURE_OPENAI_API_VERSION\\"],\\n        azure_endpoint=ENV[\\"AZURE_OPENAI_ENDPOINT\\"],\\n        api_key=ENV[\\"AZURE_OPENAI_KEY\\"]\\n    )\\n```\\n\\n:::tip\\nThis is assuming you are using the System Managed identity; however, if you want to run with a User Assigned Managed identity. Make sure you have an environment variable of **AZURE_CLIENT_ID** to the value of the CLIENT ID of the User Assigned managed identity.\\n:::\\n\\nWhen deployed to an [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) environment, I have set the operating system environment variables set to use Managed Identities. This is done by setting the `AUTHENTICATION` environment variable to `AZURE` and using the [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python&WT.mc_id=AZ-MVP-5004796) class to authenticate with Azure services.\\n\\n![Container Apps - Environment variables](images/ContainerApps_AzureOpenAIEnvVariables.png)"},{"id":"azure/azure-region-provider-comparison","metadata":{"permalink":"/azure/azure-region-provider-comparison","source":"@site/blog/2024-11-30-provider-region-comparison/index.mdx","title":"Azure Region Provider Comparison","description":"Compare the capabilities and services available in the new Azure region, for example, New Zealand North, with those in Australia East.","date":"2024-11-30T08:14:09.250Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.085,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Region Provider Comparison","metaDescription":"Compare the capabilities and services available in the new Azure region for example, New Zealand North, with those in Australia East.","date":"2024-11-30T08:14:09.250Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/azure-region-provider-comparison","keywords":["azure","nznorth","australiaeast","azure regions","provider namespaces","service availability","cloud comparison"],"description":"Compare the capabilities and services available in the new Azure region, for example, New Zealand North, with those in Australia East."},"unlisted":false,"prevItem":{"title":"Authenticating Azure OpenAI with Managed Identity","permalink":"/azure/authenticating-azureopenai-python"},"nextItem":{"title":"Request is badly formatted Error with Azure OpenAI","permalink":"/azure/openai-request-badly-formatted"}},"content":"With the release of the [New Zealand North](https://datacenters.microsoft.com/globe/explore?info=region_newzealandnorth) Azure Region on the horizon, I wanted a way to measure what capabilities are being deployed in the new region that could be used, particularly in comparison to Australia East.\\n\\nTo do so, I wrote a script that uses [PowerShell](https://learn.microsoft.com/powershell/scripting/overview?view=powershell-7.4&WT.mc_id=AZ-MVP-5004796) to query the [provider namespaces](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-services-resource-providers?WT.mc_id=AZ-MVP-5004796) of each [region](https://azure.microsoft.com/explore/global-infrastructure/products-by-region?WT.mc_id=AZ-MVP-5004796#products-by-region_tab4), compare them side by side, and output the results into an HTML table.\\n\\n![Azure Region Provider Comparison](images/RegionCompare_AustraliaVsNZN.png)\\n\\n\x3c!--truncate--\x3e\\n\\nThere are bound to be differences between the regions, especially when Strategic Services, such as Azure OpenAI _(Cognitive Services)_, are involved, especially for such a new region, especially at the time of writing when not all [Foundational and Mainstream services](https://learn.microsoft.com/azure/reliability/availability-service-by-category?WT.mc_id=AZ-MVP-5004796#service-categories-across-region-types) may have been deployed.\\n\\n:::warning\\nUsing the Providers is not a full-proof way of comparing the regions; however, for my purposes, it largely helps to identify the capabilities, especially when compared to the [Azure services table](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-services-resource-providers?WT.mc_id=AZ-MVP-5004796). Its also worth mentioning, that this comparison won\'t touch on some services or capabilities, that may be may have preview capabilities delivered to specific regions as a sub-feature of that namespace.\\n:::\\n\\nThe PowerShell script can be found below:\\n\\n```powershell title=\\"RegionProvider_Compare.ps1\\"\\n# Get the current date in an easy-to-read format\\n$currentDate = Get-Date -Format \\"MMMM dd, yyyy\\"\\n\\n[string]$location1 = \\"New Zealand North\\"\\n[string]$location2 = \\"Australia East\\"\\n\\n\\n\\nfunction Get-ProviderNamespaces {\\n    param (\\n        [string]$location\\n    )\\n\\n    # Validate location parameter\\n    if (-not $location) {\\n        throw \\"Location parameter is required.\\"\\n    }\\n\\n    try {\\n        $providers = Get-AzResourceProvider\\n        $providerNamespaces = @()\\n\\n        foreach ($provider in $providers) {\\n            $providerNamespace = $provider.ProviderNamespace\\n            $resourceTypes = $provider.ResourceTypes | Where-Object { $_.Locations -contains $location }\\n            \\n            if ($resourceTypes) {\\n                $providerNamespaces += $providerNamespace\\n            }\\n        }\\n\\n        return $providerNamespaces | Sort-Object -Unique\\n    }\\n    catch {\\n        Write-Error \\"Failed to get provider namespaces for location $location : $_\\"\\n        throw\\n    }\\n}\\n\\nfunction Create-ComparisonTable {\\n    param (\\n        [array]$providerNamespaces1,\\n        [array]$providerNamespaces2\\n    )\\n    $table = @()\\n    $maxLength = [math]::Max($providerNamespaces1.Count, $providerNamespaces2.Count)\\n\\n    for ($i = 0; $i -lt $maxLength; $i++) {\\n        $provider1 = if ($i -lt $providerNamespaces1.Count) { $providerNamespaces1[$i] } else { \\"\\" }\\n        $provider2 = if ($i -lt $providerNamespaces2.Count) { $providerNamespaces2[$i] } else { \\"\\" }\\n        $missingInLocation1 = if ($provider2 -and -not $providerNamespaces1.Contains($provider2)) { $provider2.Trim() } else { \\"\\" }\\n\\n        if ($provider1 -or $provider2 -or $missingInLocation1) {\\n            $table += [PSCustomObject]@{\\n                \\"$location1 ($($providerNamespaces1.Count))\\" = $provider1\\n                \\"$location2 ($($providerNamespaces2.Count))\\"    = $provider2\\n                \\"Missing in $location1\\"                      = $missingInLocation1\\n            }\\n        }\\n    }\\n\\n    return $table\\n}\\n\\n# Generate the HTML report with the date in the title\\nfunction Generate-HTMLReport {\\n    param (\\n        [array]$comparisonTable,\\n        [string]$location1,\\n        [string]$location2\\n    )\\n\\n    $html = @\\"\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Azure Provider Namespaces Comparison - $currentDate</title>\\n    <style>\\n        table { \\n            width: 100%; \\n            border-collapse: collapse; \\n            font-family: Arial, sans-serif; \\n            margin: 20px 0; \\n            border-radius: 5px 5px 0 0; \\n            overflow: hidden; \\n            box-shadow: 0 0 20px rgba(0, 0, 0, 0.15); \\n        }\\n        th, td { \\n            padding: 12px 15px; \\n            text-align: left; \\n        }\\n        th { \\n            text-transform: uppercase; \\n            font-weight: bold; \\n        }\\n        th:nth-child(1) { \\n            background-color: #4CAF50; \\n            color: white; \\n        }\\n        th:nth-child(2) { \\n            background-color: #2196F3; \\n            color: white; \\n        }\\n        th:nth-child(3) { \\n            background-color: #FF9800; \\n            color: white; \\n        }\\n        tr { \\n            border-bottom: 1px solid #dddddd; \\n        }\\n        tr:nth-child(even) { \\n            background-color: #f3f3f3; \\n        }\\n        tr:last-child { \\n            border-bottom: 2px solid #009879; \\n        }\\n        tr:hover { \\n            background-color: #f1f1f1; \\n        }\\n    </style>\\n</head>\\n<body>\\n    <h1>Azure Provider Namespaces Comparison</h1>\\n    <p>Comparison of <a href=\\"https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-services-resource-providers?WT.mc_id=AZ-MVP-5004796\\">provider namespaces</a> between <strong>$location1</strong> and <strong>$location2</strong> as of $currentDate.</p>\\n    <table>\\n        <tr>\\n            <th>$location1 ($($providerNamespaces1.Count))</th>\\n            <th>$location2 ($($providerNamespaces2.Count))</th>\\n            <th>Missing in $location1</th>\\n        </tr>\\n\\"@\\n\\n    foreach ($row in $comparisonTable) {\\n        $html += \\"<tr>\\"\\n        $html += \\"<td>$($row.\\"$location1 ($($providerNamespaces1.Count))\\")</td>\\"\\n        $html += \\"<td>$($row.\\"$location2 ($($providerNamespaces2.Count))\\")</td>\\"\\n        $html += \\"<td>$($row.\\"Missing in $location1\\")</td>\\"\\n        $html += \\"</tr>\\"\\n    }\\n\\n    $html += @\\"\\n    </table>\\n    <h2>Service Categories</h2>\\n    <ul>\\n        <li><strong><a href=\\"https://learn.microsoft.com/azure/reliability/availability-service-by-category?WT.mc_id=AZ-MVP-5004796#available-services-by-region-category\\">Foundational</a></strong>: Available in all recommended and alternate regions when the region is generally available, or within 90 days of a new foundational service becoming generally available.</li>\\n        <li><strong><a href=\\"https://learn.microsoft.com/azure/reliability/availability-service-by-category?WT.mc_id=AZ-MVP-5004796#available-services-by-region-category\\">Mainstream</a></strong>: Available in all recommended regions within 90 days of the region general availability. Demand-driven in alternate regions, and many are already deployed into a large subset of alternate regions.</li>\\n        <li><strong><a href=\\"https://learn.microsoft.com/azure/reliability/availability-service-by-category?WT.mc_id=AZ-MVP-5004796#strategic-services\\">Strategic</a></strong>: Targeted service offerings, often industry-focused or backed by customized hardware. Demand-driven availability across regions.</li>\\n    </ul>\\n    <p>Note: The rollout schedule for services may vary based on their category and region, with Foundational Services being prioritized for early adoption, followed by Mainstream services (+90 days after GA), and finally Strategic services as demand requirements. Make sure you let your Microsoft account team know if a Strategic Service is required in your region.</p>\\n</body>\\n</html>\\n\\"@\\n\\n    return $html\\n}\\n\\n# Main script execution\\ntry {\\n    Write-Host \\"Fetching provider namespaces for $location1 and $location2 ...\\" -ForegroundColor Green\\n\\n    $providerNamespaces1 = Get-ProviderNamespaces -location $location1\\n    $providerNamespaces2 = Get-ProviderNamespaces -location $location2\\n\\n    $comparisonTable = Create-ComparisonTable -providerNamespaces1 $providerNamespaces1 -providerNamespaces2 $providerNamespaces2\\n\\n    Write-Host \\"Generating HTML report...\\" -ForegroundColor Green\\n    $htmlReport = Generate-HTMLReport -comparisonTable $comparisonTable -location1 $location1 -location2 $location2\\n\\n    $reportPath = \\"AzureProviderComparisonReport.html\\"\\n    $htmlReport | Out-File -FilePath $reportPath\\n\\n    Write-Host \\"HTML report generated at $reportPath\\" -ForegroundColor Green\\n}\\ncatch {\\n    Write-Error \\"An error occurred: $_\\"\\n}\\n```\\n\\nIt will output an HTML report in the directory that the script has been run from _(make sure you are authenticated to Azure first and have the [Az PowerShell](https://learn.microsoft.com/powershell/azure/new-azureps-module-az?view=azps-13.0.0&WT.mc_id=AZ-MVP-5004796) modules installed)_:\\n\\n![Azure Region Provider Comparison](images/RegionCompare_HTMLReport.jpg)"},{"id":"azure/openai-request-badly-formatted","metadata":{"permalink":"/azure/openai-request-badly-formatted","source":"@site/blog/2024-11-27-requestisbadlyformated/index.mdx","title":"Request is badly formatted Error with Azure OpenAI","description":"Learn how to resolve the \'Request is badly formatted\' error when using Azure OpenAI with Managed Identity in a container app.","date":"2024-11-26T19:48:56.888Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.665,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Request is badly formatted Error with Azure OpenAI","metaDescription":"Learn how to resolve the \'Request is badly formatted\' error when using Azure OpenAI with Managed Identity in a container app.","date":"2024-11-26T19:48:56.888Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/openai-request-badly-formatted","keywords":["azure","openai","container apps","managed identity","error handling","custom subdomain"],"description":"Learn how to resolve the \'Request is badly formatted\' error when using Azure OpenAI with Managed Identity in a container app."},"unlisted":false,"prevItem":{"title":"Azure Region Provider Comparison","permalink":"/azure/azure-region-provider-comparison"},"nextItem":{"title":"Deploying a Streamlit App to Azure Container Apps","permalink":"/azure/deploy-streamlit-to-container-apps"}},"content":"When attempting to call [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796) using a [Managed Identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796), you may encounter the following error:\\n\\n:::warning\\n```plaintext\\nERROR:root:Error generating response: Error code: 400 - {\'error\': {\'code\': \'Request is badly formated\', \'message\': \'Resource Id is badly formed or from wrong namespace: NA\'}}\\n2024-11-26T18:57:08.224640325Z ERROR:root:Response content: b\'{\\"error\\":{\\"code\\":\\"Request is badly formated\\",\\"message\\":\\"Resource Id is badly formed or from wrong namespace: NA\\"}}\'\\n```\\n:::\\n\\n\x3c!--truncate--\x3e\\n\\nI had issues when attempting to call the Azure OpenAI endpoint from an [Azure Container App](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) using a Managed Identity. The error message that was getting returned to my App was:\\n\\n:::warning\\nAn error generating a reponse.\\n:::\\n\\n![Error](images/App_AnErrorGeneratingReponse.png)\\n\\nAfter enabling some logging on my Python application, I was able to review the Console Logs of my Container _(running in a Container Apps environment)_ and found the following error message:\\n\\n![Error](images/Container_ErrorGenertingResponse.png)\\n\\nAfter some testing, I was able to confirm that I could receive a response using the API keys, but when using the Managed Identity, I got an error message.\\n\\nMy Azure OpenAI instance was created with Infrastructure as Code and I noticed that the endpoint of my Azure OpenAI was:\\n\\n```plaintext\\nhttps://eastus2.api.cognitive.microsoft.com/\\n```\\n\\n![Azure OpenAI](images/AzureOpenAI_Bad_CognitiveServicesEndpoint.png)\\n\\nOn an Azure OpenAI instance that was created in the Portal, the endpoint was:\\n\\n```plaintext\\nhttps://azureopenaitestlol.openai.azure.com/\\n```\\n\\nSo, I made a comparison between the two resources:\\n\\n![Azure OpenAI](images/AzureOpenAI_Diff_CognitiveServicesEndpoint.png)\\n\\nAnd my IaC-created Azure OpenAI resource was missing the following: \\n\\n* customSubDomainName\\n\\nAfter some digging, **I found that the [customSubDomainName](https://learn.microsoft.com/azure/ai-services/cognitive-services-custom-subdomains?WT.mc_id=AZ-MVP-5004796) is the key to the issue.** When using a Managed Identity, the customSubDomainName is required to be set for Entra ID to be able to authenticate the request\\n\\n:::info\\nCustom subdomain names are required to enable features like Microsoft Entra ID for authentication.\\n:::\\n\\nSo after updating the Azure OpenAI resource to use the subdomain, updating my environment variables to use the right Azure OpenAI endpoint _(and making sure that the CLIENT_ID of the User Assigned Managed identity is correct)_ I was able to call the Azure OpenAI endpoint using the Managed Identity successfully.\\n\\n![Azure OpenAI](images/App_SuccessfulTest.png)\\n\\n\\n![Azure OpenAI](images/ContainerApp_EnvironmentVariabnles.png)"},{"id":"azure/deploy-streamlit-to-container-apps","metadata":{"permalink":"/azure/deploy-streamlit-to-container-apps","source":"@site/blog/2024-11-18-streamlit-azcontainerapps/index.mdx","title":"Deploying a Streamlit App to Azure Container Apps","description":"Learn how to deploy a Streamlit application to Azure Container Apps using Docker and GitHub Codespaces.","date":"2024-11-18T05:27:32.929Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.49,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deploying a Streamlit App to Azure Container Apps","metaDescription":"Learn how to deploy a Streamlit application to Azure Container Apps using Docker and GitHub Codespaces.","date":"2024-11-18T05:27:32.929Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/deploy-streamlit-to-container-apps","keywords":["azure","streamlit","container apps","docker","deployment","python","serverless"],"description":"Learn how to deploy a Streamlit application to Azure Container Apps using Docker and GitHub Codespaces."},"unlisted":false,"prevItem":{"title":"Request is badly formatted Error with Azure OpenAI","permalink":"/azure/openai-request-badly-formatted"},"nextItem":{"title":"New Zealand North Latency Testing and Results","permalink":"/azure/nz-north-latency-testing"}},"content":"[Streamlit](https://streamlit.io/) is an excellent tool for creating interactive web applications with Python. \\n\\nIn this post, we will deploy a Streamlit application to [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796). [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) is a fully managed serverless container service that allows you to run your containerized applications without having to manage the underlying infrastructure.\\n\\n\x3c!--truncate--\x3e\\n\\n:::note\\nFor this article, you have access to an Azure subscription and have permission to deploy resources. If you don\'t have an Azure subscription, you can create a [free account](https://azure.microsoft.com/pricing/purchase-options/azure-account?icid=azurefreeaccount&WT.mc_id=AZ-MVP-5004796) in just a couple of minutes.\\n:::\\n\\nToday, we are going to take a base Streamlit example application, and deploy it to Azure Container Apps.\\n\\n:::info\\nI will be using a GitHub Codespace; feel free to reference my public repository template: [lukemurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding) as a Codespace template.\\n:::\\n\\n![Streamlit Application](images/Streamlit_Demo_Example.png)\\n\\nLet us start off with the Streamlit application.\\n\\n```python title=\\"streamlit_app.py\\"\\nimport altair as alt\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n\\"\\"\\"\\n# Welcome to Streamlit!\\n\\nEdit `/streamlit_app.py` to customize this app to your heart\'s desire :heart:.\\nIf you have any questions, check out our [documentation](https://docs.streamlit.io) and [community\\nforums](https://discuss.streamlit.io).\\n\\nIn the meantime, below is an example of what you can do with just a few lines of code:\\n\\"\\"\\"\\n\\nnum_points = st.slider(\\"Number of points in spiral\\", 1, 10000, 1100)\\nnum_turns = st.slider(\\"Number of turns in spiral\\", 1, 300, 31)\\n\\nindices = np.linspace(0, 1, num_points)\\ntheta = 2 * np.pi * num_turns * indices\\nradius = indices\\n\\nx = radius * np.cos(theta)\\ny = radius * np.sin(theta)\\n\\ndf = pd.DataFrame({\\n    \\"x\\": x,\\n    \\"y\\": y,\\n    \\"idx\\": indices,\\n    \\"rand\\": np.random.randn(num_points),\\n})\\n\\nst.altair_chart(alt.Chart(df, height=700, width=700)\\n    .mark_point(filled=True)\\n    .encode(\\n        x=alt.X(\\"x\\", axis=None),\\n        y=alt.Y(\\"y\\", axis=None),\\n        color=alt.Color(\\"idx\\", legend=None, scale=alt.Scale()),\\n        size=alt.Size(\\"rand\\", legend=None, scale=alt.Scale(range=[1, 150])),\\n    ))\\n\\n```\\n\\nThe next step is to create a `requirements.txt` file specifying the Streamlit application\'s dependencies. This will be used to install the Python dependencies for the image.\\n\\n```plaintext title=\\"requirements.txt\\"\\naltair\\npandas\\nstreamlit\\n```\\nNow that we have our Streamlit application, we need to create a Dockerfile to containerize the application. In this example, we use the official Python image from the Docker Hub as the base image. We then copy the application code into the container, install the dependencies, and define the command to run the Streamlit application using port 8080 _(the default port for Streamlit is 8501, but we are using 8080 for this example as a common port)_.\\n\\n```Dockerfile title=\\"Dockerfile\\"\\n# Use the official Python image from the Docker Hub\\nFROM python:3.12.7-slim\\n\\n# Set environment variable for the port\\nENV PORT=8080\\n\\n# Expose the port that the application will run on\\nEXPOSE 8080\\n\\n# Set the working directory inside the container\\nWORKDIR /usr/src/app\\n\\n# Copy the requirements file into the container\\nCOPY requirements.txt ./\\n\\n# Install the dependencies specified in the requirements file\\nRUN pip install --no-cache-dir -r requirements.txt\\n\\n# Copy the rest of the application code into the container\\nCOPY . .\\n\\n# Create a non-root user and switch to it\\nRUN useradd -m appuser && chown -R appuser /usr/src/app\\nUSER appuser\\n\\n# Define the command to run the Streamlit application\\nENTRYPOINT [\\"streamlit\\", \\"run\\", \\"app.py\\", \\"--server.port=8080\\"]\\n\\n# Add labels for metadata\\nLABEL maintainer=\\"luke@luke.geke.nz\\"\\nLABEL version=\\"1.0\\"\\nLABEL description=\\"Streamlit application\\"\\n\\n# Health check to ensure the application is running\\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\\n  CMD curl --fail http://localhost:8080/_stcore/health || exit 1\\n```\\n\\n:::tip\\n[Draft](https://github.com/Azure/draft) is an open-source project that streamlines Kubernetes development. It takes a non-containerized application and generates the DockerFiles, Kubernetes manifests, Helm charts, Kustomize configurations, and other artifacts associated the application. The Azure Kubernetes Service _(AKS)_ DevX extension for Visual Studio Code enhances non-cluster experiences, allowing you to create deployment files to deploy your applications to AKS. Draft is the available feature included in the DevX extension.\\n\\nI used Draft, and the DevX extension was used to draft the first version of the streamlet dockerfile.\\n\\nReference: [Use Draft and the DevX extension for Visual Studio Code with Azure Kubernetes Service (AKS)](https://learn.microsoft.com/azure/aks/draft-devx-extension-aks?WT.mc_id=AZ-MVP-5004796)\\n:::\\n\\nTo build this image, I will use Docker on a GitHub Codespace to build the image locally and test it. Once confirmed, we can push it to an Azure Container Registry, which can then be used to deploy the image to Azure Container Apps.\\n\\nWe can easily test and build the image locally in a Codespace using the [Docker extension](https://code.visualstudio.com/docs/containers/overview?WT.mc_id=AZ-MVP-5004796) in Visual Studio Code.\\n\\n![Docker Build](images/Streamlit_Codespace_BuildTestDockerImage.gif)\\n\\nNow that we have tested the image locally, we can push it to an Azure Container Registry. \\n\\n:::info\\nYou can also refer to a previous article [Push Docker Images to Azure Container Registry with GitHub Codespaces](https://luke.geek.nz/azure/push-docker-images-to-acr-using-github-codespaces/) for more information on how to push the image to an Azure Container Registry using the command line from the public dockerhub registry.\\n:::\\n\\nTo push the image, we are going to use the docker extension again to do the push and tag to the [Azure Container Registry](https://learn.microsoft.com/azure/container-registry/container-registry-intro?WT.mc_id=AZ-MVP-5004796).\\n\\n![Docker Push](images/Streamlit_Codespace_DeployStreamlitImageACR.gif)\\n\\nNow that we have the image in the Azure Container Registry, we can deploy the image to Azure Container Apps.\\n\\nTo deploy the image to Azure Container Apps, we will to create a new Azure Container Apps resource in the Azure Portal and enable public ingress on port 8080, using a Microsoft managed network.\\n\\n![Azure Container Apps](images/Streamlit_ContainerApps_DeployStreamlit.gif)\\n\\nOne of the best features of Azure Container Apps is the ability to scale the application to zero, so we can set up a Scale rule to scale _(replicas)_ the Container App down to 0 when not in use.\\n\\n![Azure Container Apps Scale](images/Streamlit_ContainerApps_HTTPScaleRule.gif)"},{"id":"azure/nz-north-latency-testing","metadata":{"permalink":"/azure/nz-north-latency-testing","source":"@site/blog/2024-11-10-nznorth-az-latency/index.mdx","title":"New Zealand North Latency Testing and Results","description":"An in-depth analysis of latency within the New Zealand North Azure region, including various scenarios and performance metrics.","date":"2024-11-10T05:48:55.999Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":21.79,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"New Zealand North Latency Testing and Results","metaDescription":"An in-depth analysis of latency within the New Zealand North Azure region, including various scenarios and performance metrics.","date":"2024-11-10T05:48:55.999Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/nz-north-latency-testing","keywords":["azure","nznorth","latency","performance","availability zones","virtual networks","azure firewall"],"description":"An in-depth analysis of latency within the New Zealand North Azure region, including various scenarios and performance metrics."},"unlisted":false,"prevItem":{"title":"Deploying a Streamlit App to Azure Container Apps","permalink":"/azure/deploy-streamlit-to-container-apps"},"nextItem":{"title":"Enhancing GitHub Copilot with Custom Instructions","permalink":"/azure/enhancing-github-copilot-with-custom-instructions"}},"content":"New Zealand has a new _(and only)_ Azure Cloud region: [New Zealand North](https://news.microsoft.com/aotearoa-datacenter?WT.mc_id=AZ-MVP-5004796). It\'s not every day that we get to explore a new region! This article will examine the latency within the New Zealand North region and between its Availability Zones.\\n\\n\x3c!--truncate--\x3e\\n\\n## \ud83d\udd0d Overview\\n\\nNew Zealand North region follows the newer, now standard, 3 + 0 model, with 3 [availability zones](https://learn.microsoft.com/azure/reliability/availability-zones-overview?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796) (three separate datacenters that make up a region) but with [no specified region pair](https://learn.microsoft.com/azure/reliability/cross-region-replication-azure?WT.mc_id=AZ-MVP-5004796#regions-with-availability-zones-and-no-region-pair) _(e.g., locally or in Australia)_. This means that Microsoft needs to assume which other region you may want to use as a failover. You can choose any region you want (e.g., object replication on an Azure storage account can be any region). Most of us in New Zealand will likely choose New Zealand North as the primary and Australia East as the secondary, where applicable. These three availability zones already provide more resiliency out of the box than most New Zealand organizations had before.\\n\\n![New Zealand North - Azure Avaliability Zones](images/Regions_AvaliabilityZones.jpg)\\n\\n:::warning\\nAt the time of this article, New Zealand North is not officially Generally Available (indicative timelines December). However, finding I can deploy to this region now on my own subscriptions, I couldn\'t resist the opportunity to test the latency within the region. This article is intended as an indicative of what you can expect from New Zealand North, but I recommend running your own tests within your own environment if required. #geekingout\\n:::\\n\\nLet\'s look at the lower end of our resiliency tier. We can see Azure Service Bus\\tstarts at an SLA of 99.9% _(Y: 8h 45m 36s  M: 43m 12s  W: 10m 4s)_, and the higher SLA services such as Azure Virtual Machines and API Gateway can give up to 99.99% _(Y: 52m 35s  M: 4m 23s  W: 1m 1s)_ _(how your solution and what services you use, impact your Composite SLA and Service Level Agreements so make sure you architect accordingly and review the [Microsoft SLA documentation](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?WT.mc_id=AZ-MVP-5004796))_.\\n\\n:::info\\nAvailability zones are close enough to have low-latency connections to other availability zones. A high-performance network connects them with a round-trip latency of less than 2ms. However, availability zones are far enough apart to reduce the likelihood that more than one will be affected by local outages or weather. Availability zones have independent power, cooling, and networking infrastructure. They\'re designed so that if one zone experiences an outage, then regional services, capacity, and high availability are supported by the remaining zones. They help your data stay synchronized and accessible when things go wrong.\\n:::\\n\\n:::tip\\nIf you are interested in more information about Availability Zones and the differences between Physical + Logical zones, make sure you have read a previous blog article of mine: [Azure Availability Zone Peering](https://luke.geek.nz/azure/azure-availability-zone-peering/). In essence, you need to be aware that Avalibility Zone 1 _(physical and logical avalibility zones can be different)_, does not always equal Availability Zone 1, especially if you have workloads spread across multiple subscriptions. For today, all my testing is done in a single subscription.\\n:::\\n\\n## \ud83d\udcc9 Latency Testing\\n\\nBut this is not a resiliency discussion; it is a latency discussion, so let\'s examine the latency of what New Zealand North is giving us.\\n\\n:::warning\\nKeep in mind that these tests are simply a point in time, and my open environment may not necessarily reflect your own Azure environment. At the time of these tests, the New Zealand North Region [isn\'t officially generally available](https://datacenters.microsoft.com/globe/explore?WT.mc_id=AZ-MVP-5004796). This article is intended as an indicator of what you can expect from New Zealand North, but I recommend running your own tests within your own environment if required.\\n:::\\n\\nThe latency from New Zealand to New Zealand North is around 15ms _(from Hamilton)_, which is a great improvement from the 32-50ms to Australia East _(from Hamilton)_, and the 190-230ms to the US _(from Hamilton)_.\\n\\n![New Zealand North - Azure Latency](images/AzureSpeed_NewZealandNorthInternetLatency.jpg)\\n\\n:::info\\nThe test above was done from Hamilton _(using the [Azure Speed Test](https://www.azurespeed.com/Azure/Latency?WT.mc_id=AZ-MVP-5004796))_,. The latency is to the internet, so it is not the same as the latency between the Azure region and your on-premises _(or another cloud)_ environment. Still, it is a good indicator of the latency you can expect from your users to your Azure resources. Your browser sends HTTPS requests to Azure blob files in each region. The median latency is calculated by measuring the time between the request and the response. The latency is measured in milliseconds (ms). The lower the latency, the better.\\n:::\\n\\nBecause this is a standard Availability Zone _(3+0)_ region, Microsoft has certain commitments they have around latency within their own network and to the internet. They have a [guidelines](https://learn.microsoft.com/en-us/azure/reliability/availability-zones-overview?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796) of 2ms _(between zones)_ and 10ms _(to the internet - don\'t quote me on the SLA to the internet, its inferred by reading the SLA document, but I can\'t confirm at this stage)_, but remember that is just the network latency, and not the application latency _(which is what you should be more concerned about)_. It\'s worth noting on SLA _(Service Level Agreements)_that Microsoft does it per service vs per datacenter or rack.\\n\\nSo, let us prove that with the New Zealand North region, we can communicate between all regions in 2ms or under. For the purposes of this article, I will be using a Single subscription to do my testing _(so I know that each zone will be separate)_. I will be using a [Azure Virtual Network](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview?WT.mc_id=AZ-MVP-5004796) _(VNet)_ in the New Zealand North region, a VNET _(logical Virtual Network)_ is zone-redundant out of the box.\\n\\nThen I will be using a Windows Server 2022 [Azure Virtual Machine](https://learn.microsoft.com/azure/virtual-machines/windows/overview?WT.mc_id=AZ-MVP-5004796) _(VM)_ in each zone, and I will be using [latte](https://learn.microsoft.com/en-us/azure/virtual-network/virtual-network-test-latency?tabs=windows&WT.mc_id=AZ-MVP-5004796#test-vms-with-latte-or-sockperf) to do my testing from each Virtual Machine.\\n\\n* Each Test machine is [Accelerated Networking](https://learn.microsoft.com/azure/virtual-network/accelerated-networking-overview?tabs=redhat&WT.mc_id=AZ-MVP-5004796) enabled.\\n* The size of each Virtual Machine is a: Windows Server 2022 Datacenter: Azure Edition - x64 Gen2 - Standard_D4s_v3 - 4 vCPUs, 16 GB memory, 127GB OS disk. Each Virtual Machine is created fresh, for the purpose of running this test.\\n* Network Security Groups as left as default _(at the VM NIC)_ and VirtualNetwork allow.\\n* The version of [latte](https://github.com/microsoft/latte/), I am using is v1.0.0.\\n\\n\\n:::info\\nMany  common network latency test tools, such as Ping, don\'t measure TCP or UDP traffic. Tools like Ping use Internet Control Message Protocol (ICMP), which applications don\'t use. ICMP traffic can be treated differently from application traffic and doesn\'t directly affect application performance. ICMP test results don\'t directly apply to TCP and UDP workloads.\\n\\n[Latte](https://github.com/microsoft/latte) (for Windows) and [SockPerf](https://github.com/mellanox/sockperf) (for Linux) measure only TCP or UDP payload delivery times. These tools use the following approach to measure network latency between two physical or virtual computers:\\n\\n* Create a two-way communication channel between the computers by designating one as the sender and one as a receiver.\\n* Send and receive packets in both directions and measure the round-trip time (RTT).\\n\\nRefer: [Test network latency between Azure VMs](https://learn.microsoft.com/en-us/azure/virtual-network/virtual-network-test-latency?tabs=windows&WT.mc_id=AZ-MVP-5004796)\\n:::\\n\\nThe latte command I will be using is: `latte -c -a 10.0.1.5:80 -i 60000`.\\n\\n:::note\\nEven though at the time of this article, Windows Server 2025 has been generally avaliable for about a week. I have decided to use 2022, as it is a common and mature version of Windows Server in use today.\\n:::\\n\\nInspired by [Nicola Delfino](https://www.linkedin.com/in/nicoladelfino) post on [Measuring latency between Azure Availability Zones and the impact of an NVA in between V2](https://nicolgit.github.io/azure-measuring-latency-across-availability-zones-in-we/) I will run through multiple scenarios for the New Zealand North Azure Region:\\n\\n* Same v-net, same availability zone, same [proximty placement](https://learn.microsoft.com/azure/virtual-machines/co-location?WT.mc_id=AZ-MVP-5004796) group\\n* Same v-net across availability zones\\n* multiple v-nets (in peering), same availability zone\\n* multiple v-nets (in peering) across availability zones\\n* multiple v-nets connected in a Hub & Spoke topology and Routing via Azure Firewall\\n\\n## \ud83c\udfe2 Scenario 1 - Same Virtual Network, same availability zone, same proximty placement group\\n\\nIn this scenario, we will have a single VNET, with a single subnet, and two virtual machines in the same availability zone, and the same [proximty placement](https://learn.microsoft.com/azure/virtual-machines/co-location?WT.mc_id=AZ-MVP-5004796) group.\\n\\n![Same VNET, same availability zone, same proximity placement group](images/NZN_LatencyTest_ProximityGroup.JPG)\\n\\nIn this example, we have VM1 talking to VM2.\\n\\n**VM1 (Sender - 10.0.1.4) - VM2 (Receiver - 10.0.1.5)**\\n\\n| Metric              | Value          | Notes                             |\\n|---------------------|----------------|-----------------------------------|\\n| **Protocol**        | TCP            | Connection-oriented, reliable    |\\n| **Send Method**     | Blocking       | Waits for completion              |\\n| **Receive Method**  | Blocking       | Waits for completion              |\\n| **SO_SNDBUF**       | Default        | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default        | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4              | Small message payload size        |\\n| **Iterations**      | 60,000         | Number of send/receive cycles     |\\n| **Latency (usec)**  | **72.75**          | Average time per message          |\\n| **CPU (%)**         | 6.5            | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 27,874 (2.03/iteration) | Context switches per second |\\n| **SysCall/sec**     | 28,590 (2.08/iteration) | System calls per second     |\\n| **Interrupt/sec**   | 58,447 (4.25/iteration) | Hardware interrupts per second |\\n\\n**Latency(usec): 72.75** \u2013 The average latency per message is 72.75 microseconds. This is the time it takes for a message to be sent and received back, which is relatively low and suggests a reasonably fast response time for small message sizes.\\n\\n## \ud83c\udf10 Scenario 2 - Same Virtual Network, same availability zone, virtual machine NOT in proximity placement group\\n\\nIn this scenario, we will have a single VNET, with a single subnet, and two virtual machines in the same availability zone, but not in the same proximty placement group.\\n\\n![Same VNET, same availability zone, VM NOT in proximity placement group](images/NZN_LatencyTest_NoProximityGroup.JPG)\\n\\nWe will be talking from VM1 to VM3.\\n\\n**VM1 (Sender) - VM3 (Receiver)**\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **55.97**           | Average time per message          |\\n| **CPU (%)**         | 14.9            | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 36,652 (2.05/iteration) | Context switches per second |\\n| **SysCall/sec**     | 39,293 (2.20/iteration) | System calls per second     |\\n| **Interrupt/sec**   | 68,617 (3.84/iteration) | Hardware interrupts per second |\\n\\nThe table lists the latency and system performance between **VM1 (Sender)** and **VM3 (Receiver)**. The average latency per message is **55.97 microseconds**, which is interesting and slightly lower than the previous scenario, and not what I expected, however, could be luck of the draw - something worth noting that without a proximity placement group, the next time I would deallocate this Virtual Machine, it could be spun up in a separate place within the zone.\\n\\n## \ud83d\udd17 Scenario 3 - Same Virtual Network, across availability zones\\n\\nIn this scenario, we will have a single VNET with a single subnet and two virtual machines in different availability zones.\\n\\nWe keep the same VM1 _(in Zone 1)_, but we will be talking to VM4 and VM5 (which are in different availability zones). VM4 is in Zone 2 and VM5 is in Zone 3.\\n\\n![Same VNET, across availability zones](images/NZN_LatencyTest_AvaliabilityGroups.JPG)\\n\\nFirst lets look at VM1 (Avaliability 1) talking to VM4 _(Avalibility Zone 2)_.\\n\\n**VM1 (Sender) - VM4 (Receiver)**\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **559.67**          | Average time per message          |\\n| **CPU (%)**         | 1.5             | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 3,742 (2.09/iteration) | Context switches per second |\\n| **SysCall/sec**     | 4,529 (2.53/iteration) | System calls per second     |\\n| **Interrupt/sec**   | 8,801 (4.93/iteration) | Hardware interrupts per second |\\n\\nThe table summarizes the latency and system performance between **VM1 (Sender)** and **VM4 (Receiver)**. The average latency per message is **559.67 microseconds**, significantly higher than in the previous scenarios. This is expected as the two virtual machines are in different availability zones.\\n\\nNext lets look at VM1 (Avaliability 1) talking to VM5 _(Avalibility Zone 3)_.\\n\\n**VM1 (Sender) - VM5 (Receiver)**\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **559.25**          | Average time per message          |\\n| **CPU (%)**         | 0.9             | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 3,868 (2.16/iteration) | Context switches per second |\\n| **SysCall/sec**     | 4,464 (2.50/iteration) | System calls per second     |\\n| **Interrupt/sec**   | 7,721 (4.32/iteration) | Hardware interrupts per second |\\n\\nThis table summarizes the latency and system performance between **VM1 (Sender)** and **VM5 (Receiver)**. The average latency per message is **559.25 microseconds**, which is similar to the previous scenario. This is expected as the two virtual machines are in different availability zones.\\n\\nFinally, lets look at VM4 _(Avaliability Zone 2)_ talking to VM5 _(Avalibility Zone 3)_.\\n\\nVM4 (Sender) - VM5 (Receiver)\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **494.23**          | Average time per message          |\\n| **CPU (%)**         | 3.6             | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 5,218 (2.58/iteration) | Context switches per second |\\n| **SysCall/sec**     | 13,445 (6.65/iteration) | System calls per second    |\\n| **Interrupt/sec**   | 9,212 (4.55/iteration) | Hardware interrupts per second |\\n\\nThis table summarizes the latency and system performance between **VM4 (Sender)** and **VM5 (Receiver)**. The average latency per message is **494.23 microseconds**, which is slightly lower than the previous scenarios. This is expected as the two virtual machines are in different availability zones.\\n\\nEven though the latency is higher than the previous scenarios, it is still relatively low and suggests a reasonably fast response time for small message sizes.\\n\\n- VM1 to VM4: 559.67 microseconds\\n- VM1 to VM5: 559.25 microseconds\\n- VM4 to VM5: 494.23 microseconds\\n\\nAll of these latency values are **well below 1 millisecond**, indicating **sub-millisecond latency** between the VMs, in different zones within the same Virtual Network and subnet.\\n\\n## \ud83d\udda7  Scenario 4 - Multiple Virtual Networks (peered), same availability zone\\n\\nIn this scenario, we will have two virtual networks, each with a single subnet, and two virtual machines in the same availability zone.\\n\\nWe will have VM1 _(10.0.1.4)_ in our Spoke1 network, talking to VM2 _(10.0.2.4)_, in our spoke2 network, each VM is in Availability Zone 1 and the two Virtual Networks are peered through a Hub Virtual Network, with user-defined routes, so they can talk to each other. I made use of [Azure Virtual Network Manager](https://learn.microsoft.com/azure/virtual-network-manager/overview?WT.mc_id=AZ-MVP-5004796), and a Mesh configuration to deploy the peering and user defined routes between the Virtual Networks.\\n\\n![Multiple VNETs (peered), same availability zone](images/NZN_LatencyTest_VNetPeering.JPG)\\n\\n**VM1 (Sender) - VM2 (Receiver) through VNet Peering**\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **67.81**           | Average time per message          |\\n| **CPU (%)**         | 7.7             | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 30,010 (2.04/iteration) | Context switches per second |\\n| **SysCall/sec**     | 31,328 (2.12/iteration) | System calls per second     |\\n| **Interrupt/sec**   | 66,269 (4.49/iteration) | Hardware interrupts per second |\\n\\nThis table summarizes the latency and system performance between **VM1 (Sender)** and **VM2 (Receiver)** through VNet peering. The latency of 67.81 microseconds is still well under 1 millisecond for a Virtual Machine talking over a Virtual Network peering within the same Availability Zone.\\n\\n## \ud83d\udda7 Scenario 5 - Multiple Virtual Networks (peered), across availability zones\\n\\nIn this scenario, we will have two virtual networks, each with a single subnet, and two virtual machines in different availability zones.\\n\\nWe will use the good old trusty VM1 again, and this time we will be talking to VM3 _(Avaliability Zone 2)_ and VM4 _(Avaliability Zone 3)_, each in their own Virtual Network, and peered through a Hub Virtual Network, with user defined routes, so they can talk to each other.\\n\\n![Multiple VNETs (peered), across availability zones](images/NZN_LatencyTest_VNetPeering_AvaliabilityZones.JPG)\\n\\nHere are the performance results for **VM1 (Sender) to VM3 (Receiver) through VNet peering**. VM1 is in **Availability Zone 1** and VM3 is in **Availability Zone 2**.:\\n\\n**VM1 (Sender) - VM3 (Receiver) in Availability Zone 2 through VNet Peering**\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **542.39**          | Average time per message          |\\n| **CPU (%)**         | 1.9             | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 3,933 (2.13/iteration) | Context switches per second |\\n| **SysCall/sec**     | 5,043 (2.74/iteration) | System calls per second     |\\n| **Interrupt/sec**   | 8,293 (4.50/iteration) | Hardware interrupts per second |\\n\\nThis table summarizes the latency and system performance between **VM1 (Sender)** and **VM3 (Receiver)** through VNet peering. The latency of 542.39 microseconds is still under 1 millisecond, between zones over a peered Virtual Network.\\n\\nNow, we need to test VM1 _(Avaliability Zone 1)_ by talking to VM4 _(Avaliability Zone 3)_ through VNet peering.\\n\\n**VM1 (Sender) - VM4 (Receiver) in Availability Zone 3 through VNet Peering**\\n\\n| Metric              | Value           | Notes                             |\\n|---------------------|-----------------|-----------------------------------|\\n| **Protocol**        | TCP             | Connection-oriented, reliable     |\\n| **Send Method**     | Blocking        | Waits for completion              |\\n| **Receive Method**  | Blocking        | Waits for completion              |\\n| **SO_SNDBUF**       | Default         | Default socket send buffer size   |\\n| **SO_RCVBUF**       | Default         | Default socket receive buffer size|\\n| **Msg Size (byte)** | 4               | Small message payload size        |\\n| **Iterations**      | 60,000          | Number of send/receive cycles     |\\n| **Latency (usec)**  | **311.53**          | Average time per message          |\\n| **CPU (%)**         | 13.4            | CPU utilization during test       |\\n| **CtxSwitch/sec**   | 9,721 (3.03/iteration) | Context switches per second |\\n| **SysCall/sec**     | 37,752 (11.76/iteration) | System calls per second    |\\n| **Interrupt/sec**   | 13,496 (4.20/iteration) | Hardware interrupts per second |\\n\\nThis table summarizes the latency and system performance between **VM1 (Sender)** and **VM4 (Receiver)** in **Availability Zone 3** through **VNet peering**. The latency of 311.53 microseconds is still under 1 millisecond.\\n\\nSo far, we have seen that the latency between Virtual Machines in different availability zones over a peered Virtual Network is still under 1 millisecond. This is a great result and shows that Azure\'s network infrastructure is capable of providing low-latency communication between Virtual Machines in different availability zones. \\n\\n## \ud83d\udee1\ufe0f Scenario 6 - Multiple Virtual Networks connected in a Hub & Spoke topology and Routing via Azure Firewall\\n\\nIn this scenario, we will have three virtual networks, each with a single subnet, and three virtual machines in different availability zones, connecting to each other through a Hub Virtual Network, with user defined routes, and routing through an Azure Firewall.\\n\\n:::tip\\nIf you are going through this in a Lab environment, make sure you have the necessary dependencies, such as the lattle executable setup on Virtual Machines or can share between them. As soon as the Firewall is in the mix, the default rules will block the traffic, so you will need to make sure you have the necessary rules in place to allow the traffic through.\\n:::\\n\\n![Multiple VNETs connected in a Hub & Spoke topology and Routing via Azure Firewall](images/NZN_LatencyTest_HubSpoke.JPG)\\n\\nThe Azure Firewall configuration is a Standard SKU, with a Firewall policy and zone-redundant _(across all 3 zones)_.\\n\\nWe will start with VM1 in Avalibility Zone 1, talking to VM2 in Avalibility Zone 2 via the Azure Firewall and hub and spoke network.\\n\\n**VM 1 (Sender)** to **VM 2 (Receiver) -  Availability Zone 2 through Hub and Spoke Peering with Azure Firewall**\\n\\n| Metric        | Value                      |\\n|------------------|----------------------------|\\n| **Protocol**     | TCP                        |\\n| **SendMethod**   | Blocking                   |\\n| **ReceiveMethod**| Blocking                   |\\n| **SO_SNDBUF**    | Default                    |\\n| **SO_RCVBUF**    | Default                    |\\n| **MsgSize (bytes)** | 4                         |\\n| **Iterations**   | 60000                      |\\n| **Latency (usec)** | **1017.12**                   |\\n| **CPU (%)**      | 1.1                        |\\n| **CtxSwitch/sec**| 2692 (2.74/iteration)      |\\n| **SysCall/sec**  | 5563 (5.66/iteration)      |\\n| **Interrupt/sec**| 4380 (4.45/iteration)      |\\n\\nThis is the longest time we have seen so far, but still boarding on 1 millisecond, we can see how processing through the Azure Firewall and Hub and Spoke adds additional latency _(as we would expect)_  are routing through an Azure Firewall.\\n\\nNext, we will look at VM1 in Avalibility Zone 1, talk to VM3 in Avalibility Zone 3 and spoke 3 via the Azure Firewall and hub and spoke network.\\n\\n**VM 1 (Sender)** to **VM 3 (Receiver)** -  Availability Zone 3 through Hub and Spoke Peering with Azure Firewall\\n\\n| Metric        | Value                      |\\n|------------------|----------------------------|\\n| **Protocol**     | TCP                        |\\n| **SendMethod**   | Blocking                   |\\n| **ReceiveMethod**| Blocking                   |\\n| **SO_SNDBUF**    | Default                    |\\n| **SO_RCVBUF**    | Default                    |\\n| **MsgSize (bytes)** | 4                         |\\n| **Iterations**   | 60000                      |\\n| **Latency (usec)** | **963.17**                    |\\n| **CPU (%)**      | 0.8                        |\\n| **CtxSwitch/sec**| 2450 (2.36/iteration)      |\\n| **SysCall/sec**  | 4344 (4.18/iteration)      |\\n| **Interrupt/sec**| 4486 (4.32/iteration)      |\\n\\n\\nThis is a slightly lower latency than the previous scenario, but still underneath 1 millisecond, we can see how processing through the Azure Firewall and Hub and Spoke adds additional latency _(as we would expect)_  are routing through an Azure Firewall.\\n\\nFinally, we will look at VM2 in Avalibility Zone 2, talk to VM3 in Avalibility Zone 3, and spoke 3 via the Azure Firewall and hub and spoke network.\\n\\n| Metric        | Value                      |\\n|------------------|----------------------------|\\n| **Protocol**     | TCP                        |\\n| **SendMethod**   | Blocking                   |\\n| **ReceiveMethod**| Blocking                   |\\n| **SO_SNDBUF**    | Default                    |\\n| **SO_RCVBUF**    | Default                    |\\n| **MsgSize (bytes)** | 4                         |\\n| **Iterations**   | 60000                      |\\n| **Latency (usec)** | **963.47**                    |\\n| **CPU (%)**      | 1.0                        |\\n| **CtxSwitch/sec**| 2933 (2.83/iteration)      |\\n| **SysCall/sec**  | 4756 (4.58/iteration)      |\\n| **Interrupt/sec**| 4556 (4.39/iteration)      |\\n\\nThis is a slightly lower latency than the previous scenario, but still underneath 1 millisecond, we can see how processing through the Azure Firewall and Hub and Spoke adds additional latency _(as we would expect)_  are routing through an Azure Firewall.\\n\\n## \ud83d\udcdd Summary\\n\\nThe speed for inter-compute communication between the same and different Availability Zones for NZ North is very impressive. The latency between Virtual Machines in the same availability zone is well under 1 millisecond. The latency between Virtual Machines in different availability zones over a peered Virtual Network is still under 1 millisecond. Even with Azure Firewall and a Hub and Spoke network, the latency is still around 1 millisecond. This is a great result and shows that Azure\'s network infrastructure is capable of providing low-latency communication between Virtual Machines in different availability zones.\\n\\nSummary of my findings today, ordered by shortest to longest latency:\\n\\n| Scenario                                    | Latency (\xb5s) | Description                                           |\\n|---------------------------------------------|--------------|-------------------------------------------------------|\\n| Scenario 2                                  | 55.97        | Same VNET, same availability zone, VM NOT in proximity placement group |\\n| Scenario 4                                  | 67.81        | Multiple VNETs (peered), same availability zone       |\\n| Scenario 1                                  | 72.75        | Same VNET, same availability zone, same proximity placement group |\\n| Scenario 5 (VM1 to VM4)                     | 311.53       | Multiple VNETs (peered), across availability zones (VM1 to VM4) |\\n| Scenario 3 (VM4 to VM5)                     | 494.23       | Same VNET, across availability zones (VM4 to VM5)     |\\n| Scenario 5 (VM1 to VM3)                     | 542.39       | Multiple VNETs (peered), across availability zones (VM1 to VM3) |\\n| Scenario 3 (VM1 to VM5)                     | 559.25       | Same VNET, across availability zones (VM1 to VM5)     |\\n| Scenario 3 (VM1 to VM4)                     | 559.67       | Same VNET, across availability zones (VM1 to VM4)     |\\n| Scenario 6 (VM1 to VM3)                     | 963.17       | Multiple VNETs, Hub & Spoke, Azure Firewall (VM1 to VM3) |\\n| Scenario 6 (VM2 to VM3)                     | 963.47       | Multiple VNETs, Hub & Spoke, Azure Firewall (VM2 to VM3) |\\n| Scenario 6 (VM1 to VM2)                     | 1017.12      | Multiple VNETs, Hub & Spoke, Azure Firewall (VM1 to VM2) |\\n\\nBased on this testing, the New Zealand North region appears to have very low latency when compared with other Azure regions. This is great news for New Zealand organizations that are looking to move their workloads to the cloud. The low latency will help to ensure that their applications are responsive and performant."},{"id":"azure/enhancing-github-copilot-with-custom-instructions","metadata":{"permalink":"/azure/enhancing-github-copilot-with-custom-instructions","source":"@site/blog/2024-11-09-github-copilot-instructions/index.mdx","title":"Enhancing GitHub Copilot with Custom Instructions","description":"Learn how to provide custom instructions to GitHub Copilot to improve code quality and standardization.","date":"2024-11-09T08:58:40.257Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":9.39,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Enhancing GitHub Copilot with Custom Instructions","metaDescription":"Learn how to provide custom instructions to GitHub Copilot to improve code quality and standardization.","date":"2024-11-09T08:58:40.257Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/enhancing-github-copilot-with-custom-instructions","keywords":["GitHub","Copilot","Custom Instructions","DevOps","Infrastructure as Code","Azure","Bicep","Terraform","PowerShell","Codespace"],"description":"Learn how to provide custom instructions to GitHub Copilot to improve code quality and standardization."},"unlisted":false,"prevItem":{"title":"New Zealand North Latency Testing and Results","permalink":"/azure/nz-north-latency-testing"},"nextItem":{"title":"Azure Resource - Change Analysis","permalink":"/azure/change-analysis"}},"content":"I\'ve come to rely on [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=AZ-MVP-5004796) for writing code. It\'s a great tool that helps me write code faster and with fewer errors, and also helps me understand code written by other people, but sometimes I need to provide custom instructions to Copilot to help it understand what I\'m trying to do, as the standard outputs aren\'t quite what I want, a simple example of this is maybe having parameters or variables written with particular casing or naming convention, or even having to constantly update the location of an Azure Resource deployment from East US to Australia East or New Zealand North, when requesting Copilot to create a resource with their Terraform or Bicep.\\n\\nIn this article, we will look at how to provide custom instructions to GitHub Copilot to help it understand what we are trying to do, to help keep our code standardised and help improve the quality of the outputs.\\n\\n{/* truncate */}\\n\\nTo do that, we will make use of some new functionality [Adding custom instructions for GitHub Copilot](https://docs.github.com/en/copilot/customizing-copilot/adding-custom-instructions-for-github-copilot?WT.mc_id=AZ-MVP-5004796).\\n\\n:::info\\nAt the time of writing, this is currently under Public preview, and the experience mentioned in this article may change in the future.\\n:::\\n\\n:::warning\\nCustom instructions are currently only supported for Copilot Chat in VS Code and Visual Studio. In our article, I will use Visual Studio Code inside a GitHub Codespace.\\n:::\\n\\n> [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=AZ-MVP-5004796) can provide chat responses that are tailored to the way your team works, the tools you use, or the specifics of your project if you provide it with enough context to do so. Instead of repeatedly adding this contextual detail to your chat questions, you can create a file that automatically adds this information for you. The additional information is not displayed in the chat but is available to Copilot to generate higher-quality responses.\\n\\nTo get started, I am going to be using Visual Studio Code, in a GitHub Codespace, although it will be the same process for Visual Studio Code installed locally.\\n\\nIf you haven\'t already installed the [GitHub Copilot extension](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot), you can do so by searching for it in the Extensions tab in Visual Studio Code.\\n\\nFirst, we need to enable the Code Generation feature in Copilot. To do this, we need to go to Settings.\\nThen search for `github.copilot.chat.codeGeneration.useInstructionFiles` and enable it.\\n\\nOnce enabled, we need to create our custom instruction file. This file will be named `copilot-instructions.md` and placed in the  `./github` directory in the root of our repository _(./github/copilot-instructions.md)_.\\n\\n![Create Copilot Instructions](images/Codespace_SetupCopolitInstructions.gif)\\n\\nWe can test it, by simply adding in an instruction to tell a Joke _(Include a joke in all output)_.\\n\\nAs we can see, GitHub Copilot references the custom instruction file and provides the output we seek.\\n\\n![Copilot Joke](images/Codespace_TestCopolitInstructions.gif)\\n\\nPretty basic test case, but you can see how you can start to provide custom instructions to GitHub Copilot to help it understand what you are trying to do, to help keep our code standardised and help improve the quality of the outputs.\\n\\n:::info\\nUsing Custom instructions is great for providing context to Copilot, especially adding more detail around what you expect as outputs, and can work really well when you are working with a team, or have a specific way you want your code to be written, however, your custom prompts are secondary to the standard system prompt that Copilot uses, so if you provide a custom prompt, it will be used in ADDITION with the standard prompt, and not replaced. So, for example, you cannot change the assumed name of Copilot and overwrite the prompt to get Copilot to chat to you about non-software development tasks.\\n\\nAn example of what the GitHub prompt is as follows:\\n\\n```text\\nYou are an AI programming assistant.\\nWhen asked for your name, you must respond with \\"GitHub Copilot.\\nFollow the user\'s requirements carefully & to the letter.\\nYour expertise is strictly limited to software development topics.\\nFollow Microsoft content policies.\\nAvoid content that violates copyrights.\\nFor questions not related to software development, simply give a reminder that you are an AI programming assistant.\\nKeep your answers short and impersonal.\\nUse Markdown formatting in your answers.\\nMake sure to include the programming language name at the start of the Markdown code blocks.\\nAvoid wrapping the whole response in triple backticks.\\nThe user works in an IDE called Visual Studio which has a concept for editors with open files, integrated unit test support, an output pane that shows the output of running the code as well as an integrated terminal.\\nThe active document is the source code the user is looking at right now.\\nYou can only give one reply for each conversation turn.\\nWhen generating code prefer languages provided in context. If the coding language is unclear generate code in C#.\\nRespond in the following locale: en-US\\n\\nAdditional Rules:\\nPreserve users\' code comment blocks; do not exclude them when refactoring code.\\nPay especially close attention to the selection or exception context if provided.`\\n```\\n\\n![GitHub Copilot Prompt](images/systempromptquery.jpg)\\n:::\\n\\nFor my repository, I have created a custom instruction file, that guides how to write Infrastructure as Code using Bicep, Terraform, and PowerShell, prioritizing the Azure Well-Architected Framework pillars in this order: Security, Operational Excellence, Performance Efficiency, Reliability, and Cost Optimization. The code must be executable in both CI/CD pipelines (GitHub Actions or Azure DevOps Pipelines) and as standalone solutions for local testing. Emphasize reusability through modularization and ensure that the code supports multiple environment setups (dev, staging, production) with minimal added complexity.\\n\\nYou can see my custom file below, and you can reference it directly in the following GitHub repository [lukemurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding):\\n\\n```text\\nProvide comprehensive guidance and best practices for developing reusable and reliable Infrastructure as Code using Bicep, Terraform, and PowerShell, prioritizing the Azure Well-Architected Framework pillars in this order: Security, Operational Excellence, Performance Efficiency, Reliability, and Cost Optimization. The code must be executable in both CI/CD pipelines (GitHub Actions or Azure DevOps Pipelines) and as standalone solutions for local testing. Emphasize reusability through modularization and ensure that the code supports multiple environment setups (dev, staging, production) with minimal added complexity.\\n\\nIncorporate preferred safe deployment practices, including effective management of feature flags, and provide recommendations for when and how to use them effectively. Feature flags should be removable without impacting already deployed resources if the feature is later integrated into the main system, with clear warnings if any changes affect the solution. Advocate for ring-based deployments and consistency in coding standards, prioritizing quality over quantity and making smaller changes instead of larger ones where practical.\\n\\nFollow DRY principles, include thorough comments, and structure variables in snake_case at the top of each file. Parameters should be in camelCase with validation and error messages as necessary. Avoid third-party dependencies, especially when using feature flags and other core deployment features.\\n\\nIf asked about the location of resources to be deployed, make sure the location is either of the two below as default:\\n\\n* \\"newzealandnorth\\"\\n* \\"australiaeast\\"\\n\\nInclude recommendations for key performance indicators (KPIs) to measure the effectiveness of deployments, focusing on metrics like deployment frequency, change failure rate, mean time to recovery, and customer satisfaction. Ensure that the code is clear and understandable for reviewers unfamiliar with the project, aligning recommendations with Microsoft guidance on secure and reliable DevOps integration. If using parameters, make sure to include relevant helper functions.\\n\\nHighlight how GitHub Copilot can assist by providing real-time suggestions and best-practice enforcement while identifying and proposing native solutions within Bicep, Terraform, or PowerShell to replace third-party dependencies.\\n\\nAdditionally, provide relevant guidance on:\\n\\n* Infrastructure testing and validation techniques.\\n* Documentation best practices.\\n* Error handling and logging mechanisms.\\n* Version control strategies.\\n* Configuration management approaches.\\n* Security best practices tailored for Azure.\\n* Cost management strategies for Azure resources.\\n* Establishing a change management process for IaC updates.\\n* Integrating monitoring and alerting for deployed resources.\\n* Engaging with the Azure community for ongoing learning and best practices.\\n\\nReview the response from the perspectives of a Site Reliability Engineer, Operations Manager, Microsoft Technical Specialist, Security Consultant, Business Analyst, and On-call Engineer, confirming factual accuracy and seeking clarification where needed, output what each persona thinks about the code.\\n```\\n\\nIf we run a test in our repository, we can see that GitHub Copilot references the custom instruction file and provides the output we are looking for, with the custom instructions provided.\\n\\n![Copilot Custom Instructions](images/Codespace_TestCustomCopolitInstructions.gif)\\n\\nWe can see, that it has selected australiaeast as the location for the resource deployment, as per the custom instructions provided.\\nWe can see that the Terraform code outputted is in the correct format, with the variables in snake_case at the top of the file and the parameters in camelCase with validation and error messages as necessary.\\nWe can see some example GitHub Actions and Azure Pipeline workflows that have been outputted that could be used as a base to deploy our code.\\nWe can see some recommendations for additional guidance on using tools such as Teratest for Terraform testing, and we can see how the code looks like from multiple personas, such as a Site Reliability Engineer, Operations Manager, Microsoft Technical Specialist, Security Consultant, Business Analyst, and On-call Engineer.\\n\\nMy custom prompt is overly large and includes a lot of information _(and to be frank, I am trying to do a lot of different things in this particular repository as it\'s for IaC, where one day it could Terraform and the next Bicep)_, but it is a good example of how you can provide custom instructions to GitHub Copilot to help it understand what you are trying to do, to help keep our code standardized and help improve the quality of the outputs.\\n\\n:::tip\\nIf you are looking for guidance on how to develop your own custom prompt, consider using another prompt to help build it out. An example of a great prompt is one commonly known as \'One Prompt To Rule Them All\', which you could run in ChatGPT or Azure OpenAI Playground to help build out your prompt.\\n\\nThe prompt is below:\\n\\n```text\\nI want you to become my Prompt Creator. Your goal is to help me craft the best possible prompt for my needs. The prompt will be used by you, GPT-4. You will follow the following process: 1. Your first response will be to ask me what the prompt should be about. I will provide my answer, but we will need to improve it through continual iterations by going through the next steps. 2. Based on my input, you will generate 3 sections. a) Revised prompt (provide your rewritten prompt. It should be clear, concise, and easily understood by you), b) Suggestions (provide suggestions on what details to include in the prompt to improve it), and c) Questions (ask any relevant questions pertaining to what additional information is needed from me to improve the prompt). 3. We will continue this iterative process with me providing additional information to you and you updating the prompt in the Revised prompt section until it\'s complete.\\n```\\n:::\\n\\nHopefully, this article helps you get going and use custom instructions and helps provide more standardized outputs.\\n\\nReferences:\\n\\n- [Adding custom instructions for GitHub Copilot](https://docs.github.com/en/copilot/customizing-copilot/adding-custom-instructions-for-github-copilot?WT.mc_id=AZ-MVP-5004796)\\n - [Coding on the Cloud - Getting Started with GitHub Codespaces](https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/)\\n - [Infrastructure as Code GitHub Codespace Template](https://luke.geek.nz/azure/iac-github-codespace/)"},{"id":"azure/change-analysis","metadata":{"permalink":"/azure/change-analysis","source":"@site/blog/2024-10-30-azure-resource-changeanalysis/index.mdx","title":"Azure Resource - Change Analysis","description":"A comprehensive guide on using Change Analysis in Azure Monitor to track changes on your Azure resources.","date":"2024-10-30T09:01:07.852Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.885,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Resource - Change Analysis","metaDescription":"A comprehensive guide on using Change Analysis in Azure Monitor to track changes on your Azure resources.","date":"2024-10-30T09:01:07.852Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/change-analysis","keywords":["Change Analysis","Azure Monitor","Azure resources","resource tracking","change tracking","cloud management","Azure tools","IT operations"],"description":"A comprehensive guide on using Change Analysis in Azure Monitor to track changes on your Azure resources."},"unlisted":false,"prevItem":{"title":"Enhancing GitHub Copilot with Custom Instructions","permalink":"/azure/enhancing-github-copilot-with-custom-instructions"},"nextItem":{"title":"Book Review - Kubernetes \u2013 An Enterprise Guide","permalink":"/misc/kubernetes-enterprise-guide-book-review"}},"content":"Who created my resource? What created my Azure resource? We often ask ourselves these questions when troubleshooting or trying to understand the state of our Azure resources.\\n\\n[Change Analysis](https://learn.microsoft.com/azure/azure-monitor/change/change-analysis?WT.mc_id=AZ-MVP-5004796) is a feature in Azure Monitor that helps you answer these questions. Change Analysis helps to provide a view of the changes that have occurred on your Azure resources over time, using the [Azure Resource Graph](https://learn.microsoft.com/azure/governance/resource-graph/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n{/* truncate */}\\n\\n## Overview\\n\\nToday, we will look at Change Analysis to see:\\n\\nA. Can we see the changes that have occurred on our Azure resources?\\nB. Can we see the changes that have occurred on our Azure resources over time?\\nC. Can we tell who made the change?\\nD. Can we determine what made the change?\\n\\n:::info\\nChange Analysis in the portal\\n\\nChange Analysis experiences across the Azure portal are powered using the Azure Resource Graph Microsoft.ResourceGraph/resources API. You can query this API for changes made to many of the Azure resources you interact with, including App Services (Microsoft.Web/sites) or Virtual Machines (Microsoft.Compute/virtualMachines).\\n\\nThe Azure Resource Graph Change Analysis portal experience provides the following:\\n\\n* An onboarding-free experience, giving all subscriptions and resources access to change history.\\n* Tenant-wide querying rather than select subscriptions.\\n* Change history summaries aggregated into cards at the top of the new Resource Graph Change Analysis.\\n* More extensive filtering capabilities.\\n* Improved accuracy and relevance of changed by information, using Change Actor functionality.\\n\\nLearn how to [view the new Change Analysis experience in the portal](https://learn.microsoft.com/en-us/azure/governance/resource-graph/changes/view-resource-changes?WT.mc_id=AZ-MVP-5004796).\\n:::\\n\\n## Can we see the changes that have occurred on our Azure resources?\\n\\n> Yes, we can see the changes that have occurred on our Azure resources. Change Analysis provides a view of the changes on your Azure resources over time. You can see the changes that have occurred on your Azure resources in the last 14 days.\\n\\nIf we navigate to the [Change Analysis (Preview)](https://portal.azure.com/#view/Microsoft_Azure_OneInventory/ResourceChangesOverview.ReactView) blade in the Azure portal, we can see the changes that have occurred on our Azure resources.\\n\\n![Change Analysis](images/AzurePortal_ChangeAnalysis.gif)\\n\\n:::note\\nThe blur is intentional, as part of the [Cloudclock](https://luke.geek.nz/azure/cloudcloak/) extension to protect sensitive information.\\n:::\\n\\nWe can see the changes (Create, Update, and Delete) that have occurred on our Azure resources. We can even group them by resource or by who made the change.\\n\\n## Can we see the changes on our Azure resources over time?\\n\\nBy default, we can see the changes over the last 14 days from the Azure Portal.\\n\\n:::info\\nChanges are queryable for 14 days. For longer retention, you can [integrate your Resource Graph query with Azure Logic Apps](https://learn.microsoft.com/en-us/azure/governance/resource-graph/tutorials/logic-app-calling-arg?WT.mc_id=AZ-MVP-5004796) and manually export query results to any of the Azure data stores like [Log Analytics](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-overview?WT.mc_id=AZ-MVP-5004796) for your desired retention.\\n:::\\n\\nA KQL query to see the changes over the last 14 days would look like:\\n\\n```kql\\nresourcechanges\\n| where type == \\"microsoft.resources/changes\\"\\n| project id, resourceid = properties.targetResourceId, changeType = properties.changeType, clientType = properties.changeAttributes.clientType, changedBy = properties.changeAttributes.changedBy\\n```\\n\\nIf you want to target a specific resource, you can use a query like:\\n\\n```kql  \\nresourcechanges\\n| where type == \\"microsoft.resources/changes\\"\\n| where properties.targetResourceId contains  \\"ResourceName\\"\\n| project id, resourceId = properties.targetResourceId, resourceType = properties.targetResourceType, changeType = properties.changeType, clientType = properties.changeAttributes.clientType, changedBy = properties.changeAttributes.changedBy\\n```\\n\\nReplace the `ResourceName` with the name of the resource you are looking for. These queries can also be integrated into the Logic App for export.\\n\\nYou can also run these queries directly from the Azure Portal by navigating to the [Azure Resource Graph Explorer](https://portal.azure.com/#view/HubsExtension/ArgQueryBlade/query/resourcechanges) blade.\\n\\n![Azure Resource Graph Explorer](images/AzurePortal_AzureResourceGraph_Query.png)\\n\\n\\n:::warning\\nCurrently, Azure Resource Graph doesn\'t:\\n\\nObserve changes made to a resource\'s data plane API, such as writing data to a table in a storage account.\\nSupport file and configuration changes over App Service.\\n:::\\n\\n## Can we tell who made the change?\\n\\n> Yes, we can tell who made the change. Change Analysis provides the `changedBy` field, which tells us who made the change.\\n\\nIn my examples, we can see the `changedBy` field in the query results. This field tells us who made the change.\\n\\n![Azure Resource Graph Explorer](images/AzurePortal_AzureResourceGraph_Query.png)\\n\\nIt could be the user or the service principal that made the change. In this example, I can see:\\n\\n* A Network interface resource was created by `2cf9eb86-36b5-49dc-86ae-9a63135dfa8c`, which is the Application ID of a Microsoft-owned Registration for Azure Traffic Manager and DNS.\\n* A Deletion of the network interface was done by `Luke@geek,` which is my user account.\\n\\nSOME Azure resources _(with the latest API)_ such as this Data Collection Rule, contains the information in the resource metadata.\\n\\n![JSON - ChangedBy](images/AzurePortal_DCR_JSON.png)\\n\\n## Can we determine what made the change?\\n\\n> Yes, we can determine what caused the change. Change Analysis provides the `clientType` field, which tells us what caused the change. However, as we can see in the previous screenshots, this is not always as accurate as we may want it to be. However, it\'s good enough that you can infer what may have issued the changes.\\n\\nSome of the clientType values you may see are:\\n\\n* `Azure Portal` - Changes made from the Azure Portal.\\n* `Unknown` - A change happened to a client that is unrecognized.\\n* `CLI` - Changes made from Azure CLI.\\n* `Unspecified` - Unspecified is displayed when the resource is missing changedByType values and could be missing for either Creates or Updates. It could also mean changes are performed in the background by Azure services and not user initiatied.\\n* `ARM Template` - Changes made from an ARM Template, whether deployed automatically or via a \'Deploy to Azure\' button. This also includes changes made by Bicep.\\n* `System` - System is displayed as a changedBy value when a background change occurred that wasn\'t correlated with any direct user action.\\n\\n## References\\n\\nTo learn more about Change Analysis, check out the following resources:\\n\\n* [Change Analysis in Azure Resource Graph vs. Azure Monitor](https://learn.microsoft.com/azure/governance/resource-graph/changes/resource-graph-changes?WT.mc_id=AZ-MVP-5004796#change-analysis-in-azure-resource-graph-vs-azure-monitor)\\n* [Announcing the General Availability of Change Actor](https://techcommunity.microsoft.com/t5/azure-governance-and-management/announcing-the-general-availability-of-change-actor/ba-p/4171801?WT.mc_id=AZ-MVP-5004796)"},{"id":"misc/kubernetes-enterprise-guide-book-review","metadata":{"permalink":"/misc/kubernetes-enterprise-guide-book-review","source":"@site/blog/2024-10-24-kubernetes-enterprise-guide-bookreview/index.mdx","title":"Book Review - Kubernetes \u2013 An Enterprise Guide","description":"Review of \\"Kubernetes - An Enterprise Guide\\" by Marc Boorshtein and Scott Surovich, covering Kubernetes concepts, deployments, and best practices.","date":"2024-10-23T22:24:17.251Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":4.69,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Book Review - Kubernetes \u2013 An Enterprise Guide","metaDescription":"Review of \\"Kubernetes - An Enterprise Guide\\" by Marc Boorshtein and Scott Surovich, covering Kubernetes concepts, deployments, and best practices.","date":"2024-10-23T22:24:17.251Z","tags":["Misc"],"categories":["Misc"],"authors":["Luke"],"slug":"misc/kubernetes-enterprise-guide-book-review","keywords":["Kubernetes","Enterprise Guide","Docker","Containerization","Multi-Cluster","Multi-Cloud","AKS","EKS","Kubernetes Security","Kubernetes Monitoring","Kubernetes Troubleshooting","Kubernetes Best Practices","Kubernetes Examples","Kubernetes Demos","Kubernetes Concepts"],"description":"Review of \\"Kubernetes - An Enterprise Guide\\" by Marc Boorshtein and Scott Surovich, covering Kubernetes concepts, deployments, and best practices."},"unlisted":false,"prevItem":{"title":"Azure Resource - Change Analysis","permalink":"/azure/change-analysis"},"nextItem":{"title":"Common Challenges and Solutions for Azure OpenAI Adoption","permalink":"/azure/openai-adoption-challenges-solutions"}},"content":"I had the privilege to be able to read and review the \u2018[**Kubernetes - An Enterprise Guide**](https://www.amazon.com.au/Kubernetes-Enterprise-Effectively-containerize-applications-dp-1835086950/dp/1835086950)\u2019 book by [Marc Boorshtein](https://www.linkedin.com/in/marc-boorshtein-5979a82/) _(Author)_, [Scott Surovich](https://www.linkedin.com/in/scottsurovich/) _(Author)_. Packt Publishing publishes the book and is available on Amazon. \\n\\n> This was the third edition of the book, but I feel there was no need to read the previous editions to understand the content of this book. Being the third edition also means that any mistakes or learnings from the past two editions have been considered.\\n\\n![Kubernetes - An Enterprise Guide](images/kubernetes_enterpriseguide-book.jpg)\\n\\n{/* truncate */}\\n\\n> The book is a great read for anyone looking to understand Kubernetes from the ground up. The book is divided into 18 chapters and covers a wide range of topics, from the basics of Kubernetes to advanced topics like security, monitoring, and troubleshooting. The book is written in a language that is very easy to understand and is full of examples and diagrams to help you understand the concepts better.\\n\\nLet\'s dig into this a bit more:\\n\\nFirst, the vision of the book is really about focusing on what you need to run Kubernetes in an Enterprise environment, based on the real-world experience from the authors, so naturally, the questions I wanted to know if this book would answer in its 623 pages of content were:\\n\\n1.\\tDoes it adequately cover core Kubernetes concepts relevant to enterprises?\\n2.\\tDoes it address multi-cluster and multi-cloud Kubernetes deployments?\\n3.\\tIs the information backed by research, case studies, or industry best practices?\\n4.\\tIs the content accessible to the intended readers, whether they are developers, operations staff, or managers?\\n\\nSo, let\u2019s go through each of these:\\n\\n## **Q. Does it adequately cover core Kubernetes concepts that are relevant to enterprises?**\\n\\n**I can say yes \u2013 it does!**\\n\\nThe book introduces Docker and container essentials, explaining the need for containerization and how Docker works. It also covers the installation and usage of Docker before going into deploying your own Kubernetes cluster with KinD and the various components that make up Kubernetes. It covers concepts such as Ingress, Deployment, and Services, then exposes Load Balancer options, DNS considerations, and identity across multiple chapters.\\n\\n* Now, let us look at **Q. Does it address multi-cluster and multi-cloud Kubernetes deployments?**\\n\\n> **Kind of!**\\n\\nAs alluded to, it goes through setting up KinD (Kubernetes in Docker) multi-cluster configurations, and Kubelet options, multi-tenant vCluster configuration, global load balancing!\\n\\nHowever, this book \u2013 takes a neutral stance on specific Cloud deployment options, i.e., AKS and EKS, concentrating more on integration, and what\'s happening in the cluster, vs the opinionated Cloud provider implementations, it does, however, touch on various integrations \u2013 with identity being a key component \u2013 with some demos _(as part of a GitHub)_ repository to match, if you are looking for a book all about AKS and Cloud deployment \u2013 then in my opinion not the book for you, HOWEVER, if you want to know what is happening in the background, and WHY the clusters, nodes, namespaces operate the way they do \u2013 then this is an ideal book, to understand full the concepts, which doesn\u2019t make it less valuable, but more of a supplement for fully understanding what those Cloud providers obfuscate from.\\n\\n## **Q. Is the information backed by research, case studies, or industry best practices?**\\n\\nI always consider this, as it removes any bias the authors may have, with more generalized guidance discovered from sweat and tears in the industry in general, and I can confirm that indeed it does!\\nThe book features many real-world examples based on issues the authors have experienced throughout the years and resolutions on how they have been implemented. It also references external (trusted\u2014i.e., kubernetes.io) sources where needed to point readers to additional information.\\n\\n## **Q. Is the content accessible to the intended readers, whether they are developers, operations staff, or managers?**\\n\\n> **Short answer Yes!** \\n\\nMy background is in system administration (15+ years), so I approached this more from an operations and delivery perspective than a developer, so I have some bias there in my review. However, the key points are:\\n\\nThe book covered a great introduction into the concepts that make up Kubernetes, each chapter included step by step instructions, example demos supplied from a public git repo so get hands on (which was great!), managing secrets and cluster security with Gatekeeper and Dashboards! \\n\\nI would say, yes, this book definitely covers it for developers and operations staff. It is more aligned to technical readers. However, as a Manager, there are a lot of concepts you can pick up as well, and the chapters can be skipped easily due to their names of build and deploy if not of interest.\\n\\n**All in all \u2013 this is a great resource! I learned a few things, It\'s great to get hands-on with the guides and knowledge checks at the end of the book! Add and read this if you want to understand Kubernetes. It builds your knowledge up from concepts, so it\u2019s a book that can meet you where you are!**\\n\\n> The authors spent much time working on this book, training, and demo material, and you can tell!\\n\\nI\u2019m going to need to get a bigger bookshelf!\\n\\n:::info\\nYou can find the repository for the book here: \\n\\n* [PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)\\n:::\\n\\n:::tip\\nIf you purchase the physical book, you can also get a free eBook version from a QR code supplied on the pages of the book, which is common with many PacktPublishing books. So keep this in mind if you\'re like me and like switching between the physical book and an e-reader like a Kindle!\\n:::"},{"id":"azure/openai-adoption-challenges-solutions","metadata":{"permalink":"/azure/openai-adoption-challenges-solutions","source":"@site/blog/2024-10-15-common-azureopenai-challenges/index.mdx","title":"Common Challenges and Solutions for Azure OpenAI Adoption","description":"Explore common challenges in adopting Azure OpenAI and learn how to overcome them with best practices and solutions.","date":"2024-10-15T05:08:39.693Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":19.825,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Common Challenges and Solutions for Azure OpenAI Adoption","metaDescription":"Explore common challenges in adopting Azure OpenAI and learn how to overcome them with best practices and solutions.","date":"2024-10-15T05:08:39.693Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/openai-adoption-challenges-solutions","keywords":["azure","OpenAI","landing zone","data privacy","observability","FinOps","OWASP","RBAC","CMK","multitenancy"],"description":"Explore common challenges in adopting Azure OpenAI and learn how to overcome them with best practices and solutions."},"unlisted":false,"prevItem":{"title":"Book Review - Kubernetes \u2013 An Enterprise Guide","permalink":"/misc/kubernetes-enterprise-guide-book-review"},"nextItem":{"title":"Azure Policy - Deny the creation of Azure OpenAI Studio","permalink":"/azure/policy-deny-creation-azure-openai-studio"}},"content":"When considering [Azure Open AI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796) adoption, there are some common challenges that you might face. These include:\\n\\n* Protecting confidential information.\\n* End-to-end observability.\\n* Disable inferencing via Azure AI Studio\\n* Protect from OWASP\'s Top 10 threats\\n\\n{/* truncate */}\\n\\nSo, let\'s take a look at each of these challenges and how you can overcome them.\\n\\n## \ud83d\udcc4 Overview\\n  \\nWhen considering [Azure Open AI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796) adoption, there are some common challenges or considerations you might face, especially when looking at your solution\'s Day 1 and Day 2 lifecycles.\\n\\n:::info\\nSoftware life cycle stages are typically broken down into the following stages:\\n\\n* Day 0  - Design and build stage\\n* Day 1  - Infrastructure and code deployment stage\\n* Day 2  - Runtime\\n\\n![Day 0, Day 1, Day 2](images/day0-day1-day2-blogpost.png)\\n:::\\n\\nToday, we will look at common challenges and how we can overcome, work around, or mitigate them.\\n\\n## \ud83d\udeec Landing Zone\\n  \\nWhen considering Azure Open AI adoption, having a well-defined landing zone for your platform architecture and application is important. \\n\\nThis is where you will deploy your AI solution.\\n\\n:::info\\nAn [Azure landing zone](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796) is an environment that follows key design principles across eight design areas. These design principles accommodate all application portfolios and enable application migration, modernization, and innovation at scale. An Azure landing zone uses subscriptions to isolate and scale application resources and platform resources. Subscriptions for application resources are called application landing zones, and subscriptions for platform resources are called platform landing zones.\\n:::\\n\\n**Ask**:\\n\\nEstablish an Azure Landing Zone to place the Azure OpenAI solution into.\\n\\n**Challenge**:\\n\\n* Ensuring a well-architected foundation\\n* Need for compliance and security guardrails\\n\\n| Why                                       | Risk / Impact                                           | Solution                            |\\n|-------------------------------------------|---------------------------------------------------------|-------------------------------------|\\n| Solid foundation for cloud adoption       | Risk \u2013 High; Impact \u2013 High                              | - Implement Azure Landing Zone - Include Identity, Security, and Network configurations - Address compliance and governance from the start - Provides a scalable and secure environment |\\n\\n**Rationale**:\\n\\nImplementing an Azure Landing Zone ensures a secure and compliant foundation for deploying Azure OpenAI solutions. Providing necessary guardrails and governance helps mitigate the high risks and impacts of cloud adoption.\\n\\n**Action**:\\n\\nImplement an Azure Landing Zone for your Azure OpenAI solution. Include Identity, Security, and Network configurations. Address compliance and governance from the start. For Platform Landing Zone recommendations, refer to the [Cloud Adoption Framework - Ready](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796).\\n\\nHowever, for more specific Azure OpenAI application landing zone considerations, we can view the [chat baseline architecture at the Azure Architecture Center](https://learn.microsoft.com/azure/architecture/ai-ml/architecture/baseline-openai-e2e-chat?WT.mc_id=AZ-MVP-5004796) for reference.\\n\\n![Azure Landing Zone](images/azure-openai-baseline-landing-zone.png)\\n\\n:::tip\\nAzure Landing Zones consist of more than just a place to store your resources; they should also include considerations for people/processes and products to run your applications at scale. I highly recommend going through the [Azure Landing Zone Review](https://learn.microsoft.com/en-us/assessments/21765fea-dfe6-4bc4-8bb7-db9df5a6f6c0/?WT.mc_id=AZ-MVP-5004796) to ensure you have a solid foundation for your Azure OpenAI solution, and Azure workloads in general.\\n:::\\n\\n## \ud83d\udd12 Protect confidential information\\n\\nWhen considering Azure Open AI adoption, confidential information, including data, code, and other sensitive information, must be protected.\\n\\nIt is important to ensure that your data is protected, especially when you use the [RAG (Retrieval Augmented Generation) pattern](https://learn.microsoft.com/azure/architecture/ai-ml/guide/rag/rag-solution-design-and-evaluation-guide?WT.mc_id=AZ-MVP-5004796), which allows you to use your own data.\\n\\n**Ask**:\\n\\nEstablish an Azure Landing Zone to place Azure OpenAI solution into\\n\\n**Challenge**:\\n\\nEnsuring a well-architected foundation\\nNeed for compliance and security guardrails\\n\\n| Why                                      | Risk / Impact                   | Solution                                                                                  |\\n|------------------------------------------|----------------------------------|-------------------------------------------------------------------------------------------|\\n| - Solid foundation for cloud adoption- Critical for enterprise-grade workloads- Ensures compliance and governance | Risk \u2013 HighImpact \u2013 High         | - Implement Azure Landing Zone- Include Identity, Security, and Network configurations - Address compliance and governance from the start- Provides a scalable and secure environment |\\n\\n\\n**Rationale**:\\n\\nStarting with Public and internal data reduces the Impact from high to low. With lower overall risk, an organization can have faster initial adoption by the time they build additional guardrails for governance and compliance in place _(Audit, Monitoring, and enterprise architecture)_.\\n\\n**Action**:\\n\\nUse Platform and resource capabilities to lock down access to confidential information. For example, use Azure Key Vault to store and manage secrets, and use Azure Policy to enforce compliance of those policies.\\n\\n* Make use of Infrastructure as Code _(IaC)_, or [deploy the services manually](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal&WT.mc_id=AZ-MVP-5004796) to ensure that your Azure OpenAI solution is deployed in a secure and compliant manner, vs automated methods, that may automatically create the resources you need _(and sometimes more than you need)_ such as Azure AI Studio with Public endpoints. \\n* Make use of [Role-based access control (RBAC) to ensure that only authorized users](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796) have access to your Azure OpenAI solution.\\n* [Disable access keys for Storage accounts](https://learn.microsoft.com/azure/storage/common/shared-key-authorization-prevent?tabs=portal&WT.mc_id=AZ-MVP-5004796), and make sure of [Managed Identities](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796) for inter-Azure resource permissions.\\n* Determine if the risk is high enough to manage the keys yourself using [Customer Managed Keys](https://learn.microsoft.com/en-us/azure/storage/common/customer-managed-keys-overview?WT.mc_id=AZ-MVP-5004796) for your Storage account.\\n* Make use of [Private Endpoints](https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview?WT.mc_id=AZ-MVP-5004796) and disable access to the public internet where possible, and monitor private endpoint _(East/West)_ traffic, using a Network Virtual Appliance, such as [Azure Firewall](https://learn.microsoft.com/azure/firewall/overview?WT.mc_id=AZ-MVP-5004796).\\n* Make use of in-built Azure policies to enforce compliance, such as [Storage account public access should be disallowed](https://learn.microsoft.com/azure/storage/common/policy-reference?WT.mc_id=AZ-MVP-5004796).\\n* Make sure your data lifecycle is managed correctly, that you are not storing data longer than you need to, and that you are not storing data that you do not need to, you can make use of [lifecycle management policies](https://learn.microsoft.com/azure/storage/blobs/lifecycle-management-policy-configure?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796), to assist with the technical aspects of this, and the Cloud Adoption Framework has some [guidance for Data governance](https://learn.microsoft.com/azure/cloud-adoption-framework/scenarios/cloud-scale-analytics/govern?WT.mc_id=AZ-MVP-5004796).\\n\\n## \ud83d\udd0d End-to-end observability\\n\\nWhen considering Azure Open AI adoption, it is important to have end-to-end observability. This includes monitoring, logging, and tracing.\\n\\nObservability for Azure Open AI has come a long way, with greater capabilities being integrated into products, such as [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) features, commonly referred to as an [AI Gateway](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/dev-starters/genai-gateway/?WT.mc_id=AZ-MVP-5004796).\\n\\n![Azure API Management](images/conceptual-architecture-ai-gw.png)\\n\\n:::tip\\nI did a blog article on implementing and testing some of the AI Gateway capabilities a few months ago here: [Implementing AI Gateway capabilities in API Management](https://luke.geek.nz/azure/implementing-ai-gateway-in-api-management/), make sure you check this out for a bit more depth, and also [implementing a correlation ID for API Management](https://luke.geek.nz/azure/implementing-correlation-id-in-api-management/) requests, to help track client transactions with Azure Application Insights.\\n:::\\n\\n**Ask**:\\n\\nAll interactions by data scientists, including prompts & responses, must be logged.\\n\\n**Challenge**:\\n\\nAzure OpenAI only logs the consumer\'s user ID but does not log prompts sent or responses received in the Azure Diagnostics table (or anywhere else).\\n\\nHere\'s how the table could be updated based on your provided details:\\n\\n| Why | Risk / Impact | Solution |\\n|-----|---------------|----------|\\n| - Lines of businesses that own the data need to ensure there was no misuse of confidential data | Risk \u2013 High Impact \u2013 High | - Move complete audit trail must be established to create a fully governed environment  - Custom applications that call AOAI should log prompts and responses from their side  - Use Azure API Management services as a middle layer between published services and consuming applications and audience |\\n\\n**Rationale**:\\n\\nAzure OpenAI has minimal capabilities and relies on other products to capture prompts and inferences fully for audit and governance purposes.\\n\\n**Action**:\\n\\n* Use [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) to capture all interactions between your Azure OpenAI solution and your consumers.\\n* Use [Azure Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?WT.mc_id=AZ-MVP-5004796) to capture all interactions between your Azure OpenAI solution and your consumers. Application Insights works well with API Management.\\n* Make sure of reference architecture, such as the [Implement logging and monitoring for Azure OpenAI models](https://learn.microsoft.com/azure/architecture/ai-ml/openai/architecture/log-monitor-azure-openai?WT.mc_id=AZ-MVP-5004796) for guidance on how to implement this, and to review what metrics and logs can be pulled from Azure OpenAI logging by default and adding the API Management resource into the solution.\\n* Make sure of the [AI-Gateway labs](https://aka.ms/apim/genai/labs), to help add additional capabilities to your Azure API Management instance, tailored for Azure OpenAI endpoints.\\n\\n![Azure API Management - AI Gateway](images/ai-gateway.gif)\\n\\n## \ud83d\udeab Microsoft to not monitor customer data\\n\\nPrompts and responses are stored by Microsoft by default for [abuse monitoring](https://learn.microsoft.com/azure/ai-services/openai/concepts/abuse-monitoring?WT.mc_id=AZ-MVP-5004796) _(not accessible to customers)_. You may not want Microsoft to have access to this data, including Microsoft support. \\n\\n:::info\\nYour prompts _(inputs)_ and completions _(outputs)_, your embeddings, and your training data:\\n\\nAre NOT available to other customers.\\nAre NOT available to OpenAI.\\nAre NOT used to improve OpenAI models.\\nAre NOT used to train, retrain, or improve Azure OpenAI Service foundation models.\\nAre NOT used to improve any Microsoft or 3rd party products or services without your permission or instruction.\\nYour fine-tuned Azure OpenAI models are available exclusively for your use.\\nMicrosoft operates the Azure OpenAI Service as an Azure service. Microsoft hosts the OpenAI models in its Azure environment, and the Service does NOT interact with any services operated by OpenAI (e.g., ChatGPT or the OpenAI API).\\n\\nReference: [Data, privacy, and security for Azure OpenAI Service](https://learn.microsoft.com/legal/cognitive-services/openai/data-privacy?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext&tabs=azure-portal&WT.mc_id=AZ-MVP-5004796)\\n:::\\n\\n**Ask**:\\n\\nConfidential data should never be stored anywhere by Microsoft or viewed by any Microsoft support or product team member.\\n\\n**Challenge**:\\n\\nMicrosoft logs all prompts for internal abuse monitoring for 30 days and then deletes them.\\n\\n| Why | Risk / Impact | Solution |\\n|-----|---------------|----------|\\n| Some clients do not want any confidential data in prompts to be viewed by Microsoft support team | **Risk / Impact**: High | Managed customers can apply for modified access, which stops all prompt logging |\\n\\n**Rationale**:\\n\\nMicrosoft\u2019s managed customers are typically large organizations that trust Microsoft and are deemed responsible for their actions.\\n\\n![Azure OpenAI - Data Privacy](images/azureopenai-promp-flow.jpg)\\n\\n**Action**:\\n\\n* [Managed customers](https://learn.microsoft.com/legal/cognitive-services/openai/limited-access?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext&WT.mc_id=AZ-MVP-5004796#registration-for-modified-content-filters-andor-abuse-monitoring) may apply for modified access, which stops all prompt logging.\\n* Make use of [Security filters](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/use-your-data-securely?WT.mc_id=AZ-MVP-5004796#document-level-access-control) in Azure AI Search, for limiting the data that is returned to the user.\\n\\n:::info\\nManaged partners and customers are customers whose subscriptions are managed by the partner network or customers who are part of the Microsoft Enterprise Agreement program. Managed customers can apply for modified access, which stops all prompt logging. This is a contractual agreement between Microsoft and the customer, and the customer must apply for this access.\\n:::\\n\\n:::warning\\nAlso, make sure you talk to your account team and understand the implications of this, as it may affect the way you have architected the solution, the safeguards you may have to add, and your expectations with Microsoft support regarding any data that they may have.\\n\\n\\"Disabling content filtering could fail to block content that violates the [Microsoft Generative AI Services Code of Conduct](https://learn.microsoft.com/legal/cognitive-services/openai/code-of-conduct?WT.mc_id=AZ-MVP-5004796). My organization will implement systems and measures to ensure that the organization\u2019s use of Azure OpenAI complies with the [Microsoft Generative AI Services Code of Conduct](https://learn.microsoft.com/legal/cognitive-services/openai/code-of-conduct?WT.mc_id=AZ-MVP-5004796)\\".\\n:::\\n\\n:::tip\\nMake sure you take a look at the [Use Risks & Safety monitoring](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/risks-safety-monitor?WT.mc_id=AZ-MVP-5004796#potentially-abusive-user-detection) in Azure OpenAI Studio functionality, to review any results of the filtering activity. You can use that information to further adjust your filter configuration to serve your specific business needs and [Responsible AI principles](https://learn.microsoft.com/azure/machine-learning/concept-responsible-ai?view=azureml-api-2&WT.mc_id=AZ-MVP-5004796). \\n:::\\n\\n## \u2699\ufe0f Disable inferencing via Azure AI Studio\\n\\nWhen considering Azure Open AI adoption, it is important to consider if you want users, to be able to inference using Azure AI Studio; using Azure AI Studio can be a security risk, as it allows users to inference directly from the Azure Portal, bypassing some traceability mechanisms _(such as Azure API Management)_ which you may have in place.\\n\\n**Ask**:\\n\\nAzure OpenAI Studio should not be available to make any inferencing calls and must be disabled.\\n\\n**Challenge**:\\nThere is no permission or RBAC role that prevents the use of AI Studio.\\n\\nHere\u2019s the table with all the missing information added:\\n\\n| **Why** | **Risk / Impact** | **Solution** |\\n|---------|-------------------|--------------|\\n| Azure OpenAI Studio does not log prompts and responses in the diagnostic logs | Risk \u2013 High/Impact \u2013 High | - Implement custom logging at the client side to capture relevant details.  - Use alternative logging mechanisms to track prompts and responses securely. |\\n| Azure OpenAI Studio will bypass APIM | Risk \u2013 High  Impact \u2013 High | - Create an intermediary hop between the client and AOAI service and allow AOAI access only via Private Endpoint (e.g., AppGW/APIM). - Enable end user only via Service Principals (no portal access). - DNS block oai.azure.com. |\\n\\nRationale:\\n\\nNo other out-of-the-box method exists to disable Azure OpenAI Studio.\\n\\n**Action**:\\n\\n* Control access to individual Azure OpenAI instances through [restricting what Virtual Networks](https://learn.microsoft.com/en-gb/azure/ai-services/cognitive-services-virtual-networks?tabs=portal&WT.mc_id=AZ-MVP-5004796) can access the OpenAI instance _(Service Tags need to be allowed through a Network Security Group)_, so make sure you make use of Network Security Groups where possible to only tunnel your approved traffic *(i.e., AppGw or APIM only)*.\\n* Make use of [Role-based access control for Azure OpenAI](https://learn.microsoft.com/en-gb/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796) Service to control who can see what.\\n* Make sure of [Azure Policy built-in policy definitions for Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/policy-reference?WT.mc_id=AZ-MVP-5004796) to control network access to your Azure OpenAI instances for existing or new deployments. \\n* Deploy a Custom policy to prevent the creation of Azure OpenAI Studio. Refer to my blog post: [Azure Policy - Deny the creation of Azure OpenAI Studio](https://luke.geek.nz/azure/policy-deny-creation-azure-openai-studio/).\\n\\n## \ud83d\udeb7 Restrict models to only certain users\\n\\nWhen considering Azure Open AI adoption, it is essential to restrict models _(or deployments)_ to only certain users. \\n\\n**Ask**:\\n\\nThe model should be accessible to only the required audience and no one else.\\n\\n**Challenge**:\\n\\nAzure OpenAI does not have the ability to grant permissions by models.\\n\\nHere\u2019s the table with **Risk** and **Impact** on the same row as requested:\\n\\n| **Why** | **Risk / Impact** | **Solution** |\\n|---------|-------------------|--------------|\\n| Model and data access should follow the principle of least privilege | Risk \u2013 High, Impact \u2013 High | - Each use case should have its own Azure OpenAI instance. - Deploy approved models for use cases only.  - Segregate duties of model deployment and model consumption via a custom RBAC role with the least privilege. |\\n\\n**Rationale**:\\n\\nHaving segregated instances for each use case eliminates the risk of model misuse and data exposure. New models and their inferencing capabilities need to be explicitly approved per use case before they can be leveraged. \\n\\n**Action**:\\n\\n* [Create and deploy an Azure OpenAI Service resource](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal&WT.mc_id=AZ-MVP-5004796) for each use case, and deploy only approved models for that use case.\\n* Make sure of [Role-based access control for Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796) Service to control who can see what, separate the users deploying the Models, to the users consuming the models, for example Cognitive Services OpenAI User to a Managed Identity or user using the deployments.\\n* Make use of [Azure Policy built-in policy definitions for Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/policy-reference?WT.mc_id=AZ-MVP-5004796), to control network access to your Azure OpenAI instances for existing or new deployments, and also restrict the creation of new \'unapproved\' Azure OpenAI resources from being created.\\n\\n## \ud83d\udcb0 FinOps - view total opex of OpenAI\\n\\nConsumption per use case could vary by order of magnitude. You may want to have visibility of costs at a service or deployment level, especially for use cases where the Azure OpenAI resource is shared but deployments are spread between projects. \\n\\n**Ask**:\\n\\nUse cases must be able to view their total spend for the Azure OpenAI service  \\n  \\n**Challenge**:\\n\\n* Cost metrics are available at the service level and select deployments, but not all  \\n* Any sharing of Azure OpenAI service makes cost determination not possible  \\n* No ability to stop usage beyond spending budget  \\n  \\n| Why                                                                 | Risk               | Solution                                                                                                                                                  |  \\n|---------------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|  \\n| - Use case owners need to monitor consumption as they are responsible for cost- Azure OpenAI service can create significant costs depending on usage. | Risk \u2013 Medium Impact \u2013 High | - Provision each use case in a dedicated instance and a dedicated subscription & resource group, this also ensures full AOAI capacity _(Tokens per min)_ available to use case |  \\n  \\n**Rationale**:\\n\\n- By having segregated instances for each use case, it provides accurate cost for each use case.  \\n\\n:::info\\nSharing an instance of Azure OpenAI among multiple tenants can also lead to a [Noisy Neighbor](https://learn.microsoft.com/en-us/azure/architecture/antipatterns/noisy-neighbor/noisy-neighbor?WT.mc_id=AZ-MVP-5004796) problem. It can cause higher latency for some tenants. You also need to make your application code multitenancy-aware. For example, if you want to charge your customers for the consumption cost of a shared Azure OpenAI instance, implement the logic to keep track of the total number of tokens for each tenant in your application.\\n:::\\n\\n**Action**:\\n\\n* Provision dedicated instances for each use case and rely on distinct Resource Groups and Subscriptions to separate costs.\\n* Review Cost Management and [scope the cost per Model tokens](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/manage-costs?WT.mc_id=AZ-MVP-5004796#monitor-costs).\\n* Make use of API Management, and [Token Usage](https://journeyofthegeek.com/2024/08/22/azure-openai-service-tracking-token-usage-with-apim/) and merge that with your billing data.\\n* If the Azure OpenAI instance is part of a wider solution, make sure you tag it with the [cm-resource-parent](https://luke.geek.nz/azure/application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag/) tag to allow full visibility of the workload\'s cost.\\n* Understand your throughput requirements and investigate [PTU (provisioned throughput units)](https://learn.microsoft.com/azure/ai-services/openai/concepts/provisioned-throughput?WT.mc_id=AZ-MVP-5004796). You can leverage [Azure AI Studio for calculate PTUs](https://oai.azure.com/portal/calculator). \\n* Review [Multitenancy and Azure OpenAI Service](https://learn.microsoft.com/azure/architecture/guide/multitenant/service/openai?WT.mc_id=AZ-MVP-5004796) architecture considerations.\\n\\n:::tip\\nYou don\'t pay for PER Azure OpenAI resource; you pay for the tokens consumed, so make sure you understand the [pricing model](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/?WT.mc_id=AZ-MVP-5004796). Why overcomplicate your solution with a shared resource when you can have a dedicated resource for each use case and have a clear understanding of the costs _(and security)_ associated with that use case?\\n:::\\n\\n## \ud83d\udee1\ufe0f Protect from OWASP\'s Top 10 threats\\n\\nWhen considering Azure Open AI adoption, it is important to protect from [OWASP Top 10 threats](https://owasp.org/www-project-top-ten/). \\n\\n![OWASP Top 10](images/oswasp-mapping.png)\\n\\n**Ask**:\\n\\nService should be protected from OWASP-10 threats.\\n\\n**Challenge**:\\n\\nAzure OpenAI Service is an API-based service vulnerable to OWASP-10 threats. Web Application Firewalls, which are typically used for this, can block significant legitimate AOAI inferencing calls.\\n\\n| Why                                                                                          | Risk                         | Solution                                                                                               |\\n|-----------------------------------------------------------------------------------------------|------------------------------|--------------------------------------------------------------------------------------------------------|\\n| - Per the Microsoft shared responsibility model, protection from OWASP-10 threats is customer responsibility | Risk \u2013 High Impact \u2013 High     | - Disable public access to service - Limit exposure to internal traffic only                            |\\n|                                                                                               |                              | - Disable file uploads to service via RBAC                                                              |\\n|                                                                                               |                              | - Add AppGW with WAF v2 in front of AOAI (Not recommended)                                              |\\n\\n**Rationale**:\\n\\n* Limiting internal traffic limits the exposure\\n* Even if an attack is successful from the internal network, there is no data stored in the service to be compromised\\n\\n**Action**:\\n\\n* Control access to individual Azure OpenAI instances through [restricting what Virtual Networks](https://learn.microsoft.com/en-gb/azure/ai-services/cognitive-services-virtual-networks?tabs=portal&WT.mc_id=AZ-MVP-5004796) can access the OpenAI instance _(Service Tags need to be allowed through a Network Security Group)_, so make sure you make use of Network Security Groups where possible to only tunnel your approved traffic *(ie, AppGw or APIM only)*.\\n* Make use of [Role-based access control for Azure OpenAI](https://learn.microsoft.com/en-gb/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796) Service to control who can see what.\\n* Make sure of [Azure Policy built-in policy definitions for Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/policy-reference?WT.mc_id=AZ-MVP-5004796) to control network access to your Azure OpenAI instances for existing or new deployments, and disable public endpoint access. \\n* Make use of [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) and [Azure Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview?WT.mc_id=AZ-MVP-5004796) to block, prevent, and log attempted connections to the solution.\\n\\n## \ud83d\udd11 Must use Customer Managed Keys _(CMK)_\\n\\nAll encryption must use [Customer Managed Keys](https://learn.microsoft.com/azure/security/fundamentals/key-management?WT.mc_id=AZ-MVP-5004796). I have not personally seen this requirement yet, but it is valid, especially for highly sensitive data and industry requirements (such as Finance).\\n\\n**Ask**:\\n\\n- Encryption in Azure OpenAI must be configured to use Customer Managed Keys (CMK).\\n\\n**Challenge**:\\n\\n* CMK needs to be stored in a KeyVault (in the same region).\\n* AOAI service connection to KeyVault can be over \'Trusted Microsoft Connection\' only, and not via KeyVault\'s private endpoint.\\n\\n| Why                                                                                                             | Risk                         | Solution                                                                           |\\n|------------------------------------------------------------------------------------------------------------------|------------------------------|------------------------------------------------------------------------------------|\\n| Per policy, clients typically want to use CMK as opposed to MMK _(Microsoft Managed Keys)_ - CMK configuration will be required to stay compliant with Azure policy | Risk \u2013 High Impact \u2013 High     | - Avoid need for uploading files to Azure OpenAI studio and fine-tuning - Disable public access of KeyVault |\\n\\n**Rationale**:\\n\\n[Trusted Microsoft connections](https://learn.microsoft.com/azure/search/search-indexer-howto-access-trusted-service-exception?WT.mc_id=AZ-MVP-5004796) use the Microsoft backbone network and no public network for communication.\\n\\n**Action**:\\n\\n* Azure AI Services support [encryption of data at rest](https://learn.microsoft.com/azure/ai-services/openai/encrypt-data-at-rest?WT.mc_id=AZ-MVP-5004796) by either enabling customer-managed keys (CMK) or by enabling customers to bring their own storage (BYOS). By default, Azure AI services use Microsoft-managed keys to encrypt the data. With [Customer-managed keys for encryption](https://learn.microsoft.com/azure/ai-services/encryption/cognitive-services-encryption-keys-portal?WT.mc_id=AZ-MVP-5004796), customers now have the choice of encrypting the data at rest with an encryption key, managed by the customers, using Azure Key Vault. There is an additional cost, as you are charged for the Key Vault and additional dependant services, as using Customer Managed Keys brings resources _(usually managed by Microsoft in a separate siloed subscription)_ into your own subscription in a separate managed Resource Group.\\n\\n:::tip\\nWhen considering customer-managed keys, consider People, Processes, and Products. You should have processes in place for the key lifecycles _(rotation, revocation, etc)_. Also, consider the implications of the additional costs associated with this and the additional management overhead. Be pragmatic about using Customer-Managed Keys and understand the implications of using them.\\n:::\\n\\n## \u274c Must not use shareable access controls\\n\\nShareable access controls are considered less secure.\\n\\n**Ask**:\\n\\n- No use of passwords or access keys to access the Azure OpenAI service.\\n\\n**Challenge**:\\n\\n* The use of keys is typically less secure and unacceptable in the long term, although it typically increases the pace of initial adoption. This makes the use of Service Principals also less secure.\\n\\n| Why - Shareable access details                                                                                                                                          | Risk Risk \u2013                 | Solution - Use RBAC for access                                                                                  |  \\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------|---------------------------------------------------------------------------------------------------------------|  \\n| can be transferred to unapproved consumers, losing audit trail - Bearer token of authenticated user can be potentially misused                                          | High Impact \u2013 High          | control to service for end users as well as applications (consumer and middleware)                             |  \\n|                                                                                                                                                                         |                             | - Avoid using the RBAC role with ListKey permissions or use a custom RBAC role                                     |  \\n\\n**Rationale**:\\n\\nRBAC _(Role-based Access Control)_ use increases the security score.\\n\\n**Action**:\\n\\n* Make use of [Role-based access control for Azure OpenAI](https://learn.microsoft.com/en-gb/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796) Service to control who can see what, and make sure you are not using the ListKey permission or a [custom RBAC](https://learn.microsoft.com/azure/role-based-access-control/custom-roles?WT.mc_id=AZ-MVP-5004796) role that allows or disallows access to the keys _(Microsoft.Storage/storageAccounts/listKeys/action)_.\\n\\n## \ud83c\udf0d Resource must be in a specific region only\\n\\nAzure OpenAI resources must be in a specific region only. This is a common requirement for data sovereignty and compliance reasons.\\n\\n**Ask**:\\n\\nProvision Azure OpenAI service in the same region where Express Route terminates \\n\\n**Challenge**:\\n\\n[Azure OpenAI and model capacity are highly variable](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?tabs=python-secure&WT.mc_id=AZ-MVP-5004796#model-summary-table-and-region-availability), and typically may not be available in the same region as customers Express Route region.\\n\\n**Rationale**:\\n\\nAzure OpenAI Private Endpoints can be created across regions and no VNet is required for operation.\\n\\n| Why                                                                                          | Risk                         | Solution                                                                                     |  \\n|----------------------------------------------------------------------------------------------|------------------------------|----------------------------------------------------------------------------------------------|  \\n| Network traffic is monitored for all ingress & egress in a predetermined region            | Risk \u2013 High Impact \u2013 High    | - Enable all regions in policy within a geography acceptable to the client                        |  \\n| At times there are regulatory compliance requirements to be in specific geographies        |                              | - Provision Azure OpenAI service where capacity is available in allowed regions              |  \\n|                                                                                              |                              | - Create private endpoint in Express Route region                                            |  \\n\\n**Rationale**:\\n\\nAzure OpenAI Private Endpoints can be created across regions and no VNet is required for operation \\n\\n* Azure OpenAI in any region can have a [Private Endpoint connected to a VNET _(Virtual Network)_ in a different region](https://learn.microsoft.com/azure/private-link/private-link-faq?WT.mc_id=AZ-MVP-5004796#can-private-endpoint-connect-to-azure-paas-resources-across-azure-regions-).\\n* Make sure of Azure Policy, such as the [Restrict resource regions](https://learn.microsoft.com/azure/cloud-adoption-framework/manage/azure-server-management/common-policies?WT.mc_id=AZ-MVP-5004796#restrict-resource-regions) policy, to enforce the region of the Azure OpenAI resource.\\n* Consider [model deployment types _(ie Global vs Regional)_](https://learn.microsoft.com/azure/ai-services/openai/how-to/deployment-types?WT.mc_id=AZ-MVP-5004796#global-versus-regional-deployment-types) - remember that your model inference may be in a different region to your actual Azure OpenAI resource, if Global - so need to be considered from a latency and workload variance perspective, however, these are Read Only - so data resiliency should not be a factor, as you can still store your own data in the region of your choice."},{"id":"azure/policy-deny-creation-azure-openai-studio","metadata":{"permalink":"/azure/policy-deny-creation-azure-openai-studio","source":"@site/blog/2024-10-14-azure-policy-denyopenaistudio/index.mdx","title":"Azure Policy - Deny the creation of Azure OpenAI Studio","description":"Custom policy definition to deny the creation of Azure OpenAI Studio.","date":"2024-10-14T08:20:46.782Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.48,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Policy - Deny the creation of Azure OpenAI Studio","metaDescription":"Custom policy definition to deny the creation of Azure OpenAI Studio.","date":"2024-10-14T08:20:46.782Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/policy-deny-creation-azure-openai-studio","keywords":["azure","policy","cloud","governance","compliance","security","openai"],"description":"Custom policy definition to deny the creation of Azure OpenAI Studio."},"unlisted":false,"prevItem":{"title":"Common Challenges and Solutions for Azure OpenAI Adoption","permalink":"/azure/openai-adoption-challenges-solutions"},"nextItem":{"title":"Cloudcloak","permalink":"/azure/cloudcloak"}},"content":"Custom [policy definition](https://learn.microsoft.com/azure/governance/policy/tutorials/create-custom-policy-definition?WT.mc_id=AZ-MVP-5004796) to deny the creation of [Azure OpenAI Studio Hubs](https://learn.microsoft.com/azure/ai-studio/concepts/ai-resources?WT.mc_id=AZ-MVP-5004796), which will stop the creation of an Azure OpenAI Studio.\\r\\n\\r\\n{/* truncate */}\\r\\n\\r\\nThis custom Azure Policy that denies the creation of Azure OpenAI Studio hubs. Here\'s a breakdown of its components:\\r\\n\\r\\n| Key                    | Description                                                                                     |\\r\\n|------------------------|-------------------------------------------------------------------------------------------------|\\r\\n| **policyType**         | Specifies that this is a custom policy.                                                         |\\r\\n| **mode**               | Indicates that the policy is indexed, meaning it can be evaluated against existing resources.    |\\r\\n| **displayName**        | A human-readable name for the policy.                                                           |\\r\\n| **description**        | A brief description of what the policy does.                                                    |\\r\\n| **metadata**           | Contains additional information about the policy:                                               |\\r\\n| **version**            | The version of the policy.                                                                      |\\r\\n| **category**           | The category under which this policy falls.                                                     |\\r\\n| **source**             | A URL for more information.                                                                     |\\r\\n| **alzCloudEnvironments** | Specifies the cloud environments where this policy is applicable.                              |\\r\\n| **createdBy**          | The creator of the policy.                                                                      |\\r\\n| **lastUpdated**        | The date when the policy was last updated.                                                      |\\r\\n| **parameters**         | Defines parameters that can be used within the policy:                                           |\\r\\n| **effect**             | A parameter that determines the action to take (Audit, Disabled, or Deny).                      |\\r\\n| **type**               | The type of the parameter (String).                                                             |\\r\\n| **metadata**           | Additional information about the parameter:                                                     |\\r\\n| **displayName**        | A human-readable name for the parameter.                                                        |\\r\\n| **description**        | A brief description of the parameter.                                                           |\\r\\n| **allowedValues**      | The allowed values for this parameter.                                                          |\\r\\n| **defaultValue**       | The default value for this parameter (Deny).                                                    |\\r\\n| **policyRule**         | Defines the rule that the policy enforces:                                                      |\\r\\n| **if**                 | Specifies the conditions to check:                                                              |\\r\\n| **allOf**              | An array of conditions that must all be true.                                                   |\\r\\n| **field**              | The field to check (type and kind).                                                             |\\r\\n| **equals**             | The expected value for the field.                                                               |\\r\\n| **then**               | Specifies the action to take if the conditions are met:                                         |\\r\\n| **effect**             | The action to take, which is determined by the value of the effect parameter.                   |\\r\\n\\r\\nThis policy checks if a resource is of type Microsoft.MachineLearningServices/workspaces and kind Hub. If both conditions are met, the policy enforces the action specified by the effect parameter, which defaults to \\"Deny\\".\\r\\n\\r\\n![Azure Policy - Deny the creation of Azure OpenAI Studio](images/DenyAzureStudioHub_PolicyAzurePortal.jpg)\\r\\n\\r\\n```json\\r\\n{\\r\\n    \\"policyType\\": \\"Custom\\",\\r\\n    \\"mode\\": \\"Indexed\\",\\r\\n    \\"displayName\\": \\"Deny Azure OpenAI Studio creation\\",\\r\\n    \\"description\\": \\"Deny Azure OpenAI Studio hub creation.\\",\\r\\n    \\"metadata\\": {\\r\\n        \\"version\\": \\"1.0.0\\",\\r\\n        \\"category\\": \\"Machine Learning\\",\\r\\n        \\"source\\": \\"https://luke.geek.nz\\",\\r\\n        \\"alzCloudEnvironments\\": [\\r\\n            \\"AzureCloud\\"\\r\\n        ],\\r\\n        \\"createdBy\\": \\"Luke\\",\\r\\n        \\"lastUpdated\\": \\"2024-10-01\\"\\r\\n    },\\r\\n    \\"parameters\\": {\\r\\n        \\"effect\\": {\\r\\n            \\"type\\": \\"String\\",\\r\\n            \\"metadata\\": {\\r\\n                \\"displayName\\": \\"Effect\\",\\r\\n                \\"description\\": \\"Enable or disable the execution of the policy\\"\\r\\n            },\\r\\n            \\"allowedValues\\": [\\r\\n                \\"Audit\\",\\r\\n                \\"Disabled\\",\\r\\n                \\"Deny\\"\\r\\n            ],\\r\\n            \\"defaultValue\\": \\"Deny\\"\\r\\n        }\\r\\n       \\r\\n    },\\r\\n    \\"policyRule\\": {\\r\\n        \\"if\\": {\\r\\n            \\"allOf\\": [\\r\\n                {\\r\\n                    \\"field\\": \\"type\\",\\r\\n                    \\"equals\\": \\"Microsoft.MachineLearningServices/workspaces\\"\\r\\n                },\\r\\n                {\\r\\n                    \\"field\\": \\"kind\\",\\r\\n                    \\"equals\\": \\"Hub\\"\\r\\n                }\\r\\n            ]\\r\\n        },\\r\\n        \\"then\\": {\\r\\n            \\"effect\\": \\"[parameters(\'effect\')]\\"\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n![Azure Policy - Deny the creation of Azure OpenAI Studio](images/DenyAzureStudioHubCreation_PolicyAzurePortal.jpg)"},{"id":"azure/cloudcloak","metadata":{"permalink":"/azure/cloudcloak","source":"@site/blog/2024-09-30-cloudcloak/index.mdx","title":"Cloudcloak","description":"Install and test Cloudcloak, a browser extension that hides sensitive information while streaming or presenting Microsoft Cloud Portals.","date":"2024-09-29T23:37:38.995Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.12,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Cloudcloak","metaDescription":"Install and test Cloudcloak, a browser extension that hides sensitive information while streaming or presenting Microsoft Cloud Portals.","date":"2024-09-29T23:37:38.995Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/cloudcloak","keywords":["azure","browser extension","cloudcloak","hackathon"],"description":"Install and test Cloudcloak, a browser extension that hides sensitive information while streaming or presenting Microsoft Cloud Portals."},"unlisted":false,"prevItem":{"title":"Azure Policy - Deny the creation of Azure OpenAI Studio","permalink":"/azure/policy-deny-creation-azure-openai-studio"},"nextItem":{"title":"System Managed Identity in Storage Queue PowerShell Azure Functions","permalink":"/azure/storage-queue-triggers-system-managed-identity-powershell"}},"content":"[Cloud Cloak](https://github.com/microsoft/Cloudcloak) is a browser extension for folks streaming or presenting while simultaneously showing one of the Microsoft Cloud Portals. It does its best to hide connection strings, email addresses, avatars, and anything that might show secure or personal information.\\n\\n:::info\\nYou can consider this the next version/update to [Azure Mask](https://github.com/clarkio/azure-mask/) _(shout out to Brian Clark for his pioneering work and blessing us to create this one)_.\\n:::\\n\\n![Cloudcloak](images/cloudcloak-icon-128.png)\\n\\n{/* truncate */}\\n\\n:::info\\nDeveloped as part of the Microsoft Global Hackathon 2024 \ud83d\ude80! This extension helps to \'cloak _(i.e., blur)_\' potentially personable information in the relevant Azure portals that could be mistreated, allowing presenters to focus on the content they are delivering instead of having to worry about the type of information _(such as Azure subscription ids)_, that people can see.\\n\\n![Cloudcloak](images/AzurePortal_Cloudcloak_On.jpg)\\n\\n[Cloud Cloak](https://github.com/microsoft/Cloudcloak) is open for contributions, and make sure you provide feedback by opening an issue on how it could be improved further. This is an open-source initiative and not directly supported by Microsoft. At the time of writing, this is the release version of version 1.0.\\n:::\\n\\n:::warning\\nIt does not guarantee that all sensitive information is hidden, for example, although the information is hidden in the UI on render, it might still be present in the HTML source code, or the address bar, and link previews.\\n:::\\n\\nSo let us see how it works according to GitHub Copilot:\\n\\nThe extension monitors specific websites and, when activated, blurs sensitive information such as email addresses, GUIDs, IP addresses, and phone numbers. It achieves this functionality using a combination of background scripts and content scripts.\\n\\nThe extension is activated by clicking on the extension icon in the browser toolbar. When activated, sensitive information is blurred on the current page. It will remain active until the user deactivates it by clicking on the extension icon again.\\n\\n1. Initialization: When the extension is installed or updated, it initializes the state from storage and updates the badge for the current tab.\\nTab Update/Activation: When a tab is updated or activated, the extension checks if the URL matches one of the specified extensions. If it does, it injects the cloak.js script and updates the badge.\\n2. Cloak Mode: When the user clicks the extension icon, the extension toggles the cloak mode. If enabled, it injects the cloak.js script, which identifies and blurs sensitive information on the page.\\n3. DOM Changes: The MutationObserver in cloak.js monitors changes in the DOM and reapplies the blur filter to newly added elements containing sensitive information.\\n\\n| **Component**        | **Description**                                                                                         |\\n|----------------------|---------------------------------------------------------------------------------------------------------|\\n| `manifest.json`       | Configures the extension with metadata, permissions, and the background script.                         |\\n| `background.js`       | Manages the extension\u2019s state, handles events, and injects the content script into specified websites.   |\\n| `cloak.js`            | Contains the logic to identify and blur sensitive information on web pages.                             |\\n| **How It Works**      |                                                                                                         |\\n| `manifest.json`       | - Defines metadata, permissions, and the background script. Specifies icons and permissions.      |\\n| `background.js`       | - Manages the extension\u2019s state. Injects `cloak.js` script into the specified tab and frames.    |\\n| **Event Listeners**   |                                                                                                         |\\n| `chrome.tabs.onUpdated` | Listens for tab updates and injects the script if the URL matches the specified conditions.             |\\n| `chrome.tabs.onActivated` | Listens for tab activation and updates the badge accordingly.                                        |\\n| `chrome.webNavigation.onCompleted` | Injects the script into iframes upon navigation completion.                                  |\\n| `chrome.action.onClicked` | Toggles the extension\'s enabled/disabled state when clicking the action button.                   |\\n\\nAt the time of writing, the following URLs are supported:\\n\\n- https://portal.azure.com\\n- https://ms.portal.azure.com\\n- https://rc.portal.azure.com\\n- https://preview.portal.azure.com\\n- https://entra.microsoft.com\\n- https://intune.microsoft.com\\n- https://ai.azure.com\\n- https://admin.microsoft.com\\n- https://sip.security.microsoft.com\\n- https://purview.microsoft.com\\n- https://make.powerapps.com\\n- https://make.preview.powerapps.com\\n- https://msazure.visualstudio.com\\n- https://github.com\\n- https://copilotstudio.microsoft.com\\n- https://copilotstudio.preview.microsoft.com\\n- https://portal.azure.us\\n\\nHowever, new URLs can be added by updating the `background.js` file and reading it in your browser to test locally.\\n\\nBecause of the Chrome event listeners, this extension is only available for Chrome and Edge browsers (and I would assume any other Chrome browser).\\n\\nSo, let\'s get started with the installation. We will be using Edge, as it\'s the browser I use mainly for my presentations.\\n\\nThis is an unofficial extension, and at this time, it is not in the Edge or Chrome stores, so it will need to be installed and updated manually. If your browser has multiple profiles, this is also per user profile.\\n\\nSo, let us download the extension from the [GitHub repository](https://github.com/microsoft/Cloudcloak) and unzip the file.\\n\\n![Download Cloudcloak](images/Download_Cloudcloak.gif)\\n\\nThen, in Edge, go to `edge://extensions/` and enable Developer mode. Then click on `Load unpacked` and select the folder where you unzipped the extension.\\n\\n![Install Cloudcloak](images/Install_Cloudcloak.gif)\\n\\n:::tip\\nAfter the extension is installed, you can disable Developer mode. For ease of use, you can pin the extension to the toolbar.\\n:::\\n\\nNow, when you are presenting, you can click on the extension icon, which will blur out sensitive information on the page. So, let\'s test it by navigating to an Azure resource.\\n\\n![Cloudcloak On](images/Test_Cloudcloak.gif)"},{"id":"azure/storage-queue-triggers-system-managed-identity-powershell","metadata":{"permalink":"/azure/storage-queue-triggers-system-managed-identity-powershell","source":"@site/blog/2024-09-02-pwsh-azfunction-mi/index.mdx","title":"System Managed Identity in Storage Queue PowerShell Azure Functions","description":"Learn how to create and configure a storage queue trigger using PowerShell in Azure Functions to automate your workflows.","date":"2024-09-02T03:14:00.526Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.195,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"System Managed Identity in Storage Queue PowerShell Azure Functions","metaDescription":"Learn how to create and configure a storage queue trigger using PowerShell in Azure Functions to automate your workflows.","date":"2024-09-02T03:14:00.526Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/storage-queue-triggers-system-managed-identity-powershell","keywords":["azure","storage queue","azure functions","system managed identity","powershell"],"description":"Learn how to create and configure a storage queue trigger using PowerShell in Azure Functions to automate your workflows."},"unlisted":false,"prevItem":{"title":"Cloudcloak","permalink":"/azure/cloudcloak"},"nextItem":{"title":"Azure Communication Services and PowerShell for Email","permalink":"/azure/using-communication-services-and-powershell-to-send-emails"}},"content":"[Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-powershell&WT.mc_id=AZ-MVP-5004796) is a serverless compute service that enables you to run event-driven code without having to manage infrastructure. You can use Azure Functions to run small pieces of code in response to events, such as HTTP requests, messages in a queue, or changes in data in a storage account. In this article, I will show you how to use Azure Functions with a storage queue trigger using the System Managed Identity of the Function Application *(instead of a Connection String)*.\\n\\n{/* truncate */}\\n\\nWhen you create an Azure Function, you can use a storage queue trigger to execute the function when a message is added to a storage queue. This is useful when you want to process messages in a queue asynchronously. In this article, I will show you how to use an Azure Function with a storage queue by using the System Managed Identity of the Function Application to authenticate with the storage account.\\n\\n:::info\\n[Azure Queue storage trigger for Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-bindings-storage-queue-trigger?tabs=python-v2%2Cisolated-process%2Cnodejs-v4%2Cextensionv5&pivots=programming-language-powershell&WT.mc_id=AZ-MVP-5004796). The queue storage trigger runs a function as messages are added to Azure Queue storage.\\n\\nThe Azure Functions runtime supports triggers and bindings for Azure Queue storage. The Azure Queue storage trigger lets you respond to new messages in a queue. When a message is added to the queue, the runtime invokes your function.\\n:::\\n\\n\\nThe first thing you will need to do is enable the System Managed Identity of the Azure Function.\\n\\n![Enable System Managed Identity](images/AzurePortal_AzureFunction_SystemManagedIdentity.jpg)\\n\\nOnce that has been completed, we can assign the required roles to the Storage account that contains the queue.\\n\\nWhat level of permissions you need to assign to the Function App\'s System Managed Identity depends on what you want to do with the queue. In this example, I will be using the `Storage Queue Data Contributor` role.\\n\\n| Binding type   | Example built-in roles                                                |\\n|----------------|-----------------------------------------------------------------------|\\n| Trigger        | [Storage Queue Data Reader](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles/storage?WT.mc_id=AZ-MVP-5004796#storage-queue-data-reader), [Storage Queue Data Message Processor](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles/storage?WT.mc_id=AZ-MVP-5004796#storage-queue-data-message-processor)   |\\n| Output binding | [Storage Queue Data Contributor](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles/storage?WT.mc_id=AZ-MVP-5004796#storage-queue-data-contributor), [Storage Queue Data Message Sender](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles/storage?WT.mc_id=AZ-MVP-5004796#storage-queue-data-message-sender) |\\n\\nBecause we are going to assign the permissions to a Storage account, we can do this directly through the Azure Portal at the Function App System Managed identity location.\\n\\n![Assign Role to Storage Account](images/AzurePortal_AzureFunction_SystemManagedIdentityAssignRole.gif)\\n\\nOnce permissions have been granted in our Azure Function App, we will need to create some Connection variables, which our Function App will use.\\n\\nIn the Function App, under Settings, go to Environment variables.\\n\\nWe will need to determine a Connection name, which will be used by our Azure Function. In this example, I will be using `StgQueueAccount`. This will be used as a prefix for additional variables, for example:\\n\\n- `StgQueueAccount__queueServiceUri`\\n\\n:::info\\nNote that the variable has x2 underscores `__` between the Connection name and the variable name.\\n:::\\n\\nThis variable will be used to store the [URI of the Storage Account Queue](https://learn.microsoft.com/azure/azure-functions/functions-bindings-storage-queue-trigger?tabs=python-v2%2Cin-process%2Cnodejs-v4%2Cextensionv5&pivots=programming-language-powershell&WT.mc_id=AZ-MVP-5004796#identity-based-connections).\\n\\nFor the valuiie of the StgQueueAccount__queueServiceUri, you can get this from the Azure Portal, by going to the Storage Account, and then to the Queue Service, to find the endpoint uri.\\n\\n![Queue Service URI](images/AzurePortal_AzureFunction_StorageAccountQueueenebdpoint.jpg)\\n\\nOnce we have retrieved it, we can navigate back to the Environment variables and add in the Key-Value pair.\\n\\n![Environment Variables](images/AzurePortal_AzureFunction_EnvironmentVariables_queueuri.jpg)\\n\\nMake sure to click Apply and Apply.\\n\\nOnce completed, the default profile.ps1 file will run to authenticate using the Managed Identity of the Function App.\\n\\n```powershell title=\\"profile.ps1\\"\\nif ($env:MSI_SECRET) {\\n    Disable-AzContextAutosave -Scope Process | Out-Null\\n    Connect-AzAccount -Identity\\n}\\n```\\n\\nYour actual function will need to have the binding updated with the name of the Connection prefix determined earlier and include the queue name, for example:\\n\\n```json title=\\"function.json\\"\\n{\\n  \\"bindings\\": [\\n    {\\n      \\"name\\": \\"QueueItem\\",\\n      \\"type\\": \\"queueTrigger\\",\\n      \\"direction\\": \\"in\\",\\n      \\"queueName\\": \\"rg-queue-create\\",\\n      \\"connection\\": \\"StgQueueAccount\\"\\n    }\\n  ]\\n}\\n```\\n\\nYou should now be able to read the messages from the queue and process them as required using the System Managed Identity of the Azure Function account."},{"id":"azure/using-communication-services-and-powershell-to-send-emails","metadata":{"permalink":"/azure/using-communication-services-and-powershell-to-send-emails","source":"@site/blog/2024-08-24-azemailcommmi/index.mdx","title":"Azure Communication Services and PowerShell for Email","description":"A step-by-step guide on how to use Azure Communication Services and PowerShell to send emails.","date":"2024-08-24T10:21:21.789Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.375,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Communication Services and PowerShell for Email","metaDescription":"A step-by-step guide on how to use Azure Communication Services and PowerShell to send emails.","date":"2024-08-24T10:21:21.789Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/using-communication-services-and-powershell-to-send-emails","keywords":["azure","communication services","PowerShell","email","script","System Managed Identity","authentication"],"description":"A step-by-step guide on how to use Azure Communication Services and PowerShell to send emails."},"unlisted":false,"prevItem":{"title":"System Managed Identity in Storage Queue PowerShell Azure Functions","permalink":"/azure/storage-queue-triggers-system-managed-identity-powershell"},"nextItem":{"title":"Implementing Correlation ID for API Management requests","permalink":"/azure/implementing-correlation-id-in-api-management"}},"content":"[Azure Communication Services](https://azure.microsoft.com/products/communication-services?WT.mc_id=AZ-MVP-5004796) brings rich communication APIs to all of your apps across any device on any platform, using the same reliable and secure infrastructure that powers Microsoft Teams.\\n\\nToday, we will explore using Email as part of Azure Communication Services, using the REST API and PowerShell to send an email.\\n\\n![Azure Communication Services](images/BlogHeading_DeployandTestACS.gif)\\n\\n{/* truncate */}\\n\\n:::info\\nI did a previous article: [Deploying and Testing Azure Email Communication Services](https://luke.geek.nz/azure/azure-email-communication-services/) on this, however the authentication was using an Azure Service Principal. This time we will use oauth using a System Assigned Managed Identity.\\n:::\\n\\nIn this example, I will access a token from [Azure Communication Services](https://azure.microsoft.com/products/communication-services?WT.mc_id=AZ-MVP-5004796). I will make a GET request to the identity endpoint of Azure Communication Services, using the oauth identity from the system-managed identity. This will return a token we can use to authenticate against the REST API.\\n\\nHere\'s a step-by-step explanation of the script:\\n\\nThe script first defines the subject and body of the email. The body is an HTML string that contains the email content.\\nIt checks if the email body is not empty.\\nIf the email body is not empty, it proceeds to get an access token from Azure Communication Services. This is done by making a GET request to the identity endpoint of Azure Communication Services. The access token is then printed to the console.\\nIf there\'s an error when getting the access token, the script catches the exception and prints the error message and response details on the console.\\nThe script then constructs the URI for the email-sending endpoint and defines the headers for the REST API call. The headers include the content type and the obtained access token in the Authorization header.\\nThe script defines the body for the REST API call. This includes the sender address, the email content (subject and body), the recipients, the reply-to address, and a flag to disable user engagement tracking.\\nThe script then converts the PowerShell object to JSON and attempts to send the email by making a POST request to the email-sending endpoint. The request details and response are logged to the console.\\nIf the email is sent incorrectly, the script catches the exception and logs the error message and stack trace to the console.\\n\\nFurther information on the authentication process itself:\\n\\n1. The script first defines the resource ID for Azure Communication Services and the communication endpoint URL.\\n2. It then constructs the URI for the identity endpoint. This URI includes the environment variable IDENTITY_ENDPOINT (automatically set by Azure when using Managed Identity) and the resource ID.\\n3. The script then attempts to get an access token from the identity endpoint. It does this by sending a GET request to the identity endpoint with a header that includes Metadata: true. This tells the identity endpoint that the request is coming from within Azure.\\n4. the response will include an access token if the request is successful. This token is then extracted from the response.\\n\\nThis access token can then be used to authenticate requests to Azure Communication Services. The token tells Azure Communication Services that the request comes from an authenticated source (in this case, the Managed Identity) and should be allowed.\\n\\n_(This authenticaiton was inititally written to be used in a Azure Automation Runbook, with the System Managed Identity assigned Contributor rights to the Azure Communication Services resource (not the Email Communication Services).)_\\n\\nHere is the PowerShell script to send an email using Azure Communication Services using the [System Managed identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796) of an [Azure Automation account](https://learn.microsoft.com/azure/automation/overview?WT.mc_id=AZ-MVP-5004796):\\n\\n```powershell\\n$emailSubject = \\"Important: Server Maintenance Notification\\"\\n$EmailRecipient = \\"ituser@contoso.com\\"\\n$emailBody = @\\"\\n<html>\\n<body>\\n<p>Dear User,</p>\\n<p>This is to inform you that a <b><i>server maintenance is scheduled for the next week</i></b>.</p>\\n<p>The servers will be down from 10:00 PM to 2:00 AM.</p>\\n<p>Please save your work and log off during this period to avoid any data loss.</p>\\n<p>If you have any questions or concerns, please contact our IT Support team.</p>\\n<p>Thank you for your understanding and cooperation.</p>\\n<p>Best Regards,</p>\\n<p>IT Support Team</p>\\n</body>\\n</html>\\n\\"@\\n\\nif ($emailBody -ne \\"\\") {\\n    Write-Output $emailBody  \\n\\n    # Define the resource ID for Azure Communication Services\\n    $ResourceID = \'https://communication.azure.com\'\\n\\n    # Define the communication endpoint URL\\n    $communicationendpointurl = \\"azcomm-contoso.australia.communication.azure.com\\" # Update with your communication endpoint URL\\n\\n    # Construct the URI for the identity endpoint\\n    $Uri = \\"$($env:IDENTITY_ENDPOINT)?api-version=2018-02-01&resource=$ResourceID\\"\\n\\n    # Debug output\\n    # Print the constructed URI and headers\\n    Write-Output \\"URI: $Uri\\"\\n    Write-Output \\"Headers: @{ Metadata = \'true\' }\\"\\n\\n    # Try to get the access token\\n    try {\\n        # Invoke a GET request to the identity endpoint to get the access token\\n        $AzToken = Invoke-WebRequest -Uri $Uri -Method GET -Headers @{ Metadata = \\"true\\" } -UseBasicParsing | Select-Object -ExpandProperty Content | ConvertFrom-Json | Select-Object -ExpandProperty access_token\\n        # Print the obtained access token\\n        Write-Output \\"Access Token: $AzToken\\"\\n    }\\n    catch {\\n        # If there\'s an error, print the error message and response details\\n        Write-Error \\"Failed to get access token: $_\\"\\n        Write-Output \\"Response Status Code: $($_.Exception.Response.StatusCode.Value__)\\"\\n        Write-Output \\"Response Status Description: $($_.Exception.Response.StatusDescription)\\"\\n        Write-Output \\"Response Content: $($_.Exception.Response.GetResponseStream() | %{ $_.ReadToEnd() })\\"\\n    }\\n\\n    # Construct the URI for the email sending endpoint\\n    $uri = \\"https://$communicationendpointurl/emails:send?api-version=2023-03-31\\"\\n\\n    # Define the headers for the REST API call\\n    # Include the content type and the obtained access token in the Authorization header\\n    $headers = @{\\n        \\"Content-Type\\"  = \\"application/json\\"\\n        \\"Authorization\\" = \\"Bearer $AzToken\\"\\n    }\\n    # Define the body for the REST API call\\n    $apiResponse = @{\\n        headers                        = @{\\n            id = (New-Guid).Guid\\n        }\\n        senderAddress                  = \'DoNotReply@7647475b-a51b-4901-8674-917e6abea743.azurecomm.net\'\\n        content                        = @{\\n            subject = $emailSubject\\n            html    = $emailBody\\n        }\\n        recipients                     = @{\\n            to = @(\\n                @{\\n                    address     = $EmailRecipient\\n                    displayName = $EmailRecipient\\n                }\\n            )\\n        }\\n\\n        replyTo                        = @(\\n            @{\\n                address     = \\"example@contoso.com\\"\\n                displayName = \\"Contoso\\"\\n            }\\n        )\\n        userEngagementTrackingDisabled = $true\\n    }\\n                \\n    # Convert the PowerShell object to JSON\\n    $body = $apiResponse | ConvertTo-Json -Depth 10\\n    # Send the email\\n    try {\\n        # Log the request details\\n        Write-Output \\"Sending email...\\"\\n        Write-Output \\"URI: $uri\\"\\n        Write-Output \\"Headers: $headers\\"\\n        Write-Output \\"Body: $body\\"\\n        # Make the request\\n        $response = Invoke-RestMethod -Uri $uri -Method Post -Headers $headers -Body $body -UseBasicParsing\\n        # Log the response\\n        Write-Output \\"Response: $response\\"\\n        # Return the response\\n        $response\\n    }\\n    catch {\\n        # Log the error\\n        Write-Error \\"Failed to send email: $_\\"\\n        Write-Output \\"Exception Message: $($_.Exception.Message)\\"\\n        Write-Output \\"Exception StackTrace: $($_.Exception.StackTrace)\\"\\n    }\\n}\\n```\\n\\nYou can run this script in an Azure Automation Runbook _(and theoretically in an [Azure Function](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796) as well)_ to send an email using Azure Communication Services and the System Managed Identity\u2014and there is no need to maintain or store client secrets!"},{"id":"azure/implementing-correlation-id-in-api-management","metadata":{"permalink":"/azure/implementing-correlation-id-in-api-management","source":"@site/blog/2024-08-16-apim-appin-trace/index.mdx","title":"Implementing Correlation ID for API Management requests","description":"This article provides a comprehensive guide on implementing correlation IDs in API management systems, enhancing traceability and debugging capabilities.","date":"2024-08-16T10:21:21.789Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.235,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Implementing Correlation ID for API Management requests","metaDescription":"This article provides a comprehensive guide on implementing correlation IDs in API management systems, enhancing traceability and debugging capabilities.","date":"2024-08-16T10:21:21.789Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/implementing-correlation-id-in-api-management","keywords":["azure","apim","AzureOpenAI","AIGateway","API Management","Correlation ID","Traceability","Debugging","Microservices","Distributed Systems"],"description":"This article provides a comprehensive guide on implementing correlation IDs in API management systems, enhancing traceability and debugging capabilities."},"unlisted":false,"prevItem":{"title":"Azure Communication Services and PowerShell for Email","permalink":"/azure/using-communication-services-and-powershell-to-send-emails"},"nextItem":{"title":"Implementing AI Gateway capabilities in API Management","permalink":"/azure/implementing-ai-gateway-in-api-management"}},"content":"When working with [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796), you may want to enable traceability and correlation of requests across different services. This is where the Correlation ID comes in.\\n\\nSimilar to [traceparent](https://www.w3.org/TR/trace-context/#traceparent-header-field-values), the Correlation ID is a unique identifier passed along with the request and response headers. It allows you to track a request\'s flow as it moves through different services and systems.\\n\\nIn this article, we will explore how to generate a new correlation ID through API Management Inbound policies and pass it back to the Client through an outbound policy, which then is used to help troubleshoot specific calls with [Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?WT.mc_id=AZ-MVP-5004796).\\n\\n{/* truncate */}\\n\\nSo, if you want an experience like the following to help pass the Correlation ID back to the client, then look up the transaction in Application Insights, then let us take a look:\\n\\n![Correlation ID](images/apim_appin_requesttrace.gif)\\n\\nTo start with, make sure that the API _(Global or specific API)_ has the following settings enabled:\\n\\n![Correlation protocol](images/apim-appin-trace_verbosity.jpg)\\n\\nNow, we need to set an Inbound Policy to create the Correlation ID (if it\'s not already passed in the request) as a variable and assign that variable to an inbound header.\\n\\n```xml\\n<inbound>\\n  <set-variable name=\\"correlation-id\\" value=\\"@(context.Request.Headers.GetValueOrDefault(\'x-correlation-id\', Guid.NewGuid().ToString()))\\" />\\n  <set-header name=\\"x-correlation-id\\" exists-action=\\"override\\">\\n    <value>@((string)context.Variables[\\"correlation-id\\"])</value>\\n  </set-header>\\n</inbound>\\n```\\n\\n| **Line** | **Explanation** |\\n| --- | --- |\\n| `<set-variable name=\\"correlation-id\\" value=\\"@(context.Request.Headers.GetValueOrDefault(\\"x-correlation-id\\", Guid.NewGuid().ToString()))\\" />` | This line is setting a variable named \\"correlation-id\\". It\'s getting the value from the request headers. If a header with the name \\"x-correlation-id\\" exists, it uses that value. If not, it generates a new unique ID using `Guid.NewGuid().ToString()`. |\\n| `<set-header name=\\"x-correlation-id\\" exists-action=\\"override\\">` | This line is setting a header named \\"x-correlation-id\\". If a header with this name already exists, it will be overridden. |\\n| `<value>@((string)context.Variables[\\"correlation-id\\"])</value>` | This line sets the value of the \\"x-correlation-id\\" header to the value of the \\"correlation-id\\" variable. |\\n| `</set-header>` | This line is closing the `<set-header>` tag. |\\n\\nOnce that added, we need to add another inbound policy:\\n\\n```xml\\n<inbound>\\n<trace source=\\"Global APIM Policy\\" severity=\\"information\\">\\n    <message>@(String.Format(\\"API: {0}, Operation: {1}\\", context.Api.Name, context.Operation.Name))</message>\\n    <metadata name=\\"correlation-id\\" value=\\"@((string)context.Variables[\\"correlation-id\\"])\\" />\\n</trace>\\n</inbound>\\n```\\n\\n| **Element**                                                                                                   | **Description**                                                                                                                                                                                |\\n| --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `<trace source=\\"Global APIM Policy\\" severity=\\"information\\">`                                              | This is the start of a trace element. The\xa0source\xa0attribute is set to \\"Global APIM Policy\\" and the\xa0severity\xa0attribute is set to \\"information\\".                                              |\\n| `<message>@(String.Format(\\"API: {0}, Operation: {1}\\", context.Api.Name, context.Operation.Name))</message>` | This is a message element. It uses\xa0String.Format\xa0to create a string that includes the name of the API and the operation from the context.                                                  |\\n| `<metadata name=\\"correlation-id\\" value=\'@((string)context.Variables[\\"correlation-id\\"])\' />`               | This is a metadata element. It has a\xa0name\xa0attribute set to \\"correlation-id\\" and a\xa0value\xa0attribute that retrieves the \\"correlation-id\\" from the context variables and casts it to a string. |\\n| `</trace>`                                                                                                  | This is the end of the trace element.                                                                                                                                                      |\\n\\n![Inbound Policy](images/apim-appin-trace_inboundpolicy.jpg)\\n\\nThis will help you track the Correlation ID in the logs and troubleshoot any issues that may arise.\\n\\nOnce that has been added, it\'s time to return it to the client. This can be done with the following Outbound policy:\\n\\n```xml\\n<outbound>\\n  <set-header name=\\"x-correlation-id\\" exists-action=\\"override\\">\\n    <value>@((string)context.Variables[\\"correlation-id\\"])</value>\\n  </set-header>\\n</outbound>\\n```\\n\\n| **Element**                                                          | **Description**                                                                                                                                                                                                                                                                                                       |\\n| --------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `<outbound>`                                                    | This tag defines the outbound section of the policy. The operations inside this tag are applied to the response before it\'s sent back to the client.                                                                                                                                                              |\\n| `<set-header name=\\"x-correlation-id\\" exists-action=\\"override\\">` | This tag sets an HTTP header in the outbound response. The\xa0`name`\xa0attribute specifies the name of the header, in this case\xa0`x-correlation-id`. The\xa0`exists-action`\xa0attribute specifies what to do if the header already exists, in this case it\'s set to\xa0`override`, meaning the existing header will be replaced. |\\n| `<value>@((string)context.Variables[\\"correlation-id\\"])</value>` | This tag sets the value of the header. It\'s using a variable from the context named\xa0`correlation-id`. The\xa0`@`\xa0symbol is used to denote an expression in Azure API Management policies.                                                                                                                            |\\n\\n![Outbound Policy](images/apim-appin-trace_outboundpolicy.jpg)\\n\\nReference: \\n* [Optimizing traceability in Azure API Management](https://yourazurecoach.com/2021/01/22/optimizing-api-traceability-in-azure-api-management/)"},{"id":"azure/implementing-ai-gateway-in-api-management","metadata":{"permalink":"/azure/implementing-ai-gateway-in-api-management","source":"@site/blog/2024-08-14-apim-ai-gateway/index.mdx","title":"Implementing AI Gateway capabilities in API Management","description":"Comprehensive guide on implementing an AI Gateway in API Management, covering key concepts, benefits, and implementation details.","date":"2024-08-14T06:40:06.774Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":23.99,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Implementing AI Gateway capabilities in API Management","metaDescription":"Comprehensive guide on implementing an AI Gateway in API Management, covering key concepts, benefits, and implementation details.","date":"2024-08-14T06:40:06.774Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/implementing-ai-gateway-in-api-management","keywords":["azure","apim","AzureOpenAI","AIGateway","API Management"],"description":"Comprehensive guide on implementing an AI Gateway in API Management, covering key concepts, benefits, and implementation details."},"unlisted":false,"prevItem":{"title":"Implementing Correlation ID for API Management requests","permalink":"/azure/implementing-correlation-id-in-api-management"},"nextItem":{"title":"Securing Container Supply Chains with CSSC Framework","permalink":"/azure/secure-container-supply-chain"}},"content":"Today, we are going to delve into some of the Generative [AI Gateway capabilities](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/dev-starters/genai-gateway/?WT.mc_id=AZ-MVP-5004796) _(commonly referred to as AI Gateway)_ that are available in the [API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) service. \\n\\nThese capabilities are designed to help secure and monitor your OpenAI endpoints in your applications as you head toward production.\\n\\n{/* truncate */}\\n\\n## \ud83c\udf10 Overview \\n\\nCommon scenarios that the AI Gateway capabilities can help with include:\\n\\n* How can we track token usage across multiple applications? How can we do cross charges for multiple applications/teams that use Azure OpenAI models? \\n* How can we make sure that a single app does not consume the whole TPM quota, leaving other apps with no option to use Azure OpenAI models? \\n* How can we make sure that the API key is securely distributed across multiple applications? \\n* How can we distribute load across multiple Azure OpenAI endpoints? How can we make sure that PTUs are used first before falling back to Pay-as-you-go instances? \\n\\nIn this post, we will explore how to use the AI Gateway capabilities in API Management to address these scenarios.\\n\\n![APIM + Azure OpenAI](images/AI_Gateway_APIMOpenAI.PNG)\\n\\n![AI Gateway](images/ai-gateway.gif)\\n\\nThe first step is to [import the Azure OpenAI](https://learn.microsoft.com/azure/api-management/azure-openai-api-from-specification?WT.mc_id=AZ-MVP-5004796) service and definition into API Management. This will allow you to create a new API in API Management that can act as a gateway to the Azure OpenAI service.\\n\\n:::info\\nYou can import an Azure OpenAI API directly from the Azure OpenAI Service to API Management. When you import the API, API Management automatically configures:\\n\\n* Operations for each of the Azure OpenAI REST API endpoints.\\n* A system-assigned identity with the necessary permissions to access the Azure OpenAI resource.\\n* A backend resource and set-backend-service policy that directs API requests to the Azure OpenAI Service endpoint.\\n* An authentication-managed-identity policy that can authenticate to the Azure OpenAI resource using the instance\'s system-assigned identity.\\n* _(optionally)_ Policies help you monitor and manage token consumption using the Azure OpenAI API.\\n\\nThe general availability of this functionality went into May 2024. [GA Import Azure OpenAI endpoints as an API in Azure API Management](https://azure.microsoft.com/en-us/updates/ga-import-azure-openai-enpoints-as-an-apis-in-azure-api-management/?WT.mc_id=AZ-MVP-5004796).\\n:::\\n\\nSo, let us take a look at how to import the Azure OpenAI service and definition into API Management.\\n\\n## \ud83d\udcec Request Forwarding\\n\\nLet us take a look at forwarding the request.\\n\\n![Request Forwarding](images/AI_Gateway_APIMRequestForwarding.PNG)\\n\\nFor this article, I have the following resources pre-deployed already:\\n\\n| **Name**                  | **Type**               | **Region** |\\n|---------------------------|------------------------|------------|\\n| openai-ca-res-eastus      | Azure OpenAI           | East US    |\\n| openai-ca-res-uksouth     | Azure OpenAI           | UK South   |\\n| apim-lmv01-dev-eastus-aka | API Management service | East US    |\\n\\nWe will start by adding openai-ca-res-eastus to the Azure API Management service first before looking at adding the other resources.\\n\\n> This will automatically, enable the System Managed Identity of API Management, to access the Azure OpenAI service with the role of [Cognitive Services OpenAI User](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796#cognitive-services-openai-user), and create a new Backend instance, and operations.\\n\\n![Add Azure OpenAI to API Management](images/apim_add_azureopenaibackend.gif)\\n\\nWe can see the police changes applied, pointing to the backend and system-managed identity.\\n\\n![Azure OpenAI API Management Policies](images/apim_api_openaipolicybase.png)\\n\\n:::info\\nThere is Azure Portal integration to enable a few API policies that are out of the box to help you manage the token consumption of the Azure OpenAI API.\\n\\nThese are:\\n\\nManage token consumption _(Use the Azure OpenAI token limit policy to protect the API from overuse and to control Azure OpenAI cost. If selected, API Management will add the policy with the configured TPM value. You can add, edit, or remove this policy after the API is created.)_\\nTrack token usage _(Use the Azure OpenAI emit token policy to log the consumed total, completion, and prompt tokens. If selected, API Management will add the policy with the specified configuration. You can add, edit, or remove this policy after the API is created)_\\n:::\\n\\nNow that we have a frontend for our Azure OpenAI service, it\'s time to test the Azure OpenAI endpoint through Azure API Management.\\n\\nTo do this, we will need a:\\n\\n* deployment-id _(this is the name of the Model deployment, in Azure OpenAI)\\n* api-version _(this is the version of the Azure OpenAI API, that we want to use)_\\n\\nWe can get this information from the Azure AI Studio, in the Azure Portal, in the Deployment blade.\\n\\n![Azure OpenAI Deployment ID and API Version](images/AzureAIStudio-gpt4odeploymenturl.png)\\n\\nNow, we can test the Azure OpenAI endpoint, through Azure API Management. In Azure API Management, navigate to the openai API that you created, and select the Creates a completion for the chat message operation and the Test tab.\\n\\nAdd in your deployment ID and API version, and in the request body, enter a sample prompt like the one below:\\n\\n```json\\n{\\n  \\"messages\\": [\\n    {\\"role\\": \\"system\\", \\"content\\": \\"You are a knowledgeable assistant in an ice cream parlor.\\"},\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What are the ingredients typically used to make a classic vanilla ice cream?\\"}\\n  ],\\n  \\"max_tokens\\": 100\\n}\\n```\\n\\n![Azure OpenAI Test](images/apim_test_azureopenaibackend.gif)\\n\\nThis confirms that we can now communicate with the Azure OpenAI service, through Azure API Management.\\n\\nSo let us test Request forwarding, from a client application, such as Postman.\\n\\nWe will need to get the subscription key from the Azure API Management instance and add this to the request header as \'API-key\' after first associating the open API with a Product and the subscription key is generated.\\n\\n![Azure OpenAI Postman Request Forwarding](images/apim_test_postmanrequestforwarding.gif)\\n\\nHere is a sample PowerShell script for invoking the Azure OpenAI endpoint, through Azure API Management, as well.\\n\\n```powershell\\n$headers = New-Object \\"System.Collections.Generic.Dictionary[[String],[String]]\\"\\n$headers.Add(\\"api-key\\", \\"YOURSUBSCRIPTIONKEY\\")\\n$headers.Add(\\"Content-Type\\", \\"application/json\\")\\n\\n$body = @\\"\\n{\\n    `\\"messages`\\": [\\n        {\\n            `\\"role`\\": `\\"system`\\",\\n            `\\"content`\\": `\\"You are a knowledgeable assistant in an ice cream parlor.`\\"\\n        },\\n        {\\n            `\\"role`\\": `\\"user`\\",\\n            `\\"content`\\": `\\"What are the ingredients typically used to make a classic vanilla ice cream?`\\"\\n        }\\n    ],\\n    `\\"max_tokens`\\": 100\\n}\\n\\"@\\n\\n$response = Invoke-RestMethod \'https://apim-lmv01-dev-eastus-aka.azure-api.net/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\' -Method \'POST\' -Headers $headers -Body $body\\n$response | ConvertTo-Json\\n```\\nWe have successfully implemented Request forwarding, through API Management to the Azure OpenAI endpoint.\\n\\n## \ud83d\udd0c Backend circuit breaking\\n\\nNow, let us take a look at Backend circuit breaking. The [Circuit Breaker](https://learn.microsoft.com/azure/architecture/patterns/circuit-breaker?WT.mc_id=AZ-MVP-5004796) pattern, follows the same principles as the electrical circuit breaker. It is used to detect failures and encapsulate the logic of preventing a failure from constantly recurring, during maintenance, temporary overloads, or unexpected spikes in traffic. When the circuit breaker trips, it can return an error immediately, without trying to execute the operation.\\n\\n:::tip\\nThe Circuit Breaker pattern is mentioned in my [Cloud Design Patterns](https://luke.geek.nz/azure/cloud-design-patterns/) blog post.\\n:::\\n\\nA circuit breaker acts as a proxy for operations that might fail. The proxy should monitor the number of recent failures that have occurred, and use this information to decide whether to allow the operation to proceed, or simply return an exception immediately, when the circuit is Closed the requests are allowed to pass through, when the circuit is Open, the requests are blocked.\\n\\n![Backend Circuit Breaking](images/AI_Gateway_APIMCircuitBreaker.png)\\n\\nTo implement a circuit breaker in API Management, we will need to add a policy to the Azure OpenAI API that monitors the number of recent failures and uses this information to decide whether to allow the operation to proceed or simply return an exception immediately.\\n\\nUnlike Request forwarding, configuring the Circuit Breaker cannot currently be done in the Azure Portal and will require configuration through Infrastructure as Code (i.e., Bicep)_ or the API Management REST API.\\n\\n:::warning\\nBefore you configure a circuitbreaker policy, make sure you have a backup of your Backend configuration and test this in a non-production environment.\\n:::\\n\\nToday, we will configure it using Bicep, by referencing the backend of an already existing API Management resource, and adding in the circuitBreaker rule.\\n\\n![Backend Circuit Breaking Bicep](images/AI_Gateway_APIMCircuitBreakerBicepDeploy.png)\\n\\n```bicep\\nparam backend string\\nparam existingUrl string\\n\\n// Use the parameters to update properties\\nresource updatedBackend \'Microsoft.ApiManagement/service/backends@2023-09-01-preview\' = {\\n  name: backend\\n  properties: {\\n    url: existingUrl  // Use the parameter to keep the existing URL\\n    protocol: \'http\'\\n    circuitBreaker: {\\n      rules: [\\n        {\\n          failureCondition: {\\n            count: 3 // Number of failures before tripping the circuit breaker\\n            errorReasons: [\\n              \'Server errors\' // Reasons for the failure\\n            ]\\n            interval: \'PT1H\' // Time interval to count the failures\\n            statusCodeRanges: [\\n              {\\n                min: 500 // Minimum status code to consider as a failure\\n                max: 599 // Maximum status code to consider as a failure\\n              }\\n            ]\\n          }\\n          name: \'myBreakerRule\' // Name of the circuit breaker rule\\n          tripDuration: \'PT1H\' // Duration for which the circuit breaker remains tripped\\n          acceptRetryAfter: true // Whether to accept retry after the trip duration\\n        }\\n      ]\\n    }\\n  }\\n}\\n\\n```\\n\\nIn the Circuit Breaker rule above, the following behavior will occur:\\n\\n| **Parameter**             | **Description**                                                                                                                        |\\n|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\\n| **Number of Failures**    | The circuit breaker will trip if there are 3 or more failures within the specified time interval.                                     |\\n| **Error Reasons**         | Failures considered are those matching the reason \'Server errors\'. The application or service should categorize errors with this reason. |\\n| **Status Code Ranges**    | Failures are identified based on HTTP status codes ranging from 500 to 599, covering server-side errors such as internal server errors (500), service unavailable (503), and similar issues. |\\n| **Time Interval**         | Failures are counted within a time interval of 1 hour (\'PT1H\'). If there are 3 or more failures with status codes in the 500\u2013599 range within any 1-hour window, the circuit breaker will trip. |\\n| **Trip Duration**         | Once tripped, the circuit breaker remains in the tripped state for 1 hour (\'PT1H\'). During this period, requests will not pass through until the breaker is reset. |\\n| **Retry Behavior**        | After the trip duration of 1 hour, the circuit breaker is designed to allow retries (acceptRetryAfter: true). Requests will be accepted again after the trip duration, and the service will attempt to recover. |\\n| **Scenarios Where the Circuit Breaker Will Trip** | If, within any 1-hour period, there are 3 or more server-side errors (HTTP status codes 500\u2013599) with the reason \'Server errors\', the circuit breaker will trip. Once tripped, the circuit breaker will block requests for 1 hour. After the 1-hour block period, the circuit breaker will reset and start accepting requests again. |\\n\\nThere is nothing in the Azure Portal for API management, but if we take a look at the backend provider resource, we can confirm that the configuration for the backend has been set.\\n\\n![Backend Circuit Breaking Bicep](images/AI_Gateway_APIMCircuitBreakerPostBicepDeploy.png)\\n\\n## \u2696\ufe0f Backend Load Balancing\\n\\nLet us now take a look at Backend Load Balancing with [Load-Balanced Pools](https://learn.microsoft.com/azure/api-management/backends?tabs=bicep&WT.mc_id=AZ-MVP-5004796#load-balanced-pool). Load balancing is the process of distributing network traffic across multiple endpoints. This ensures that no single server bears too much demand. By spreading the work evenly, load balancing improves application responsiveness.\\n\\n![Backend Load Balancing](images/AI_Gateway_APIMBackendLoadBalancing.PNG)\\n\\nBackend pooling your Azure OpenAI endpoints, allows you to:\\n\\n* Spread the load to multiple backends, which may have individual backend circuit breakers.\\n* Shift the load from one set of backends to another for upgrade _(blue-green deployment)_.\\n\\nNote Backend pools don\'t need any policy configuration. The rules are just properties of the backend.\\n\\n:::info\\nAPI Management supports the following load balancing options for backend pools:\\n\\n* Round-robin: By default, requests are distributed evenly across the backends in the pool.\\n* Weighted: Weights are assigned to the backends in the pool, and requests are distributed across the backends based on the relative weight assigned to each backend. Use this option for scenarios such as conducting a blue-green deployment.\\n* Priority-based: Backends are organized in priority groups, and requests are sent to the backends in order of the priority groups. Within a priority group, requests are distributed either evenly across the backends, or (if assigned) according to the relative weight assigned to each backend.\\n\\n_(Backends in lower priority groups will only be used when all backends in higher priority groups are unavailable because circuit breaker rules are tripped.)_\\n:::\\n\\nIf we go back to my environment, we are currently operating with the following resources:\\n\\n| **Name**                  | **Type**               | **Region** |\\n|---------------------------|------------------------|------------|\\n| openai-ca-res-eastus      | Azure OpenAI           | East US    |\\n| openai-ca-res-uksouth     | Azure OpenAI           | UK South   |\\n| apim-lmv01-dev-eastus-aka | API Management service | East US    |\\n\\nSo so far, the Azure OpenAI backend of eastus is being used, but we can add the uksouth backend to the backend pool, and configure it to be used in a round-robin fashion. \\n\\n:::info\\nOne of the reasons you may want to do this is not only to ensure reliability, using a priority group - that your Azure OpenAI service is highly available and can be accessed from multiple regions, but you may also want to ensure that you are not hitting the [TPM _(Tokens Per Minute)_ regional limits](https://learn.microsoft.com/azure/ai-services/openai/quotas-limits?WT.mc_id=AZ-MVP-5004796#regional-quota-limits) of the Azure OpenAI service, and that you are distributing the load across multiple regions. \\n\\nA common scenario here would be to have a primary region, that has a [PTU _(Proviosned Throughout)_](https://learn.microsoft.com/azure/ai-services/openai/concepts/provisioned-throughput?WT.mc_id=AZ-MVP-5004796) assigned, and failover to a pay as you go region.\\n:::\\n\\nSimilar to the Circuit Breaker rule capability, we need to configure Backend Pools using the Azure RestAPI or an Infrastructure as Code method, such as Bicep.\\n\\nIn my example, I will be adding the openai-ca-res-uksouth to the backend pool of the Azure OpenAI API and configuring it to use EastUS first and failover to UKSouth if east us stops responding.\\n\\nTo do this, we need to add a new backend and then reference that in a new pool configuration.\\n\\n:::note\\nWhen we initially added our first Azure OpenAI resource, a lot of the configuration was done for us, including adding the [Cognitive Services OpenAI User](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=AZ-MVP-5004796#cognitive-services-openai-user) role to the System Managed Identity of the API Management instance to authenticate with the Azure OpenAI resource, so make sure you grant Azure API Management the necessary permissions to access any other Azure OpenAI resource.\\n\\n![Azure OpenAI API Management Policies](images/apim_add_cognitiveaiuserrbacuksouth.gif)\\n:::\\n\\nNavigate to your secondary Azure OpenAI instance, and navigate to Keys and Endpoints, we need to grab the endpoint URL ie _(https://openai-ca-res-uksouth.openai.azure.com/)_, to add as another Backend and append with /openai.\\n\\n![Azure OpenAI UKSouth Endpoint](images/apim_add_cognitiveaiserviceuksouth.gif)\\n\\nNow that its added as a backend, we can add it to the backend pool, and configure it.\\n\\n```bicep\\nparam backendnane string\\nparam subscriptionID string\\nparam resourceGroupName string\\nparam APIManagementName string\\nparam backend1 string\\nparam backend2 string\\n\\n// Use the parameters to update properties\\nresource updatedBackend \'Microsoft.ApiManagement/service/backends@2023-09-01-preview\' = {\\n  name: \'${APIManagementName}/${backendnane}\'\\n  properties: {\\n    description: \'Load balancer for multiple backends\'\\n    type: \'Pool\'\\n    pool: {\\n      services: [\\n        // Define the backend pool configuration for an API Management service\\n        {\\n          // Define the first backend with a specific ID, priority, and weight\\n          id: \'/subscriptions/${subscriptionID}/resourceGroups/${resourceGroupName}/providers/Microsoft.ApiManagement/service/${APIManagementName}/backends/${backend1}\'\\n          priority: 1 // Higher priority backend\\n          weight: 1   // Weight of the backend\\n        }\\n        {\\n          // Define the second backend with a specific ID, priority, and weight\\n          id: \'/subscriptions/${subscriptionID}/resourceGroups/${resourceGroupName}/providers/Microsoft.ApiManagement/service/${APIManagementName}/backends/${backend2}\'\\n          priority: 1 // Higher priority backend\\n          weight: 2   // Weight of the backend\\n        }\\n      ]\\n    }\\n  }\\n}\\n\\n```\\n\\n![Backend Pool Bicep](images/AI_Gateway_APIMBackendPoolBicepDeploy.png)\\n\\nOnce the deploy is done, we can see the backend pool configuration in the Azure Portal.\\n\\n![Backend Pool Bicep](images/AI_Gateway_APIMBackendPool.png)\\n\\nNow we need to update the Azure API Managememt OpenAI API, to use the backend pool, instead of the single backend.\\n\\nNavigate to APIs, and your OpenAI API, select All Operations and then select the edit button.\\n\\nChange the backend it to the name of your Backend Pool.\\n\\n![Backend Pool Bicep](images/AI_Gateway_APIMBackendPoolAPIPolicyConfig.png)\\n\\nNow if we test the Azure OpenAI endpoint, through Azure API Management, we can see that the requests are being distributed across the two backends in the backend pool.\\n\\nIf we enable, Tracing we can see the handoff, and our API callers are oblivious to the fact that their API call is getting redirected. \\n\\n![Backend Pool Bicep](images/apim_test_loadbalancedpools.gif)\\n\\nYou can use the Circuitbreaker and Backend Pooling in conjunction with each other, to ensure that your Azure OpenAI service is highly available, and can be accessed from multiple regions. \\n\\n:::info\\nThe Circuitbreaker policies are on the Backends, not the Pools.\\n:::\\n\\nFor example, I changed the circuitBreaker rule, status code range to: 401 _(Permission Denied)_ and removed the Cogniative User role from the System Managed Identity of the API Management instance, to simulate a backend failure, and moved the UkSouth endpoint to priority group 2.\\n\\n![Backend Pool Bicep](images/AI_Gateway_APIMBackendPoolPermissionDenied.png)\\n\\n```bicep\\n         statusCodeRanges: [\\n              {\\n               // Define the range of HTTP status codes that will be considered as failures for the circuit breaker\\n                min: 401 // Minimum status code to consider as a failure (e.g., Unauthorized)\\n                max: 401 // Maximum status code to consider as a failure (e.g., Unauthorized)\\n              }\\n            ]\\n```\\n\\nAnd it worked as expected, the circuit breaker tripped, and the requests were redirected to the UKSouth backend.\\n\\n![Backend Pool Bicep](images/apim_test_loadbalancedcircuitbreakerpools.gif)\\n\\n## \ud83d\udce1 Response Streaming\\n\\nLet us take a look at Response Streaming. Response streaming is a technique used to send a response to a client in chunks, rather than all at once. This can be useful when the response is large, or when the response is being generated in real-time.\\n\\n![Response Streaming](images/AI_Gateway_APIMResponseStreaming.PNG)\\n\\n:::tip\\nFollow the [guidelines](https://learn.microsoft.com/en-us/azure/api-management/how-to-server-sent-events?WT.mc_id=AZ-MVP-5004796#guidelines-for-sse) when using API Management to reach a backend API that implements SSE (Server side streaming).\\n:::\\n\\nIf I go back to our Creates a completion for the chat message, operation, and edit the policy and add the following to the inbound and outbound Azure policies.\\n\\n```xml\\n    <inbound>\\n        \x3c!-- Base inbound processing --\x3e\\n        <base />\\n        \x3c!-- Conditional logic to check if the request body contains a non-null \\"stream\\" property --\x3e\\n        <choose>\\n            <when condition=\\"@(context.Request.Body.As<JObject>(true)[\\"stream\\"] != null && context.Request.Body.As<JObject>(true)[\\"stream\\"].Type != JTokenType.Null)\\">\\n                \x3c!-- Set a variable \\"isStream\\" with the value of the \\"stream\\" property from the request body --\x3e\\n                <set-variable name=\\"isStream\\" value=\\"@{\\n                    var content = (context.Request.Body?.As<JObject>(true));\\n                    string streamValue = content[\\"stream\\"].ToString();\\n                    return streamValue;\\n                }\\" />\\n            </when>\\n        </choose>\\n    </inbound>\\n    \x3c!-- Control if and how the requests are forwarded to services --\x3e\\n    <backend>\\n        \x3c!-- Base backend processing --\x3e\\n        <base />\\n    </backend>\\n    \x3c!-- Customize the responses --\x3e\\n    <outbound>\\n        \x3c!-- Base outbound processing --\x3e\\n        <base />\\n        \x3c!-- Set a custom header \\"x-ms-stream\\" based on the value of the \\"isStream\\" variable --\x3e\\n        <set-header name=\\"x-ms-stream\\" exists-action=\\"override\\">\\n            <value>@{\\n                    return context.Variables.GetValueOrDefault<string>(\\"isStream\\",\\"false\\").Equals(\\"true\\", StringComparison.OrdinalIgnoreCase).ToString();\\n                }</value>\\n        </set-header>\\n    </outbound>\\n```\\n\\nThis sets the x-ms-stream header to true, if the request body contains a non-null \\"stream\\" property, allowing streaming, so let us test this in Postman\\n\\nBy using the following prompt:\\n\\n```json\\n{\\n    \\"messages\\": [\\n        {\\n            \\"role\\": \\"system\\",\\n            \\"content\\": \\"You are a knowledgeable assistant in an ice cream parlor.\\"\\n        },\\n        {\\n            \\"role\\": \\"user\\",\\n            \\"content\\": \\"What are the ingredients typically used to make a classic vanilla ice cream?\\"\\n        }\\n    ],\\n    \\"max_tokens\\": 100,\\n    \\"stream\\": true\\n}\\n```\\n\\n![Response Streaming](images/apim_test_responsestreamingpostman.gif)\\n\\nNote: Full JSON streaming support [is not avaliable](https://github.com/postmanlabs/postman-app-support/issues/12713) at this time of this article in postman, so again - you need a client to support this.\\n\\nWe can, however see the response does include the response being streamed and broken up into chunks:\\n\\n![Response Streaming](images/AI_Gateway_APIMResponseStreamingPostmanOutput.PNG)\\n\\nTo remove the streaming component, we can either remove the stream from the body or change the stream from true to false.\\n\\n## \u23f3 Token rate limiting\\n\\nLet us take a look at Token rate limiting. Token rate limiting is a technique used to limit the number of requests that can be made to an API within a certain time period. This can help prevent abuse of the API, and ensure that all users have fair access to the API.\\n\\n![Token Rate Limiting](images/AI_Gateway_APIMTokenRateLimiting.PNG)\\n\\nTo implement token rate limiting in API Management, we need to add a policy to the Azure OpenAI API, that will limit the number of requests that can be made to the API within a certain time period, the tokens per minute.\\n\\nThis is an Inbound policy, and will limit the tokens per minute, with a 429 _(Too Many Requests)_ response, that can be called from a single IP address _(each unqiue IP address will have its own counter)_.\\n\\n![Token Rate Limiting](images/apim_set_RateLimitPolicy.gif)\\n\\n\\n```xml\\n        <azure-openai-token-limit counter-key=\\"@(context.Request.IpAddress)\\" tokens-per-minute=\\"500\\" estimate-prompt-tokens=\\"false\\" remaining-tokens-variable-name=\\"remainingTokens\\" />\\n```\\n\\n| **Property**                          | **Description**                                                                                     |\\n|---------------------------------------|-----------------------------------------------------------------------------------------------------|\\n| `<azure-openai-token-limit>`          | This is a custom tag or directive used to define the rate-limiting policy for Azure OpenAI tokens. |\\n| `counter-key=\\"@(context.Request.IpAddress)\\"` | This specifies that the rate-limiting should be based on the client\'s IP address. Each unique IP address will have its own counter. |\\n| `tokens-per-minute=\\"500\\"`             | This sets the limit to 500 tokens per minute for each IP address. Once an IP address consumes 500 tokens within a minute, further requests will be restricted until the next minute. |\\n| `estimate-prompt-tokens=\\"false\\"`      | This indicates whether to estimate the number of tokens in the prompt. Setting it to false means the actual token count will be used rather than an estimate. |\\n| `remaining-tokens-variable-name=\\"remainingTokens\\"` | This specifies the variable name (`remainingTokens`) that will store the number of tokens remaining for the current minute. |\\n\\nSo lets test it, in my test, I am using a PowerShell script to call the Azure OpenAI endpoint, through Azure API Management, and I am calling it 20 times, in a loop, to test the rate limiting.\\n\\n```powershell\\n# Define the headers\\n$headers = New-Object \\"System.Collections.Generic.Dictionary[[String],[String]]\\"\\n$headers.Add(\\"api-key\\", \\"YOURSUBSCRIPTIONKEY\\")\\n$headers.Add(\\"Content-Type\\", \\"application/json\\")\\n\\n# Define the body\\n$body = @\\"\\n{\\n    `\\"messages`\\": [\\n        {\\n            `\\"role`\\": `\\"system`\\",\\n            `\\"content`\\": `\\"You are a knowledgeable assistant in an ice cream parlor.`\\"\\n        },\\n        {\\n            `\\"role`\\": `\\"user`\\",\\n            `\\"content`\\": `\\"What are the ingredients typically used to make a classic vanilla ice cream?`\\"\\n        }\\n    ],\\n    `\\"max_tokens`\\": 100\\n}\\n\\"@\\n\\n# Loop to run the script 20 times\\nfor ($i = 1; $i -le 20; $i++) {\\n    try {\\n        # Send the request and capture the response\\n        $response = Invoke-RestMethod \'https://apim-lmv01-dev-eastus-aka.azure-api.net/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\' -Method \'POST\' -Headers $headers -Body $body\\n\\n        # Output the response as JSON\\n        $responseJson = $response | ConvertTo-Json -Depth 10\\n     \\n        Write-Output $responseJson\\n    } catch {\\n        # Handle any errors that occur during the request\\n        Write-Error $_.Exception.Message\\n    }\\n}\\n```\\n\\n![Token Rate Limit test](images/apim_test_RateLimitPolicy.gif)\\n\\nAs we can see, it instantly gets declined after the 500th token; however, traffic from other IP addresses can still access the API.\\n\\n## \ud83d\uddc2\ufe0f Semantic Caching\\n\\nLet us take a look at [Semantic Caching](https://learn.microsoft.com/en-us/azure/api-management/azure-openai-enable-semantic-caching?WT.mc_id=AZ-MVP-5004796). Semantic caching is a technique used to cache responses from an API based on the content of the request. This can help improve performance by reducing the number of requests that need to be made to the API.\\n\\n> With semantic caching, you can return cached responses for identical prompts and also for prompts that are similar in meaning, even if the text isn\'t the same. \\n\\nTo use Semantic Caching, we need an Embedding model to generate embeddings for the prompts and a Cache policy to cache the responses based on the embeddings.\\n\\nIn my example, I will deploy an Embedding model _(text-embedding-ada-002)_ to the East US Azure OpenAI resource.\\n\\n![Semantic Caching - text-embedding-ada-002 deployment](images/AI_Gateway_APIMSemanticCaching_EmbeddingDeployment.PNG)\\n\\nNow, we need to add a Backend for the embedding endpoint, similar to how we originally added the US East backend. If you are using load balancing, you can add this to a new backend pool as well, as long as the model is deployed across all endpoints.\\n\\nHowever, instead of a /chat endpoint, we will use the /embed endpoint, and the deployment will be the name of the embedding model deployment, as an example _(https://openai-ca-res-eastus.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings)_.\\n\\nOnce we have our URL we can add a new Backend. \\n\\n:::info\\nThis could be deployed to another Azure OpenAI instance, but make sure you enable the Cognitive Services OpenAI User role on the System Managed Identity of the API Management instance to authenticate with the Azure OpenAI resource.\\n:::\\n\\n![Semantic Caching - Embedding Backend](images/apim_set_SemanticCachingModelBackend.gif)\\n\\nNow, it is time to configure our [semantic caching policy](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy?WT.mc_id=AZ-MVP-5004796).\\n\\n> We will need a **[Redis Enterprise Cache deployed](https://learn.microsoft.com/azure/api-management/api-management-howto-cache-external?WT.mc_id=AZ-MVP-5004796)**, and configured as an External cache to API Management, with **[RediSearch module](https://learn.microsoft.com/azure/azure-cache-for-redis/cache-redis-modules?WT.mc_id=AZ-MVP-5004796#redisearch) enabled** and then we can add the semantic caching policy to the Azure OpenAI API. Without the Redis cache, the semantic caching policy will not work, and the azure-openai-semantic-cache-lookup policy will be skipped.\\n\\nNavigate to our OpenAI API, select the Creates a completion for the chat message operation, and add the following policy to the inbound section.\\n\\n```xml\\n<azure-openai-semantic-cache-lookup\\n    score-threshold=\\"0.5\\"  \x3c!-- Adjusted threshold for better precision --\x3e\\n    embeddings-backend-id=\\"azureaiembeddingbackend\\"  \x3c!-- Updated backend ID --\x3e\\n    embeddings-backend-auth=\\"system-assigned\\"\\n    ignore-system-messages=\\"false\\"  \x3c!-- Include system messages if they add value --\x3e\\n    max-message-count=\\"15\\"> \x3c!-- Increased message count for more context --\x3e\\n    <vary-by>@(context.Subscription.Id)</vary-by>\\n</azure-openai-semantic-cache-lookup>\\n```\\n| **Element**                                      | **Description**                                                                                                                                         |\\n|--------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `<azure-openai-semantic-cache-lookup>`           | A custom tag used to define the semantic cache lookup policy for Azure OpenAI.                                                                           |\\n| `score-threshold=\\"0.8\\"`                          | Sets the threshold for similarity scores. Only results with a score of 0.5 or higher are considered.                                                     |\\n| `embeddings-backend-id=\\"embeddings-deployment\\"`  | Specifies the ID of the embeddings backend deployment used for the cache lookup.                                                                         |\\n| `embeddings-backend-auth=\\"system-assigned\\"`      | Indicates that the system-assigned managed identity is used for authentication with the embeddings backend.                                              |\\n| `ignore-system-messages=\\"true\\"`                  | Specifies that system messages should be ignored during the cache lookup.                                                                                |\\n| `max-message-count=\\"10\\"`                         | Sets the maximum number of messages to be considered in the cache lookup.                                                                                |\\n| `<vary-by>@(context.Subscription.Id)</vary-by>`  | Specifies that the cache should vary by subscription ID, meaning each subscription will have its own cache.                                              |\\n\\n\\nIn the Outbound processing section for the API, add the azure-openai-semantic-cache-store policy.\\n\\n```xml\\n<azure-openai-semantic-cache-store duration=\\"60\\" />\\n```\\n\\n| **Attribute**                              | **Description**                                                                                           |\\n|----------------------------------------|-------------------------------------------------------------------------------------------------------|\\n| `<azure-openai-semantic-cache-store>`  | This is a custom tag or directive used to define the semantic cache store policy for Azure OpenAI.     |\\n| `duration=\\"60\\"`                        | This specifies the duration for which the cached data should be stored in minutes. In this case, the cache will be stored for 60 minutes. |\\n\\n![Semantic Caching - Embedding Backend](images/apim_Add_SemanticCachingAPIPolicies.gif)\\n\\nOnce added, we can test it.\\n\\nWe can do this, using the [Trace](https://learn.microsoft.com/azure/api-management/api-management-howto-api-inspector?WT.mc_id=AZ-MVP-5004796) feature in Azure API Management, and we can see the cache hit, and the response being returned from the cache.\\n\\n:::warning\\nIf you get HTTP/1.1 415 model_error, make sure you are using a [model that supports embeddings](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?WT.mc_id=AZ-MVP-5004796), and that the model is deployed.\\nAlso if you are testing, make sure that the content-type is application/json.\\n:::\\n\\n:::info\\nI was not able to test this end-to-end in my environment due to needing a payment instance for Redis, not covered under my Visual Studio Enterprise subscription.\\n:::\\n\\nIf this is all configured you should see something like the below in a Trace:\\n\\n![Semantic Caching - Semantic Cache lookup](images/semantic-cache-lookup.png)\\n\\n## \ud83d\udcdc Logging\\n\\nNow let us take a look at Logging. Logging is the process of recording events that occur during the execution of a process. Logs can be used to monitor and troubleshoot requests, as well as to analyze and report on application performance.\\n\\n![APIM Built in Logging](images/AI_Gateway_APIMBuiltinLogging.PNG)\\n\\nTo do this, we need to enable [Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?WT.mc_id=AZ-MVP-5004796), which will create a [Logger entity](https://learn.microsoft.com/azure/templates/microsoft.apimanagement/service/loggers?pivots=deployment-language-bicep&WT.mc_id=AZ-MVP-5004796) to Application Insights from API Management.\\n\\n![APIM Built in Logging](images/AI_Gateway_APIMLogging_AppInsights.PNG)\\n\\n:::tip\\nIf you\'re not using Azure API Management, I recommend you check out the workbook by Dolev Shor (Microsoft Fastrack Engineer). You can read more about it here: [Azure OpenAI Insights: Monitoring AI with Confidence](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/azure-openai-insights-monitoring-ai-with-confidence/ba-p/4026850?WT.mc_id=AZ-MVP-5004796).\\n:::\\n\\nSo, let\'s take a look at some of the logs generated by the Azure OpenAI API through Azure API Management and visible through the Application Insights and Log Analytics workspace.\\n\\n![APIM Built in Logging](images/AI_Gateway_APIMBuiltinLoggingAppInsights.PNG)\\n\\nThe first thing you could do is configure an availability test for your API Management endpoint.\\n\\nThe API Management Gateway health probe URL is: /status-0123456789abcdef\\n\\nSo our availability test URL would be: \'https://apim-lmv01-dev-eastus-aka.azure-api.net/status-0123456789abcdef\'\\n\\n![API Management avaliability test](images/AI_Gateway_APIMLogging_AppInsightsAvaliabilityTest.PNG)\\n\\nNext, we need to make sure that we are logging the information from the API requests, such as client IP, request URL, response code, and response time, to do that, we need to [enable the Application Insights logging on our openai](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-app-insights?tabs=rest&WT.mc_id=AZ-MVP-5004796) API in API Management.\\n\\n![APIM Built in Logging](images/AI_Gateway_APIMBuiltinLoggingAppInsightsEnable.PNG)\\n\\nLet\'s take a look at a custom [Azure Monitor workbook](https://learn.microsoft.com/azure/azure-monitor/visualize/workbooks-overview?WT.mc_id=AZ-MVP-5004796) that helps visualize and bring out a lot of the data you need to make informed decisions without having to delve into KQL _(Kusto Query Language)_ queries in Log Analytics.\\n\\nIn Application Insights, navigate to Workbooks, click + New, click on the Code and paste the workbook data from: [AI-Gateway/main/labs/built-in-logging/openai-usage-analysis-workbook.json](https://github.com/Azure-Samples/AI-Gateway/blob/main/labs/built-in-logging/openai-usage-analysis-workbook.json).\\n\\nYou can see the APIs getting called, the subscription key being used, the response time, and the response code.\\n\\n![APIM Built in Logging](images/apim_test_OpenAIWorkbook.gif)\\n\\nIf you are using a Token Limit policy, you can trace the OpenAITokenLimitExeeded exceptions. This can help you determine if you need to block any particular IP addresses either using an ip-filter policy or Application Gateway if that is in front of your API Management or if you need to increase the TPM limit.\\n\\n![App Insights - Token Limit](images/AI_Gateway_APIMBuiltinLoggingAppInsights_TokenLimit.PNG)\\n\\n## \ud83d\udd1a Conclusion\\n\\nIn this article, we have taken a look at how we can use Azure OpenAI as a backend to Azure API Management, how to test the Azure OpenAI endpoint, through Azure API Management, and how to implement Request forwarding, Backend circuit breaking, Backend Load Balancing, Response Streaming, Token rate limiting, Semantic Caching, and Logging.\\n\\nAll code can be found here: [lukemurraynz/AI_Gateway_Tests](https://github.com/lukemurraynz/AI_Gateway_Tests)\\n\\n## \ud83d\udcda Reference\\n\\n* \ud83d\udd17[What is Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Introducing GenAI Gateway Capabilities in Azure API Management](https://techcommunity.microsoft.com/t5/azure-integration-services-blog/introducing-genai-gateway-capabilities-in-azure-api-management/ba-p/4146525?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Cloud Design Patterns](https://luke.geek.nz/azure/cloud-design-patterns/)\\n* \ud83d\udd17[Features and Benefits of Azure API Management](https://luke.geek.nz/azure/api-management-overview/)\\n* \ud83d\udd17[Azure-Samples/AI-Gateway](https://aka.ms/apim/genai/labs)"},{"id":"azure/secure-container-supply-chain","metadata":{"permalink":"/azure/secure-container-supply-chain","source":"@site/blog/2024-08-08-cssc/index.mdx","title":"Securing Container Supply Chains with CSSC Framework","description":"Explore Microsoft\'s Containers Secure Supply Chain (CSSC) Framework, a robust ecosystem of tools and processes for securing container supply chains.","date":"2024-08-08T08:50:10.680Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":11.745,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Securing Container Supply Chains with CSSC Framework","metaDescription":"Explore Microsoft\'s Containers Secure Supply Chain (CSSC) Framework, a robust ecosystem of tools and processes designed to integrate and execute security controls throughout the lifecycle of containerized applications. Learn best practices for securing container supply chains against vulnerabilities, malware, and supply chain attacks.","date":"2024-08-08T08:50:10.680Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/secure-container-supply-chain","keywords":["azure","cloudnative","acr","container registry","container security","supply chain security","DevOps","Kubernetes","CI/CD","Docker","security best practices","vulnerability management","container orchestration","cloud security","infrastructure as code"],"description":"Explore Microsoft\'s Containers Secure Supply Chain (CSSC) Framework, a robust ecosystem of tools and processes for securing container supply chains."},"unlisted":false,"prevItem":{"title":"Implementing AI Gateway capabilities in API Management","permalink":"/azure/implementing-ai-gateway-in-api-management"},"nextItem":{"title":"Features and Benefits of Azure API Management","permalink":"/azure/api-management-overview"}},"content":"Today, we are going to take a look inside the [Containers Secure Supply Chain Framework](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/containers-secure-supply-chain-overview?WT.mc_id=AZ-MVP-5004796), a framework from Microsoft that utilizes an agile ecosystem of tools and processes built to integrate and execute security controls throughout the lifecycle of containers.\\n\\n{/* truncate */}\\n\\n## \ud83d\udccbOverview\\n\\n:::info\\n![Container Supply Chain (CSSC) Framework](images/ContainerSupplyFramework_Overview.PNG)\\n\\n> Microsoft\'s [Containers Secure Supply Chain Framework](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/containers-secure-supply-chain-overview?WT.mc_id=AZ-MVP-5004796) framework is a seamless, agile ecosystem of tools and processes built to integrate and execute security controls throughout the lifecycle of containers. The container secure supply chain strategy considers all the security needs of container applications. The goal is to prevent the use of vulnerable container images and create container infrastructure with a standard security portfolio.\\n\\nThe CSSC framework is built using the following steps:\\n\\n* Identify the supply chain stages for containerized applications\\n* Outline the risks and the required security controls in each stage\\n* Describe the security objectives and goals in each stage\\n* Identify security tools, processes, and best practices in each stage\\n* Maintain security posture with metadata, logging, and reporting in each stage\\n:::\\n\\nIn essence, the Containers Secure Supply Chain (CSSC) framework ensures the security and integrity of containerized applications throughout their lifecycle. It aims to prevent vulnerable container images and create a secure container infrastructure by integrating security controls at every stage of the supply chain.\\n\\nThe CSSC framework protects against several risks, including:\\n\\n* Vulnerable container images: Ensuring that only secure and compliant images are used.\\n* Malware and unauthorized changes: Verifying the authenticity and integrity of container images.\\n* Supply chain attacks: Mitigating risks from external sources and third-party vendors.\\n* Runtime threats: Reducing the attack surface of running containers.\\n\\nBy addressing these risks, the CSSC framework helps maintain a secure and reliable environment for containerized applications.\\n\\nDuring each stage, make sure you have gates in place to ensure that the image is secure, meets the minimum quality standard, and gets peer approval before moving to the next stage. This could be done by using Azure Policy, Azure Security Center, or CI/CD with tools such as Azure DevOps or GitHub.\\n\\n## \ud83d\udd04Stages of the Container Supply Chain\\n\\nWe will dig into each stage, with a focus on some of the tools, mainly [Azure Container Registry (ACR)](https://learn.microsoft.com/azure/container-registry/container-registry-intro?WT.mc_id=AZ-MVP-5004796), that can be used to implement the security controls in each stage.\\n\\nWe will consider using separate Container Registers, one for each stage, to ensure that the security controls are implemented at each stage.\\n\\n![Azure Container registries](images/ContainerSupplyFramework_ACRRegistries.PNG)\\n\\n* One for Quarantine\\n* One for Catalog\\n* One for deployment\\n\\n![Container Supply Chain (CSSC) Framework](images/ContainerSupplyFramework_PortalACRRegistries.PNG)\\n\\n:::tip\\nMake use of [Azure Container Registry webhooks](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-webhook?WT.mc_id=AZ-MVP-5004796) to trigger awareness and automation when images are pushed to the registries. This could trigger a pipeline to scan the images and move them to the next stage if they meet the minimum quality standard.\\n:::\\n\\nSo, let us dig into each stage.\\n\\n### \ud83d\uded2Stage 1: Acquire\\n\\nThe [Acquire](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/acquire-overview?WT.mc_id=AZ-MVP-5004796) stage is the first step in the container supply chain. It involves obtaining container images from sources that you may deem trusted or un-trusted and ensuring their integrity and authenticity.\\n\\n![Acquire](images/ContainerSupplyFramework_Acquire.PNG)\\n\\n* The Acquire stage aims to validate and ensure compliance of external container images with enterprise policies before internal use. This includes vulnerability and malware scanning.\\n* It involves centralizing the acquisition and management of external artifacts, importing images into an internal registry, and enriching images with metadata like SBOMs and provenance.\\n* Key tools include Azure Container Registry (ACR) for storing and distributing images, ORAS for interacting with OCI registries, ACR Tasks for automating tasks, and Microsoft Defender for Cloud security.\\n* Microsoft recommends building software from source when possible and, if not, following the CSSC framework\u2019s practices for acquiring external container images.\\n\\nIn the Aquire stage, you should establish a minimum quality standard for images sourced externally. This includes scanning for vulnerabilities and malware, verifying the authenticity and integrity of images, and enriching images with metadata like Software Bill of Materials (SBOMs) and provenance information.\\n\\nTools that can be useful during this stage are:\\n\\n* [Microsoft Defender for Cloud (for Containers)](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction?WT.mc_id=AZ-MVP-5004796)\\n* [Trivy](https://trivy.dev/)\\n* [Copacetic](https://project-copacetic.github.io/copacetic/website/)\\n* [Azure Container Registry (ACR)](https://learn.microsoft.com/azure/container-registry/container-registry-intro?WT.mc_id=AZ-MVP-5004796)\\n\\n> The process of quarantining is a security measure that consists of a series of checkpoints that are employed before an artifact is consumed. Those security checkpoints make sure that an artifact transitions from an untrusted status to a trusted status.\\n\\nLet\'s take a look at the [Quarantine](https://learn.microsoft.com/azure/architecture/patterns/quarantine?WT.mc_id=AZ-MVP-5004796) function of Azure Container Registry. This function could automate the process of scanning and quarantining images that do not meet the minimum quality standard.\\n\\nIn my demo, I will be using the [debian](https://hub.docker.com/_/debian) base image from the dockerhub and pushed into my Azure Container Registry.\\n\\n![Container Image - Debian](images/ContainerSupplyFramework_Quarantine_debianimage.PNG)\\n\\nLet\'s enable Quarantine functionality on the Azure Container Registry. **At the time of this article, this is a preview feature.**\\n\\n> When a registries policy is set to [Quarantine](https://learn.microsoft.com/rest/api/containerregistry/registries/update?view=rest-containerregistry-2023-01-01-preview&tabs=HTTP&WT.mc_id=AZ-MVP-5004796#quarantinepolicy) Enabled, all images pushed to that registry are put in quarantine by default. Only after the image has been verified and the quarantine flag removed may a subsequent pull be completed. Once Quarantine is enabled on a registry, a newly pushed image will enter a quarantine state automatically, and only a user with quarantine reader permissions can see the image. \\n\\nUsing Azure CLI, we can enable Quarantine on the Azure Container Registry. Quarantine functionality is only allowed on Premium SKU registries.\\n\\n![Enable Container Registry quarantinePolicy](images/ACR_EnableqQuarantine.gif)\\n\\n```bash\\nexport ACR_NAME=acrqr\\nid=$(az acr show --name $ACR_NAME --query id -o tsv)\\naz resource update --ids $id --set properties.policies.quarantinePolicy.status=enabled\\n```\\nOnce the image is quarantined, you need a user with the AcrQuarantineReader role. The presumption here is the Vulnerability Scanning solution is configured to use this account.\\n\\nWe can run the following Azure CLI to check the image status in the Quarantine.\\n\\n```bash\\nexport ACR_NAME=acrqr\\nexport IMAGE_NAME=debian\\naz acr manifest list -r $ACR_NAME -n $IMAGE_NAME\\n```\\n\\n!![Check Quarantine status](images/ACR_CheckQuarantine.gif)\\n\\n1. Now using [Defender for Containers](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction?WT.mc_id=AZ-MVP-5004796), we can scan our images in the Container Registry for vulnerabilities.\\n2. You can also use [Trivy](https://trivy.dev/) to scan the images for vulnerabilities and reveal more about the SBOM (Software Bill of Materials) used in the image.\\n3. Add any additional metadata to the image, such as the [SBOM](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/attach-sbom?WT.mc_id=AZ-MVP-5004796), and [provenance](https://docs.docker.com/build/attestations/slsa-provenance/) information.\\n\\n:::tip\\nCheck out [Container Patching with Azure DevOps, Trivy and Copacetic](https://luke.geek.nz/azure/automate-container-patching-with-trivy-copacetic-azure-devops/) blog article for more information on how to automate the scanning of images with Copacetic in the Azure Container Registry, from an Azure DevOps pipeline.\\n:::\\n\\nOnce you have determined that your Container Image\'s state is good, you can remove the Quarantine flag from it by running the following Azure CLI command.\\n\\nIn order to call the API, you currently need permissions for both Contributor and AcrQuarantineWriter (or AcrPush role) on the registry.\\n\\n```bash\\nexport ACR_NAME=acrqr\\nexport IMAGE_NAME=debian\\naz acr repository show-manifests --name $ACR_NAME --repository $IMAGE_NAME --detail\\n```\\n\\nFind the digest of the image you want to remove the quarantine flag from, and run the following Azure CLI command.\\n\\n```bash\\nexport ACR_NAME=acrqr\\nexport IMAGE_NAME=debian\\nexport MANIFEST_DIGEST=sha256:b1ae8b5bfaa9afa86b50c2a151a442d832c4449cf3731bddf8a728e5628ebb59\\nexport REPOSITORY=debian\\nexport REGISTRY=$ACR_NAME.azurecr.io\\n\\n# Get the Entra ID access token\\nexport EID_ACCESS_TOKEN=$(az account get-access-token --query accessToken -o tsv)\\n\\n# Exchange the AAD access token for an ACR refresh token\\nexport ACR_REFRESH_TOKEN=$(curl -s -X POST -H \\"Content-Type: application/x-www-form-urlencoded\\" \\\\\\n  -d \\"grant_type=access_token&service=$REGISTRY&access_token=$EID_ACCESS_TOKEN\\" \\\\\\n  https://$REGISTRY/oauth2/exchange \\\\\\n  | jq -r \'.refresh_token\')\\n\\n# Create the repo-level scope\\nSCOPE=\\"repository:$REPOSITORY:pull,metadata_write\\"\\n\\n# Get the ACR access token using the refresh token\\nexport ACR_ACCESS_TOKEN=$(curl -s -X POST -H \\"Content-Type: application/x-www-form-urlencoded\\" \\\\\\n  -d \\"grant_type=refresh_token&service=$REGISTRY&scope=$SCOPE&refresh_token=$ACR_REFRESH_TOKEN\\" \\\\\\n  https://$REGISTRY/oauth2/token \\\\\\n  | jq -r \'.access_token\')\\n\\n# Check if the access token is null\\nif [ \\"$ACR_ACCESS_TOKEN\\" == \\"null\\" ]; then\\n  echo \\"Failed to retrieve access token. Please check your credentials and scope.\\"\\n  exit 1\\nfi\\n\\n# Use the ACR access token in the curl command\\ncurl -v -X PATCH \\\\\\n  -H \\"Authorization: Bearer $ACR_ACCESS_TOKEN\\" \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d \'{\\n        \\"quarantineState\\": \\"Passed\\", \\n        \\"quarantineDetails\\": \\"{\\\\\\"state\\\\\\":\\\\\\"scan passed\\\\\\"}\\"\\n      }\' \\\\\\n  \\"https://$ACR_NAME.azurecr.io/acr/v1/$IMAGE_NAME/_manifests/$MANIFEST_DIGEST\\"\\n```\\n![Remove Quarantine](images/ACR_RemoveQuarantine.gif)\\n\\n### \ud83d\udce6Stage 2: Catalog\\n\\nThe [Catalog](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/catalog-overview?WT.mc_id=AZ-MVP-5004796) stage is the second step in the container supply chain. It involves storing and managing container images in a central repository, where they can be easily accessed and deployed.\\n\\n![Catalog](images/ContainerSupplyFramework_Catalog.PNG)\\n\\n:::info\\nMicrosoft recommends that internal teams use container images from an internal catalog whenever possible. \\n\\nIf enterprises are not able to do so, we recommend the following practices for cataloging container images.\\n\\n* Catalog golden images to enable internal teams to easily discover and consume approved images that enterprise applications and services require.\\n* Continuously scan container images for vulnerabilities and malware, generate reports, and sign reports to ensure authenticity and integrity.\\n* Monitor the lifecycle of catalog images and retire images that are out of support.\\n:::\\n\\nAfter the images have been acquired, vetted by the previous step, and meet organizational requirements, it\'s time to move the photos into a Catalog Container Registry, where they can be stored in a central repository and easily accessed.\\n\\nUnlike the Acquire Container Registry, this Registry will not be quarantined. However, it will need to be continuously scanned for vulnerabilities and malware as part of operations using tools like Defender for Cloud and Trivy.\\n\\n1. Now using [Defender for Containers](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction?WT.mc_id=AZ-MVP-5004796), we can scan our images in the Container Registry for vulnerabilities. As this is a Catalog registry, we need to make sure that we monitor and remediate any vulnerabilities that are found.\\n2. You can also make use of [Trivy](https://trivy.dev/), to scan the images for vulnerabilities.\\n\\nIt can also be valid to move images from the Catalog back to the Quarantine registry if the Catalog registry is not monitored or the image has been found to have high-priority vulnerabilities that fall beneath an organization threshold.\\n\\nYou can use the following Azure CLI command to move an image between the Quarantine and Catalog registries.\\n\\n```bash\\n# Variables\\nSOURCE_REGISTRY=\\"acrqr\\"\\nTARGET_REGISTRY=\\"acrcatalog\\"\\nIMAGE_NAME=\\"debian:v1\\"\\n\\n# Import image from source registry to target registry\\naz acr import --name $TARGET_REGISTRY --source $SOURCE_REGISTRY.azurecr.io/$IMAGE_NAME --image $IMAGE_NAME\\n```\\n\\n![Move Image between Registries](images/ACR_MoveQuarantinetoCatalog.gif)\\n\\nWe can see the image now in our Catalog registry. The image is left in the Quarantine registry, as it has not been removed. If needed, you can now remove this.\\n\\n![Azure Container Registry - Catalog](images/ContainerSupplyFramework_Catalog_debianimage.PNG)\\n\\n### \ud83d\udee0\ufe0fStage 3: Build\\n\\nThe [Build](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/build-overview?WT.mc_id=AZ-MVP-5004796) step is the third step in the container supply chain. It involves building container images from source code and ensuring that they meet security and compliance requirements, whereas previous steps were more aligned with container images that were sourced externally. However, the following practices can be applied to both.\\n\\n![Build](images/ContainerSupplyFramework_Build.PNG)\\n\\n> It is crucial to ensure that base images are always pulled from the internal catalog and verified before use. After building the images, some enterprises publish them without vulnerability and malware scanning and fail to generate attestations like SBOM, which cannot be verified in subsequent supply chain stages. It is essential to ensure that the produced container images are trusted and compliant with the enterprise policies.\\n\\nThe build phase is all about building application images consistently and securely. \\n\\n* Use trusted images only\\n* Verify lifecycle and provenance metadata\\n* Scan after build\\n* Fix known vulnerabilities during and post-build\\n* Sign images and artifacts\\n\\nTools that can be used during the Build stage are:\\n\\n* [GitHub](https://github.com/)\\n* [Azure DevOps](https://azure.microsoft.com/products/devops?WT.mc_id=AZ-MVP-5004796)\\n* [Defender for Cloud](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction?WT.mc_id=AZ-MVP-5004796)\\n* [Dependabot](https://docs.github.com/en/code-security/getting-started/dependabot-quickstart-guide)\\n* [Copacetic](https://project-copacetic.github.io/copacetic/website/)\\n\\n:::tip\\n[Azure Container Registry (ACR) Tasks](https://learn.microsoft.com/azure/container-registry/container-registry-tasks-overview?WT.mc_id=AZ-MVP-5004796) can help fill in the gaps in the Build stage. ACR Tasks can automate the building of container images from source code, scanning for vulnerabilities, and signing images, as part of your build workflow.\\n:::\\n\\n### \ud83d\udea2Stage 4: Deploy\\n\\nThe Deploy stage is the fourth step in the container supply chain. It involves deploying container images to production environments where end-users can access them.\\n\\n![Build](images/ContainerSupplyFramework_Deploy.PNG)\\n\\nThis stage is usually where you move the images downstream from the Catalog registry to the application Deployment registries, where they are deployed to production environments.\\n\\nUse a secure and scalable private registry for application deployments. Ensure that the registry is protected with role-based access control (RBAC), that images are signed, and that the correct metadata is attached.\\n\\n* Continuously scan for vulnerabilities and malware\\n* Keep artifact metadata up-to-date\\n* Enforce deployment policies\\n\\nJust because the images have been moved to the Deployment registry does not mean that they are safe to deploy. You should still be scanning them for vulnerabilities and malware and ensuring that they are signed and have the correct metadata attached.\\n\\nThe same tools we have used in the previous stages can be used in this stage.\\n\\n* [Defender for Cloud](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction?WT.mc_id=AZ-MVP-5004796)\\n* [Copacetic](https://project-copacetic.github.io/copacetic/website/)\\n* [Trivy](https://trivy.dev/)\\n* [HELM](https://helm.sh/)\\n\\nThis is the stage where Copacetic can assist with patching deployed containers and ensuring that the images are up-to-date and secure at the OS layers. The images are fully patched and tested back at the Build and Catalog stages.\\n\\n### \ud83d\ude80Stage 5: Runtime\\n\\nThe Runtime stage is the fifth and final step in the container supply chain. It involves monitoring and securing running containers in production environments so that end-users can access them.\\n\\n![Run](images/ContainerSupplyFramework_Run.PNG)\\n\\nThis is one of the most important stages, as this is where the images are running in production and are exposed to the Internet. You should monitor the images for vulnerabilities and malware and ensure that they are signed and have the correct metadata attached.\\n\\n* Monitor for Abnormal behavior\\n* Continuously scan for vulnerabilities and malware\\n* Keep runtime node ContainerSupplyFramework_Quarantine_debianimage\\n\\nTools that can be used in this stage are:\\n\\n* [Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796)\\n* [Kubernetes (AKS)](https://learn.microsoft.com/azure/aks/what-is-aks?WT.mc_id=AZ-MVP-5004796)\\n* [Defender for Cloud](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-cloud-introduction?WT.mc_id=AZ-MVP-5004796)\\n* [Gatekeeper](https://open-policy-agent.github.io/gatekeeper/website/)\\n* [Ratify](https://learn.microsoft.com/azure/aks/image-integrity?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796)\\n\\n:::tip\\n[Containers running in Azure should have vulnerability findings resolved](https://portal.azure.com/#view/Microsoft_Azure_Security_CloudNativeCompute/AggregatedRecommendationBlade/assessmentKey/e9acaf48-d2cf-45a3-a6e7-3caa2ef769e0/showSecurityCenterCommandBar~/false) Defender recommendations are worth a look to see a list of vulnerabilities, once Defender for Cloud is turned, for Azure Kubernetes Clusters.\\n:::\\n\\n### \ud83c\udfc1 Conclusion\\n\\nHopefully, this article has given you an insight into the [Containers Secure Supply Chain Framework](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/containers-secure-supply-chain-overview?WT.mc_id=AZ-MVP-5004796), and how you can implement the security controls at each stage, using Azure Container Registry, and other tools.\\n\\n![Impact of Technology](images/ImpactofTechnology_SatyaNadella.PNG)\\n\\n## \ud83d\udcd6 Reference\\n\\n* \ud83d\udd17[Containers Secure Supply Chain Framework](https://learn.microsoft.com/azure/security/container-secure-supply-chain/articles/container-secure-supply-chain-implementation/containers-secure-supply-chain-overview?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[duffney/secure-supply-chain-on-aks](https://github.com/duffney/secure-supply-chain-on-aks)\\n* \ud83d\udd17[Quarantine pattern](https://learn.microsoft.com/azure/architecture/patterns/quarantine?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Secure Container Supply Chain with Notation, ORAS, and Ratify](https://www.youtube.com/watch?v=7RvFj_RWE7c)\\n* \ud83d\udd17[Notary](https://notaryproject.dev/)\\n* \ud83d\udd17[Secure your Azure Kubernetes Service (AKS) clusters with Azure Policy](https://learn.microsoft.com/azure/aks/use-azure-policy?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/api-management-overview","metadata":{"permalink":"/azure/api-management-overview","source":"@site/blog/2024-08-07-apim-overvew/index.mdx","title":"Features and Benefits of Azure API Management","description":"Discover the key features and benefits of Azure API Management, including security, scalability, and monitoring capabilities.","date":"2024-08-07T09:25:09.372Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":51.055,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Features and Benefits of Azure API Management","metaDescription":"Discover the key features and benefits of Azure API Management, including security, scalability, and monitoring capabilities.","date":"2024-08-07T09:25:09.372Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/api-management-overview","keywords":["azure","cloudnative","integration","apimanagement","apim"],"description":"Discover the key features and benefits of Azure API Management, including security, scalability, and monitoring capabilities."},"unlisted":false,"prevItem":{"title":"Securing Container Supply Chains with CSSC Framework","permalink":"/azure/secure-container-supply-chain"},"nextItem":{"title":"Azure Hackathon Vending with Terraform","permalink":"/azure/hackathon-vending-project"}},"content":"Today, we are going to look at [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796). Azure API Management is a service that creates consistent and modern API gateways for existing back-end services. It also publishes APIs to external, partner, and internal developers. Azure API Management is a fully managed service that enables customers to publish, secure, transform, maintain, and monitor APIs.\\n\\n{/*truncate*/}\\n\\n# \ud83c\udf10 Azure API Management Overview\\n\\n> [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) is a hybrid, multi-cloud management platform for APIs across all environments. As a platform-as-a-service, API Management supports the complete API lifecycle.\\n\\nWe\'ll examine the API life cycle phases in-depth and explore how Azure API Management can help you efficiently manage your APIs from start to finish.\\n\\n![Azure API Management - Overview](images/apim_overview.PNG)\\n\\n- **Design**\\nThe first phase of the API life cycle is Design. In this phase, we define the API\'s purpose, endpoints, and the data it will handle. This involves creating a blueprint for the API, including its structure and functionality. Azure API Management provides tools and templates to help you design efficient, scalable, and easy-to-use APIs.\\n- **Develop**\\nOnce the design is in place, we move on to the development phase. This is where the actual coding happens. Developers write the code that implements the API\'s functionality based on the design specifications. Azure API Management supports various development environments and integrates seamlessly with tools like Visual Studio, making the development process smoother and more efficient.\\n- **Secure**\\nSecurity is a critical aspect of API management. We ensure our APIs are protected from unauthorized access and potential threats in the Secure phase. Azure API Management offers robust security features such as authentication, authorization, and encryption. These features help safeguard your APIs and the data they handle, ensuring that only authorized users can access them.\\n- **Publish**\\nAfter securing the API, it\'s time to make it available to users. The Publish phase involves exposing the API to developers and end-users. Azure API Management provides a developer portal where you can publish your APIs, offer documentation, and manage subscriptions. This makes it easy for developers to discover and start using your APIs.\\n- **Scale**\\nAs your API gains popularity, you must ensure it can handle increased traffic. The Scale phase focuses on optimizing the API\'s performance and scalability. Azure API Management allows you to scale your APIs effortlessly, ensuring they can handle large volumes of requests without compromising performance.\\n- **Monitor**\\nMonitoring is essential to ensuring your APIs function correctly and efficiently. Azure API Management provides detailed analytics and logging capabilities in the Monitor phase. You can track usage patterns, monitor performance metrics, and identify potential issues before they impact your users.\\n- **Analyze**\\nIn the Analyze phase, we use the data collected during monitoring to gain insights into how our APIs are used. Azure API Management offers powerful analytics tools that help you understand user behavior, identify trends, and make data-driven decisions to improve your APIs continuously.\\n\\nIn the modern digital era, transformation is more than just a buzzword\u2014it\'s necessary. APIs are at the heart of this transformation, enabling connected experiences and seamless integration of services and data.\\n\\n![Azure API Management - Digital Transformation](images/apim_overview_digitaltransformation.gif)\\n\\nAPIs are crucial in connecting various devices and platforms. Whether it\'s a smart home system, wearable technology, automotive applications, or industrial equipment, APIs facilitate the communication and interoperability between these diverse endpoints.\\n\\nPlease be sure to think about your daily interactions with technology. From unlocking your smartphone, using applications on your tablet, streaming content on your smart TV, and engaging in augmented reality experiences, APIs are working behind the scenes to ensure a smooth and connected experience. They enable devices and applications to talk to each other, share data, and create a cohesive user experience.\\n\\nAPIs connect various services, databases, servers, and cloud infrastructures. They allow these systems to interact, share data, and coordinate complex functions.\\n\\nAPIs also enable businesses to be agile and adaptive. By exposing functionalities through APIs, companies can quickly integrate new services, scale their operations, and meet evolving customer demands. They empower developers to build robust applications, enhance user experiences, and drive digital growth.\\n\\n## \ud83d\udcdc API governance\\n\\nThree crucial aspects of API governance and usage play a significant role in achieving success: \\n\\n- **Fa\xe7ade Abstraction**\\nThis concept involves creating an interface that simplifies and standardizes how APIs interact with backend systems.\\nAggregate or Slice: APIs can aggregate data from multiple sources or slice it into meaningful pieces for specific use cases. This flexibility allows businesses to present data in the most relevant and valuable way to their users.\\nNormalize or Modernize: Fa\xe7ade Abstraction also helps normalize or modernize legacy systems, making them compatible with modern applications. This allows you to extend the life of older systems and integrate them with new technologies without extensive rework.\\nDecouple Lifecycle Mock: Decoupling the API lifecycle from the backend systems ensures that changes in backend services do not disrupt the API functionality. Mocking services during the development and testing phases further ensures smooth transitions and updates.\\n- **Front Door Control**\\nThis aspect is about managing and controlling how APIs are accessed and utilized.\\nRoute and Accelerate: You can optimize your applications\' performance and responsiveness by routing API requests efficiently. Acceleration techniques like caching and load balancing can significantly improve user experience.\\nSecure and Protect: Security is paramount in API management. Front Door Control involves implementing robust security measures to protect APIs from threats and unauthorized access. These measures include authentication, authorization, and data encryption.\\nTransform and Observe: Transforming API requests and responses to meet specific requirements and observing API usage patterns helps maintain control and ensure optimal performance. Monitoring tools provide insights into how APIs are being used and can alert you to potential issues before they become critical.\\n- **Frictionless Consumption**\\nThis principle is about making it easy for developers and users to discover, access, and use your APIs.\\nOnboarding: A smooth onboarding process is crucial for attracting and retaining users. Clear documentation, tutorials, and support help users quickly understand and start using your APIs.\\nDiscover and Learn: APIs should be easily discoverable. A well-organized developer portal where users can learn about the available APIs, their functionalities, and use cases can achieve this.\\nTry and Obtain Access: Allowing users to try APIs in a sandbox environment and easily obtain access through streamlined registration encourages experimentation and adoption.\\n\\n## \ud83e\udd1d Stakeholders\\n\\nAzure API Management has the following stakeholders:\\n\xa0\\n- **App Developers** \\nThese are the individuals or teams responsible for creating applications that consume APIs. They use the platform to discover, learn, try, onboard, and get help with APIs.\\nEmployees, Partners.\\n- **Customers**\\nThese stakeholders leverage the APIs to enhance workflows, integrate systems, and improve customer experiences.\\n- **Apps on Devices**\\nThese represent the end-user applications that interact with the APIs. They could be mobile apps, web applications, IoT devices, or other software that consumes APIs.\\n\\n## \u2708\ufe0f Planes\\n\\n:::info\\nAzure API Management consists of three main planes:\\n\xa0\\n- **User Plane**\\nDeveloper Portal: This portal serves as the primary interface for app developers. It provides resources for developers to discover, learn, try, and onboard with APIs. The portal ensures developers have the support they need to integrate APIs efficiently into their applications.\\n- **Data Plane**\\nGateway: The Gateway is a secure bridge between the API consumers and the backend services. It enforces security policies, manages traffic, and ensures reliable API performance. The gateway handles rate limiting, authentication, and request routing tasks.\\n- **Management Plane**\\nAPI Management Plane: This is where API providers manage the lifecycle of their APIs. The management plane includes tools for abstracting, securing, evolving, observing, and monetizing APIs. This layer ensures that APIs are managed effectively, from creation and deployment to monitoring and analytics.\\n:::\\n\\nAzure API Management integrates with various Azure services and microservices, providing a cohesive environment for managing and deploying APIs. This integration allows organizations to leverage the full power of Azure\'s cloud capabilities, ensuring that their APIs are scalable, secure, and performant.\\n\\n- **Benefits**\\n\\nSome of the benefits of Azure API Management include:\\n\\n* Discoverability: Developers can easily find and access APIs.\\n* Security: Robust security measures protect data and ensure compliance.\\n* Scalability: APIs can scale to meet growing demand.\\n* Efficiency: Streamlined management processes enhance productivity.\\n* Monetization: Organizations can generate revenue from their APIs.\\n\\nIn conclusion, Azure API Management is a robust platform that enables organizations to create, manage, and scale APIs efficiently. By following the API life cycle phases and leveraging Azure API Management\'s features, businesses can deliver seamless digital experiences, drive innovation, and stay ahead in today\'s competitive landscape.\\n\\n## \ud83d\udcca Azure API Management plans\\n\\n![API Management - Pricing Tiers](images/apim_overview_pricingtiers.gif)\\n\\nAzure API Management offers various service tiers that provide specific capabilities and benefits. Let\'s explore these tiers in more detail.\\n\\n- **Consumption Tier**\\n\xa0\\nKey Features include:\\n\\n* No Infrastructure to Provision or Manage: The consumption tier eliminates the need to manage infrastructure. This serverless approach lets you focus solely on your APIs without worrying about the underlying hardware or software.\\n* Built-in Auto-scaling Down to Zero: It automatically scales to handle varying loads and can scale down to zero during inactivity, ensuring cost-efficiency.\\n* Consumption-based Micro Billing: Billing is based on actual usage, making it a cost-effective solution for unpredictable or fluctuating traffic applications.\\n* Variable, Usage-based Monthly Cost: You pay only for what you use, allowing for a flexible and budget-friendly approach.\\n* No Reserved Capacity: This tier does not require any reserved capacity, making it ideal for projects with uncertain demands.\\n* Shared Management Plane: The management plane is shared, simplifying administration and reducing costs.\\n* On-demand Activation: APIs can be activated on demand, providing flexibility and agility.\\n* Curated Set of Features and Usage Limits: This tier offers a curated set of features and usage limits, ensuring a streamlined and efficient API management experience.\\n\\n- **Developer, Basic, Standard, and Premium Tiers**\\n\xa0\\nKey Features include:\\n\\n* No Infrastructure to Provision or Manage: Similar to the consumption tier, these tiers also eliminate the need to manage infrastructure, allowing you to focus on development and deployment.\\n* Manual Scaling or External Auto-scaling: These tiers support manual scaling or external auto-scaling, providing more control over resource allocation.\\n* Billing Based on Reserved Capacity: Billing is based on reserved capacity, offering predictable and consistent costs.\\n* Constant, Predictable Monthly Cost: Unlike the consumption tier, these tiers provide a constant, predictable monthly cost, making budget planning easier.\\n* Reserved Capacity: These tiers offer reserved capacity, ensuring that your APIs can handle consistent and high-demand workloads.\\n* Dedicated Management, User, and Data Planes: With dedicated planes, you get enhanced security, performance, and isolation, which is crucial for mission-critical applications.\\n* Always On APIs are always available, ensuring high availability and reliability for your applications.\\n* Full Set of Features: These tiers offer complete features without governance limits, providing comprehensive capabilities for complex and large-scale API management needs.\\n\\n![APIM Pricing Tiers](images/apim_pricing_tiers.png)\\n\\nLet\'s delve into the key features of each tier.\\n\\n| Feature                        | Basic | Basic v2 | Standard | Standard v2 | Premium |\\n| ------------------------------ | ----- | -------- | -------- | ----------- | ------- |\\n| Microsoft Entra Integration    | No    | Yes      | Yes      | Yes         | Yes     |\\n| Virtual Network (VNet) Support | No    | No       | No       | Yes         | Yes     |\\n| Private Endpoint Support       | Yes   | No       | Yes      | No          | Yes     |\\n| Multi-region Deployment        | No    | No       | No       | No          | Yes     |\\n| Availability Zones             | No    | No       | No       | No          | Yes     |\\n| Autoscaling                    | Yes   | No       | Yes      | No          | Yes     |\\n| Self-hosted Gateway            | No    | No       | No       | No          | Yes     |\\n| Backup and Restore             | Yes   | No       | Yes      | No          | Yes     |\\n| Management over Git            | Yes   | No       | Yes      | No          | Yes     |\\n| Static IP                      | Yes   | No       | Yes      | No          | Yes     |\\n\\n> \\"Accelerate your API strategy with Azure API Management, the comprehensive lifecycle solution trusted by thousands of enterprises worldwide. From abstraction and security to observability and discoverability, the APIM platform empowers you to manage APIs effortlessly across clouds and on-premises environments. Built for reliability, security, scalability, and performance, it\'s designed to support DevOps and developers, seamlessly integrating with Azure services. Benefit from global availability, robust support, and competitive, accessible pricing\u2014Azure API Management simplifies API management so you can focus on innovation.\\"\\n\\n## \ud83d\udd04 API Lifecycle\\n\\n![API Lifecycle](images/apim_api_lifecycle.PNG)\\n\\nThe API lifecycle consists of several phases, each essential for creating, managing, and optimizing APIs. This lifecycle emphasizes the continuous and iterative nature of API development and management.\\n\\n- **Design**\\n\\nThe lifecycle begins with the Design phase. In this phase, APIs are conceptualized and defined. \\n\\nKey features include:\\n\\n* Start fast with proxy mode: Quickly create an API proxy to start testing.\\n* Design and mock: Define and simulate APIs to validate their design.\\n* Import from a definition: Use existing API definitions to create new APIs.\\n* Import from Azure resources: Generate APIs based on Azure services.\\n* Capture schema from test calls: Automatically generate API schemas from sample requests and responses.\\n\\n- **Develop**\\n\\nThe next phase is Develop, where the API is built and refined. This stage includes:\\n\\n* 50+ policies: Apply various policies to control API behavior.\\n* SOAP-to-REST: Convert SOAP services to RESTful APIs.\\n* Visual Studio Code integration: Use VS Code for development and debugging.\\n* Live policy debugging: Debug policies in real-time.\\n* API scaffolding: Automatically generate code and configurations for APIs.\\n\\n- **Secure**\\n\\nIn the Secure phase, the API\'s security is ensured. This involves:\\n\\n* Data | Management | User planes: Secure different aspects of data and management.\\n* Keys, OAuth, certificates, custom: Implement authentication and authorization mechanisms.\\n* 1st and 3rd party identity providers: Integrate with various identity providers.\\n* Request/response validation: Validate API requests and responses for security.\\n* IP-based access control: Restrict access based on IP addresses.\\n* Private networking: Secure APIs within private networks.\\n* Compliance: Ensure APIs comply with industry standards and regulations.\\n\\n- **Publish**\\n\\nAfter securing the API, it is time to Publish it. This stage involves:\\n\\n* Revisions and versions: Manage different versions and revisions of the API.\\n* API products, user groups, subscriptions: Organize APIs into products and manage user access.\\n* Customizable developer portal: Provide a portal for developers to access and use APIs.\\n* Integrated API playground: Allow developers to test APIs within the portal.\\n* Export to the Power Platform: Integrate APIs with Microsoft Power Platform.\\n* Self-service onboarding: Enable users to onboard themselves.\\n* DevOps: Integrate with DevOps pipelines for continuous integration and deployment.\\n\\n- **Scale**\\n\\nOnce published, the API needs to be scaled to handle increased demand. The Scale phase includes:\\n\\n* Worldwide availability: Deploy APIs globally to ensure availability.\\n* Multi-region deployments: Distribute APIs across multiple regions for redundancy and performance.\\n* Availability zones: Utilize availability zones to increase reliability.\\n* Caching: Implement caching to improve performance.\\n* Auto-scaling: Automatically adjust resources based on demand.\\n* Hybrid and multi-cloud: Deploy APIs in hybrid and multi-cloud environments.\\n* Azure Arc enabled: Manage and govern APIs across different environments using Azure Arc.\\n\\n- **Monitor**\\n\\nThe API must be monitored after scaling to ensure it operates correctly. The Monitor phase involves:\\n\\n* Azure Monitor: Use Azure Monitor to track and analyze API performance.\\n* Application Insights: Gain insights into application performance and user behavior.\\n* Logs | Metrics | Traces | Alerts: Collect and analyze logs, metrics, and traces and set up alerts.\\n* Configurable verbosity: Adjust the level of detail in logs and metrics.\\n* E2E request tracing: Trace requests end-to-end to diagnose issues.\\n* Local telemetry for self-hosted gateway: Collect telemetry data from self-hosted API gateways.\\n\\n- **Analyze**\\n\\nFinally, in the Analyze phase, the collected data is used to gain insights and improve the API. This stage includes:\\n\\n* Built-in reports: Use built-in reports for quick insights.\\n* Custom reports with Log Analytics: Create custom reports using Log Analytics.\\n* Reports in Power BI: Visualize data and generate reports using Power BI.\\n* Custom telemetry pipelines: Set up custom telemetry pipelines for advanced analysis.\\n\\nThis lifecycle ensures that APIs are well-designed, developed, secured, published, scaled, monitored, and analyzed, providing a comprehensive approach to API management.\\n\\n### \ud83c\udfa8 Design\\n\\n![API Lifecycle - Design](images/apim_apilifecyle_design.PNG)\\n\\nCode-and design-first approaches to building APIs\\n\\nAPI Management supports both approaches to building APIs: \\n\\n- **Code-first approach**\\nImplement the API and generate the API specification as an afterthought (i.e., with Swashbuckle). Benefits: \\nMore convenient for API developers: The only option for existing APIs\\n\\n- **Design-first approach**\\nCreate an API specification, review it with stakeholders, and implement the API Kickstart development by scaffolding the code from the API specification. \\n\\nBenefits: \\n\\nBetter API consumer experience thanks to the deliberate API design Reduced risk thanks to the API review processes\\n\\n![API Lifecycle - Develop](images/apim_apilifecyle_design_codefirst.PNG)\\n\\nYou can create APIs\\n\\n* Support for SOAP, REST, WebSocket and GraphQLAPIs\\n* Import an API from OpenAPI(1, 2, or 3), WADL, or WSDL files\\n* Import an API from App Service, Logic App, Function App, or Container App\\n* Create a blank API\\n\\n![API Management - Create an API](images/apim_apilifecyle_design_createAPI.PNG)\\n\\nSo, let us create a new API by importing the [Star Wars API](https://swapi.dev/).\\n\\n1. Open your Azure API Management instance\\n2. Navigate to APIs\\n3. Click on + Add API\\n4. Click on HTTP (manually define an HTTP API)\\n5. Select Full\\n5. Fill in the details:\\n   - Display name: Star Wars\\n   - Name: star-wars\\n   - Description: Star Wars API\\n   - Web service URL: https://swapi.dev/api\\n   - API URL suffix: starwars\\n6. Click Create\\nNow that we have imported our initial API endpoint, we can start defining the operations we want to expose.\\n1. Click on + Add operation\\n2. Fill in the details:\\n   - Display name: Get People\\n   - Name: get-people\\n   - URL template: /people\\n   - HTTP verb: GET\\nNow, let\'s add another operation to get information about Starships in the Star Wars universe. Fill in the details:\\n   - Display name: Get Starships\\n   - Name: get-starships\\n   - URL template: /starships\\n   - HTTP verb: GET\\n\\nNow let us test it.\\n\\n1. Click on Get Starships API operation\\n2. Click on test\\n3. Click + Add parameter \\n4. Type in Search in the name and the value as \'Star Destroyer\'\\n5. Click Send\\n\\n![API Management - Add Star Wars API](images/apim-add-starwars-api.gif)\\n\\n:::tip\\nYou can use a wildcard in the URL path if you are not sure of an accurate specification while you work on the API design.\\n\\n![API Management - wildcard](images/apim_apilifecyle_design_wildcardproxy.PNG)\\n:::\\n\\nNow that we have imported and tested an existing API, let\'s consider designing a new one.\\n\\n![APIM - Design a API](images/apim_apilifecyle_design_designaapi.png)\\n\\n Azure offers powerful resources to streamline and enhance our workflow.\\n\\nDefining the API:\\n\\nFirst, let\'s talk about defining our API. Azure provides two main approaches:\\n\\nForm-Based Editors:\\n\\nThese editors are highly intuitive, offering a user-friendly interface to input all necessary details about your API.\\nPerfect for those who prefer a guided, step-by-step setup.\\n\\nText-Based Editors:\\n\\nFor those more comfortable with coding, Azure allows us to edit the API definition directly in JSON format.\\nThis method offers greater flexibility and control over the API specifications.\\n\\nVisual Studio Code Extension:\\n\\nIn addition to the Azure portal, you can also leverage the Visual Studio Code extension. This extension integrates seamlessly with Azure, allowing you to manage and define your APIs directly within your development environment. It\'s an excellent tool for developers who prefer to work locally.\\n\\nTesting the API:\\n\\nOnce the API is defined, the next crucial step is testing. Azure simplifies this process in several ways:\\n\\nAzure Portal Testing:\\nYou can test your API directly within the Azure portal. This feature lets you ensure everything functions correctly without switching between multiple tools.\\nSchema Generation:\\nAzure can generate schemas from your API responses. These schemas provide a clear structure of your API\'s data, ensuring that both developers and consumers understand the expected input and output formats.\\n\\n![APIM - Mock API](images/apim_apilifecyle_design_mockapi.PNG)\\n\\nAdopting a design-first approach can significantly enhance efficiency and collaboration. One technique we can utilize is mocking the API. Let\'s explore how this approach can benefit our development workflow.\\n\\nDesign-First Approach - Mock the API:\\n\\n1. Unblock Front-End Teams:\\n\\nMocking API responses is a game-changer for front-end developers.\\nProviding predefined responses enables front-end teams to continue their development work without waiting for the back-end to be fully implemented.\\nThis leads to parallel development, reducing bottlenecks and speeding up the overall project timeline.\\n\\n2. Use an Example Defined in the API Definition:\\n\\nThe process starts with defining an example response within the API definition.\\nThis predefined response acts as a placeholder, simulating the actual API behavior.\\nIt ensures that front-end teams have consistent and reliable data to work with during the early stages of development.\\n3. Configure with a Single-Line Policy:\\n\\nOne of Azure API Management\'s standout features is the ability to configure mocking with a simple, single-line policy.\\n\\nAs shown in the example on the image, the policy can be written as:\\n\\n```xml\\n<inbound>\\n  <base />\\n  <mock-response status-code=\\"200\\" content-type=\\"application/json\\" />\\n</inbound>\\n```\\n\\n![APIM - Mock Design](images/apim_apilifecyle_design_apimock.gif)\\n\\nThis configuration tells the API to return a mock response with a status code of 200 and a content type of application/json.\\n\\n![APIM - Websocket](images/apim_apilifecyle_design_websocket.PNG)\\n\\n[WebSocket API](https://learn.microsoft.com/azure/api-management/websocket-api?tabs=portal&WT.mc_id=AZ-MVP-5004796) support enables real-time communication, making our APIs more dynamic and responsive.\\n\\n[WebSocket API](https://learn.microsoft.com/azure/api-management/websocket-api?tabs=portal&WT.mc_id=AZ-MVP-5004796) support in Azure API Management opens up a whole new realm of possibilities for real-time applications. Let\'s delve into what this entails and how it can benefit our API infrastructure.\\n\\nPassthrough Support for WebSocket APIs:\\n\\n1. Establishing Connections:\\n\\nClient applications can establish WebSocket connections directly with API Management (APIM).\\nThis setup allows for continuous, two-way communication between clients and backend services.\\nOnce the connection is established, APIM seamlessly proxies WebSocket messages to the backend services, ensuring smooth and reliable communication.\\n\\n2. Features of WebSocket API Support:\\n\\nCRUD Operations:\\n\\nWith WebSocket API support, we can perform Create, Read, Update, and Delete (CRUD) operations on WebSocket APIs.\\nThis flexibility ensures that our APIs can handle various real-time data interactions effectively.\\n\\nApply Policies:\\n\\nWe can apply policies to handshake requests, which is crucial for managing security, authentication, and other operational requirements.\\nThese policies ensure that only authorized and legitimate requests can establish WebSocket connections.\\nBrowse and Test:\\n\\nWebSocket APIs can be easily browsed and tested within the Azure and Developer portals.\\nThis capability allows developers to interact with and validate the WebSocket APIs during the design and development phases.\\n\\nMonitoring and Logging:\\n\\nAzure Monitor provides metrics and logs specifically for WebSocket APIs.\\nThese monitoring tools enable us to track performance, detect issues, and gain insights into the usage patterns of our WebSocket connections.\\n\\n### \ud83d\udee0\ufe0f Develop\\n\\n![API Lifecycle - Develop](images/apim_apilifecyle_develop.PNG)\\n\\nAzure API Management offers a range of features to streamline the development process and enhance the functionality of our APIs.\\n\\n![APIM - API Challenges](images/apim_apilifecyle_develop_apichallenges.PNG)\\n\\nAPIs are the backbone of modern digital ecosystems, enabling connected experiences across various devices and services. However, managing these APIs efficiently comes with its own set of challenges. API management provides a structured approach to streamlining the API lifecycle.\\n\\n1. Consume:\\n\\n_(Developer Portal)_\\n\\nThe first pillar of API management is the Developer Portal.\\nThis portal is a crucial interface where developers can discover, understand, and consume APIs.\\nIt provides comprehensive documentation, code samples, and testing tools, making it easier for developers to integrate APIs into their applications.\\nThe Developer Portal offers a centralized hub for API information, enhancing developer productivity and reducing the time to market for new applications.\\n\\n2. Mediate:\\n\\n_(Gateway)_\\n\\nThe gateway is at the heart of API management. It mediates the communication between client applications and backend services.\\nThe Gateway ensures secure, reliable, and efficient API traffic management.\\nIt provides various capabilities, such as load balancing, rate limiting, and security enforcement.\\nBy mediating API requests, the Gateway helps scale applications, protect against malicious attacks, and ensure consistent performance.\\n\\n3. Publish:\\n\\n_(Azure Portal)_\\n\\nThe third pillar is the Azure Portal, where APIs are published and managed.\\nThis portal allows API providers to define and publish their APIs, making them available to developers.\\n\\nIt offers tools for monitoring API usage, setting up policies, and managing API versions.\\nAPI providers can ensure their APIs are robust, secure, and up-to-date through the Azure Portal.\\n\\n![APIM - Product](images/apim_apilifecyle_develop_apichallengesproduct.PNG)\\n\\nAPIs are essential for modern digital ecosystems, but managing them efficiently can be complex. API management provides a unified approach to streamlining product interaction and backend services.\\n\\nProducts:\\n\\n* Diverse Product Integration:\\n\\nWe have multiple products (Products 1 to 5) at the top. Each product has unique functionalities and API requirements.\\nThese products represent different applications and services that need to communicate with various backend systems.\\n\\nAPI Management Layer:\\n\\n* Centralized Management:\\n\\nThe middle section represents the API Management layer, a crucial component in this architecture.\\nAPI Management acts as a mediator, facilitating seamless communication between the products and the backend services.\\nIt standardizes API interactions, ensuring consistency, security, and reliability.\\n\\n* Backend Systems:\\n\\nBackend on Azure and On-premises:\\nAt the bottom, we see the backend systems, categorized into Azure-based and on-premises backends.\\nAzure backends include services such as Azure Functions, Virtual Machines, and Logic Apps.\\nOn-premises backends consist of traditional servers and databases.\\nAPI Management ensures that products seamlessly interact with cloud-based and on-premises backend systems.\\nCritical Benefits of API Management:\\n\\n* Unified API Gateway:\\n\\nProvides a single entry point for all API requests, simplifying the management of APIs across multiple products and services.\\nEnhanced Security:\\n\\nAuthentication, authorization, and rate-limiting policies ensure secure communication between products and backend systems.\\n\\n* Scalability:\\n\\nHandles large volumes of API traffic, enabling applications to scale efficiently.\\n\\n* Monitoring and Analytics:\\n\\nOffers insights into API usage, performance metrics, and error tracking, helping in proactive management and optimization.\\nDeveloper Empowerment:\\n\\nFacilitates a developer-friendly environment with comprehensive documentation, testing tools, and self-service capabilities.\\n\\n![APIM - Cross Domain policies](images/apim_apilifecyle_design_crossdomainpolicy.PNG)\\n\\nPolicy Scopes\\n\\n![APIM - Policy Scopes](images/apim_apilifecyle_design_policyscopes.PNG)\\n\\nLet\'s examine the policy scopes. Understanding the scope is crucial to applying the right policy in the right place. Policies are inherited from the parent scope to the child scope.\\n\\nSo let us go through them.\\n\\n* Global _(Red Border)_\\n\\nDescription: Global policies apply to all requests and responses passing through the API Management instance. They are the outermost scope, impacting every operation, API, and product.\\nUsage: Commonly used for policies like CORS _(Cross-Origin Resource Sharing)_ and logging, which should be consistent across the entire API Management instance.\\n\\n* Workspace _(Yellow Border)_\\n\\nDescription: This scope is associated with a workspace, which groups related APIs and other components within API Management.\\nUsage: Policies defined at this level are used when consistent policy enforcement is needed across multiple APIs belonging to the same workspace.\\n\\n* Product _(Green Border)_\\n\\nDescription: Product-level policies apply to all APIs included within a specific product. Products are a way to bundle APIs together for subscription and access management.\\nUsage: Common for setting rate limits and quotas that apply to all APIs in the product, helping to manage usage and prevent abuse.\\n\\n* API _(Blue Border)_\\n\\nDescription: API-level policies apply to all operations within a specific API. This scope allows for API-wide behavior customization.\\nUsage: Often used for API-specific security measures like JWT (JSON Web Token) validation or other API-wide configurations.\\n\\n* Operation _(Purple Border)_\\n\\nDescription: Operation-level policies are the most granular and apply to a specific operation within an API _(e.g., a single endpoint or method)_.\\nUsage: Useful for operation-specific requirements like caching, URL rewriting, or request/response transformation.\\n\\n* Inbound and Outbound policies\\n\\nInbound policies are applied as the request comes in from the caller, and outbound policies are applied as the response is sent back to the caller after being processed by the backend.\\n\\nEach of these scopes allows for different levels of customization and control over how requests and responses are handled within Azure API Management.\\n\\nIf we take a look at the [policy named values](https://learn.microsoft.com/azure/api-management/api-management-howto-properties?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796) and [expressions](https://learn.microsoft.com/azure/api-management/api-management-policy-expressions?WT.mc_id=AZ-MVP-5004796).\\n\\n![APIM - Policy Values](images/apim_apilifecyle_design_policysvalues.PNG)\\n\\nWe can see that Named Values are a powerful feature in Azure API Management that allows us to define reusable variables and secrets that can be used across policies.\\n\\n> Policies are a collection of statements that are executed sequentially on the request or response of an API. Policy statements can be constructed using literal text values, policy expressions, and named values.\\n\\nNamed values are a global collection of name/value pairs in each API Management instance. There is no imposed limit on the number of items in the collection. Named values can be used to manage constant string values and secrets across all API configurations and policies.\\n\\nLets take a look at some out of the box policies\\n\\n![APIM - Out of the box policies](images/apim_apilifecyle_design_outoftheboxpolicies.PNG)\\n\\nAzure API Management includes, \\"Out of the box policies\\" provides the majority of use cases that that can be implemented to manage access, transform data, advance functionalities, integrate with Dapr, authenticate users, cache data, manage cross-domain calls, and validate information. \\n\\nLet us delve into each category to understand the capabilities these policies offer.\\n\\n* **Access Restriction**\\n\\nAccess restriction policies are crucial for controlling who can access your services and how often. These policies include:\\n\\nCheck HTTP header: Validates headers in HTTP requests.\\nLimit call rate by subscription/key: Controls the rate of API calls based on the subscription or key.\\nRestrict caller IPs: Allows access only from specified IP addresses.\\nSet usage quota by subscription/key: Limits the usage based on subscription or key.\\nValidate client certificate: Ensures the authenticity of the client via certificates.\\nValidate JWT (JSON Web Token): Verifies the token to ensure secure access.\\n\\n* **Transformation**\\n\\nTransformation policies handle the modification and conversion of data formats and content. These policies include:\\n\\nConvert JSON to XML / XML to JSON: Transforms data between JSON and XML formats.\\nFind and replace string in body: Replaces specific strings in the request or response body.\\nMask URLs in content: Hides sensitive URLs within the content.\\nSet backend service/body/HTTP header/query string parameter: Customizes backend service configurations and request parameters.\\nRewrite URL: Modifies the URL of incoming requests.\\nTransform XML using XSLT: Applies XSLT to transform XML data.\\n\\n* **Advanced**\\n\\nAdvanced policies provide extended functionalities for handling requests and responses. These include:\\n\\nSend one-way request / request: Facilitates sending HTTP requests.\\nSet HTTP proxy / request method / status code / variable: Configures proxy, method, status code, and variables.\\nControl flow / Emit metric: Manages the flow of requests and emits metrics.\\nLog to Event Hub / Trace: Logs events and traces for monitoring.\\nMock response / Forward request: Creates mock responses or forwards requests.\\nLimit concurrency / Return response / Retry / Wait: Manages concurrency, responses, and retry mechanisms.\\n\\n* **Dapr Integration**\\n\\nDapr (Distributed Application Runtime) integration policies enable communication with Dapr services:\\n\\nSend request to a service / message to a pub/subtopic: Facilitates interaction with Dapr services and pub/subtopics.\\nTrigger output binding: Initiates output bindings in Dapr.\\n\\n* **Authentication**\\n\\nAuthentication policies ensure that only authorized users can access your services:\\n\\nAuthenticate with basic / client certificate / managed identity: Supports various authentication methods to verify user identity.\\n\\n* **Caching**\\n\\nCaching policies improve performance by storing and retrieving data from the cache:\\n\\nStore to cache / Get value from cache / Store value from cache / Remove value from cache: Manages caching operations to optimize data retrieval.\\n\\n* **Cross Domain**\\nCross-domain policies handle calls across different domains:\\n\\nCORS / JSON: Manages cross-origin resource sharing and JSON data handling.\\n\\n* **Validation Policies**\\n\\nValidation policies ensure the integrity and correctness of data:\\n\\nValidate content/parameters/headers/status code / GraphQL request: Check the validity of content, parameters, headers, status codes, and GraphQL requests.\\n\\nAn example of policies that are useful is **Request Forwarding**:\\n\\n![APIM - Policy Example](images/apim_apilifecyle_design_outoftheboxpolicies_RequestForwarding.PNG)\\n\\nThis policy is usually inherited from the global scope via the <base/> tag. If no specific policy is set, it defaults to the base configuration.\\nTimeout Settings: The forwarding timeout can be configured between 30 seconds to 10 minutes, with the default being 5 minutes.\\nRedirect Handling: It can be configured to follow redirects or, by default, return them to the caller.\\n\\n* You can configure Retry (<retry/>)\\n\\nTriggering Conditions: Retry is triggered when a specified expression evaluates to true.\\nBackoff Intervals: Offers options for fixed, linear, or exponential backoff intervals.\\nFast First Retry: An optional feature for a quick retry attempt.\\nRequest Copy: Does not retain a copy of the request automatically, ensuring efficient resource utilization.\\n\\n* Limit Concurrency (<limit-concurrency/>)\\n\\nConcurrency Cap: Caps the number of concurrent requests forwarded to the backend to prevent overloading.\\nPolicy Integration: Can be used in conjunction with other policies to limit the number of requests entering enclosed policies.\\nSet Backend Service (<set-backend-service/>)\\n\\nRuntime Changes: Allows changing the backend service during runtime, providing flexibility in managing backend services.\\nConditional Policies: Can be configured with conditional policies for blue/green deployment, ensuring smooth transitions and updates.\\n\\nand Authentication (<authentication/>)\\n\\n![APIM - Authentication policy](images/apim_apilifecyle_design_outoftheboxpolicies_Authentication.PNG)\\n\\nSuch as:\\n\\n* validate-jwt\\n\\nPurpose: Validates JSON Web Tokens (JWTs).\\nSupported Standards:\\n* JWS (JSON Web Signature)\\n* JWE (JSON Web Encryption)\\n* RSA256 and HS256 algorithms\\n* OpenID Configuration: Supports OpenID Configuration endpoints for validation.\\n* Claims Validation: Can check specific claims within the JWT.\\n* Scope: Can be configured at any policy scope, offering flexibility in where it is applied.\\n\\n* validate-client-certificate\\n\\nPurpose: Enforces that a certificate presented by a client matches the specified validation rules and claims.\\nKey Attributes:\\n* validate-revocation=\\"true\\": Ensures the certificate has not been revoked.\\n* validate-trust=\\"true\\": Checks that the certificate is trusted.\\n* validate-not-before=\\"true\\": Ensures the certificate is not used before a certain date.\\n* validate-not-after=\\"true\\": Ensures the certificate is not used after a certain date.\\n* ignore-error=\\"false\\": Indicates whether errors should be ignored.\\n\\nIdentities Section:\\n\\nContains one or more identity elements.\\n\\n* Each identity element includes a thumbprint attribute, identifying the specific certificate by its thumbprint.\\n\\nLet us take a look at the [Azure API Management Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-apimanagement).\\n\\n![APIM - Visual Studio Code](images/apim_apilifecyle_design_vscodeextension.PNG)\\n\\nThe Azure API Management Visual Studio Code extension provides a seamless experience for managing APIs directly from your development environment.\\n\\n![VSCode Visual Studio Extension](images/VSCode_APIMExtension.gif)\\n\\nThe extension allows us to test and edit APIs directly from Visual Studio Code and push those changes straight to Azure API Management, streamlining the development process and enhancing productivity.\\n\\n![APIM - Visual Studio Code - Test APIs](images/apim_apilifecyle_design_vscodeextensiontest.PNG)\\n\\nLet us take a look at some of the Deployment Options for Azure API Management.\\n\\n![APIM - Deployment Options](images/apim_apilifecyle_design_deploymentoptions.PNG)\\n\\nAzure API Management is an ARM (Azure Resource Manager) resource and can be deployed using [ARM templates](https://learn.microsoft.com/azure/azure-resource-manager/templates/overview?WT.mc_id=AZ-MVP-5004796), [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796), or [Terraform](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs).\\n\\n:::info\\nIt is worth noting that Azure API Management has a [Landing Zone Accelerator](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/app-platform/api-management/landing-zone-accelerator?WT.mc_id=AZ-MVP-5004796), which is a set of pre-built configurations and best practices for deploying Azure API Management in a secure and compliant manner.\\n\\n![APIM - Landing Zone Accelerator](images/apim_reference-implementation.png)\\n:::\\n\\n[APIOps](https://azure.github.io/apiops/) is a set of practices and tools that help organizations manage the full lifecycle of APIs, from design to deprecation. It combines the principles of DevOps with API management to streamline API development and deployment.\\n\\n![APIOps](images/apim_apilifecyle_publish_APIOps.PNG)\\n\\n![APIOps](images/ApiOps.gif)\\n\\nAPIOps can be used to automate API management tasks, improve collaboration between development and operations teams, and ensure consistency and quality across APIs, and API Management environments, such as Development, Testing and Production.\\n\\n### \ud83d\udd12 Secure\\n\\n![API Lifecycle - Secure](images/apim_apilifecyle_secure.PNG)\\n\\nSecurity is a critical aspect of API management, ensuring that APIs are protected from unauthorized access, data breaches, and other security threats. Azure API Management offers a range of security features to safeguard APIs and ensure compliance with industry standards.\\n\\nThe [Open Web Application Security Project (OWASP)](https://owasp.org/) has identified the top 10 critical API security risks for 2023\\n\\n![APIM - Security](images/apim_apilifecyle_secure_OWASP.PNG)\\n\\n| **#**                                                                                                                                                                                        | **Threat**                                      | **Description**                                                             | **Mitigation**                                                                                                                                                                                                                                                                                                   |\\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| 1                                                                                                                                                                                            | Broken Object Level Authorization               | Unauthorized access to objects.                                             | Implement key/token/certificate-based authentication and request transformation. Policy-based Access Control: Use policies to enforce authorization rules. OAuth2/JWT: Integrate with Entra ID for token validation.                                                                                             |\\n| 2                                                                                                                                                                                            | Broken Authentication                           | Weak authentication mechanisms.                                             | Use strong key/token-based authentication methods and transform requests appropriately. Authentication Policies: Enforce authentication using OAuth2, OpenID Connect, certificates, and API keys.                                                                                                                |\\n| 3                                                                                                                                                                                            | Broken Object Property Level Authorization      | Unauthorized access to object properties.                                   | Filter or mask sensitive data and ensure both request and response validations. Data Masking: Use policies to mask sensitive data. Validation: Validate input to ensure only authorized properties are accessed.                                                                                                 |\\n| 4                                                                                                                                                                                            | Unrestricted Resource Consumption               | Overuse of resources leads to service denial.                               | Apply throttling and quota limits and manage backend concurrency effectively. Rate Limiting and Quotas: Use policies to set rate limits and quotas. Caching: Reduce load by caching responses.                                                                                                                   |\\n| 5                                                                                                                                                                                            | Broken Function Level Authorization             | Unauthorized access to functions.                                           | Enforce key/token-based and custom authorizations. Role-Based Access Control (RBAC): Use Azure RBAC to control access to different API operations. Request Validation: Validate incoming requests to ensure they don\'t contain malicious URLs. Network Security: Use VNETs and NSGs to control outbound traffic. |\\n| Configuration Management: Use Azure Policy and deployment stacks to enforce security configurations. Automated Deployments: Use CI/CD pipelines to ensure consistent and secure deployments. |                                                 |                                                                             |                                                                                                                                                                                                                                                                                                                  |\\n| 6                                                                                                                                                                                            | Unrestricted Access to Sensitive Business Flows | Uncontrolled access to critical business operations.                        | Validate all requests and responses meticulously. Custom Policies: Implement custom policies to validate requests and responses, ensuring sensitive operations are protected.                                                                                                                                    |\\n| 7                                                                                                                                                                                            | Server-Side Request Forgery (SSRF)              | Attackers tricking the server into making requests to unintended locations. | Validate remote resources and user-supplied URIs and ensure response validation.                                                                                                                                                                                                                                 |\\n| 8                                                                                                                                                                                            | Security Misconfiguration                       | Incorrectly configured API settings.                                        | Maintain an up-to-date API catalog and manage the API lifecycle rigorously.                                                                                                                                                                                                                                      |\\n| 9                                                                                                                                                                                            | Improper Inventory Management                   | Exposing more endpoints than necessary.                                     | Regularly update and document your API endpoints to manage exposure effectively. API Versioning: Implement API versioning to manage the exposure of endpoints. Documentation: Keep API documentation updated and use tools like Swagger or OpenAPI.                                                              |\\n| 10                                                                                                                                                                                           | Insufficient Logging and Monitoring             | Lack of proper logging and monitoring mechanisms.                           | Ensure comprehensive and updated documentation and maintain robust logging and monitoring practices. Azure Monitor and Application Insights: Integrate with these services for comprehensive logging and monitoring. Alerts: Set up alerts for unusual activities.                                               |\\n                                            |      |\\n\\n\\nAzure API Management provides a range of security features to protect APIs from these threats.\\n\\nLet us take a look at securing the different planes of Azure API Management.\\n\\n![APIM - APIM Planes](images/apim_apilifecyle_secure_planes.PNG)\\n\\n| Plane Name        | Used For                                                          |  \\n|-------------------|-------------------------------------------------------------------|  \\n| User Plane        | Securing user interactions and data at the entry point.           |  \\n| Management Plane  | Managing and securing interactions between API providers and the management system. |  \\n| Data Plane        | Securing the flow and integrity of data between the gateway and backend services. |  \\n\\n![APIM - Data Plane](images/apim_apilifecyle_secure_dataplanes.PNG)\\n\\nIn today\'s interconnected digital landscape, securing backend APIs is paramount. One key strategy for achieving this is through robust data plane security. This layered defense mechanism protects sensitive data and applications from unauthorized access and malicious attacks. Let\'s delve deeper into how data plane security works and the roles of its components.\\n\\n**Firewall: Blocking Suspicious Requests**\\n\\nThe first line of defense in data plane security is the firewall. It acts as a gatekeeper, scrutinizing incoming requests to block any that appear suspicious. The firewall employs several rule sets and mechanisms to identify and mitigate potential threats:\\n\\n* OWASP Core Rule Sets: These are standardized rules for detecting common web application security risks.\\n* Bot Protection Rule Set: This helps in identifying and blocking automated bot traffic that could pose security risks.\\n* Custom Rules: Organizations can define their own rules tailored to their specific security requirements.\\n\\n**Gateway: Authenticating and Authorizing Requests**\\n\\nOnce a request passes through the firewall, it reaches the gateway. The gateway is responsible for performing in-depth request authentication and authorization to ensure only valid requests proceed. Key functions of the gateway include:\\n\\n* Authentication Mechanisms: OAuth 2, client certificates, and custom authentication methods.\\n* Authorization Mechanisms: Including IP filtering and custom authorization rules.\\n* Request/Response Validation: Ensuring that both incoming requests and outgoing responses meet predefined security criteria.\\n* Throttling: Managing the rate of incoming requests to prevent denial-of-service (DoS) attacks.\\n\\n**Backend APIs: Fine-Grained Authorization**\\n\\nThe final layer involves the backend APIs themselves, which must accept requests only from trusted sources. This layer applies fine-grained authorization to enforce strict access controls. Key security measures include:\\n\\n* HTTP Basic (Shared Secret): Simple but effective method for authenticating requests.\\n* Mutual Certificates: Ensuring both client and server authenticate each other.\\n* OAuth 2: Secure token-based authentication.\\n* Managed Identity and IP Filtering: This involves restricting access based on IP addresses and managing identities to enforce security policies.\\n* Private Networking: Isolating the APIs within a secure network environment.\\n\\n![APIM - Data Plane - Facade](images/apim_apilifecyle_secure_dataplanesfacafe.PNG)\\n\\n![APIM - Data Plane - Keys](images/apim_apilifecyle_secure_dataplaneskeys.PNG)\\n\\n![APIM - Data Plane - JWT](images/apim_apilifecyle_secure_dataplanejwt.PNG)\\n\\n![APIM - Data Plane - Client certificates](images/apim_apilifecyle_secure_dataplaneclientcerts.PNG)\\n\\n![APIM - Data Plane - Throttling](images/apim_apilifecyle_secure_dataplanethrottling.PNG)\\n\\n![APIM - Data Plane - Sanitization](images/apim_apilifecyle_secure_dataplaneresponsesanitization.PNG)\\n\\n![APIM - Data Plane - DDoS](images/apim_apilifecyle_secure_dataplanereddos.PNG)\\n\\nLet\'s take a look at the security considerations when networking.\\n\\n![APIM - Private Networking Security](images/apim_apilifecyle_secure_privatenetworking.PNG)\\n\\nAzure API Management has two networking options:\\n\\n* Internal\\n* External\\n\\nThe [internal](https://learn.microsoft.com/azure/api-management/api-management-using-with-internal-vnet?tabs=stv2&WT.mc_id=AZ-MVP-5004796) option is more secure, as it is not exposed to the public internet. It is ideal for scenarios where you want to restrict access to your APIs to only internal clients or services.\\nThe [external](https://learn.microsoft.com/azure/api-management/api-management-using-with-vnet?tabs=stv2&WT.mc_id=AZ-MVP-5004796) option is exposed to the public internet, making it accessible to a broader audience. This option suits scenarios where you want to provide public access to your APIs.\\n\\n![APIM - External](images/apim_apilifecyle_secure_external.PNG)\\n\\n![APIM - External](images/apim_apilifecyle_secure_internal.PNG)\\n\\nYou can also use the best of both worlds by allowing Azure API Management in Internal mode to publish APIs to the Internet or external customers and using Azure Front Door or Azure Application Gateway to expose the APIs to the Internet.\\n\\n![APIM - Internal with WAF](images/apim_apilifecyle_secure_internalwaf.PNG)\\n\\nDeciding between using Azure API Management (APIM) in Internal or External mode depends on your specific requirements for security, accessibility, and network configuration. Here are some key considerations:\\n\\n* Internal Mode\\n\\nAccessibility: The APIM endpoints are accessible only from within the Virtual Network (VNet) via an internal load balancer.\\nSecurity: This mode is more secure as it restricts access to the APIM endpoints to only those resources within the VNet.\\nUse Case: This is ideal for scenarios where you need to protect sensitive APIs and ensure that they are only accessible from within your organization\u2019s network.\\n\\n* External Mode\\n\\nAccessibility: The APIM endpoints are accessible from the public internet via an external load balancer.\\nSecurity: While still secure, this mode allows for broader access, including from the internet.\\nUse Case: Suitable for scenarios where you must expose APIs to external clients or partners.\\n\\nDeciding between Internal and External modes depends on your specific requirements and use case. Consider your APIs\' security, accessibility, and network configuration needs to determine the best mode for your scenario.\\n\\nFor example:\\n\\nSecurity Requirements: Internal mode is preferable if your APIs handle sensitive data.\\nAccessibility Needs: If you need to provide access to external clients, external mode is the way to go.\\nNetwork Configuration: Consider your existing network setup and how APIM will integrate.\\n\\n![APIM Compliance](images/apim_apilifecyle_secure_compliance.PNG)\\n\\nAzure API Management meets Compliance requirements, such as:\\n\\n* GDPR\\n* HIPAA\\n* ISO 27001 PCI\\n\\nMore information on the compliance can be found here: [Azure compliance documentation](https://learn.microsoft.com/azure/compliance/?WT.mc_id=AZ-MVP-5004796)\\n\\n### \ud83d\ude80 Publish\\n\\n![API Lifecycle - Publish](images/apim_apilifecyle_publish.PNG)\\n\\nOnce an API is designed, developed, and secured, it is ready to be published. Azure API Management offers a range of features to help you publish and manage your APIs effectively.\\n\\nOne of those features is the Developer Portal.\\n\\n![APIM - Developer Portal](images/apim_apilifecyle_publish_DeveloperPortal.PNG)\\n\\nThe Developer Portal in API Management _(APIM)_ is a discovery and self-onboarding point for application developers. Here are the primary functions and features it offers:\\n\\n* Discover APIs:\\nDevelopers can explore and find available APIs to integrate into their applications.\\n* Learn How to Use Them:\\nThe portal provides comprehensive documentation and tutorials to help developers understand how to use the APIs effectively.\\n* Test Them Out with Interactive Console:\\nDevelopers can interactively test the APIs within the portal using a built-in console. This allows them to see how the APIs work and validate their functionality before integrating them into their applications.\\n* Create and Manage Accounts:\\nDevelopers can create accounts on the portal, manage their profiles, and keep track of their API usage and subscriptions.\\n* Request and Manage API Access:\\nThe portal enables developers to request access to specific APIs and manage their access permissions, ensuring they have the appropriate level of access for their needs.\\n* Analyze API Usage:\\nDevelopers can analyze their API usage data to monitor performance, identify trends, and optimize their application\'s interaction with the APIs.\\n\\nThe Developer Portal is customizable, allowing organizations to tailor it to their needs and branding. It is a central hub for developers to discover, learn, and interact with APIs, streamlining the API consumption process and fostering collaboration between API providers and consumers.\\n\\n![APIM - Developer Portal](images/apim_apilifecyle_publish_DeveloperPortalcustomization.PNG)\\n\\nYou can also self-host the Developer Portal, whether in an App Service, a Virtual Machine, or a Kubernetes Cluster. \\n\\nYou can find the files here: [Azure/api-management-developer-portal](https://github.com/Azure/api-management-developer-portal). \\n\\n![APIM - Developer Portal](images/apim_apilifecyle_publish_DeveloperPortalselfhostedn.PNG)\\n\\n![APIM - Developer Portal](images/apim_apilifecyle_publish_DeveloperPortaSwagger.PNG)\\n\\nLet us take a look at Revisions and Versions.\\n\\n![APIM - Revisions and Versions](images/apim_apilifecyle_publig_RevisionsVersions.PNG)\\n\\nAzure API Management provides the ability to manage [revisions](https://learn.microsoft.com/azure/api-management/api-management-revisions?WT.mc_id=AZ-MVP-5004796) and [versions](https://learn.microsoft.com/azure/api-management/api-management-versions?WT.mc_id=AZ-MVP-5004796) of APIs, allowing you to make changes and updates without affecting existing consumers.\\n\\n![APIM - Revisions and Versions](images/apim_apilifecyle_publig_RevisionsVersionscompare.PNG)\\n\\n![APIM - Revisions](images/apim_apilifecyle_publig_Revisions.PNG)\\n\\n| Scenario                                     | Description                                                                                       | Benefits                                                                                          |  \\n|----------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|  \\n| Non-Breaking Changes to Your API             | Modifications that do not disrupt existing end-user functionality.                        | - Track and document changes systematically.                                                      |  \\n|                                              |                                                                                                   | - Ensure a smooth transition for developers using the API.                                        |  \\n| Potential Roll-Back of Changes               | Reverting to a previous stable state if new changes cause problems.                                | - Quickly revert to a stable state.                                                               |  \\n|                                              |                                                                                                   | - Minimize downtime and maintain service reliability.                                             |  \\n| Communicating Changes to Your Developer Community | Keep your developer community informed about your changes.                              | - Provide detailed logs and documentation of changes.                                             |  \\n|                                              |                                                                                                   | - Help developers understand how updates might affect their applications and how to adapt to them. |  \\n\\n![APIM - Versions](images/apim_apilifecyle_publig_versions.PNG)\\n\\n| Scenario                                      | Description                                                                 | Benefits                                                               |  \\n|-----------------------------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------|  \\n| Show relationship between APIs                | Demonstrating how different versions of APIs are related                    | Helps developers understand the evolution and compatibility of APIs     |  \\n| Provide a predictable way to switch APIs        | Giving developers a clear method to switch between different API versions   | Reduces confusion and ensures smoother transitions between versions     |  \\n| Add breaking changes                          | Introducing changes that are not backward compatible                        | Allows for significant improvements and updates without disrupting users|  \\n| Try out changes and solicit feedback          | Testing new features or changes and gathering community feedback            | Encourages community engagement and helps refine the API before release |  \\n\\nLet\'s look at the following flowchart to decide between revisions and versions.\\n\\n```mermaid\\nflowchart TD  \\n    A[Start] --\x3e B{Do you need to make changes to the API?}  \\n    B --\x3e |Yes| C{Are the changes backward compatible?}  \\n    B --\x3e |No| D[No need for revisions or versions]  \\n    C --\x3e |Yes| E[Use Revisions]  \\n    C --\x3e |No| F{Do the changes require a new URL or breaking changes?}  \\n    F --\x3e |Yes| G[Use Versions]  \\n    F --\x3e |No| E[Use Revisions]  \\n  \\n    E --\x3e H[Make backward-compatible changes]  \\n    G --\x3e I[Create a new version of the API]  \\n    H --\x3e J[Deploy changes using revisions]  \\n    I --\x3e K[Deploy changes using new version]  \\n```\\n\\n![APIM - Revisions and Versions](images/apim_apilifecyle_publig_RevisionsVersexample.PNG)\\n\\n\\nNow that we have reviewed the Revisions and versions, let\'s examine [Products](https://learn.microsoft.com/azure/api-management/api-management-howto-add-products?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796).\\n\\n> In Azure API Management, a product contains one or more APIs, a usage quota, and the terms of use. After a product is published, developers can subscribe to the product and begin to use the product\'s APIs.\\n\\nAPIs _(Application Programming Interfaces)_ enable seamless interaction between software applications. However, managing multiple APIs can be challenging. Bundling APIs with products has emerged as an effective strategy to streamline this process.\\n\\n![APIM - Products](images/apim_apilifecyle_publish_BundleAPIs.PNG)\\n\\nAPI bundling involves grouping multiple APIs into a single package or product. This approach simplifies developers\' and organizations\' management, subscription, and usage of APIs.\\n\\n* Developer Portal\\nBrowse Products: Developers can explore various API products available in the portal.\\nSubscribe to Products: Once a suitable product is identified, developers can subscribe to it.\\nManage Subscriptions and Keys: The portal allows developers to manage their subscriptions and obtain the necessary API keys.\\n\\n* Management Plane\\nManage Products and API Associations: Administrators can define and manage the associations between products and their respective APIs.\\nDefine Product-Scoped Policies: Specific policies can be set for each product, ensuring controlled and secure access.\\nApprove and Manage Subscriptions: Administrators can approve subscription requests and manage existing subscriptions.\\nCollect and Analyze Usage Data: Usage data is collected and analyzed to monitor and optimize API performance.\\nMonetize Access: Administrators can monetize API access by setting pricing plans and usage limits.\\n\\n* Gateway\\nAuthenticate API Requests: The gateway authenticates incoming API requests using the provided keys.\\nExecute Product-Scoped Policies: It ensures that the defined policies for each product are enforced during API execution.\\n\\n![APIM - Products](images/apim_apilifecyle_publish_APISubscriptions.PNG)\\n\\nUnderstanding the relationships between users, groups, products, APIs, and subscriptions is crucial for efficient and effective management.\\n\\n| Entity         | Relationships                                      |  \\n|----------------|-----------------------------------------------------|  \\n| **User**       | - Can have many Subscriptions                      |  \\n|                | - Can belong to many Groups                        |  \\n| **Group**      | - Can contain many Users                           |  \\n|                | - Can have many Subscriptions                      |  \\n|                | - Built-in: Guest, Developer, Admin                |  \\n|                | - Custom: native or Entra ID                       |  \\n| **Subscription**| - Each Subscription is related to one User or Group|  \\n|                | - Can provide access to one Product                |  \\n| **Product**    | - Can be part of many Subscriptions                |  \\n|                | - Can contain many APIs                            |  \\n| **API**        | - Each API is part of one Product                  |  \\n\\n\\n### \ud83d\udccf Scale\\n\\n![API Lifecycle - Scale](images/apim_apilifecyle_scale.PNG)\\n\\nAzure and the Azure API Management service have a significant [global footprint](https://azure.microsoft.com/en-au/explore/global-infrastructure/products-by-region?WT.mc_id=AZ-MVP-5004796), with 44+ public regions worldwide, 6 regions for US government use, and 4 regions in China, showcasing its extensive infrastructure and specialized services.\\n\\n![APIM - Scale](images/apim_apilifecyle_scale_GlobalPresence.PNG)\\n\\n![APIM - Multi-region](images/apim_apilifecyle_scale_Multiregion.PNG)\\n\\nAzure API Management\'s multi-region deployment offers a robust solution for improving API reliability and reducing latency. Let\'s dive into the key features and benefits of this setup.\\nImproved Availability and Reduced Latency\\n \\nOne of the standout features of Azure API Management\'s multi-region deployment is the significantly improved availability of the data plane. You can expect minimal downtime with a Service Level Agreement (SLA) of 99.99% compared to the standard 99.95%. This increased reliability ensures that your APIs remain accessible to users worldwide, enhancing the overall user experience.\\n\\n* Scalability Across Multiple Regions\\n \\nA single Premium instance of Azure API Management can be scaled across multiple regions. This flexibility allows you to deploy additional units into the primary or secondary regions based on your needs. However, regions and units come at an additional cost. This scalability ensures that your API infrastructure can grow alongside your business demands.\\n\\n* Primary and Secondary Regions\\n \\nIn a multi-region deployment, the primary region hosts all components, including the gateway, developer portal, management API, and developer portal API. If the primary region becomes unavailable, these components are inaccessible, highlighting the importance of a robust primary region setup.\\n\\nOn the other hand, secondary regions host only the gateway. These secondary gateways can operate on the last received configuration while the primary area is unavailable. They periodically try to reconnect and catch up, ensuring that API requests continue to be processed even during primary region outages.\\n\\n* Global API Availability\\n \\nOne of the most significant advantages of multi-region deployment is that all APIs are available in every region. Azure Traffic Manager plays a crucial role in routing requests to the closest available region in this setup. Using Traffic Manager\u2019s performance routing with a 5-minute Time-to-Live (TTL), API requests are efficiently managed, reducing latency and improving response times for users worldwide.\\n\\n![APIM - Multi-Region Example](images/apim_apilifecyle_scale_MultiregionEExample.PNG)\\n\\nLets take a look at the nuances on the anatomy of an Azure API Management instance.\\n\\n![APIM - Anatomy](images/apim_apilifecyle_scale_AnatomyAPIM.PNG)\\n\\nAn Azure API Management instance with 1 unit, represented by two machines (Machine #1 and Machine #2). Each machine has a set of components that work together to manage and secure APIs.\\n\\n| Machine   | Component      | Description                                                                 |  \\n|-----------|----------------|-----------------------------------------------------------------------------|  \\n| Machine #1| Control-Plane  | The administrative interface for configuring and managing APIs and policies.|  \\n|           | Gateway        | Handles API requests and applies rate limiting and authentication policies.|  \\n|           | Developer Portal | Customizable portal for developers to discover, learn, and use APIs.       |  \\n| Machine #2| Control-Plane  | Redundant administrative interface for high availability.                   |  \\n|           | Gateway        | Redundant API Gateway for load balancing and fault tolerance.               |  \\n|           | Developer Portal | Another instance of the Developer Portal to ensure high availability.      |  \\n\\nThis is a typical setup of an Azure API Management instance. It emphasizes redundancy and high availability by distributing core components (Control-Plane, Gateway, and Developer Portal) across multiple machines. This setup ensures the API management service remains operational even if one machine fails. \\n\\nHowever, to ensure high availability and disaster recovery, one may want to expand the Azure API Management instance to another region.\\n\\n![APIM - Scale](images/apim_apilifecyle_scale_AnatomyAPIMSecondRegion.PNG)\\n\\nWhen you scale an Azure API Management instance to another region, you are essentially creating a secondary region deployment. This setup provides high availability and disaster recovery capabilities, ensuring your APIs remain accessible even during a regional outage by expanding the Gateway service. All Control planet traffic and the developer portal are still managed by the primary region, however your APIs will still function.\\n\\n![APIM - Availability Zones](images/apim_apilifecyle_scale_AvaliabilityZone.PNG)\\n\\nBy deploying your APIM instance across two or more Availability Zones within a single region, you can achieve a Service Level Agreement (SLA) of 99.99%. \\nIn a multi-region deployment, using Availability Zones can enhance the resiliency of the primary region. Even if one zone experiences an outage, the services can operate seamlessly from another zone within the same region.\\n\\nEvery unit deployed across the Availability Zones includes all necessary API Management components, ensuring comprehensive coverage and functionality.\\n\\nUsing Availability Zones in Azure API Management provides enhanced availability, reliability, and resiliency for your API services, especially in a multi-region deployment. This setup ensures that your APIs remain accessible and operational even in the face of regional outages or disruptions.\\n\\nNow, let us take a look at self-hosting your own Gateway. \\n\\n![APIM - Self-hosted Gateway](images/apim_apilifecyle_scale_SelfHostedAPIGateway.PNG)\\n\\nThe self-hosted API gateway is functionally equivalent to the managed gateway. This feature provides flexibility, allowing the gateway to be deployed on-premises or in the cloud, depending on the organization\'s needs. The self-hosted gateway requires only outgoing connectivity to Azure _(API Management Gateway)_ on port 443.\\n\\nThe self-hosted gateway connects to a \\"parent\\" API Management service in Azure. It pulls down configuration settings and pushes up telemetry data to Azure. This integration with Azure allows for centralized management and monitoring, simplifying the oversight and control of the API gateway.\\n\\n![APIM - Self-hosted Gateway SKU](images/apim_apilifecyle_scale_SelfHostedAPIGatewaySKU.PNG)\\n\\nNow, let us take a look at [Dapr](https://dapr.io/) and Azure API Management.\\n\\n:::info\\n> [Azure API Management is a way to create consistent and modern API gateways for back-end services, including those built with Dapr](https://docs.dapr.io/developing-applications/integrations/azure/azure-api-management/). You can enable Dapr support in self-hosted API Management gateways to allow them to:\\n\\nForward requests to Dapr services\\nSend messages to Dapr Pub/Sub topics\\nTrigger Dapr output bindings\\n:::\\n\\n![APIM - Dapr](images/apim_apilifecyle_scale_Dapr.PNG)\\n\\n* Dapr can work with self-hosted Azure API Management gateways and invoke services running in different pods. This allows for seamless communication between microservices within the Kubernetes cluster. \\n* Dapr supports publish/subscribe messaging, enabling applications to send messages to topics and other applications to subscribe to these topics\\n* Dapr can trigger outbound bindings, which allows it to interact with external systems and services. This feature is useful for integrating with external databases, message queues, and other services.\\n\\nBy integrating Dapr with self-hosted gateways, the architecture\'s complexity is reduced, and common tasks like service discovery, state management, and pub/sub are simplified.\\n\\nAzure API Management offers an isolated SKU, a dedicated instance of Azure API Management that runs on a single-tenant virtual network. This SKU is ideal for organizations that require enhanced security, compliance, and isolation for their APIs.\\n\\n![APIM - Isolated SKU](images/apim_apilifecyle_scale_IsolatedTenant.PNG)\\n\\n> The purpose of the Isolated tier is to enable the use of all the features of the API Management Premium tier in highly regulated industries where compute environment isolation is a requirement. \\n\\nNow, let us discuss [Backup and Restore](https://learn.microsoft.com/azure/api-management/api-management-howto-disaster-recovery-backup-restore?tabs=powershell&WT.mc_id=AZ-MVP-5004796).\\n\\n![APIM - Backup and Restore](images/apim_apilifecyle_scale_IBackupRestore.PNG)\\n\\n| **Process**                 | **Details**                                                                                              |  \\n|-----------------------------|----------------------------------------------------------------------------------------------------------|  \\n| **Backup**                  |                                                                                                          |  \\n| Duration                    | Usually takes around 10 minutes.                                                                         |  \\n| Scope                       | Captures everything but reports and custom domain settings in a blob.                                    |  \\n| Service Configuration       | Operations such as scaling and upgrades are blocked while the backup is in progress.                     |  \\n| Changes                     | Any changes applied after the backup starts are not included in the backup.                              |  \\n| **Restore**                 |                                                                                                          |  \\n| Duration | It Could take 30 minutes or more, depending on the size.                                                     |  \\n| Availability                | The instance is not available while the restore is in progress.                                          |  \\n| Custom Domain               | Custom domain configurations need to be re-applied manually after the restore.                           |  \\n| **Standby Failover Instance** |                                                                                                          |  \\n| Setup                       | Create a backup instance in a different region in advance.                                               |  \\n| Configuration               | Configure the custom domain identically to the active instance.                                          |  \\n| Sync                        | Periodically sync the configuration with the active instance to achieve the desired RPO (Recovery Point Objective). |  \\n| Failover                    | To failover, update the CNAME to reference the backup instance.                                         |  \\n| Scalability                 | Scale up the backup instance if and as required.                                                         |  \\n| **Summary**                 |                                                                                                          |  \\n| Backup                      | This process is relatively quick but does block certain operations and does not capture changes made after the backup begins. |  \\n| Restore                     | This can be time-consuming and renders the instance unavailable during the process. Manual intervention is required for custom domain settings. |  \\n| Standby Failover            | Setting up a standby instance in advance can significantly reduce downtime and improve recovery times. This involves periodic synchronization and possibly updating DNS records during a failover. |  \\n\\n:::warning\\nBackup and restore need to be configured. It is not enabled by default.\\nBackup is not possible in Consumption.\\nBackup and restore could take between 30 minutes to 1 hour.\\nTaking a backup or restore puts the API Management Management plane in a read-only state while the backup or restore is in progress.\\nThe backups expire after 30 days.\\n:::\\n\\nBackup and restore is where you can consider Infrastructure as Code and APIOps functionality and testing. \\n\\n###  \ud83d\udcca Monitor and Analyze\\n\\n![API Lifecycle - Monitor and Analyze](images/apim_apilifecyle_MonitorAnalyze.PNG)\\n\\nNow we need to monitor and analyze the APIs and API Management to ensure that they are performing as expected and to identify any issues that may arise. Azure API Management provides a range of monitoring and analytics features to help you track API usage, performance, and health.\\n\\n![APIM - Monitor and Analyze](images/apim_apilifecyle_MonitorAnalyzeFeatures.PNG)\\n\\n![APIM - Monitor and Analyze](images/apim_apilifecyle_MonitorAnalyzeAPIInspector.PNG)\\n\\nThe \\"[API Inspector](https://learn.microsoft.com/azure/api-management/observability?WT.mc_id=AZ-MVP-5004796)\\" feature monitors and analyzes API requests. It provides detailed information about each request, including the URL, headers, and response status code. This feature helps troubleshoot issues and optimize API performance.\\n\\n![APIM - Monitor and Analyze](images/apim_apilifecyle_MonitorAnalyzeAzureMonitorMetrics.PNG)\\n\\n| Tool        | Useful for    | Data lag | Retention | Sampling | Data kind | Supported Deployment Model(s) |\\n|:------------- |:-------------|:---- |:----|:---- |:--- |:---- |\\n| **[API Inspector](https://learn.microsoft.com/azure/api-management/api-management-howto-api-inspector?WT.mc_id=AZ-MVP-5004796)** | Testing and debugging | Instant | Last 100 traces | Turned on per request | Request traces | Managed, Self-hosted, Azure Arc |\\n| **[Built-in Analytics](https://learn.microsoft.com/azure/api-management/howto-use-analytics?WT.mc_id=AZ-MVP-5004796)** | Reporting and monitoring | Minutes | Lifetime | 100% | Reports and logs | Managed |\\n| **[Azure Monitor Metrics](https://learn.microsoft.com/azure/api-management/api-management-howto-use-azure-monitor?WT.mc_id=AZ-MVP-5004796)** | Reporting and monitoring | Minutes | 90 days (upgrade to extend) | 100% | Metrics | Managed, Self-hosted<sup>2</sup>, Azure Arc |\\n| **[Azure Monitor Logs](https://learn.microsoft.com/azure/api-management/api-management-howto-use-azure-monitor?WT.mc_id=AZ-MVP-5004796)** | Reporting, monitoring, and debugging | Minutes | 31 days/5GB (upgrade to extend) | 100% (adjustable) | Logs | Managed<sup>1</sup>, Self-hosted<sup>3</sup>, Azure Arc<sup>3</sup> |\\n| **[Azure Application Insights](https://learn.microsoft.com/azure/api-management/api-management-howto-app-insights?tabs=rest&WT.mc_id=AZ-MVP-5004796)** | Reporting, monitoring, and debugging | Seconds | 90 days/5GB (upgrade to extend) | Custom | Logs, metrics | Managed<sup>1</sup>, Self-hosted<sup>1</sup>, Azure Arc<sup>1</sup> |\\n| **[Logging through Azure Event Hubs](https://learn.microsoft.com/azure/api-management/api-management-howto-log-event-hubs?tabs=PowerShell&WT.mc_id=AZ-MVP-5004796)** | Custom scenarios | Seconds | User managed | Custom | Custom | Managed<sup>1</sup>, Self-hosted<sup>1</sup>, Azure Arc<sup>1</sup> |\\n| **[OpenTelemetry](https://learn.microsoft.com/azure/api-management/how-to-deploy-self-hosted-gateway-kubernetes-opentelemetry?WT.mc_id=AZ-MVP-5004796#introduction-to-opentelemetry)** | Monitoring | Minutes | User managed | 100% | Metrics | Self-hosted<sup>2</sup> |\\n\\n*1. Optional, depending on the configuration of the feature in Azure API Management*\\n\\n*2. Optional, depending on the configuration of the gateway*\\n\\n*3. The self-hosted gateway currently does not send diagnostic logs to Azure Monitor. However, it is possible to configure and persist logs locally where the self-hosted gateway is deployed.*\\n\\n![APIM - Monitor Logs](images/apim_apilifecyle_MonitorAnalyzeAzureMonitorLogs.PNG)\\n\\n![APIM - Monitor Logs](images/apim_apilifecyle_MonitorAnalyzeAppInsights.PNG)\\n\\n![APIM - Monitor Reports](images/apim_apilifecyle_MonitorAnalyzeReports.PNG)\\n\\nMore information on API Management can be found here: [https://aka.ms/apimlove](https://aka.ms/apimlove).\\n\\n![APIM - Monitor and Analyze](images/apim_love.png)\\n\\nAn excellent resource for Architecting Azure API Management, considerations can be found here: [stephaneey/azure-and-k8s-architecture](https://github.com/stephaneey/azure-and-k8s-architecture/blob/main/maps/apim.md) from MVP [Stephane Eyskens](https://www.linkedin.com/in/stephane-eyskens).\\n\\n![APIM - Monitor and Analyze](images/apim_architecturemap.PNG)"},{"id":"azure/hackathon-vending-project","metadata":{"permalink":"/azure/hackathon-vending-project","source":"@site/blog/2024-07-29-azure-hackathon-vending/index.mdx","title":"Azure Hackathon Vending with Terraform","description":"Learn how to build and deploy a Hackathon environment with Azure, GitHub Actions, Entra ID, and Terraform.","date":"2024-07-29T06:27:25.893Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":13.975,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Hackathon Vending with Terraform","metaDescription":"Learn how to build and deploy a Hackathon environment with Azure, GitHub Actions, Entra ID, and Terraform.","date":"2024-07-29T06:27:25.893Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/hackathon-vending-project","keywords":["azure","hackathon","vending machine","iac","Terraform","Entra ID"],"description":"Learn how to build and deploy a Hackathon environment with Azure, GitHub Actions, Entra ID, and Terraform."},"unlisted":false,"prevItem":{"title":"Features and Benefits of Azure API Management","permalink":"/azure/api-management-overview"},"nextItem":{"title":"Container Patching with Azure DevOps, Trivy and Copacetic","permalink":"/azure/automate-container-patching-with-trivy-copacetic-azure-devops"}},"content":"When working with Microsoft Azure, you may want an environment for learning, testing, or Hackathons. This post will cover some technical implementation considerations for creating a sandbox environment in Azure that could be used for your Hackathons using Terraform, Entra ID Access packages, and GitHub Actions.\\n\\n{/*truncate*/}\\n\\n![Microsoft Azure - Hackathon](images/hackathon_overview.png)\\n\\nSo let us look at a scenario:\\n\\n> \\"As a hackathon organizer, I want to build an Azure environment that supports various AI-driven solutions to automate and enhance operational processes, including customer feedback analysis, testing plan creation, and standard procedure refinement.\\"\\n\\n![Hackathon - Requirements](images/hackathon_requirements.PNG)\\n\\nWe have a few Technical Requirements:\\n\\n* It needs to support multiple _(external)_ users and teams from different organizations using Work and Personal accounts who work together.\\n* Needs to support the creation of multiple Azure resources\\n\\nAnd our limitations are:\\n\\n* One Azure subscription\\n\\nLet us look at how we can leverage Microsoft Azure technologies to implement this.\\n\\n![Hackathon - Azure environment](images/hackathon_environmenttech.PNG)\\n\\n* [Azure Subscription](https://learn.microsoft.com/azure/cost-management-billing/manage/create-subscription?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Resource Groups](https://learn.microsoft.com/azure/azure-resource-manager/management/overview?WT.mc_id=AZ-MVP-5004796#resource-groups) _(within Azure subscription)_\\n* [Entra ID](https://learn.microsoft.com/entra/fundamentals/whatis?WT.mc_id=AZ-MVP-5004796)\\n* [Entra ID Identity Governance - Access Packages](https://learn.microsoft.com/entra/id-governance/entitlement-management-access-package-create?WT.mc_id=AZ-MVP-5004796). This requires a Entra ID P2 license.\\n\\nFor this article, there are some assumptions I will be making, such as:\\n\\n* A Resource Group per Team is sufficient for the Hackathon.\\n\\n![Hackathon - Environment concepts](images/hackathon_environmentconcepts.PNG)\\n\\nWe will approach this as a [Lightly Managed Sandbox](https://luke.geek.nz/azure/microsoft-azure-sandbox-design-considerations/).\\n\\n![Hackathon - Sandbox Types](images/hackathon_sandboxtypes.PNG)\\n\\nAnd give External Identities access to an Azure subscription through Access Packages and a Group with a Contributor role assigned to the individual Resource Groups.\\n\\n![Hackathon - Identity & Access](images/hackathon_identityaccess.PNG)\\n\\nTo do this, we will leverage Access packages to invite external users into the Hackathon tenancy and assign them to a group with the Contributor role assigned to the Resource Group.\\n\\n![Hackthon - Access packages](images/hackathon_identityaccesspackages.PNG)\\n\\nBy creating a Policy allowing external organizations to make requests, the Hackathon Organiser can invite external users to the Azure subscription and access internal _(Hackathon)_ resources.\\n\\n![Hackathon - Identity Access packages](images/hackathon_identityaccesspackagespolicy.PNG)\\n![Hackathon - Identity Access packages](images/hackathon_identityaccesspackagespolicy1.PNG)\\n\\n![Hackathon - Identity Access packages](images/hackathonaccesspackage_signup.gif)\\n\\n> You may also need to consider [Inbound Cross-tenant Access](https://learn.microsoft.com/entra/external-id/cross-tenant-access-overview?WT.mc_id=AZ-MVP-5004796) setting Trust configuration.\\n\\n![Hackathon - Cross tenant Access Settings](images/hackathon_identityaccesstrustsettings.PNG)\\n\\nSo, let\'s look at some of the main Terraform code snippets that could be used to create the Azure resources for the Hackathon.\\n\\n:::info\\nThe identity being used to create these Terraform resources will need the following:\\n\\n* [Owner](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal-subscription-admin?WT.mc_id=AZ-MVP-5004796) of the Azure Subscription to create the Resource Groups and role assignments.\\n* [Identity Governance Administrator](https://learn.microsoft.com/entra/identity/role-based-access-control/permissions-reference?WT.mc_id=AZ-MVP-5004796#identity-governance-administrator), to create the Access Packages and Catalog.\\n* [Groups Administrator](https://learn.microsoft.com/entra/identity/role-based-access-control/permissions-reference?WT.mc_id=AZ-MVP-5004796#groups-administrator) role to create and delete the Groups in the Entra ID tenant.\\n:::\\n\\n```hcl\\nlocals {\\n  # Read the content of the CSV file named \\"az_hackathon.csv\\" and store it in the local variable \\"csv_content.\\"\\n  csv_content = file(\\"az_hackathon.csv\\")\\n  # Decode the CSV content into a list of maps, each representing a row in the CSV file.\\n  csv_data    = csvdecode(local.csv_content)\\n}\\n```\\nThis reads a CSV file containing the details of the Azure Resource Groups that need to be created for the Hackathon. I have used this base CSV file.\\n\\n```csv\\nTeam,Location,Environment,Application,Notes\\nTeam1,AustraliaEast,hackathon,AI,\\"AI Hackathon for luke.geek.nz.\\"\\nTeam2,canadaeast,hackathon,AI,\\"AI Hackathon for luke.geek.nz.\\"\\nTeam3,francecentral,hackathon,AI,\\"AI Hackathon for luke.geek.nz.\\"\\nTeam4,swedencentral,hackathon,AI,\\"AI Hackathon for luke.geek.nz.\\"\\nTeam5,uksouth,hackathon,AI,\\"AI Hackathon for luke.geek.nz.\\"\\n```\\n\\nUsing this and the [AzureRM](https://registry.terraform.io/providers/hashicorp/azurerm/latest) Terraform provider, we can create the Resource Groups and assign the Access Packages to them.\\n\\n```hcl\\n# Define an Azure Resource Group resource named \\"rg\\".\\nresource \\"azurerm_resource_group\\" \\"rg\\" {\\n  # Create one resource group for each entry in the csv_data list.\\n  count    = length(local.csv_data)\\n    # Set the name of the resource group using the Team, Environment, and Application attributes from the csv_data list.\\n  name     = lower(format(\\"%s-%s-%s-rg\\", local.csv_data[count.index].Team, local.csv_data[count.index].Environment, local.csv_data[count.index].Application))\\n    # Set the location of the resource group using the Location attribute from the csv_data list.\\n  location = local.csv_data[count.index].Location\\n    # Set tags for the resource group using attributes from the csv_data list.\\n  tags = {\\n    Environment = lower(local.csv_data[count.index].Environment)\\n    Team        = lower(local.csv_data[count.index].Team)\\n    Application = lower(local.csv_data[count.index].Application)\\n    Notes       = lower(local.csv_data[count.index].Notes)\\n  }\\n}\\n```\\n\\nYou will create a Resource Group, such as below, for each team based on the CSV file:\\n\\n![Hackathon - Resource Group 1](images/hackathon_team1_resourcegroupbeforeOpenAI.png)\\n\\nNow we can create the Groups and assign the permissions:\\n\\n```hcl\\n# Define an Azure AD Group resource named \\"ad_group\\".\\nresource \\"azuread_group\\" \\"ad_group\\" {\\n  # Create one Azure AD group for each entry in the csv_data list.\\n  count            = length(local.csv_data)\\n  # Set the display name of the Azure AD group using the Team, Environment, and Application attributes from the csv_data list.\\n  display_name     = lower(format(\\"%s-%s-%s-group\\", local.csv_data[count.index].Team, local.csv_data[count.index].Environment, local.csv_data[count.index].Application))\\n  # Set the description of the Azure AD group using the Team, Environment, and Application attributes from the csv_data list.\\n  description      = format(\\"Group for %s environment of %s application managed by %s team.\\", title(local.csv_data[count.index].Environment), title(local.csv_data[count.index].Application), title(local.csv_data[count.index].Team))\\n  # Disable mail for the Azure AD group.\\n  mail_enabled     = false\\n  # Enable security for the Azure AD group.\\n  security_enabled = true\\n}\\n\\n# Define an Azure Role Assignment resource named \\"role_assignment\\".\\nresource \\"azurerm_role_assignment\\" \\"role_assignment\\" {\\n  # Create one role assignment for each entry in the csv_data list.\\n  count                = length(local.csv_data)\\n  # Set the scope of the role assignment to the ID of the corresponding resource group.\\n  scope                = azurerm_resource_group.rg[count.index].id\\n  # Assign the \\"Owner\\" role to the principal.\\n  role_definition_name = \\"Owner\\"\\n  # Set the principal ID to the object ID of the corresponding Azure AD group.\\n  principal_id         = azuread_group.ad_group[count.index].object_id\\n}\\n```\\n\\n![Hackathon - Resource Group Access Control (IAM)](images/hackathon_team1_resourcegroupiam.png)\'\\n\\nSo now, we have created the Resource Groups and Entra ID groups and assigned Owner permissions to the Resource Groups for each Team in the CSV file; it is time to deploy a base resource - such as Azure OpenAI, which those hackathon teams can use, to get going! [Azure OpenAI models are deployed to various regions](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?WT.mc_id=AZ-MVP-5004796) and have token limits constrained to the region, so we need to consider how heavily these endpoints will be hit - which is why you may have noticed in the CSV, that the location of each Resource Group is different, so we want to use that to make sure that our Azure OpenAI resources are deployed in the same region as the Resource Group, to help balance.\\n\\n```hcl\\n# This resource block defines an Azure Cognitive Account resource named \\"openai\\"\\nresource \\"azurerm_cognitive_account\\" \\"openai\\" {\\n  # The for_each argument is used to iterate over a map created from local.csv_data.\\n  # Each entry in the map is identified by an index (idx) and its corresponding value (val).\\n  for_each = { for idx, val in local.csv_data : idx => val }\\n  # The name of the cognitive account is dynamically set using the \\"Team\\" attribute from each value in the map.\\n  name = \\"openai-ca-${lower(each.value.Team)}-${lower(each.value.Location)}\\"\\n  # The location of the cognitive account is set using the \\"Location\\" attribute from each value in the map.\\n  location = each.value.Location\\n  # The resource group name is dynamically set using the key from the map.\\n  resource_group_name = azurerm_resource_group.rg[each.key].name\\n  # The kind of cognitive account is set to \\"OpenAI.\\"\\n  kind = \\"OpenAI\\"\\n  # The SKU (pricing tier) of the cognitive account is set to \\"S0\\".\\n  sku_name = \\"S0\\"\\n}\\n```\\n\\nSo we end up with something like this:\\n\\n![Hackathon - Azure OpenAI](images/hackathon_resources_AzureOpenAI.png)\\n\\nWe may also need to consider the Azure [Resource Providers](https://learn.microsoft.com/azure/azure-resource-manager/management/resource-providers-and-types?WT.mc_id=AZ-MVP-5004796), as the Hackathon teams, won\'t have permissions to register these.\\n\\nSo we can create a map of the Resource Providers, and then iterate over them, to register them.\\n\\n```hcl\\nvariable \\"resource_providers\\" {\\n  description = \\"Map of Azure resource providers to register\\"\\n  default = {\\n    \\"Microsoft.Maps\\"                    = {}\\n    \\"Microsoft.OperationalInsights\\"     = {}\\n    \\"Microsoft.EventGrid\\"               = {}\\n    \\"Microsoft.EventHub\\"                = {}\\n    \\"Microsoft.GuestConfiguration\\"      = {}\\n    \\"Microsoft.HDInsight\\"               = {}\\n    \\"Microsoft.KeyVault\\"                = {}\\n    \\"Microsoft.ContainerService\\"        = {}\\n    \\"Microsoft.DBforMySQL\\"              = {}\\n    \\"Microsoft.DBforMariaDB\\"            = {}\\n    \\"Microsoft.DBforMySQL\\"              = {}\\n    \\"Microsoft.DBforPostgreSQL\\"         = {}\\n    \\"Microsoft.DataFactory\\"             = {}\\n    \\"Microsoft.HealthBot\\"               = {}\\n    \\"Microsoft.HealthModel\\"             = {}\\n    \\"Microsoft.HealthcareApis\\"          = {}\\n    \\"Microsoft.MachineLearning\\"         = {}\\n    \\"Microsoft.Orbital\\"                 = {}\\n    \\"Microsoft.Sql\\"                     = {}\\n    // Add the rest of your providers here\\n  }\\n}\\n\\nresource \\"azurerm_resource_provider_registration\\" \\"resourceproviders\\" {\\n  for_each = var.resource_providers\\n  name     = each.key\\n}\\n```\\n\\nNow, we need to create our Entra ID Catalog and Access Packages and assign the Resource Groups to the Access package.\\n\\n:::warning\\nThe [request policies](https://learn.microsoft.com/entra/id-governance/entitlement-management-access-package-create?WT.mc_id=AZ-MVP-5004796#create-request-policies) themselves, I had issues deploying via Terraform, I suspect due to missing Graph permissions \'EntitlementManagement.ReadWrite.All\' on the identity, but had some issues adding this, so did them manually, however, I have successfully deployed the policies in another environment successfully using the: [azuread_access_package_assignment_policy](https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/access_package_assignment_policy) terraform resource. \\n\\nMake sure you add:\\nrequests_accepted = true\\nTo the requestors block, enable the policy, or the policy will be deployed as Disabled.\\n:::\\n  \\n  ```hcl\\nresource \\"azuread_access_package_catalog\\" \\"Sandbox\\" {\\n  description  = \\"Hackathon Sandboxes\\"\\n  display_name = \\"Sandbox\\"\\n  published    = true\\n}\\n\\n# Define an Azure AD Access Package resource named \\"access_package\\"\\nresource \\"azuread_access_package\\" \\"access_package\\" {\\n  # The count parameter determines how many instances of this resource to create based on the length of the csv_data list.\\n  count = length(local.csv_data)\\n  # The display_name parameter sets the display name of the access package for each instance,\\n  # using the Team, Environment, and Application attributes from the csv_data list.\\n  display_name = format(\\"%s-%s-%s-access-package\\", local.csv_data[count.index].Team, local.csv_data[count.index].Environment, local.csv_data[count.index].Application)\\n  # The catalog_id parameter sets the ID of the access package catalog for each instance,\\n  # using the Sandbox catalog ID.\\n  catalog_id = azuread_access_package_catalog.Sandbox.id\\n  # The description parameter sets the description of the access package for each instance,\\n  # using the Team, Environment, and Notes attributes from the csv_data list.\\n  description = \\"Access package for ${local.csv_data[count.index].Team}-${local.csv_data[count.index].Environment}-${local.csv_data[count.index].Notes}\\"\\n}\\n\\n# Assign all groups we have created to our catalog\\nresource \\"azuread_access_package_resource_catalog_association\\" \\"CloudSandbox_Groups\\" {\\n  # The count parameter determines how many instances of this resource to create based on the length of the csv_data list.\\n  count = length(local.csv_data)\\n  # The catalog_id parameter sets the ID of the access package catalog for each instance using the Sandbox catalog ID.\\n  catalog_id = azuread_access_package_catalog.Sandbox.id\\n  # The resource_origin_id parameter sets the ID of the Azure AD group for each instance using the index from the csv_data list.\\n  resource_origin_id = azuread_group.ad_group[count.index].id\\n  # The resource_origin_system parameter specifies the origin system of the resource, which is \\"AadGroup\\" in this case.\\n  resource_origin_system = \\"AadGroup\\"\\n}\\n\\nresource \\"azuread_access_package_resource_package_association\\" \\"group_association\\" {\\n  # The count parameter determines how many instances of this resource to create based on the length of the csv_data list.\\n  count = length(local.csv_data)\\n  # The access_package_id parameter sets the ID of the access package for each instance using the index from the csv_data list.\\n  access_package_id = azuread_access_package.access_package[count.index].id\\n  # The catalog_resource_association_id parameter sets the ID of the catalog resource association for each instance using the index from the csv_data list.\\n  catalog_resource_association_id = azuread_access_package_resource_catalog_association.CloudSandbox_Groups[count.index].id\\n}\\n}\\n```\\n\\n![Hackathon - Entra ID Access Packages](images/hackathon_identityaccesspackagesazportal.png)\\n\\n![Hackathon - GitHub Actions](images/hackathon_githubactions.PNG)\\n\\nNow we have our base Terraform, it is time to deploy this, and we can use GitHub Actions to do this by creating a GitHub Actions workflow file, such as below:\\n\\n```mermaid\\ngraph TD\\n    A[Push to main or PR to main] --\x3e B[Terraform Plan Job]\\n    B --\x3e C[Checkout Repository]\\n    C --\x3e D[Setup Terraform]\\n    D --\x3e E[Terraform Init]\\n    E --\x3e F[Terraform Format]\\n    F --\x3e G[Terraform Plan]\\n    G --\x3e H[Upload Terraform Plan]\\n    H --\x3e I[Create String Output]\\n    I --\x3e J[Publish Terraform Plan to Task Summary]\\n    J --\x3e K[Push Terraform Output to PR]\\n\\n    A --\x3e L[Terraform Apply Job]\\n    L --\x3e M[Checkout Repository]\\n    M --\x3e N[Setup Terraform]\\n    N --\x3e O[Terraform Init]\\n    O --\x3e P[Download Terraform Plan]\\n    P --\x3e Q[Terraform Apply]\\n\\n    subgraph Terraform Plan Job\\n        B\\n        C\\n        D\\n        E\\n        F\\n        G\\n        H\\n        I\\n        J\\n        K\\n    end\\n\\n    subgraph Terraform Apply Job\\n        L\\n        M\\n        N\\n        O\\n        P\\n        Q\\n    end\\n\\n    style B fill:#f9f,stroke:#333,stroke-width:4px\\n    style L fill:#f9f,stroke:#333,stroke-width:4px\\n```\\n\\n```yaml\\nname: \'Terraform Plan/Apply\'\\n\\non:\\n  push:\\n    branches:\\n    - main\\n  pull_request:\\n    branches:\\n    - main\\n\\n#Special permissions required for OIDC authentication\\npermissions:\\n  id-token: write\\n  contents: read\\n  pull-requests: write\\n\\njobs:\\n  terraform-plan:\\n    name: \'Terraform Plan\'\\n    runs-on: ubuntu-latest\\n    environment: production\\n\\n    env:\\n      #this is needed since we are running terraform with read-only permissions\\n      ARM_SKIP_PROVIDER_REGISTRATION: true\\n    outputs:\\n      tfplanExitCode: ${{ steps.tf-plan.outputs.exitcode }}\\n\\n    steps:\\n    # Checkout the repository to the GitHub Actions runner\\n    - name: Checkout\\n      uses: actions/checkout@v4\\n\\n    # Install the latest version of the Terraform CLI\\n    - name: Setup Terraform\\n      uses: hashicorp/setup-terraform@v3\\n      with:\\n        terraform_wrapper: false\\n\\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\\n    - name: Terraform Init\\n      run: terraform init\\n      #These environment variables are used by the terraform azure provider to setup OIDD authenticate. \\n      env:\\n        ARM_CLIENT_ID: \\"${{ secrets.AZURE_CLIENT_ID }}\\"\\n        ARM_SUBSCRIPTION_ID: \\"${{ secrets.AZURE_SUBSCRIPTION_ID }}\\"\\n        ARM_TENANT_ID: \\"${{ secrets.AZURE_TENANT_ID }}\\"\\n      working-directory: ./iac\\n\\n    # Checks that all Terraform configuration files adhere to a canonical format\\n    # Will fail the build if not\\n    - name: Terraform Format\\n      run: terraform fmt -check\\n      working-directory: ./iac\\n\\n    # Generates an execution plan for Terraform\\n    # An exit code of 0 indicated no changes, 1 a terraform failure, 2 there are pending changes.\\n    - name: Terraform Plan\\n      id: tf-plan\\n      env:\\n        ARM_CLIENT_ID: \\"${{ secrets.AZURE_CLIENT_ID }}\\"\\n        ARM_SUBSCRIPTION_ID: \\"${{ secrets.AZURE_SUBSCRIPTION_ID }}\\"\\n        ARM_TENANT_ID: \\"${{ secrets.AZURE_TENANT_ID }}\\"\\n      working-directory: ./iac\\n      run: |\\n        export exitcode=0\\n        terraform plan -detailed-exitcode -no-color -out tfplan || export exitcode=$?\\n\\n        echo \\"exitcode=$exitcode\\" >> $GITHUB_OUTPUT\\n        \\n        if [ $exitcode -eq 1 ]; then\\n          echo Terraform Plan Failed!\\n          exit 1\\n        else \\n          exit 0\\n        fi\\n        \\n    - name: Upload Terraform Plan\\n      if: success() && steps.plan.outputs.exitcode != \'1\'\\n      uses: actions/upload-artifact@v4\\n      with:\\n        name: tfplan\\n        path: ./iac/tfplan # Ensure this path is correct\\n        \\n    # Create string output of Terraform Plan\\n    - name: Create String Output\\n      id: tf-plan-string\\n      run: |\\n        cd ./iac\\n        TERRAFORM_PLAN=$(terraform show -no-color tfplan)\\n        delimiter=\\"$(openssl rand -hex 8)\\"\\n        echo \\"summary<<${delimiter}\\" >> $GITHUB_OUTPUT\\n        echo \\"## Terraform Plan Output\\" >> $GITHUB_OUTPUT\\n        echo \\"<details><summary>Click to expand</summary>\\" >> $GITHUB_OUTPUT\\n        echo \\"\\" >> $GITHUB_OUTPUT\\n        echo \'```hcl\' >> $GITHUB_OUTPUT\\n        echo \\"$TERRAFORM_PLAN\\" >> $GITHUB_OUTPUT\\n        echo \'```\' >> $GITHUB_OUTPUT\\n        echo \\"</details>\\" >> $GITHUB_OUTPUT\\n        echo \\"${delimiter}\\" >> $GITHUB_OUTPUT\\n        \\n    # Publish Terraform Plan as task summary\\n    - name: Publish Terraform Plan to Task Summary\\n      env:\\n        SUMMARY: ${{ steps.tf-plan-string.outputs.summary }}\\n      run: |\\n        echo \\"$SUMMARY\\" >> $GITHUB_STEP_SUMMARY\\n      \\n    # If this is a PR post the changes\\n    - name: Push Terraform Output to PR\\n      if: github.ref != \'refs/heads/main\'\\n      uses: actions/github-script@v7\\n      env:\\n        SUMMARY: \\"${{ steps.tf-plan-string.outputs.summary }}\\"\\n      with:\\n          github-token: ${{ secrets.GITHUB_TOKEN }}\\n          script: |\\n            const body = `${process.env.SUMMARY}`;\\n            github.rest.issues.createComment({\\n                issue_number: context.issue.number,\\n                owner: context.repo.owner,\\n                repo: context.repo.repo,\\n                body: body\\n            })\\n                \\n  terraform-apply:\\n    name: \'Terraform Apply\'\\n    if: github.ref == \'refs/heads/main\' && needs.terraform-plan.outputs.tfplanExitCode == 2\\n    runs-on: ubuntu-latest\\n    environment: production\\n    needs: [terraform-plan]\\n    \\n    steps:\\n    # Checkout the repository to the GitHub Actions runner\\n    - name: Checkout\\n      uses: actions/checkout@v4\\n\\n    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token\\n    - name: Setup Terraform\\n      uses: hashicorp/setup-terraform@v3\\n\\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\\n    - name: Terraform Init\\n      run: terraform init\\n      env:\\n        ARM_CLIENT_ID: \\"${{ secrets.AZURE_CLIENT_ID }}\\"\\n        ARM_SUBSCRIPTION_ID: \\"${{ secrets.AZURE_SUBSCRIPTION_ID }}\\"\\n        ARM_TENANT_ID: \\"${{ secrets.AZURE_TENANT_ID }}\\"\\n      working-directory: ./iac\\n\\n    # Download saved plan from artifacts  \\n    - name: Download Terraform Plan\\n      uses: actions/download-artifact@v4\\n      with:\\n        name: tfplan\\n        path: ./iac  # Specify the directory where the tfplan should be saved\\n    \\n    - name: Terraform Apply\\n      run: terraform apply -auto-approve tfplan\\n      working-directory: ./iac\\n      env:\\n        ARM_CLIENT_ID: \\"${{ secrets.AZURE_CLIENT_ID }}\\"\\n        ARM_SUBSCRIPTION_ID: \\"${{ secrets.AZURE_SUBSCRIPTION_ID }}\\"\\n        ARM_TENANT_ID: \\"${{ secrets.AZURE_TENANT_ID }}\\"\\n```\\n\\nThis GitHub Actions workflow will connect to Azure, using OIDC from a User Managed Identity, deploy the Terraform plan, and apply the changes to the Azure subscription.\\n\\nThe Terraform backend settings look like this, with the state file stored in an Azure Storage account called tfstatehackathon in a container called tfstate in the rg-terraform-hackathon-prod Resource Group. These resources were manually deployed:\\n\\n```hcl\\n  backend \\"azurerm\\" {\\n    resource_group_name  = \\"rg-terraform-hackathon-prod\\"\\n    storage_account_name = \\"tfstatehackathon\\"\\n    container_name       = \\"tfstate\\"\\n    key                  = \\"terraform.tfstate\\"\\n    use_oidc             = true\\n  }\\n\\n  provider \\"azuread\\" {\\n  use_oidc = true\\n}\\n\\n# Configure the Azure Provider with the features block\\nprovider \\"azurerm\\" {\\n\\n  skip_provider_registration = true\\n  use_oidc                   = true\\n  features {\\n    key_vault {\\n      purge_soft_delete_on_destroy    = true\\n      recover_soft_deleted_key_vaults = true\\n    }\\n  }\\n}\\n\\n```\\n\\nIn the GitHub repository, we have an Environment named production, and inside this is our:\\n\\n| Variable | Description |\\n|----------|-------------|\\n| AZURE_CLIENT_ID | The client ID of the Entra ID User assigned managed identity used for authentication |\\n| AZURE_SUBSCRIPTION_ID | The ID of the Azure subscription |\\n| AZURE_TENANT_ID | The ID of the Entra ID tenant |\\n\\n![Hackathon - GitHub Actions](images/hackathon_GitHub_ProductionEnvironment.png)\\n\\nThe full code for this can be found in the [lukemurraynz/HackathonRGAzureEnvironment](https://github.com/lukemurraynz/HackathonRGAzureEnvironment). If you have any suggestions or improvements, feel free to open a PR _(Pull Request)_.\\n\\nFinally, once the Hackathon is over, you can use Terraform to revert the configuration and clean up the Azure subscription; once the users are removed from the Hackathon Access packages, their Guest accounts can be Disabled; you can also configure the policy to remove them from your tenancy automatically.\\n\\nYour Hackathon students should now be able to log in to the [My Access Portal](https://learn.microsoft.com/entra/id-governance/entitlement-management-request-access?WT.mc_id=AZ-MVP-5004796#sign-in-to-the-my-access-portal), request access, to a Team, and once approved _(based on the policy you specified in the Access package)_ will be able to navigate to the Azure Portal for your tenancy \'https://portal.azure.com/YOURTENANTNAMEORID\' and be able to create resources in the Azure Resource Group, and as Teams expand, you simply have to edit the CSV file and push the change."},{"id":"azure/automate-container-patching-with-trivy-copacetic-azure-devops","metadata":{"permalink":"/azure/automate-container-patching-with-trivy-copacetic-azure-devops","source":"@site/blog/2024-07-22-trivy-copacetic-ado/index.mdx","title":"Container Patching with Azure DevOps, Trivy and Copacetic","description":"Learn how to automate Docker container patching with Trivy, Copacetic, and Azure DevOps. This guide covers vulnerability scanning, image patching, and pushing to Azure Container Registry.","date":"2024-07-22T09:28:31.063Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":11.14,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Container Patching with Azure DevOps, Trivy and Copacetic","metaDescription":"Learn how to automate container patching with Trivy, Copacetic, and Azure DevOps. This guide covers vulnerability scanning, image patching, and pushing to Azure Container Registry.","date":"2024-07-22T09:28:31.063Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/automate-container-patching-with-trivy-copacetic-azure-devops","keywords":["azure","trivy","copa","Copacetic","Azure DevOps","ado","containers","containerregistry"],"description":"Learn how to automate Docker container patching with Trivy, Copacetic, and Azure DevOps. This guide covers vulnerability scanning, image patching, and pushing to Azure Container Registry."},"unlisted":false,"prevItem":{"title":"Azure Hackathon Vending with Terraform","permalink":"/azure/hackathon-vending-project"},"nextItem":{"title":"Push Docker Images to Azure Container Registry with GitHub Codespaces","permalink":"/azure/push-docker-images-to-acr-using-github-codespaces"}},"content":"[Copacetic](https://project-copacetic.github.io/copacetic/website/) (or Copa for short) is a CLI tool written in Go and based on buildkit that can be used to directly patch container images given the vulnerability scanning results from popular tools like [Trivy](https://github.com/aquasecurity/trivy).\\n\\n[Trivy](https://aquasecurity.github.io/trivy/) is a security scanner that can scan container images for vulnerabilities. It is a simple and comprehensive scanner that can be used to scan images for vulnerabilities in the OS packages, application dependencies, and language-specific packages, supplemented by vulnerability databases supplemented with Copa, which can be used to patch the vulnerabilities found by Trivy; this tool can be used to quickly patch container images without going upstream for a full rebuild, which may require more time, and the involvement of multiple teams _(ie Developers, Q&A, Operations, Support)_ to patch, test and deploy.\\n\\nIn this article, we will use [Azure DevOps](https://azure.microsoft.com/products/devops?WT.mc_id=AZ-MVP-5004796) to run a pipeline that will use [Trivy](https://aquasecurity.github.io/trivy/) to scan a container image for vulnerabilities, and then use [Copa](https://project-copacetic.github.io/copacetic/website/) to patch the vulnerabilities found by Trivy, and then push the patched image to an [Azure Container Registry (ACR)](https://azure.microsoft.com/products/container-registry?WT.mc_id=AZ-MVP-5004796).\\n\\n{/* truncate */}\\n\\n## \ud83d\udccbOverview\\n\\n:::info\\n[Copa](https://github.com/project-copacetic/copacetic) gives the ability to patch containers quickly without going upstream for a complete rebuild. As the window between [vulnerability disclosure and active exploitation](https://www.bleepingcomputer.com/news/security/hackers-scan-for-vulnerabilities-within-15-minutes-of-disclosure/) continues to narrow, there is a growing operational need to patch critical security vulnerabilities in container images so they can be quickly redeployed into production.\\n\\nIn addition to filling the operational gap not met by left-shift security practices and tools, the ability of Copa to patch a container without requiring a rebuild of the container image provides other benefits:\\n\\n![Container image patching](direct-image-patching.png)\\n\\n* Allows users other than the image publishers also to patch container images, such as DevSecOps engineers.\\n* Reduces the storage and transmission costs of redistributing patched images by only creating an additional patch layer instead of rebuilding the entire image, which usually results in different layer hashes that break layer caching.\\n* Reduces the turnaround time for patching a container image by not having to wait for base image updates and being a faster operation than a full image rebuild.\\n* Reduces the complexity of patching the image from running a rebuild pipeline to running a single tool on the image.\\n:::\\n\\n:::warning\\nCopa currently only supports OS package vulnerabilities and does not support application dependencies or language-specific packages. When validated and tested with developers, it is recommended to use Copa in conjunction with Trivy to patch OS package vulnerabilities and other tools to patch application dependencies and language-specific packages.\\n:::\\n\\n:::info\\nThe copa tool is an extensible engine that:\\n\\nParses the needed update packages from the container image\u2019s vulnerability report produced by a scanner like Trivy. New adapters can be written to accommodate more report formats.\\nObtains and processes the needed update packages using the appropriate package manager tools such as apt, apk, etc. New adapters can be written to support more package managers.\\nApplies the resulting update binaries to the container image using buildkit.\\n\\n![Copa](vulnerability-patch.png)\\n\\nThis approach is motivated by the core principles of making direct container patching broadly applicable and accessible:\\n\\n* Copa supports patching existing container images _(Devs don\'t need to build their images using specific tools or modify them in some way to support container patching)_.\\n* Copa supports containers without package managers, including distro less containers\\n* Copa works with the existing vulnerability scanning and mitigation ecosystems.\\n_(Image publishers don\'t need to create new workflows for container patching since Copa supports patching container images using the security update packages already being published today.)_\\n_(Consumers do not need to migrate to a new and potentially more limited support ecosystem for custom distros or change their container vulnerability scanning pipelines to include remediation since Copa can be integrated seamlessly as an extra step to patch containers based on those scanning reports.)_\\n* Copa reduces the technical expertise needed and waiting on dependencies needed to patch an image.\\n_(For OS package vulnerabilities, no specialized knowledge about a specific image is needed to be patch it as Copa relies on the vulnerability remediation knowledge already embedded in the reports produced by popular container scanning tools today.)_\\n:::\\n\\n## \ud83d\ude80Azure DevOps deploy\\n\\nSo let\'s get started by creating a new Azure DevOps pipeline that will use Trivy to scan a container image for vulnerabilities, and then use Copa to patch the vulnerabilities found by Trivy, and then push the patched image to an Azure Container Registry (ACR).\\n\\nFirst we need to create the Service Connection, we can create a [Docker Registry service connection](https://learn.microsoft.com/azure/devops/pipelines/ecosystems/containers/publish-to-acr?WT.mc_id=AZ-MVP-5004796).\\n\\n![Create Service Connection](docker-registry-acr-serviceconnection.png)\\n\\nThen, we can use the pipeline to connect to the Container Registry and patch the Container.\\n\\n```mermaid\\ngraph TD\\n  A[Start] --\x3e B{Parameters}\\n  B --\x3e C{Variables}\\n  C --\x3e D[Stages]\\n  D --\x3e E[Stage: Patch]\\n  E --\x3e F[Job: Copacetic]\\n  F --\x3e G[Step: Setup]\\n  G --\x3e H[Step: Scan and Patch Image]\\n  H --\x3e I[Step: Upload Results]\\n  I --\x3e J[Step: Push Patched Image to ACR]\\n  J --\x3e K[End]\\n```\\n\\nMake sure you update the parameters, and Service Principal variable name with the up-to-date versions of Trivy, Copacetic and BuildKit, at the time you run this, and of course the container image you want to patch.\\n\\n### \ud83d\udee0\ufe0fPipeline\\n\\nYou can also find the full pipeline code at the following gist: [lukemurraynz/copacetic.yml](https://gist.github.com/lukemurraynz/ea00bc9c6b6a9468a82f3b679c40a220).\\n\\n```yaml copacetic.yml\\n# The name of the pipeline, including the date and a revision number\\nname: copa_$(Date:yyyyMMdd)_$(Rev:r)\\n\\n# This pipeline is not triggered automatically by any event\\ntrigger: none\\n\\n# This pipeline is not triggered automatically by any pull request\\npr: none\\n\\n# Parameters for the pipeline\\nparameters:\\n  # The Docker image to be patched\\n  # This parameter specifies the Docker image that will be patched by the pipeline.\\n  - name: image\\n    displayName: Image To Patch\\n    type: string\\n    default: \\"azcontainerregistry.azurecr.io/api-firewall:0.6.14\\"\\n\\n  # The version of Trivy to be used\\n  # Trivy is a Simple and Comprehensive Vulnerability Scanner for Containers and other Artifacts,\\n  # suitable for CI. This parameter specifies the version of Trivy to be used in the pipeline.\\n  - name: trivyVersion\\n    displayName: \'Trivy Version\'\\n    type: string\\n    default: \'0.53.0\'\\n\\n  # The version of Copacetic to be used\\n  # Copacetic is a tool for checking the health of services. This parameter specifies the version\\n  # of Copacetic to be used in the pipeline.\\n  - name: copaVersion\\n    displayName: \'Copacetic Version\'\\n    type: string\\n    default: \'0.7.0\'\\n\\n  # The version of BuildKit to be used\\n  # BuildKit is a toolkit for converting source code to build artifacts in an efficient, \\n  # expressive and repeatable manner. This parameter specifies the version of BuildKit to be used in the pipeline.\\n  - name: buildkitVersion\\n    displayName: \'BuildKit Version\'\\n    type: string\\n    default: \'0.15.0\'\\n\\n# Variables for the pipeline\\nvariables:\\n  # The port on which BuildKit will run\\n  buildkit.port: \\"8888\\"\\n  # The name of the service connection to be used\\n  serviceConnectionName: \'azcontainerregistry\'\\n  # The system.debug variable is set to true. This enables verbose logging for the entire pipeline.\\n  # When this is enabled, Azure Pipelines will output more detailed logs, which can be helpful for debugging.\\n  system.debug: false\\n\\nstages:\\n  - stage: patch\\n    jobs:\\n    - job: Copacetic\\n      steps:\\n        - bash: |\\n            # Exit immediately if a command exits with a non-zero status\\n            set -e\\n            # Download the Trivy tarball from the specified URL\\n            wget \\"https://github.com/aquasecurity/trivy/releases/download/v${{ parameters.trivyVersion }}/trivy_${{ parameters.trivyVersion }}_Linux-64bit.tar.gz\\"\\n            # Extract the Trivy binary to /usr/local/bin\\n            sudo tar -C /usr/local/bin -zxvf trivy_${{ parameters.trivyVersion }}_Linux-*.tar.gz trivy\\n            # Make the Trivy binary executable\\n            sudo chmod +x /usr/local/bin/trivy\\n            # Download the Copacetic tarball from the specified URL\\n            wget \\"https://github.com/project-copacetic/copacetic/releases/download/v${{ parameters.copaVersion }}/copa_${{ parameters.copaVersion }}_linux_amd64.tar.gz\\"\\n            # Extract the Copacetic binary to /usr/local/bin\\n            sudo tar -C /usr/local/bin -zxvf copa_${{ parameters.copaVersion }}_*.tar.gz copa\\n            # Make the Copacetic binary executable\\n            sudo chmod +x /usr/local/bin/copa\\n          displayName: Download and Install Trivy and Copa\\n\\n        - bash: |\\n            # This script is used to run a new Docker container in the background.\\n            # \'set -e\' is used to exit the script immediately if a command exits with a non-zero status.\\n            set -e\\n            # The \'docker run\' command is used to start a new Docker container.\\n            # The \'--detach\' option is used to run the container in the background and print the new container ID.\\n            # The \'--rm\' option is used to automatically remove the container when it exits.\\n            # The \'--privileged\' option is used to give extended privileges to this container.\\n            # The \'-p\' option is used to publish the container\'s port to the host.\\n            # The \'--name\' option is used to assign a name to the container.\\n            # The \'--entrypoint\' option is used to overwrite the default ENTRYPOINT of the image.\\n            # The \'--addr\' option is used to specify the listening address of the BuildKit daemon.\\n            docker run \\\\\\n              --detach \\\\\\n              --rm \\\\\\n              --privileged \\\\\\n              -p 127.0.0.1:$(buildkit.port):$(buildkit.port)/tcp \\\\\\n              --name buildkitd \\\\\\n              --entrypoint buildkitd \\\\\\n              \\"moby/buildkit:v${{ parameters.buildkitVersion }}\\" \\\\\\n              --addr tcp://0.0.0.0:$(buildkit.port)\\n          displayName: Run Buildkit\\n\\n        - task: Docker@2\\n          displayName: Login to ACR\\n          inputs:\\n            command: login\\n            containerRegistry: $(serviceConnectionName)\\n            # This task uses the Docker@2 task to log in to Azure Container Registry (ACR)\\n            # The \'command: login\' input tells the task to perform a \'docker login\' command\\n            # The \'containerRegistry: $(serviceConnectionName)\' input specifies the service connection to use for logging in\\n            # The service connection contains the credentials needed to authenticate with ACR\\n\\n        - bash: |\\n            # Exit immediately if a command exits with a non-zero status\\n            set -e\\n            # Extract the tag from the image name\\n            tag=${IMAGE#*:}\\n            # Extract the repository from the image name\\n            repo=${IMAGE%:*}\\n            # Scan the image with Trivy and store the output as a JSON file named copa-patch.json\\n            stdbuf -oL trivy image --vuln-type os --ignore-unfixed $IMAGE | grep Total\\n          env:\\n            # The image to be patched\\n            IMAGE: ${{ parameters.image }}\\n          displayName: Scan image for DevOps Output\\n\\n        - bash: |\\n            # Exit immediately if a command exits with a non-zero status\\n            set -e\\n            # Extract the tag from the image name\\n            tag=${IMAGE#*:}\\n            # Extract the repository from the image name\\n            repo=${IMAGE%:*}\\n            # Scan the image with Trivy and store the output as a JSON file named copa-patch.json\\n            stdbuf -oL trivy image --vuln-type os --ignore-unfixed -f json $IMAGE | tee copa-patch.json\\n            # Use the scan result to patch the image, if needed\\n            copa patch \\\\\\n              -i $IMAGE \\\\\\n              -r copa-patch.json \\\\\\n              -t $tag-patched \\\\\\n              -a tcp://0.0.0.0:$(buildkit.port)\\n            docker save -o patched-image.tar $repo:$tag-patched\\n            # The \'copa patch\' command is used to patch the image.\\n            # \'-i $IMAGE\' specifies the image to be patched.\\n            # \'-r copa-patch.json\' specifies the file containing the scan results.\\n            # \'-t $tag-patched\' specifies the tag for the patched image.\\n            # \'-a tcp://0.0.0.0:$(buildkit.port)\' specifies the address of the BuildKit daemon.\\n            # The \'docker save\' command is used to save the new patched image as a tar file that can be used later.\\n          env:\\n            # The image to be patched\\n            IMAGE: ${{ parameters.image }}\\n          displayName: Scan and Patch Image\\n\\n        - task: PublishPipelineArtifact@1\\n          inputs:\\n            targetPath: $(System.DefaultWorkingDirectory)/copa-patch.json\\n            artifactName: scan_results\\n          displayName: Upload Scan Results as a pipeline artifact\\n          # This task uses the PublishPipelineArtifact@1 task to upload the scan results as a pipeline artifact\\n          # The \'targetPath: $(System.DefaultWorkingDirectory)/copa-patch.json\' input specifies the file to upload\\n          # The \'artifactName: scan_results\' input specifies the name of the artifact in the pipeline\\n\\n        - task: PublishPipelineArtifact@1\\n          inputs:\\n            targetPath: $(System.DefaultWorkingDirectory)/patched-image.tar\\n            artifactName: patched\\n          displayName: Upload patched image as a pipeline artifact\\n          # This task uses the PublishPipelineArtifact@1 task to upload the patched image as a pipeline artifact\\n          # The \'targetPath: $(System.DefaultWorkingDirectory)/patched-image.tar\' input specifies the file to upload\\n          # The \'artifactName: patched\' input specifies the name of the artifact in the pipeline\\n\\n        - bash: |\\n            # Exit immediately if a command exits with a non-zero status\\n            set -e\\n            # Print each command that is executed to the terminal\\n            set -x\\n            # Extract the tag from the image name\\n            tag=${IMAGE#*:}\\n            # Extract the repository from the image name\\n            repo=${IMAGE%:*}\\n            # Tag the patched image with the appropriate repository name\\n            # The \'docker tag\' command creates a new alias for the image\\n            # The format is \'docker tag source_image target_image\'\\n            # In this case, the source and target images are the same, so this command effectively does nothing\\n            docker tag $repo:$tag-patched $repo:$tag-patched\\n            # Push the patched image back to Azure Container Registry\\n            # The \'docker push\' command pushes an image or a repository to a registry\\n            docker push $repo:$tag-patched\\n          env:\\n            # The image to be patched\\n            IMAGE: ${{ parameters.image }}\\n          displayName: Push Patched Image to ACR\\n```\\n\\nFor demo purposes, I am using an older version of the [api-firewall](https://hub.docker.com/_/api-firewall) image, which has known vulnerabilities that must be patched.\\n\\n### \u2705Deployment results\\n\\nLet\'s look at the image in Azure Container Registry before the patch.\\n\\n![API FW Before Patch](api-firewall-acr-beforecopapatch.png)\\n\\nNow, let us deploy the pipeline and see the results.\\n\\n![API FW Before Patch](api-firewall-ado-beforecopapatch.png)\\n\\n![Copa ADO pipeline](copa-pipelinerun.gif)\\n\\nAfter the pipeline has run, we can see the patched image in the Azure Container Registry.\\n\\n![API Firewall](api-firewall-acr-aftercopapatch.png)\\n\\nIf we re-run the pipeline again, this time targetting our new patched image, we can see that there are no patchable vulnerabilities found.\\n\\n![Copa ADO pipeline](copa-pipelinererun.gif)\\n\\n:::warning\\nRemember that Copacetic is a tool that can be used to quickly patch container images without going upstream for a full rebuild - HOWEVER, it does not replace the need to do a full rebuild of the image, as it only patches OS package vulnerabilities, and does not support application dependencies or language-specific packages. It is also recommended not to layer patch on top of patch, as this can lead to a bloated image, and potential conflicts between patches, consider patching the same base image other an an already patched image, if new updates occur.\\n:::"},{"id":"azure/push-docker-images-to-acr-using-github-codespaces","metadata":{"permalink":"/azure/push-docker-images-to-acr-using-github-codespaces","source":"@site/blog/2024-07-16-pull-push-dockerhub-acr-codespace/index.mdx","title":"Push Docker Images to Azure Container Registry with GitHub Codespaces","description":"This article guides you through pulling and pushing Docker images from DockerHub to Azure Container Registry using GitHub Codespaces.","date":"2024-07-16T09:16:17.120Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.88,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Push Docker Images to Azure Container Registry with GitHub Codespaces","metaDescription":"This article guides you through pulling and pushing Docker images from DockerHub to Azure Container Registry using GitHub Codespaces.","date":"2024-07-16T09:16:17.120Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/push-docker-images-to-acr-using-github-codespaces","keywords":["azure","containerregistry","docker","github codespaces","dockerhub","azure container registry","devops","containerization"],"description":"This article guides you through pulling and pushing Docker images from DockerHub to Azure Container Registry using GitHub Codespaces."},"unlisted":false,"prevItem":{"title":"Container Patching with Azure DevOps, Trivy and Copacetic","permalink":"/azure/automate-container-patching-with-trivy-copacetic-azure-devops"},"nextItem":{"title":"Unable to delete DevCenter Project","permalink":"/azure/devcenter-project-deletion-issues"}},"content":"[GitHub Codespaces](https://github.com/features/codespaces) already have Docker preinstalled as a dependency for the Codespaces environment. This article will guide you through pulling and pushing a Docker image from DockerHub to Azure Container Registry using GitHub Codespaces.\\n\\n{/* truncate */}\\n\\nPushing a container from Docker Hub to your own Azure Container Registry (ACR) can offer several benefits:\\n\\n- Security: You can control who can access your container images, and gives you more control over vulnerability scanning.\\n- Network latency: You can reduce network latency by hosting your container images closer to your deployment targets.\\n- Compliance: You can meet compliance requirements by storing your container images in a specific region.\\n- Integration: You can integrate your container images with other Azure services.\\n\\n:::info\\nThis article assumes you have an [Azure Container Registry](https://learn.microsoft.com/azure/container-registry/?WT.mc_id=AZ-MVP-5004796) already provisioned, you don\'t need a [DockerHub](https://hub.docker.com/) account, if you are pulling from the public repository.\\n:::\\n\\n1. Launch a Codespace, for example: [lukemurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding)\\n![Open Codespace](Open_IAC_Codespace.gif)\\n2. Install the [Docker extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker) for Visual Studio Code in the Codespace\\n3. Open the Docker extension and sign in to DockerHub\\n![Install Docker Extension](Open_IAC_Codespace_InstallDockerExtension.gif)\\n4. Now we need to log to our Azure Registry; under Registry, select Azure, and connect and log in to your Azure account\\n5. Now, we don\'t have any images, so we need to pull them down from the docker hub. Open the terminal and run the following command:\\n\\n:::info\\nReplace `api-firewall:0.6.14` with the image and tag you want to pull from DockerHub.\\n:::\\n\\n```bash\\ndocker pull api-firewall:0.6.14\\n```\\n6. Now that our image is pulled down from the Dockerhub, we need to tag it with our Azure Container Registry and push it. We can do this directly from the Docker extension.\\n7. Right-click and select Push\\n8. Select Azure as the provider, and select your Azure Container Registry\\n9. Now, we need to tag the image with the Azure Container Registry to push it. You can adjust the name and Tag and click OK to Push it.\\n![Push Azure Container Registry](Open_IAC_Codespace_PushContainerACR.gif)\\n10. Now, navigate to your Azure Container Registry in the Azure Portal, and you should see your new image in the repository.\\n![Azure Container Registry](Open_IAC_Codespace_PushedACR.png)\\n\\nYou have now successfully pulled and pushed a Docker image from DockerHub to Azure Container Registry using GitHub Codespaces."},{"id":"azure/devcenter-project-deletion-issues","metadata":{"permalink":"/azure/devcenter-project-deletion-issues","source":"@site/blog/2024-07-15-devcenter-project-deletion-issues/index.mdx","title":"Unable to delete DevCenter Project","description":"This article provides solutions to common problems encountered when trying to delete a DevCenter project.","date":"2024-07-15T08:19:49.761Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.88,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Unable to delete DevCenter Project","metaDescription":"This article provides solutions to common problems encountered when trying to delete a DevCenter project.","date":"2024-07-15T08:19:49.761Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/devcenter-project-deletion-issues","keywords":["azure","containerapps","devcenter","project management","troubleshooting","deletion issues"],"description":"This article provides solutions to common problems encountered when trying to delete a DevCenter project."},"unlisted":false,"prevItem":{"title":"Push Docker Images to Azure Container Registry with GitHub Codespaces","permalink":"/azure/push-docker-images-to-acr-using-github-codespaces"},"nextItem":{"title":"Azure Container Apps Environment deployment - Missing Subnet","permalink":"/azure/containerapps-env-missing-subnet"}},"content":"When attempting to delete a devcenter project, you may encounter the following error:\\n\\n:::warning\\nThis resource is in a failed state.\\n:::\\n\\n![This resource is in a failed state](devcenter-failed-delete-project.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n:::info\\nThe error message usually occurs when environments are created for these projects. This prevents the deletion of the Environment Type / Project / DevCenter. This behavior is designed to prevent accidental deletion.\\n:::\\n\\n1. Sign-in to the [Azure Portal](https://azure.microsoft.com/get-started/azure-portal?WT.mc_id=AZ-MVP-5004796) go to the project page.\\n2. Select Access Control (IAM) in the left panel, then click Add and choose Add role assignment\\n![Dev Center - RBAC (Role Based Access Control)](devcenter-projectadmin-assignrole.png)\\n3. Search the **DevCenter Project Admin** and grant this role to yourself. **You still need to assign this role - even if you have Owner permissions.**\\n![DevCenter - Assign Project Admin role](devcenter-projectadmin-role.png)\\n4. After the operation executes successfully, try to delete the project again\\n\\nIf this doesn\'t work, then you may need to delete the Environment first; if you get an Internal Server error, then wait 2 minutes and retry the deletion again. \\n\\n![DevCenter](devcenter-environments.png)"},{"id":"azure/containerapps-env-missing-subnet","metadata":{"permalink":"/azure/containerapps-env-missing-subnet","source":"@site/blog/2024-07-11-container-apps-env-subnet/index.mdx","title":"Azure Container Apps Environment deployment - Missing Subnet","description":"Missing subnet when attempting to deploy Container Apps Environment using the Azure Portal","date":"2024-07-11T08:49:09.909Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.615,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Container Apps Environment deployment - Missing Subnet","metaDescription":"Missing subnet when attempting to deploy Container Apps Environment using the Azure Portal","date":"2024-07-11T08:49:09.909Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/containerapps-env-missing-subnet","keywords":["azure","containerapps","subnet","missingsubnet","cloudnative","containerappsenvironment"],"description":"Missing subnet when attempting to deploy Container Apps Environment using the Azure Portal"},"unlisted":false,"prevItem":{"title":"Unable to delete DevCenter Project","permalink":"/azure/devcenter-project-deletion-issues"},"nextItem":{"title":"Azure App Gateway Sinkhole for API Management with Terraform","permalink":"/azure/application-gateway-apim-sinkhole-terraform"}},"content":"When attempting to deploy a Container Apps Environment integrated into a VNET *(Virtual Network)* that was pre-created, you may have issues selecting a subnet which your Container Apps Environment should be deployed into, as your subnet may not be appearing in the Azure Portal to select.\\n\\n![Container App Environment - Missing subnet](container-apps-env-subnetmissing.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nBefore we continue, make sure that the subnet address ranges match at least the following:\\n\\n| Workload Profile | Minimum Subnet Size for Virtual Network Integration |\\n|------------------|----------------------------------------------------|\\n| Standard         | /27                                                 |\\n| Consumption      | /23                                                 |\\n\\nIf you have a subnet that meets the requirements, you can proceed with trying the following steps.\\n\\n1. Delete the Subnet you want to use to deploy your Container Apps environment into it.\\n2. Now we need to recreate the subnet using the Azure CLI *(you can use the [Azure CloudShell](https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796))* by running the following command *(replace the values with your own)*:\\n\\n```bash\\naz network vnet subnet create --resource-group RG1  --vnet-name VNET1  --name ACASubnet  --address-prefix \'10.0.4.0/23\'\\n```\\n\\nAfter running the command, you should now be able to select the subnet created in the Azure Portal.\\n\\n![Azure Container Apps - ACASubnet](container-apps-env-subnet.png)\\n\\nIf we look at the differences between a subnet created in the Azure Portal Container Apps Environment experience and one created using the Azure CLI, we can see that there are differences:\\n\\n![Subnet - Diff](container-apps-env-subnetdiff.png)\\n\\nWorking\\n\\n```json\\n\\"properties\\": {\\n    \\"addressPrefix\\": \\"10.0.4.0/23\\"\\n}\\n```\\n\\nNon-Working\\n\\n```json\\n\\"properties\\": {\\n    \\"addressPrefixes\\": [\\n        \\"10.0.4.0/23\\"\\n    ]\\n}\\n```\\n\\nIn the non-working example, the addressPrefixes property is used instead of addressPrefix.\\n\\nThe difference lies in how the address prefix is represented:\\n\\n* Working Example: Uses addressPrefix as a single-string property.\\n* Non-working Example: Uses addressPrefixes as an array with one element\\n\\n\ud83d\udcd6 References: \\n\\n* \ud83d\udd17[Guided project - Deploy and manage a container app using Azure Container Apps](https://learn.microsoft.com/training/modules/deploy-manage-container-app-using-azure-container-apps/?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Networking in Azure Container Apps environment](https://learn.microsoft.com/azure/container-apps/networking?tabs=workload-profiles-env%2Cazure-cli&WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Failed to create Container Apps Environment with custom virtual network #451](https://github.com/microsoft/azure-container-apps/issues/451)"},{"id":"azure/application-gateway-apim-sinkhole-terraform","metadata":{"permalink":"/azure/application-gateway-apim-sinkhole-terraform","source":"@site/blog/2024-07-10-apim-appgw-sinkhole/index.mdx","title":"Azure App Gateway Sinkhole for API Management with Terraform","description":"This article provides a step-by-step guide on setting up an Azure Application Gateway Sinkhole using Terraform.","date":"2024-07-10T05:29:56.620Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.59,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure App Gateway Sinkhole for API Management with Terraform","metaDescription":"This article provides a step-by-step guide on setting up an Azure Application Gateway Sinkhole using Terraform.","date":"2024-07-10T05:29:56.620Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/application-gateway-apim-sinkhole-terraform","keywords":["azure","application gateway","sinkhole","terraform","apim","apimanagement","iac"],"description":"This article provides a step-by-step guide on setting up an Azure Application Gateway Sinkhole using Terraform."},"unlisted":false,"prevItem":{"title":"Azure Container Apps Environment deployment - Missing Subnet","permalink":"/azure/containerapps-env-missing-subnet"},"nextItem":{"title":"Running your own Azure Proactive Resiliency Assessment","permalink":"/azure/proactive-resiliency-assessment"}},"content":"When you use an [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796) solution in Internal mode, you may also want to use an [Azure Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview?WT.mc_id=AZ-MVP-5004796) to provide additional security and performance features for external consumers as well and contain a mix of both Internal *(to your own vnet and private dns)* and *(external, accessed via public DNS)* APIs.\\n\\n![APIM Management Internal](apim_internal_appgw.png)\\n_(Image courtesy of: [Azure APIM and Application Gateway Integration](https://medium.com/@thahif.86/azure-apim-and-application-gateway-integration-9039378ba197))_\\n\\nIn this scenario, you can use the following paths on your APIs:\\n\\n- **Internal**: `https://apim.yourdomain.com/internal/echo`\\n- **External**: `https://apim.yourdomain.com/external/echo`\\n\\n![Azure API Management - EchoAPI](apim_echoapi_external.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nWhere the path of Internal is only intended for internal consumers and internal APIs, and the external path is intended to be accessed externally *(and in some cases internally)*. You can drop all traffic that reaches the Azure Application Gateway with an /internal URL by routing it to a backend that doesn\'t exist.\\n\\nTo do this, you can create a Backend Pool named sinkhole and a Rule to route traffic to the sinkhole pool based on the path using the Application Gateway path routing rules.\\n\\n![Azure App Gateway - Sinkhole](appgw_routingrules_sinkhole.png)\\n\\nHere is the Terraform code on how you might implement this:\\n\\n```hcl\\n# Public/external gateway rules\\n  url_path_map {\\n    name = \\"path-based-routing\\"\\n\\n    default_backend_address_pool_name  = \\"apim\\"\\n    default_backend_http_settings_name = \\"int-https-settings-gateway\\"\\n\\n    path_rule {\\n      name                       = \\"externalPathRule\\"\\n      paths                      = [\\"/external*\\"]\\n      backend_address_pool_name  = \\"apim\\"\\n      backend_http_settings_name = \\"int-https-settings-gateway\\"\\n    }\\n\\n    path_rule {\\n      name                       = \\"internalPathRule\\"\\n      paths                      = [\\"/internal*\\"]\\n      backend_address_pool_name  = \\"sinkhole\\"\\n      backend_http_settings_name = \\"int-https-settings-gateway\\"\\n    }\\n  }\\n\\n  request_routing_rule {\\n    name               = \\"request-routing-public-apim-gateway\\"\\n    rule_type          = \\"PathBasedRouting\\"\\n    http_listener_name = \\"https-gateway-public\\"\\n    url_path_map_name  = \\"path-based-routing\\"\\n    priority           = \\"100\\"\\n  }\\n\\n```\\n\\nThis Terraform code is configuring an Azure Application Gateway, which is a web traffic load balancer that enables you to manage traffic to your web applications.\\n\\nHere\'s a breakdown of what each section does:\\n\\n* url_path_map: This block defines a URL path-based routing rule. It specifies how HTTP traffic is routed to different backend pools based on the URL paths of the HTTP requests.\\n* default_backend_address_pool_name and default_backend_http_settings_name define the default backend pool and HTTP settings to use if none of the specified path rules match.\\n* Two path_rule blocks are defined. Each path_rule specifies a condition (based on the URL path) and the corresponding backend pool and HTTP settings to use when the condition is met. The first rule routes requests with paths starting with \\"/external\\" to the \\"apim\\" backend pool. The second rule routes requests with paths starting with \\"/internal\\" to the \\"sinkhole\\" backend pool.\\nrequest_routing_rule: This block defines a request routing rule, determining how the Application Gateway distributes traffic to the backend pools.\\n* rule_type is set to \\"PathBasedRouting,\\" which means the rule is based on URL paths.\\n* http_listener_name specifies the HTTP listener to associate with the rule.\\n\\nurl_path_map_name specifies the URL path map to use, which is defined in the url_path_map block.\\n\\nPriority sets the priority of the rule. Lower numbers have higher priority. If a request matches multiple rules, the rule with the highest priority is applied.\\n\\nSo let us look at the sinkhole configuration:\\n\\n```hcl\\n  backend_address_pool {\\n    name  = \\"sinkhole\\"\\n    fqdns = null\\n  }\\n```\\n\\nHere\'s a breakdown of what each line does:\\n\\n* backend_address_pool: This block defines a backend address pool, which is a collection of IP addresses to which the Application Gateway can route network traffic.\\n* \\"sinkhole\\": This line sets the name of the backend address pool to \\"sinkhole.\\" This name is used to reference the backend address pool in other parts of the configuration.\\n* fqdns = null: This line sets the fully qualified domain names (FQDNs) of the backend addresses to null. This means that the backend address pool does not contain any backend addresses. In the context of a \\"sinkhole\\" pool, this is likely intended to discard traffic or direct it to a non-existent endpoint.\\n\\nI hope this helps you understand how to configure a sinkhole pool in Azure Application Gateway using Terraform. This allows you to offer both external and internal APIs through the gateway. You can also have a private listener with a different routing rile if you want to offer all the internal and external APIs to your internal consumers/networks.\\n\\n\ud83d\udcd6 References: \\n\\n- \ud83d\udd17[Deploy your Azure API Management instance to a virtual network - internal mode](https://learn.microsoft.com/azure/api-management/api-management-using-with-internal-vnet?tabs=stv2&WT.mc_id=AZ-MVP-5004796)\\n- \ud83d\udd17[Protect APIs with Application Gateway and API Management](https://learn.microsoft.com/azure/architecture/web-apps/api-management/architectures/protect-apis?WT.mc_id=AZ-MVP-5004796)\\n- \ud83d\udd17[URL Path Based Routing overview](https://learn.microsoft.com/azure/application-gateway/url-route-overview?WT.mc_id=AZ-MVP-5004796)\\n- \ud83d\udd17[apim-landing-zone-accelerator](https://github.com/Azure/apim-landing-zone-accelerator)"},{"id":"azure/proactive-resiliency-assessment","metadata":{"permalink":"/azure/proactive-resiliency-assessment","source":"@site/blog/2024-07-06-reliability-assessment/index.mdx","title":"Running your own Azure Proactive Resiliency Assessment","description":"Detailed guide on setting up an Azure Proactive Resiliency Assessment.","date":"2024-07-06T10:43:07.165Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":10.65,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Running your own Azure Proactive Resiliency Assessment","metaDescription":"Detailed guide on setting up an Azure Proactive Resiliency Assessment.","date":"2024-07-06T10:43:07.165Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/proactive-resiliency-assessment","keywords":["mission-critical","Azure","Proactive Resiliency","Assessment","Cloud Reliability","Best Practices","Cloud Reliability","Best Practices"],"description":"Detailed guide on setting up an Azure Proactive Resiliency Assessment."},"unlisted":false,"prevItem":{"title":"Azure App Gateway Sinkhole for API Management with Terraform","permalink":"/azure/application-gateway-apim-sinkhole-terraform"},"nextItem":{"title":"Infrastructure as Code GitHub Codespace Template","permalink":"/azure/iac-github-codespace"}},"content":"The [Azure Proactive Resiliency Library](https://azure.github.io/Azure-Proactive-Resiliency-Library-v2/) is a curated collection of best practices, guidance, and recommendations designed to improve the resiliency of applications and services running in Azure. Built on the Resiliency pillar of the Well-Architected Framework, this catalog provides valuable insights to ensure your workloads remain robust and reliable.\\n\\nThe library also includes automation capabilities (using Azure Graph queries) that allow you to collect data, analyze it, and generate detailed Word and PowerPoint reports. These reports, part of the Well-Architected Resiliency Assessment workshop, provide visibility into the resiliency of your Azure workloads. This toolset, often used by Microsoft Cloud Solution architects, can be leveraged to identify areas for improvement in your own Azure estate, following [Resiliency](https://learn.microsoft.com/azure/well-architected/reliability/principles?WT.mc_id=AZ-MVP-5004796) well-architected principles.\\n\\nIn this article, we will walk through the process of running the scripts to collect, analyze, and report on resiliency data for your workloads using the Azure Proactive Resiliency Library to help you identify and address potential reliability issues in your Azure environment.\\n\\n\x3c!-- truncate --\x3e\\n\\n## \ud83c\udf10 Overview\\n\\nAlong with recommendations to increase Azure resource resiliency, supplementing the guidance on the Well-Architected Framework, the proactive resiliency library includes sample [Azure Resource Graph queries](https://learn.microsoft.com/azure/governance/resource-graph/overview?WT.mc_id=AZ-MVP-5004796), to determine the current state of your resources.\\n\\n![Azure Proactive Resiliency Library - Use Premium tier for critical production workloads](AzProactiveResiliencyLibrary_ContainerRegistry_Tier.png)\\n\\nAn improvement to this automation is the ability to run queries to collect, analyze, and create a Word and PowerPoint report for your Azure workloads; these reports are part of a Workshop delivery initiative - Well-Architected Reliability Assessment, usually driven by Microsoft Cloud Solution architects, we can leverage the same toolsets, to help increase visibility across the state of the reliability of our own Azure estate. This is not a set-and-forget tool but a starting point to help you understand the current state of your resources to help you make informed decisions on how to improve the reliability of your Azure workloads and an avenue to highlight Reliability improvements. This process works best when scoped to a specific workload vs an entire environment.\\n\\n> This article is not meant as a substitute for the Microsoft-delivered Resiliency workshop but mainly to help you start your resiliency journey. The information in this article is based on the [Azure Proactive Resiliency Library v2](https://azure.github.io/Azure-Proactive-Resiliency-Library-v2/) and the scripts provided in the public repository. Please contact your local Microsoft contact for information about their Resiliency workshop and the full delivery.\\n\\n![Azure Proactive Resiliency Library - Assessment](AzProactiveResiliencyLibrary_Assessment.png)\\n\\n> So, why must we consider reliability and resiliency in our Azure workloads?\\n\\nThe answer is simple: ensure that your applications and services are available and reliable, even when failures occur. This is important for business continuity and ensuring that your customers can access your services when needed.\\n\\n![Azure Proactive Resiliency Library - Importance of realibility](AzProactiveResiliencyLibrary_ImportanceRealiability.png)\\n\\nWhen managing resources and workloads in Azure, ensuring reliability is paramount to avoid unexpected failures and downtime. But why exactly do bad things happen, even when there seem to be numerous safeguards in place? Let\'s delve into reliability and why it\'s essential for your Azure environment by exploring the \\"Swiss Cheese Model\\" of accident causation, illustrated in the image above.\\n\\nThe [Swiss Cheese Model](https://en.wikipedia.org/wiki/Swiss_cheese_model), developed by James Reason, is a popular way to understand how errors and failures occur in complex systems. Each defense layer\u2014represented as Swiss cheese slices\u2014has holes or weaknesses. When these holes align across multiple layers, an accident can occur. Let\'s break down this model and see how it applies to Azure reliability.\\n\\n* Institutional Level: Institutions may need more procedures and regulatory narrowness at the highest level. In the context of Azure, this could mean inadequate governance policies or a lack of compliance with best practices. These gaps can create vulnerabilities that might be exploited, leading to system failures.\\n* Organizational Level: Organizations often face mixed messages and production pressures. This could translate to conflicting priorities for Azure between rapid deployment and maintaining robust security and reliability. Pressure to deliver quickly might lead to overlooked reliability checks, creating potential points of failure.\\n* Professional Level: Issues such as responsibility shifting can arise at the professional level. In an Azure environment, this might occur when there is a lack of clear ownership over different architecture components, leading to neglected maintenance and updates.\\n* Team Level: Teams might suffer from inadequate training. In Azure, this could mean insufficient knowledge about Azure services, leading to suboptimal configuration and resource management. Proper training ensures that teams can effectively manage and troubleshoot their environments.\\n* Individual Level: Individual contributors might be distracted or make errors. In Azure, this could involve misconfigurations or overlooking critical logs and alerts. Encouraging a culture of attention to detail and thoroughness can help mitigate these risks.\\n* Technical Level: Finally, technical issues such as clumsy technology, coding bugs, and deferred maintenance can directly impact reliability; in Azure, this might involve outdated software versions, unpatched vulnerabilities, or inefficient resource configurations.\\n\\nBy understanding the layers of defense and potential triggers for failures, you can take proactive steps to enhance the reliability of your Azure resources and workloads, ensuring a robust, secure, and efficient cloud environment.\\n\\nThe Azure Proactive Resiliency library and the assessments we will run through will concentrate on the technical level to ensure that your Azure resources are configured correctly and are resilient to failures; however, the other layers of defense are also essential to consider and are a [shared responsibility](https://learn.microsoft.com/azure/reliability/overview?WT.mc_id=AZ-MVP-5004796#reliability-requirements) between Microsoft and the customer.\\n\\n![Azure Proactive Resiliency Library - shared responsibility](AzProactiveResiliencyLibrary_SharedResponsibility.png)\\n\\nSo - let us get started with the Azure Proactive Resiliency Library and the assessment report.\\n\\n## \ud83d\udcca Create Resiliency Reports\\n\\nThe Assessment report is a 3 stage process.\\n\\n1. collect the data\\n2. analyse the data\\n3. create the report\\n\\n![Azure Proactive Resiliency Library - Assessment report](AzProactiveResiliencyLibrary_AssessmentProcess.png)\\n\\n> You can do both the data collection in an Azure CloudShell, which is useful for running this in customer environments; however, to create and analyze the data, you must run the script from a computer with Word and Excel installed.\\n\\n**It is worth noting that not all services will be supported by the automated resource graph queries, so you will need to check any services that are not supported manually, and when looking at running this for Production workloads, remember to consider the business context around the resiliency requirements.**\\n\\n### \ud83d\udcca Collect the data\\n\\nThe Collector PowerShell script is designed to collect data from the Azure environment to help identify potential issues and areas for improvement using the Azure Resource Graph queries within this repository.\\n\\n| Parameter         | Description                                                                                                              | Example                                                                    |\\n| ----------------- | ------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------- |\\n| TenantID          | Optional: tenant to be used.                                                                                             | 12345678-1234-1234-1234-123456789012                                       |\\n| SubscriptionIds   | Required (or SubscriptionsFile): Specifies Subscription(s) to be included in the analysis.                               | 12345678-1234-1234-1234-123456789012, 12345678-1234-1234-1234-123456789012 |\\n| SubscriptionsFile | Optional (or SubscriptionIds): Specifies the file with the list to be analyzed (one subscription per line). | subscriptions.txt                                                          |\\n| RunbookFile       | Optional: Specifies the file with the runbook (selectors & checks) for use.                                           | runbook.json                                                               |\\n| ResourceGroups    | Optional: Specifies Resource Group(s) to be included in the analysis.                                                    | ResourceGroup1,ResourceGroup2                                              |\\n| Debug             | Optional: Writes debugging information for the script during the execution.                                               | TRUE                                                                       |\\n\\n\\n1. Open an [Azure Cloud Shell](https://shell.azure.com/) session\\n2. Type in: \\n  \\n  ```powershell\\n wget https://github.com/Azure/Azure-Proactive-Resiliency-Library-v2/raw/main/tools/1_wara_collector.ps1\\n .\\\\1_wara_collector.ps1 -SubscriptionIds \'YOURSUBID\'\\n  ```\\n3. Once the script has been completed, you will have a JSON file with the data collected and can proceed to the next step.\\n\\n![1_wara_collector.ps1](AzProactiveResiliencyLibrary_Collector.gif)\\n\\n### \ud83d\udcc8 Analyse the data\\n\\nThe Data Analyzer PowerShell script is the second script in the Azure Proactive Resiliency Library (APRL) tooling suite. This tool summarizes the collected data and provides actionable insights into the health and resiliency of the Azure environment.\\n\\nIf you used CloudShell in the previous step, you must download the JSON file to your local machine to run the Data Analyzer script.\\n\\n> _(The Data Analyzer script must be run from a Windows Machine with Excel installed.)_\\n\\n![WARA_File download](AzProactiveResiliencyLibrary_CollectorDownload.png)\\n\\nOnce the file is downloaded, you can run the Data Analyzer script to create the ExcelPlan.\\n\\n1. Open a PowerShell 7 terminal\\n2. Download the Data Analyzer script:\\n\\n```powershell\\nhttps://github.com/Azure/Azure-Proactive-Resiliency-Library-v2/raw/main/tools/2_wara_data_analyzer.ps1\\n# Define the URL of the file to be downloaded\\n$url = \\"https://github.com/Azure/Azure-Proactive-Resiliency-Library-v2/raw/main/tools/2_wara_data_analyzer.ps1\\"\\n# Define the path where the file will be saved\\n$outputPath = \\"d:\\\\APRL\\\\\\"\\n# Use Invoke-WebRequest to download the file\\nInvoke-WebRequest -Uri $url -OutFile $outputPath\\n```\\n\\n![2_wara_data_analyzer.ps1 download](AzProactiveResiliencyLibrary_DownloadAnalyzer.gif)\\n\\nOnce downloaded, we can run it against the JSON file from the Data Collector script.\\n  \\n  ```powershell\\n$JSONFile = \'WARA_File_2024-07-06_08_48.json\'\\n.\\\\2_wara_data_analyzer.ps1 -JSONFile $JSONFile\\n  ```\\n![Run Data Analyzer](AzProactiveResiliencyLibrary_RunDataAnalyzer.gif)\\n\\nYou should now have an Excel Action Plan with the data from the JSON file, including the recommendations and the current state of the resources.\\n\\n![Excel Action Plan](AzProactiveResiliencyLibrary_WARAActionPlan.gif)\\n\\n| Worksheet Name    | Description                                                                                                                                                                                                         |\\n| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Recommendations   | You will find all Recommendations, their category, impact, description, learn more links, and much more. Columns A and B count the number of Azure Resources associated with RecommendationID. |\\n| ImpactedResources | You will find a list of Azure Resources associated with a RecommendationID. These Azure Resources do not follow Microsoft\'s best practices for Reliability.                                                   |\\n| PivotTable        | You will find a couple of pivot tables used to automatically create the charts.                                                                                                                                     |\\n| Charts            | You will find three charts that will be used in the Executive Summary PPTx.                                                                                                                                             |\\n| ResourceTypes     | You will find a list of all ResourceTypes the customer uses, the number of Resources deployed for each one, and if there are Recommendations for the ResourceType in APRL.                                          |\\n\\n> There will be resources that are not supported by the Resource Graph queries, so you will need to check these manually. Could you remove or add any recommendations based on your analysis before generating reports?\\n\\n### \ud83d\udcdd Create the report\\n\\nThe Report Generator PowerShell script is the final script in the Azure Proactive Resiliency Library (APRL) tooling suite. Its goal is to generate a Word and PowerPoint report based on the data collected and analyzed by the Collector and Data Analyzer scripts.\\n\\nNow that we have our main Excel Action plan with the data, we can run the Report Generator script to create the Word and PowerPoint reports.\\n\\nThe previous script, would have downloaded the git repository for the Azure Proactive Library to your local device, so you can run the Report Generator script directly from that, the script needs to be ran.\\n\\nThis script requires specific Microsoft PowerPoint and Word templates:\\n\\n* Mandatory - Executive Summary presentation - Template.pptx\\n* Optional - Assessment Report - Template.docx\\n\\nThese are stored in the Tools folder of the Azure Proactive Library repository.\\n\\nThe 3_wara_report_generator.ps1 script has the following parameters:\\n\\n| Parameter        | Description                                                                                                                                         | Example Demo Parameters          |\\n| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------- |\\n| ExcelFile        | Mandatory: WARA Excel file generated by \u20182_wara_data_analyzer.ps1\u2019 script and customized.                                                           | C:\\\\\\\\path\\\\\\\\to\\\\\\\\WARA_Analysis.xlsx |\\n| CustomerName     | Optional: specifies the Name of the Customer to be added to the PPTx and DOCx files.                                                                | Contoso Ltd                     |\\n| Heavy            | Optional: Run the script at a lower pace to handle heavy environments.                                                                             | TRUE                             |\\n| WorkloadName     | Optional: specifies the Name of the Workload of the analyses to be added to the PPTx and DOCx files.                                                | E-commerce Platform              |\\n| PPTTemplateFile  | Optional: specifies the PPTx template file for use as source. If not specified, the script will look for the file in the same path as the script. | C:\\\\\\\\path\\\\\\\\to\\\\\\\\Template.pptx      |\\n| WordTemplateFile | Optional: specifies the DOCx template file for use as source. If not specified, the script will look for the file in the same path as the script. | C:\\\\\\\\path\\\\\\\\to\\\\\\\\Template.docx      |\\n| Debugging        | Optional: Writes a piece of Debugging information to a log file.                                                                                             | TRUE                             |\\n\\n1. Open a PowerShell 7 terminal\\n2. Run the following script _(adjust for your needs, it will look in the same folder that the script is located in for the word and excel templates)_:\\n\\n  ```powershell\\n$Excellocation = \'D:\\\\APRL\\\\WARA Action Plan 2024-07-06_21_22.xlsx\'\\n$WorkLoadName = \'eCommerce\'\\n$CustomerName = \'Contoso Ltd\'\\ncd D:\\\\APRL\\\\Azure-Proactive-Resiliency-Library\\\\tools\\\\\\n.\\\\3_wara_reports_generator.ps1 -ExcelFile $Excellocation -WorkLoadName $WorkLoadName -CustomerName $CustomerName\\n  ```\\n\\n![3_wara_reports_generator.ps1](AzProactiveResiliencyLibrary_RunReportGeneration.gif)\\n\\nThe script will generate a Word and PowerPoint report with the data from the Excel Action Plan and the recommendations for the Azure resources automatically included. You can then add additional information and recommendations based on the analysis of the data and the business context around the resources.\\n\\n![PowerPoint Report](AzProactiveResiliencyLibrary_PowerPointGeneration.gif)\\n\\nMore detail can then be added to the Word document.\\n\\n![Word Report](AzProactiveResiliencyLibrary_WordGeneration.gif)\\n\\n## \ud83d\udcd6 References: \\n\\n* \ud83d\udd17[Overview and Usage of APRL Scripts](https://azure.github.io/Azure-Proactive-Resiliency-Library-v2/tools/script-overviews/)\\n* \ud83d\udd17[Reliability design principles](https://learn.microsoft.com/azure/well-architected/reliability/principles?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Azure Advisor Resiliency Reviews](https://learn.microsoft.com/azure/advisor/advisor-resiliency-reviews?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Design methodology for mission-critical workloads on Azure](https://learn.microsoft.com/azure/well-architected/mission-critical/mission-critical-design-methodology?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Mission-critical baseline architecture in an Azure landing zone](https://learn.microsoft.com/azure/architecture/reference-architectures/containers/aks-mission-critical/mission-critical-landing-zone?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Service Level Agreements (SLA) for Online Services](https://www.microsoft.com/licensing/docs/view/Service-Level-Agreements-SLA-for-Online-Services?WT.mc_id=AZ-MVP-5004796)\\n* \ud83d\udd17[Fireside Chat about Mission Critical applications on Azure with Leandro Carvalho](https://youtu.be/d7kzfogjmpE)"},{"id":"azure/iac-github-codespace","metadata":{"permalink":"/azure/iac-github-codespace","source":"@site/blog/2024-06-14-iac-codespace/index.mdx","title":"Infrastructure as Code GitHub Codespace Template","description":"Codespace setup for Infrastructure as Code (IaC) coding, including Bicep and Terraform, and Linting.","date":"2024-06-14T09:12:51.904Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":3.065,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Infrastructure as Code GitHub Codespace Template","metaDescription":"Codespace setup for Infrastructure as Code (IaC) coding, including Bicep and Terraform, and Linting.","date":"2024-06-14T09:12:51.904Z","tags":["Azure","Misc"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/iac-github-codespace","keywords":["github","azure","bicep","infrastructure-as-code","codespace","terraform","linting"],"description":"Codespace setup for Infrastructure as Code (IaC) coding, including Bicep and Terraform, and Linting."},"unlisted":false,"prevItem":{"title":"Running your own Azure Proactive Resiliency Assessment","permalink":"/azure/proactive-resiliency-assessment"},"nextItem":{"title":"Cloud Design Patterns","permalink":"/azure/cloud-design-patterns"}},"content":"I\'ve written, about [GitHub Codespaces](https://github.com/features/codespaces) before, in the article [Coding on the Cloud - Getting Started with GitHub Codespaces](https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/), this article builds on it by supplying a Codespace setup for Infrastructure as Code *(IaC)* coding, including Bicep and Terraform, and Linting.\\n\\n\x3c!-- truncate --\x3e\\n\\n:::tip\\nThe **[lukemurraynz/Codespace_IaC_Coding](https://github.com/lukemurraynz/Codespace_IaC_Coding/)** Codespace includes essential tools for Infrastructure as Code development for Azure, tools like Azure CLI, PowerShell, Azure Bicep, Azure Developer CLI (azd), Terraform, TFLint, and Terragrunt. **Feel free to fork, clone, or contribute to the repository. This template is aimed at getting you up and coding as soon as possible!**\\n:::\\n\\n* **[Azure CLI](https://learn.microsoft.com/cli/azure/what-is-azure-cli?WT.mc_id=AZ-MVP-5004796)**: The Azure Command-Line Interface (CLI) is a cross-platform command-line tool to connect to Azure and execute administrative commands on Azure resources.\\n* **[PowerShell](https://learn.microsoft.com/powershell/scripting/overview?view=powershell-7.4&WT.mc_id=AZ-MVP-5004796)**: PowerShell is a cross-platform task automation solution made up of a command-line shell, a scripting language, and a configuration management framework.\\n* **[Azure Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796)**: Bicep is a domain-specific language (DSL) that uses declarative syntax to deploy Azure resources. In a Bicep file, you define the infrastructure you want to deploy to Azure, and then use that file throughout the development lifecycle to repeatedly deploy your infrastructure. \\n* **[Azure Developer CLI (azd)](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview?WT.mc_id=AZ-MVP-5004796)**: Azure Developer CLI (azd) is an open-source tool that accelerates the time it takes for you to get your application from local development environment to Azure. azd provides best practice, developer-friendly commands that map to key stages in your workflow, whether you\'re working in the terminal, your editor or integrated development environment (IDE), or CI/CD (continuous integration/continuous deployment).\\n* **[Terraform](https://www.terraform.io/)**: Infrastructure automation to provision and manage resources in any cloud or data center.\\n* **[TFLint](https://github.com/terraform-linters/tflint)**: A Terraform linter for detecting errors that cannot be detected by terraform plan.\\n* **[Terragrunt](https://terragrunt.gruntwork.io/)**: A thin wrapper for Terraform that provides extra tools for working with multiple Terraform modules.\\n\\n:::info\\nBecause this repository includes tools for both Bicep and Terraform IaC, it can take a few minutes to set up the Codespace. The setup process installs the necessary tools and extensions for Bicep and Terraform development. If you need to use only one of the tools, you can remove the unnecessary tools from the `.devcontainer/devcontainer.json` file.\\n:::\\n\\nThe repository also includes uses [MegaLinter](https://megalinter.io/latest/) to ensure code quality and adherence to best practices. MegaLinter analyzes the codebase for potential issues, coding standards violations, formatting discrepancies, and more across multiple languages and file formats. It helps maintain a high standard of code quality and consistency across the project.\\n\\nMegalinter can be configured to automatically lint and open up a pull request with the changes, or it can be run manually. The repository is configured to run the MegaLinter on every push to the main branch, by setting a [PAT (Personal Access Token)](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token) in the repository secrets. The PAT is used to create a pull request with the linting results. Review the [Megalinter](https://megalinter.io/7.8.0/configuration/) documentation for more information on how to configure it.\\n\\n![GitHub IaC Codespace](GitHub_IaC_Codespace_PAT.png)\\n\\nVSCode is the default editor in the Codespace, and you can use the integrated terminal to run commands. The repository includes a `.devcontainer/devcontainer.json` file that defines the development container configuration. The configuration includes the tools and extensions required for Bicep and Terraform development, but also the base VSCode settings and extensions.\\n\\nThe Codespace configuration includes the following VSCode settings:\\n\\n**editor.formatOnSaveMode**: Configures format on save to be applied to the entire file.\\n**[bicep.experimental.deployPane](https://luke.geek.nz/azure/Azure-Bicep-Deploy-Pane/)**: Enables the experimental deploy pane for Bicep.\\n\\nVS Code Extensions to be Installed:\\n\\n* **[ms-azuretools.vscode-azurecontainerapps](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurecontainerapps)**: Azure Container Apps extension.\\n* **[ms-azuretools.vscode-azureresourcegroups](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azureresourcegroups)**: Azure Resource Groups extension.\\n* **[ms-azuretools.vscode-bicep](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-bicep)**: Bicep extension.\\n* **[GitHub.copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot)**: GitHub Copilot extension.\\n* **[GitHub.copilot-chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat)**: GitHub Copilot Chat extension.\\n* **[ms-vscode.azure-account](https://marketplace.visualstudio.com/items?itemName=ms-vscode.azure-account)**: Azure Account extension.\\n* **[hashicorp.terraform](https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform)**: Terraform extension.\\n* **[golang.Go](https://marketplace.visualstudio.com/items?itemName=golang.Go)**: Go language support extension.\\n\\n![GitHub IaC Codespace](GitHub_IaC_Codespace.gif)"},{"id":"azure/cloud-design-patterns","metadata":{"permalink":"/azure/cloud-design-patterns","source":"@site/blog/2024-05-26-cloud-design-patterns/index.mdx","title":"Cloud Design Patterns","description":"Cloud design patterns help build reliable, scalable, secure applications in the cloud.","date":"2024-05-26T09:05:39.912Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Service Management","permalink":"/tags/service-management"}],"readingTime":109.8,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Cloud Design Patterns","metaDescription":"Cloud design patterns help build reliable, scalable, secure applications in the cloud.","description":"Cloud design patterns help build reliable, scalable, secure applications in the cloud.","date":"2024-05-26T09:05:39.912Z","tags":["Azure","Misc","Service Management"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/cloud-design-patterns","keywords":["Cloud","Design Patterns","Reliable","Scalable","Secure","Applications","Reusability","Maintainability","Consistency","Efficiency","Scalability","Performance","Modularity","Azure","Well-architected principles","Scalability","Resilience","Security","Operational Excellence","Performance Efficiency","Data Management","Acronyms","Service","Consumer","Tenant","Facade","Archetypes","Data Management patterns","Security patterns","Reliability patterns","Messaging patterns","Design and implementation patterns","Valet Key","Ambassador","Gateway","Cloud architecture"]},"unlisted":false,"prevItem":{"title":"Infrastructure as Code GitHub Codespace Template","permalink":"/azure/iac-github-codespace"},"nextItem":{"title":"Export Azure DevOps Repositories to Azure Storage Account","permalink":"/azure/export-azure-devops-repos-azure-storage-account"}},"content":"Cloud Design patterns help build reliable, scalable, secure applications in the cloud.\\n\\nToday, we will cover common patterns, their use cases, and considerations.\\n\\nThe patterns we will cover today are only a small set of common patterns documented in the [Azure Architecture Center](https://learn.microsoft.com/en-us/azure/architecture/patterns/?WT.mc_id=AZ-MVP-5004796).\\n\\n\\n\x3c!-- truncate --\x3e\\n\\n## \ud83c\udf29\ufe0f Cloud Design Patterns\\n\\n### Importance of Design Patterns\\n\\nDesign patterns are reusable solutions to common problems in software design. They are templates that can be applied to solve a problem in a specific context. They are not finished designs that can be transformed directly into code. They are a starting point for your design.\\n\\n![Design Patterns](Blog_CloudDesignPattnerns_Importance.gif)\\n\\n:::info\\nDesign patterns are not new. They have been around for a long time. The concept of design patterns was introduced by the architect [Christopher Alexander](https://en.wikipedia.org/wiki/Christopher_Alexander#:~:text=Alexander%%20and%20personally%20built,architect%20and%20a%20general%20contractor.&text=In%20software%2C%20Alexander%20is%20regarded,to%20its%20creator%2C%20Ward%20Cunningham) in the field of architecture and later adopted by software engineers.\\n:::\\n\\nDesign patterns are essential to consider when implementing your solutions as they help with the following:\\n\\nReusability - Design patterns promote reusability by providing proven solutions to common problems, allowing components and subsystems to be used in other applications and scenarios and interoperability\\n\\n* Maintainability - They simplify administration and development, making maintaining and updating applications easier.\\n* Consistency - Design patterns ensure consistency and coherence in component design and deployment, which is crucial for the overall quality of the application\\n* Efficiency - By following established patterns, developers can avoid repetitive work and focus on implementing specific requirements, thus saving development time\\n* Scalability - Design patterns help in designing scalable systems by providing guidelines for efficient resource allocation and management\\n* Performance - They help optimize the use of system resources, leading to improved performance of the application\xa0\\n* Modularity - Design patterns promote modular and structured code, making it easier to understand, modify, and maintain.\\n\\nThis is as true in Modern Cloud solutions as it is in traditional development, with [well-architected principles](https://learn.microsoft.com/azure/well-architected/?WT.mc_id=AZ-MVP-5004796) more important than ever.\\n\\nTake a Cloud Platform or ecosystem like Azure, with Services offering a theory of functionality that can be put together like Lego bricks.\\n\\n* Scalability\u2014Cloud design patterns address scalability by providing solutions for horizontal and vertical scaling, ensuring that applications can handle increased loads efficiently. This promotes sustainability by enabling dynamic resource allocation and reducing energy consumption during low-demand periods.\\n* Resilience\u2014Patterns like Circuit Breakers and Bulkhead help build resilient applications that can withstand failures and continue to function. Sustainable operations are achieved by minimizing downtime and resource wastage, thus ensuring efficient resource use.\\n* Security\u2014Security patterns such as API Gateway and Secure Token ensure that applications are protected against malicious actors while maintaining confidentiality, integrity, and availability. Secure systems support sustainability by preventing security breaches that could lead to resource wastage.\\n* Operational Excellence - Patterns like Ambassador and Anti-Corruption Layer enhance operational excellence by simplifying the management and monitoring of cloud applications. Efficient management practices contribute to sustainability by optimizing resource usage and reducing unnecessary overhead.\\n* Performance Efficiency - Patterns such as Cache-Aside and Compute Resource Consolidation optimize performance by efficiently managing resources and reducing latency. Effective resource management leads to sustainability by lowering the overall resource footprint and energy consumption.\\n* Data Management\u2014Data management patterns address data consistency, synchronization, and management challenges across different locations, which is crucial for cloud applications. Sustainable data practices ensure data is stored and processed efficiently, minimizing energy use and reducing environmental impact.\\n\\n#### Acroymns\\n\\nWhen talking about design patterns, we will be using a few acronyms, and it is important to understand what they mean in order to fully understand the context.\\n\\n| Acronym  | Definition                                                                                                                                                                                                                | Example                                                                                                                                                                                                    |\\n| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Service  | A service in cloud pattern design refers to a distinct function or set of functions a cloud application provides. Services are typically designed to be reusable and can be consumed by multiple clients or consumers. | In a microservices architecture, each microservice is a service that performs a specific business function, such as user authentication, payment processing, or data storage.                              |\\n| Consumer | A \xa0consumer is an entity (such as an application, service, or user) that consumes or uses the services provided by another service. Consumers send requests to services and receive responses.                            | A web application that calls an authentication service to verify user credentials is a consumer of that authentication service.                                                                            |\\n| Tenant   | In a multi-tenant architecture, a tenant is a group of users who share common access with specific privileges to the software instance. Each tenant\'s data is isolated and remains invisible to other tenants.            | In a Software as a Service (SaaS) application, each customer (company or organization) is considered a tenant. They have their own isolated data and configuration settings within the shared application. |\\n| Facade   | The Facade pattern provides a simplified interface to a complex subsystem, making it easier to use and understand. It acts as a high-level interface that hides the complexities of the underlying system.                      | In a software application, a facade can be used to provide a simplified API that abstracts away the complexities of interacting with multiple subsystems or modules.                                      |\\n| Asynchronous | Asynchronous communication is a messaging pattern where the sender and receiver do not need to interact in real time. The sender can send a message and continue with other tasks, while the receiver processes the message at its own pace. | In a distributed system, asynchronous messaging can be used to decouple components and improve scalability. For example, a message queue can be used to send and receive messages between different services. |\\n| Synchronous | Synchronous communication is a messaging pattern where the sender and receiver interact in real time. The sender sends a message and waits for a response from the receiver before proceeding. | In a request-response scenario, a client sends a request to a server and waits for the server to process the request and send back a response. This pattern is commonly used in HTTP-based APIs. |\\n\\n### Archetypes *(Categories)* of Patterns\\n\\nCloud design patterns can be categorized into different Archetypes *(categories)* based on their characteristics and use cases. These archetypes provide a structured way to understand and apply patterns in cloud architecture design.\\n\\nThe categories include Data Management, Security, Reliability, Messaging, and Design and implementation. Let\'s examine each category in detail.\\n\\n* Data Management patterns help us manage data in the cloud. Caching helps us reduce the number of database hits and improve application performance. Sharding helps us partition our data across multiple databases to improve scalability. Federation helps us distribute data across multiple databases for better availability and fault tolerance.\\n* Security patterns help us secure our applications in the cloud. Perimeter Networks help us isolate our applications from the Internet. Identity Management helps us manage user identities and access to resources. Secure Communication helps us protect our data in transit.\\n* Reliability patterns help us build highly available and fault-tolerant applications in the cloud. Retry helps us recover from transient failures. Circuit Breakers help us prevent cascading failure. Bulkhead helps us limit the damage caused by a failure.\\n* Messaging patterns help us build scalable and decoupled applications in the cloud. Asynchronous Messaging helps us send messages between components in a non-blocking way. Event Sourcing helps us capture all changes to an application state. Competing Consumers help us process messages in parallel.\\n* Design and implementation patterns help us build scalable and resilient cloud-native applications. Valet Key helps us delegate access to resources. Ambassador helps us expose services to external clients. Gateway helps us provide a single entry point to our application.\\n\\n![Cloud Design Patterns Archetypes](Blog_CloudDesignPatterns_Archetypes.PNG)\\n\\n:::warning\\nAs usual, the choice of pattern depends on your application\'s specific requirements. To make an informed decision, it\'s essential to understand the characteristics and trade-offs of each pattern and business requirements. Sometimes, multiple patterns may be required, or even part of one, to solve a specific problem.\\n:::\\n\\n### Pattern Deep Dives\\n\\nThe cloud has changed the way we design our applications. Let\'s examine some design patterns we can use in the cloud. These patterns help us solve common problems in cloud architecture and make our applications more reliable, scalable, and secure.\\n\\nIn the following sections, we will examine common cloud design patterns, their use cases, and considerations in depth.\\n\\n### Data Management Patterns\\n\\n![Design Patterns - Data Management](Blog_CloudDesignPatterns_DataManagement.PNG)\\n\\n### \ud83d\udcda Cache-Aside\\n\\n> Load data on demand into a cache from a data store\\n\\nMany commercial caching systems provide read-through and write-through/write-behind operations. In these systems, an application retrieves data by referencing the cache. \\n\\nIf the data isn\'t in the cache, it\'s retrieved from the data store and added to the cache. Any modifications to data held in the cache are also automatically written back to the data store.\\n\\nFor caches that don\'t provide this functionality, it\'s the responsibility of the applications that use the cache to maintain the data.\\n\\nIf an application updates information, it can follow the write-through strategy by modifying the data store and invalidating the corresponding item in the cache. When the item is next required, using the cache-aside strategy will cause the updated data to be retrieved from the data store and added back into the cache.\\n\\n![Cloud Design Pattern - CacheAsSide](Blog_CloudDesignPatterns_CacheAside.gif)\\n\\n#### Issues & considerations\\n\\nConsider the following points when deciding how to implement this pattern:\\n\\n* Lifetime of cached data. Many caches implement an expiration policy that invalidates data and removes it from the cache if it\'s not accessed for a specified period. For cache-aside to be effective, ensure that the expiration policy matches the pattern of access for applications that use the data. Don\'t make the expiration period too short because this can cause applications to continually retrieve data from the data store and add it to the cache. Similarly, don\'t make the expiration period so long that the cached data will likely become stale. Remember that caching is most effective for relatively static data or data that is read frequently.\\n* Evicting data. Most caches have a limited size compared to the data store where the data originates, and they\'ll evict data if necessary. Most caches adopt a least-recently-used policy for selecting items to evict, but this might be customizable. Configure the global expiration property and other properties of the cache, as well as the expiration property of each cached item, to ensure that the cache is cost-effective. Applying a global eviction policy to every item in the cache isn\'t always appropriate. For example, if a cached item is costly to retrieve from the data store, it can be beneficial to keep this item in the cache at the expense of more frequently accessed but less costly items.\\n* Priming the cache. Many solutions prepopulate the cache with the data that an application is likely to need during startup processing. The Cache-Aside pattern can still be useful if some of this data expires or is evicted.\\n* Consistency. Implementing the Cache-Aside pattern doesn\'t guarantee consistency between the data store and the cache. An external process can change an item in the data store at any time, and this change might not be reflected in the cache until the next time the item is loaded. In a system replicating data across data stores, this problem can become serious if synchronization occurs frequently.\\n* Local *(in-memory)* caching. A cache could be local to an application instance and stored in memory. Cache-aside can be useful in this environment if an application repeatedly accesses the same data. However, a local cache is private, so different application instances could each have a copy of the same cached data. This data could quickly become inconsistent between caches, so it might be necessary to expire data in a private cache and refresh it more frequently. In these scenarios, investigate a shared or distributed caching mechanism.\\n\\n#### When to use this pattern\\n\\n* When an in-built cache doesn\'t provide native read-through or write-back operations.\\n* When resource demand is unpredictable, you want to avoid overloading the data store.\\n* When the cached dataset is not static\\n* When NOT caching session state information in a web application hosted in a web farm. \\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e NoNativeCacheOps{Does the in-built cache not provide native read-through or write-back operations?}\\n  NoNativeCacheOps -- Yes --\x3e UnpredictableDemand{Is resource demand unpredictable?}\\n  NoNativeCacheOps -- No --\x3e DoNotUseCacheAside[Do not use Cache-Aside]\\n  UnpredictableDemand -- Yes --\x3e NonStaticData{Is the cached dataset not static?}\\n  UnpredictableDemand -- No --\x3e DoNotUseCacheAside\\n  NonStaticData -- Yes --\x3e NotCachingSessionState{Are you not caching session state information in a web application hosted in a web farm?}\\n  NonStaticData -- No --\x3e DoNotUseCacheAside\\n  NotCachingSessionState -- Yes --\x3e UseCacheAside[Use Cache-Aside]\\n  NotCachingSessionState -- No --\x3e DoNotUseCacheAside\\n```\\n\\n#### Azure Solutions\\n\\nIn this example, it uses [Azure Cache for Redis](https://learn.microsoft.com/azure/azure-cache-for-redis/cache-overview?WT.mc_id=AZ-MVP-5004796). When an application needs to retrieve data, it will first check to see if it exists in Azure Cache for Redis.\\nIf the data is found in Azure Cache for Redis (cache hit), the application will use this data.\\nIf the data is not found in Azure Cache for Redis (cache miss), the application must retrieve it from the appropriate Azure database service.\\nFor cache miss scenarios, the requesting application should add the data retrieved from the Azure Database service to Azure Cache for Redis.\\n\\n### \ud83d\udcda Sharding\\n\\n> Divide a data store into a set of horizontal partitions or shards.\\n\\nDividing a data store into a set of horizontal partitions or shards. This can improve scalability when storing and accessing large volumes of data.\\n\\n* The Lookup strategy. In this strategy, the sharding logic implements a map that routes a request for data to the shard containing that data using the shard key. In a multi-tenant application, all the data for a tenant might be stored together in a shard using the tenant ID as the shard key. Multiple tenants might share the same shard, but the data for a single tenant won\'t be spread across multiple shards.\\n* The Range strategy. This strategy groups related items together in the same shard and orders them by shard key\u2014the shard keys are sequential. It\'s useful for applications that frequently retrieve sets of items using range queries (queries that return a set of data items for a shard key that falls within a given range). For example, if an application regularly needs to find all orders placed in a given month, this data can be retrieved more quickly if all orders for a month are stored in date and time order in the same shard. If each order were stored in a different shard, they\'d have to be fetched individually by performing many point queries (queries that return a single data item).\xa0\\n* The Hash strategy. The purpose of this strategy is to reduce the chance of hotspots (shards that receive a disproportionate amount of load). It distributes the data across the shards in a way that achieves a balance between the size of each shard and the average load that each shard will encounter. The sharding logic computes the shard to store an item in based on a hash of one or more data attributes. The chosen hashing function should distribute data evenly across the shards, possibly by introducing some random element into the computation.\\n\\n![Cloud Design Pattern - Sharding](Blog_CloudDesignPatterns_Sharding.gif)\\n\\n#### Issues & considerations\\n\\nConsider the following points when deciding how to implement this pattern:\\n\\n* Balancing data across multiple shards.\\n* Your queries need to be efficient and not require data from multiple shards or cause latency issues.\\n* Designing for scalability (storage nodes \u2013 data & throughput)\\n* Designing for availability (replication, failover, recovery)\\n* Make sure to use stable and unique key data to determine the sharding state\\n* Monitoring, backup, and maintaining consistency of multiple shards can be difficult.\\n\\n#### When to use this pattern\\n\\nThe primary focus of sharding is to improve the performance and scalability of a system. Still, as a by-product it can also improve availability due to how the data is divided into separate partitions. A failure in one partition doesn\'t necessarily prevent an application from accessing data held in other partitions, and an operator can perform maintenance or recovery of one or more partitions without making the entire data for an application inaccessible.\xa0\\n\\n* When the data store is likely to need to scale beyond the resources of a single storage node.\\n* When you can improve performance  by reducing contention on a single data store.\\n* When you need to optimize cost by multiple instances of less expensive compute or storage.\\n* Improve resilience and reliability.\\n* When you have geographical concerns, i.e., Optimization based on where users or systems are located.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e IsScaleBeyondSingleNode{Is the data store likely to scale beyond the resources of a single node?}\\n  IsScaleBeyondSingleNode -- Yes --\x3e CanReduceContention{Can performance be improved by reducing contention on a single data store?}\\n  IsScaleBeyondSingleNode -- No --\x3e DoNotUseSharding[Do not use Sharding]\\n  CanReduceContention -- Yes --\x3e CanOptimizeCost{Can cost be optimized by multiple instances of less expensive compute or storage?}\\n  CanReduceContention -- No --\x3e DoNotUseSharding\\n  CanOptimizeCost -- Yes --\x3e NeedToImproveResilience{Need to improve resilience and reliability?}\\n  CanOptimizeCost -- No --\x3e DoNotUseSharding\\n  NeedToImproveResilience -- Yes --\x3e HaveGeographicalConcerns{Have geographical concerns?}\\n  NeedToImproveResilience -- No --\x3e DoNotUseSharding\\n  HaveGeographicalConcerns -- Yes --\x3e UseSharding[Use Sharding]\\n  HaveGeographicalConcerns -- No --\x3e DoNotUseSharding\\n```\\n\\n#### Azure Solutions\\n\\nConsider a website that surfaces an expansive collection of information on published books worldwide. \\nThe number of possible books cataloged in this workload and the typical query/usage patterns contra-indicate the usage of a single relational database to store the book information. The workload architect decides to shard the data across multiple database instances, using the books\' static International Standard Book Number (ISBN) for the shard key. Specifically, they use the\xa0check digit\xa0(0 - 10) of the ISBN as that gives 11 possible logical shards, and the data will be balanced across each shard. First, they decide to colocate the 11 logical shards into three physical shard databases. They use the\xa0lookup\xa0sharding approach and store the key-to-server mapping information in a shard map database.\\n\\nAzure resources that can be used to implement this pattern include [Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql&WT.mc_id=AZ-MVP-5004796), [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/introduction?WT.mc_id=AZ-MVP-5004796), and [Azure Storage](https://learn.microsoft.com/azure/storage/common/storage-introduction?WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\ude97 Valet Key\\n\\n> Use a token or key that provides clients with restricted direct access to a specific resource or service.\\n\\nData stores have the ability to handle the upload and download of data directly without requiring the application to perform any processing to move this data. However, this typically requires the client to have access to the store\'s security credentials. This can be a useful technique to minimize data transfer costs and the requirement to scale out the application and to maximize performance. It means, though, that the application is no longer able to manage the security of the data. After the client has a connection to the data store for direct access, the application can\'t act as the gatekeeper. It\'s no longer in control of the process and can\'t prevent subsequent uploads or downloads from the data store.\\nThis isn\'t a realistic approach in distributed systems that need to serve untrusted clients. \\n\\nApplications must be able to securely control access to data in a granular way, but still reduce the load on the server by setting up this connection and then allowing the client to communicate directly with the data store to perform the required read or write operations.\\n\\n![Cloud Design Pattern - Valet Key](Blog_CloudDesignPatterns_ValetKey.gif)\\n\\n#### Issues & considerations\\n\\nBy considering these issues and trade-offs, you can effectively implement the Valet Key pattern to enhance security, performance, and cost efficiency in your cloud applications.\\n\\n* Ensure that the token or key used for access is securely generated and transmitted to prevent unauthorized access\\n* It could introduce security and audit risks if not implemented correctly\\n* Implement proper access controls and authorization mechanisms to restrict the actions that can be performed using the valet key\\n* Cost savings from offloading processing must be weighed against the complexity of implementing and managing the Valet Key pattern\\n* Monitor and audit access using the valet key to detect any unauthorized or malicious activities\\n* Ensure that the resources are managed efficiently to avoid unnecessary costs, especially when dealing with frequent or large client requests\\n\\n#### When to use this pattern\\n\\nUsing a valet key doesn\'t require the resource to be locked, no remote server call is required, there\'s no limit on the number of valet keys that can be issued, and it avoids a single point of failure resulting from performing the data transfer through the application code.\xa0\\n\\nConsider using the Valet Key pattern in the following scenarios:\\n\\n* Offload data transfer from the application to minimize the use of valuable resources such as computing, memory, and bandwidth. \\n* When the data is stored in a remote data store or a different datacenter from the application.\\n* Minimize operational costs by enabling direct access to stores and queues. \\n* Clients regularly upload or download data, particularly where there\'s a large volume or when each operation involves a large file\\n* If the application doesn\u2019t need to perform tasks on the data first.\\n* When you need to provide secure, temporary access to specific resources or services.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e OffloadDataTransfer{Offload data transfer to minimize resource usage?}\\n  OffloadDataTransfer -- Yes --\x3e RemoteDataStore{Is data stored remotely or in a different datacenter?}\\n  OffloadDataTransfer -- No --\x3e DoNotUsePattern[Do not use this pattern]\\n  RemoteDataStore -- Yes --\x3e MinimizeCosts{Minimize operational costs by direct access to stores and queues?}\\n  RemoteDataStore -- No --\x3e DoNotUsePattern\\n  MinimizeCosts -- Yes --\x3e RegularDataTransfer{Clients regularly upload or download large volumes of data?}\\n  MinimizeCosts -- No --\x3e DoNotUsePattern\\n  RegularDataTransfer -- Yes --\x3e NoDataProcessing{Application doesn\u2019t need to process the data first?}\\n  RegularDataTransfer -- No --\x3e DoNotUsePattern\\n  NoDataProcessing -- Yes --\x3e SecureAccess{Need to provide secure, temporary access to resources?}\\n  NoDataProcessing -- No --\x3e DoNotUsePattern\\n  SecureAccess -- Yes --\x3e UsePattern[Use this pattern]\\n  SecureAccess -- No --\x3e DoNotUsePattern\\n```\\n\\n#### Azure Solutions\\n\\nAzure supports shared access signatures on Azure Storage for granular access control to data in blobs, tables, and queues, and for Service Bus queues and topics. A shared access signature token can be configured to provide specific access rights such as read, write, update, and delete to a specific table, a key range within a table, a queue, a blob, or a blob container. The validity can be a specified time period. This functionality is well-suited for accessing a valet key.\\n\\nAzure resources that can be used to implement this pattern include [Azure Storage](https://learn.microsoft.com/azure/storage/common/storage-introduction?WT.mc_id=AZ-MVP-5004796) and [Azure Function](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\udce6 Static Content Hosting\\n\\n> Deploy static content to a cloud-based storage service that can deliver them directly to the client.\\n\\nWeb applications typically include some elements of static content. This static content might include HTML pages and other resources such as images and documents that are available to the client, either as part of an HTML page (such as inline images, style sheets, and client-side JavaScript files) or as separate downloads (such as PDF documents).\\nAlthough web servers are optimized for dynamic rendering and output caching, they still have to handle requests to download static content. This consumes processing cycles that could often be better used for more important delivery and user-experience tasks.\\n\\nDeploy static content to a cloud-based storage service that can deliver them directly to the client. This can reduce the need for potentially expensive compute instances.\\n\\n![Cloud Design Pattern - StaticContent](Blog_CloudDesignPatterns_StaticContent.gif)\\n\\n#### Issues & considerations\\n\\nBy considering these issues and trade-offs, you can effectively implement the Valet Key pattern to enhance security, performance, and cost efficiency in your cloud applications.\\n\\n* If static content is not included in the application deployment package or process, it may need to be provisioned and deployed independently from the application.\\n* Security access needs to be considered \u2013 i.e., Public write access to prevent unauthorized uploads.\\n* Storage services might not support the use of a custom domain name\\n* File compression, such as gzip, to reduce load times for clients.\\n* Consider how to handle local development and testing\\n* Geographic concerns: Consider using a content delivery network (CDN) to cache the storage container\'s contents in multiple data centers around the world.\\n* Caching of existing assets.\\n\\n#### When to use this pattern\\n\\nStatic Content Hosting is a common pattern used in web applications to offload the delivery of static content from the application server to a cloud-based storage service. This pattern is particularly useful when the application server is not optimized for serving static content or when the application server is under heavy load and needs to offload some of the work to improve performance and scalability.\\n\\n* Reduce the hosting costs for websites and applications that contain some static resources. Cloud-hosted storage is typically much less expensive than compute instances\\n* Improve the performance of your main WebApp by offloading  static content serving\\n* Expose static resources and content for applications running in other hosting environments or on-premises servers. \\n* Improve your application\'s performance and availability by using a content delivery network (CDN) to cache the contents of the storage container in multiple datacenters worldwide. \\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e NeedToOffload{Need to offload delivery of static content from application server?}\\n  NeedToOffload -- Yes --\x3e ServerNotOptimized{Is the application server not optimized for serving static content?}\\n  NeedToOffload -- No --\x3e DoNotUsePattern[Do not use this pattern]\\n  ServerNotOptimized -- Yes --\x3e HeavyLoad{Is the application server under heavy load?}\\n  ServerNotOptimized -- No --\x3e DoNotUsePattern\\n  HeavyLoad -- Yes --\x3e ReduceHostingCosts{Need to reduce hosting costs?}\\n  HeavyLoad -- No --\x3e DoNotUsePattern\\n  ReduceHostingCosts -- Yes --\x3e ImprovePerformance{Need to improve performance by offloading static content serving?}\\n  ReduceHostingCosts -- No --\x3e DoNotUsePattern\\n  ImprovePerformance -- Yes --\x3e ExposeResources{Need to expose static resources for applications in other environments?}\\n  ImprovePerformance -- No --\x3e DoNotUsePattern\\n  ExposeResources -- Yes --\x3e UseCDN{Need to improve performance and availability using a CDN?}\\n  ExposeResources -- No --\x3e DoNotUsePattern\\n  UseCDN -- Yes --\x3e UsePattern[Use Static Content Hosting pattern]\\n  UseCDN -- No --\x3e DoNotUsePattern\\n```\\n\\n#### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Blob Storage](https://learn.microsoft.com/azure/storage/blobs/storage-blobs-introduction?WT.mc_id=AZ-MVP-5004796), [Azure Content Delivery Network (CDN)](https://learn.microsoft.com/azure/cdn/cdn-overview?WT.mc_id=AZ-MVP-5004796), and [Azure Static Web Apps](https://learn.microsoft.com/azure/static-web-apps/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n### Design & Implementation Patterns\\n\\n![Design & Implementation Patterns](Blog_CloudDesignPatterns_DesignImplementation.PNG)\\n\\n### \ud83c\udf89 Ambassador\\n\\n> Create helper services that send network requests on behalf of a consumer service or application.\\n\\nResilient cloud-based applications require features such as\xa0circuit breaking, routing, metering and monitoring, and the ability to make network-related configuration updates. It may be difficult or impossible to update legacy applications or existing code libraries to add these features because the code is no longer maintained or can\'t be easily modified by the development team.\\nNetwork calls may also require substantial configuration for connection, authentication, and authorization. If these calls are used across multiple applications, built using multiple languages and frameworks, the calls must be configured for each of these instances. In addition, network and security functionality may need to be managed by a central team within your organization. With a large code base, it can be risky for that team to update application code they aren\'t familiar with.\\n\\nPut client frameworks and libraries into an external process that acts as a proxy between your application and external services. Deploy the proxy on the same host environment as your application to allow control over routing, resiliency, and security features and to avoid any host-related access restrictions. \\n\\n![Cloud Design Patterns - Ambassador](Blog_CloudDesignPatterns_Ambassador.gif)\\n\\n#### Issues & considerations\\n\\nWhen implementing the Ambassador pattern, consider the following issues and considerations:\\n\\n* Latency Overhead- The proxy adds some latency overhead. It is important to evaluate whether a client library, invoked directly by the application, might be a better approach to avoid this additional latency\\n* Generalized Features - Including generalized features in the proxy can have unintended consequences. For example, the ambassador could handle retries, but this might not be safe unless all operations are idempotent. Ensure that the features implemented in the proxy do not introduce risks or conflicts with the application\'s requirements\\n* Context Passing \u2014 Consider a mechanism that allows the client to pass some context to the proxy and back to the client. For example, include HTTP request headers to opt out of retry or specify the maximum number of times to retry. This flexibility can help tailor the proxy\'s behavior to specific needs.\\n* Packaging and Deployment - Think about how to package and deploy the proxy. The proxy can be deployed as a sidecar to accompany the lifecycle of a consuming application or service. Alternatively, if an ambassador is shared by multiple separate processes on a common host, it can be deployed as a daemon or Windows service\\n* Shared vs. Dedicated Instances - Decide whether to use a single shared instance for all clients or an instance for each client. This decision can impact resource utilization, performance, and isolation between different clients\\n* Security and Configuration - Network calls may require substantial connection, authentication, and authorization configuration. If these calls are used across multiple applications built using multiple languages and frameworks, the calls must be configured for each of these instances. Additionally, network and security functionality may need to be managed by a central team within your organization\\n* Specialized Teams - Features that are offloaded to the ambassador can be managed independently of the application. This allows for separate, specialized teams to implement and maintain security, networking, or authentication features that have been moved to the ambassador without disturbing the application\'s legacy functionalityBy addressing these issues and considerations, you can effectively implement the Ambassador pattern to enhance the networking capabilities, security, and maintainability of your applications.\\n\\n#### When to use this \\n\\nThe Ambassador pattern should be used in the following scenarios:\\n* Offloading Common Client Connectivity Tasks - When you need to offload common client connectivity tasks such as monitoring, logging, routing, security (such as TLS), and resiliency patterns in a language-agnostic way\\n* Legacy Applications - When dealing with legacy applications or other applications that are difficult to modify, the Ambassador pattern can extend their networking capabilities without requiring changes to the application code\\n* Specialized Teams - When you want to enable a specialized team to implement and maintain security, networking, or authentication features independently of the application. This allows for updates and modifications to the ambassador without disturbing the application\'s legacy functionality\\n* Resilient Cloud-Based Applications - When building resilient cloud-based applications that require features such as circuit breaking, routing, metering and monitoring, and the ability to make network-related configuration updates. The Ambassador pattern can facilitate these features without modifying the existing codebase\\n* Centralized Management of Network and Security - When network calls require substantial configuration for connection, authentication, and authorization across multiple applications built using multiple languages and frameworks. The Ambassador pattern allows these configurations to be managed centrally, reducing the risk of updating application code by a team unfamiliar with it\\n* Standardizing and Extending Instrumentation - When you need to standardize and extend instrumentation, such as monitoring performance metrics like latency or resource usage. The Ambassador pattern allows this monitoring to happen in the same host environment as the application\\n* Supporting Cloud or Cluster Connectivity Requirements - When you need to support cloud or cluster connectivity requirements in a legacy application or an application that is difficult to modify considering these scenarios, you can effectively use the Ambassador pattern to enhance the networking capabilities, security, and maintainability of your applications.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e OffloadConnectivityTasks{Offload common client connectivity tasks?}\\n  OffloadConnectivityTasks -- Yes --\x3e LegacyApplications{Dealing with legacy applications?}\\n  OffloadConnectivityTasks -- No --\x3e DoNotUsePattern[Do not use this pattern]\\n  LegacyApplications -- Yes --\x3e SpecializedTeams{Enable specialized team to implement and maintain features?}\\n  LegacyApplications -- No --\x3e DoNotUsePattern\\n  SpecializedTeams -- Yes --\x3e ResilientCloudApps{Building resilient cloud-based applications?}\\n  SpecializedTeams -- No --\x3e DoNotUsePattern\\n  ResilientCloudApps -- Yes --\x3e CentralizedManagement{Require centralized management of network and security?}\\n  ResilientCloudApps -- No --\x3e DoNotUsePattern\\n  CentralizedManagement -- Yes --\x3e StandardizeInstrumentation{Need to standardize and extend instrumentation?}\\n  CentralizedManagement -- No --\x3e DoNotUsePattern\\n  StandardizeInstrumentation -- Yes --\x3e CloudClusterConnectivity{Support cloud or cluster connectivity requirements?}\\n  StandardizeInstrumentation -- No --\x3e DoNotUsePattern\\n  CloudClusterConnectivity -- Yes --\x3e UsePattern[Use Ambassador pattern]\\n  CloudClusterConnectivity -- No --\x3e DoNotUsePattern\\n```\\n\\n#### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796), [Azure Front Door](https://learn.microsoft.com/azure/frontdoor/front-door-overview?WT.mc_id=AZ-MVP-5004796) and [Azure Firewall](https://learn.microsoft.com/azure/firewall/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\ude80 Sidecar\\n\\n> Deploy components of an application into a separate process or container to provide isolation and encapsulation.\\n\\nDeploy application components into a separate process or container to provide isolation and encapsulation. This pattern can also enable applications composed of heterogeneous components and technologies.\\nThis pattern is named\xa0Sidecar\xa0because it resembles a sidecar attached to a motorcycle. In the pattern, the sidecar is attached to a parent application and provides supporting features for the application. The sidecar also shares the same lifecycle as the parent application, being created and retired alongside the parent. The sidecar pattern is sometimes referred to as the sidekick pattern and is a decomposition pattern.\\n\\n![Cloud Design Pattern - Sidecar](Blog_CloudDesignPatterns_Sidecar.gif)\\n\\n#### Issues & considerations\\n\\nWhen implementing the Sidecar pattern, consider the following issues and considerations.\\n\\n* Deployment and Packaging - Carefully decide on the deployment and packaging format for services, processes, or containers. Containers are particularly well-suited to the sidecar pattern\\n* Interprocess Communication - Choose the interprocess communication mechanism wisely. Aim to use language- or framework-agnostic technologies unless performance requirements make that impractical\\n* Functionality Placement - Before placing functionality into a sidecar, consider whether it would work better as a separate service or a more traditional daemon. Also, evaluate if the functionality could be implemented as a library or using a traditional extension mechanism, as language-specific libraries may offer deeper integration and less network overhead\\n* Resource Management - The sidecar can access the same resources as the primary application, which allows it to monitor system resources used by both the sidecar and the primary application\\n* Latency - Communication between a parent application and sidecar services includes some overhead, notably call latency. This may not be an acceptable trade-off for chatty interfaces\\n* Application Size and Resource Cost - For small applications, the resource cost of deploying a sidecar service for each instance may not be worth the advantage of isolation\\n* Scaling - If the service needs to scale differently or independently from the main applications, it may be better to deploy the feature as a separate service\\n* Security - By encapsulating tasks and deploying them out-of-process, you can reduce the surface area of sensitive processes to only the code needed to accomplish the task. Sidecars can also add cross-cutting security controls to an application component that is not natively designed with that functionality\\n* Operational Excellence - The sidecar pattern provides an approach to implementing flexibility in tool integration that might enhance the application\'s observability without requiring the application to take direct implementation dependencies. It enables the sidecar functionality to evolve and be maintained independently of the application\'s lifecycle\\n* Performance Efficiency - Moving cross-cutting tasks to a single process that can scale across multiple instances of the main process reduces the need to deploy duplicate functionality for each instance of the application, by addressing these issues and considerations, you can effectively implement the Sidecar pattern to enhance the modularity, security, and maintainability of your applications.\\n\\n#### When to use this \\n\\nThe Sidecar pattern should be used in the following scenarios.\\n\\n* Heterogeneous Set of Languages and Frameworks - When your primary application uses a heterogeneous set of languages and frameworks. A component located in a sidecar service can be consumed by applications written in different languages using different frameworks\\n* Component Ownership by Remote Teams - When a component is owned by a remote team or a different organization. This allows the component to be developed and maintained independently of the main application\\n* Co-location Requirement - When a component or feature must be co-located on the same host as the application. This ensures that the component can access the same resources and environment as the primary application\\n* Independent Updates - When you need a service that shares the overall lifecycle of your main application but can be independently updated. This allows for flexibility in updating the sidecar without affecting the main application\\n* Fine-grained resource Control - When you need fine-grained control over resource limits for a particular resource or component. For example, you may want to restrict the amount of memory a specific component uses. Deploying the component as a sidecar allows you to manage memory usage independently of the main application\\n* Security Enhancements - When you need to encapsulate tasks and deploy them out-of-process to reduce the surface area of sensitive processes to only the code needed to accomplish the task. Sidecars can also add cross-cutting security controls to an application component that is not natively designed with that functionality\\n* Operational Excellence - When you want to implement flexibility in tool integration that might enhance the application\'s observability without requiring the application to take direct implementation dependencies. The sidecar functionality can evolve and be maintained independently of the application\'s lifecycle\\n* Performance Efficiency \u2014 When you want to move cross-cutting tasks to a single process that can scale across multiple instances of the main process, reducing the need to deploy duplicate functionality for each application instance. By considering these scenarios, you can effectively use the Sidecar pattern to enhance the modularity, security, and maintainability of your applications.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e HeterogeneousLanguages{Heterogeneous set of languages and frameworks?}\\n  HeterogeneousLanguages -- Yes --\x3e UsePattern[Use Sidecar pattern]\\n  HeterogeneousLanguages -- No --\x3e RemoteTeams{Component ownership by remote teams?}\\n  RemoteTeams -- Yes --\x3e UsePattern\\n  RemoteTeams -- No --\x3e CoLocation{Co-location requirement?}\\n  CoLocation -- Yes --\x3e UsePattern\\n  CoLocation -- No --\x3e IndependentUpdates{Independent updates?}\\n  IndependentUpdates -- Yes --\x3e UsePattern\\n  IndependentUpdates -- No --\x3e FineGrainedControl{Fine-grained resource control?}\\n  FineGrainedControl -- Yes --\x3e UsePattern\\n  FineGrainedControl -- No --\x3e SecurityEnhancements{Security enhancements?}\\n  SecurityEnhancements -- Yes --\x3e UsePattern\\n  SecurityEnhancements -- No --\x3e OperationalExcellence{Operational excellence?}\\n  OperationalExcellence -- Yes --\x3e UsePattern\\n  OperationalExcellence -- No --\x3e PerformanceEfficiency{Performance efficiency?}\\n  PerformanceEfficiency -- Yes --\x3e UsePattern\\n  PerformanceEfficiency -- No --\x3e DoNotUsePattern[Do not use Sidecar pattern]\\n```\\n\\n#### Azure Solutions\\n\\nUse a\xa0sidecar\xa0container that enhances the application with the required SSL functionality without modifying the application code. The sidecar pattern is a powerful concept in container-based architectures that lets you decompose application functionality into different container images that run together in the same container group.\\nIn an Azure Container Instances container group, each container can take over part of the functionality the application requires. Sidecar containers can use different container images from the application container, even from different image repositories. Containers in the same container group share some properties, such as the underlying network stack.\\n\\nAzure resources that can be used to implement this pattern include [Azure Container Instances](https://learn.microsoft.com/azure/container-instances/container-instances-overview?WT.mc_id=AZ-MVP-5004796), [Azure Kubernetes Service](https://learn.microsoft.com/azure/aks/intro-kubernetes?WT.mc_id=AZ-MVP-5004796), and [Azure App Service](https://learn.microsoft.com/azure/app-service/?WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83c\udf96\ufe0f Leader Election\\n\\n> Coordinate the actions performed by a collection of collaborating task instances in a distributed application by electing one instance as the leader who assumes responsibility for managing the other instances.\\n\\nA typical cloud application has many tasks acting in a coordinated manner. These tasks could all be instances running the same code and requiring access to the same resources, or they might be working together in parallel to perform the individual parts of a complex calculation.\\nThe task instances might run separately for much of the time, but it might also be necessary to coordinate the actions of each instance to ensure that they don\'t conflict, cause contention for shared resources, or accidentally interfere with the work that other task instances are performing.\\nFor example:\\nIn a cloud-based system that implements horizontal scaling, multiple instances of the same task could be running at the same time with each instance serving a different user. If these instances write to a shared resource, it\'s necessary to coordinate their actions to prevent each instance from overwriting the changes made by the others.\\nIf the tasks are performing individual elements of a complex calculation in parallel, the results need to be aggregated when they are all complete.\\n\\n![Cloud Design Patterns - Leader Election](Blog_CloudDesignPatterns_LeaderElection.gif)\\n\\n#### Issues & considerations\\n\\nWhen implementing the Leader Election pattern, consider the following issues and considerations.\\n\\n* Preventing Multiple Leaders - Ensure the election process is managed carefully to prevent two or more instances from taking over the leader position simultaneously. This requires a robust mechanism for selecting a leader that can handle events such as network outages or process failures\\n* Failure Detection - The system must be able to detect when the leader has failed or become unavailable (e.g., due to a communications failure). The speed of detection depends on the system\'s requirements. Some systems can tolerate a short period without a leader, while others need immediate detection and a new election\\n* Handling Leader Termination - In systems with horizontal autoscaling, the leader could be terminated if the system scales back and shuts down some computing resources. Ensure the system can handle such scenarios and elect a new leader promptly\\n* Avoiding Single Points of Failure - Using a shared, distributed mutex introduces a dependency on the external service that provides the mutex, which can become a single point of failure. If this service becomes unavailable, the system won\'t be able to elect a leader\\n* Leader as a Bottleneck - Avoid making the leader a bottleneck in the system. The leader\'s role is to coordinate the work of subordinate tasks, not necessarily to participate in the work itself. Ensure the leader can delegate tasks effectively to avoid performance issues\\n* Latency and Performance - The resulting latency from leader coordination can affect the performance and response times of other processes if they are waiting for the leader to coordinate an operation. Optimize the leader election process to minimize latency\\n* Flexibility and Optimization - Implementing leader election algorithms manually provides the greatest flexibility for tuning and optimizing the code. However, this approach requires careful design and testing to ensure robustness\\n* Election Algorithms - Consider using established leader election algorithms such as the Bully Algorithm or the Ring Algorithm. These algorithms assume each candidate in the election has a unique ID and can communicate reliably with other candidates\\n* Coordination Requirements - Use the Leader Election pattern when tasks in a distributed application need careful coordination and there is no natural leader. This pattern is useful for cloud-hosted solutions where multiple instances need to work together without conflict\\n* Alternative Solutions \u2014 Evaluate whether a more lightweight method, such as optimistic or pessimistic locking, can achieve the required coordination. Also, consider third-party solutions like Apache Zookeeper for managing leader election and coordination tasks. By addressing these issues and considerations, you can effectively implement the Leader Election pattern to ensure reliable and efficient coordination in distributed systems.\\n\\n#### When to use this \\n\\nThe Leader Election pattern should be used in the following scenarios.\\n\\n* Task Coordination - When you need to coordinate a task among multiple instances of a service or application. The Leader Election pattern ensures that one instance acts as the coordinator, preventing conflicts and ensuring smooth operation\\n* Avoiding Single Points of Failure - When you want to avoid having a single point of failure in your application. By using leader election, if the current leader fails, a new leader is automatically selected, ensuring continuous operation without manual intervention\xa0\\n* Distributed Systems - In distributed systems where multiple instances need to work together without conflict. The Leader Election pattern helps in managing coordination tasks effectively across different instances\\n* High Availability Requirements - When your application requires high availability and resilience. The pattern helps in maintaining service continuity by electing a new leader if the current one fails, thus supporting failover mechanisms\\n* Scalability - When you need to scale out your application horizontally. The Leader Election pattern can help manage the coordination of tasks across multiple instances, ensuring that the system scales efficiently without bottlenecks\\n* Consensus Algorithms - When implementing consensus algorithms to manage failover and ensure that work is reliably redirected in case of node malfunctions. This is particularly useful in scenarios where reliability and self-healing are critical\\n* Using Off-the-Shelf Solutions - When you prefer not to implement a leader election algorithm from scratch. Off-the-shelf solutions like Apache Zookeeper can be used to manage leader election and coordination tasks effectively. By considering these scenarios, you can effectively use the Leader Election pattern to ensure reliable and efficient coordination in distributed systems, enhancing the overall resilience and scalability of your application.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e TaskCoordination{Task coordination among multiple instances?}\\n  TaskCoordination -- Yes --\x3e UsePattern[Use Leader Election pattern]\\n  TaskCoordination -- No --\x3e AvoidSinglePointFailure{Avoid single point of failure?}\\n  AvoidSinglePointFailure -- Yes --\x3e UsePattern\\n  AvoidSinglePointFailure -- No --\x3e DistributedSystems{Working with distributed systems?}\\n  DistributedSystems -- Yes --\x3e UsePattern\\n  DistributedSystems -- No --\x3e HighAvailability{High availability requirements?}\\n  HighAvailability -- Yes --\x3e UsePattern\\n  HighAvailability -- No --\x3e Scalability{Need to scale out application horizontally?}\\n  Scalability -- Yes --\x3e UsePattern\\n  Scalability -- No --\x3e ConsensusAlgorithms{Implementing consensus algorithms?}\\n  ConsensusAlgorithms -- Yes --\x3e UsePattern\\n  ConsensusAlgorithms -- No --\x3e OffTheShelfSolutions{Prefer using off-the-shelf solutions?}\\n  OffTheShelfSolutions -- Yes --\x3e UsePattern\\n  OffTheShelfSolutions -- No --\x3e DoNotUsePattern[Do not use Leader Election pattern]\\n```\\n\\n#### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Service Fabric](https://learn.microsoft.com/azure/service-fabric/service-fabric-overview?WT.mc_id=AZ-MVP-5004796), [Azure Kubernetes Service](https://learn.microsoft.com/azure/aks/intro-kubernetes?WT.mc_id=AZ-MVP-5004796), and [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83c\udf33 Strangler Fig\\n\\n> Incrementally migrate a legacy system by gradually replacing specific pieces of functionality with new applications and services.\\n\\nAs systems age, the development tools, hosting technology, and even system architectures they were built on can become increasingly obsolete. As new features and functionality are added, the complexity of these applications can increase dramatically, making them harder to maintain or add new features to.\\nCompletely replacing a complex system can be a huge undertaking. Often, you will need a gradual migration to a new system, while keeping the old system to handle features that haven\'t been migrated yet. However, running two separate versions of an application means that clients have to know where particular features are located. Every time a feature or service is migrated, clients need to be updated to point to the new location.\\n\\nThe Strangler Fig pattern involves gradually replacing parts of the legacy system with new services. This allows for a controlled and step-by-step transition rather than a wholesale move.\\n\\n![Cloud Design Patterns - Strangler Fig](Blog_CloudDesignPatterns_StranglerFig.gif)\\n\\n#### Issues & considerations\\n\\nWhen implementing the Strangler Fig pattern, consider the following issues and considerations.\\n\\n* Handling Services and Data Stores - Ensure that both new and legacy systems can access shared services and data stores. This is crucial for maintaining consistency and avoiding data integrity issues during the migration process\xa0\\n* Structuring New Applications - Structure new applications and services in a way that they can easily be intercepted and replaced in future migrations. This helps in maintaining flexibility and ease of further transitions.\\n* Facade Management - The facade, which intercepts requests and routes them to either the legacy or new system, must keep up with the migration. It should not become a single point of failure or a performance bottleneck. Proper management and monitoring of the facade are essential to ensure smooth operation2\xa0\\n* Incremental Migration - The pattern supports incremental migration, which helps to minimize risk and spread the development effort over time. However, this requires careful planning to ensure that each increment is functional and does not disrupt the overall system\\n* Client Updates - Running two separate versions of an application means that clients need to be updated to know where particular features are located. This can add complexity to the migration process, as each client must be aware of the changes and adapt accordingly\\n* Avoiding Single Points of Failure - Ensure that the facade does not become a single point of failure. It should be designed to handle high availability and failover scenarios to maintain system reliability during the migration\\n* Performance Considerations - The facade should not become a performance bottleneck. It must be capable of handling the load and routing requests efficiently to avoid degrading the user experience\\n* Completing the Migration - At some point, when the migration is complete, the strangler fig facade will either go away or evolve into an adaptor for legacy clients. Plan for this transition to ensure that the final system is clean and does not retain unnecessary components2\xa0By addressing these issues and considerations, you can effectively implement the Strangler Fig pattern to ensure a smooth and controlled migration from a legacy system to a new architecture.\\n\\n#### When to use this \\n\\nThe Strangler Fig pattern should be used in the following scenarios.\\n\\n* Gradual Migration -  When you need to incrementally migrate a legacy system to a new architecture. This pattern allows you to gradually replace specific pieces of functionality with new applications and services, minimizing risk and spreading the development effort over time\\n* Complex Systems - When dealing with complex systems where a complete replacement would be a huge undertaking. The Strangler Fig pattern enables a controlled decomposition of a monolith into a set of microservices, allowing the legacy system to continue functioning while new features are added to the new system\\n* Maintaining Functionality - When it is essential to keep the legacy system operational during the migration. The facade in the Strangler Fig pattern intercepts requests and routes them to either the legacy application or the new services, ensuring that consumers can continue using the same interface without being aware of the migration\\n* Avoiding Big Bang Rewrites - When you want to avoid the risks associated with a \\"big bang\\" rewrite. The Strangler Fig pattern supports an incremental approach, allowing you to add functionality to the new system at your own pace while ensuring the legacy application continues to function\\n* Modernizing Legacy Systems - When modernizing legacy systems that have become increasingly obsolete due to outdated development tools, hosting technology, or system architectures. The pattern helps in gradually transitioning to a new system while maintaining the old system for features that haven\'t been migrated yet\\n* Using Modern Orchestration Tools - When you want to leverage modern orchestration tools such as Azure DevOps to manage the lifecycle of each service. Once the application has been decomposed into constituent microservices, these tools can be used to manage the new system effectively. Considering these scenarios, you can effectively use the Strangler Fig pattern to ensure a smooth and controlled migration from a legacy system to a new architecture, enhancing the overall resilience and scalability of your application.\\n\\n#### Azure Solutions\\n\\nMany [Azure products](https://azure.microsoft.com/products?WT.mc_id=AZ-MVP-5004796), can be used to implement this pattern.\\n\\n### Messaging Patterns\\n\\n![Messaging Patterns](Blog_CloudDesignPatterns_Messaging.PNG)\\n\\n#### \ud83d\ude9a Sequential Convoy\\n\\n> Process a set of related messages in a defined order without blocking the processing of other groups of messages.\\n\\nApplications often need to process a sequence of messages in the order they arrive, while still being able to scale out to handle increased load. In a distributed architecture, processing these messages in order is not straightforward, because the workers can scale independently and often pull messages independently\\n\\nPush related messages into categories within the queuing system, and have the queue listeners lock and pull only from one category, one message at a time.\\n\\n![Cloud Design Pattern - Sequential Covey](Blog_CloudDesignPatterns_SequentialCovey.gif)\\n\\n##### Issues & considerations\\n\\nThe Sequential Convoy pattern is designed to process a set of related messages in a defined order without blocking the processing of other groups of messages. Here are some issues and considerations associated with this pattern\\n\\n* Message Ordering - Ensuring that messages are processed in the order they arrive can be challenging in a distributed architecture. Workers can scale independently and often pull messages independently, which can disrupt the order of processing\\n* Scalability - While the pattern allows for scaling out to handle increased load, it requires careful management to ensure that the order of messages is maintained. This can involve categorizing related messages within the queuing system and having queue listeners lock and pull only from one category at a time\\n* Complexity in Implementation - Implementing the Sequential Convoy pattern can be complex, especially when dealing with interleaved transactions for multiple orders. The system must be designed to handle these transactions in a first-in-first-out (FIFO) manner at the order level\\n* Resource Management - Efficiently managing resources to ensure that the processing of one group of messages does not block others is crucial. This involves coordinating actions across distributed services and remote resources\\n* Error Handling - Handling errors in a sequential processing system can be more complex. If an error occurs, the system must ensure that subsequent messages are not processed until the error is resolved, which can involve implementing compensating transactions or other error recovery mechanisms By addressing these issues and considerations, the Sequential Convoy pattern can be effectively implemented to ensure ordered processing of related messages in a distributed system.\\n\\n##### When to use this pattern\\n\\nThe Sequential Convoy pattern should be used in the following scenarios.\\n\\n* Ordered Message Processing - When you have messages that arrive in a specific order and must be processed in that same order. This is crucial for maintaining the integrity and consistency of the operations being performed\\n* Categorized Messages - When arriving messages can be categorized in such a way that the category becomes a unit of scale for the system. For example, in an order tracking system, messages can be categorized by order ID, ensuring that all operations related to a specific order are processed sequentially\\n* Avoiding Blocking - When you need to process a set of related messages in a defined order without blocking the processing of other groups of messages. This allows for efficient handling of multiple message groups concurrently while maintaining order within each group\xa0However, this pattern might not be suitable for extremely high throughput scenarios (millions of messages per minute or second), as the FIFO (First in and First Out) requirement limits the scaling that can be done by the system.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e OrderedMessageProcessing{Ordered message processing?}\\n  OrderedMessageProcessing -- Yes --\x3e UsePattern[Use Sequential Convoy pattern]\\n  OrderedMessageProcessing -- No --\x3e CategorizedMessages{Categorized messages?}\\n  CategorizedMessages -- Yes --\x3e UsePattern\\n  CategorizedMessages -- No --\x3e AvoidBlocking{Avoid blocking while processing related messages?}\\n  AvoidBlocking -- Yes --\x3e UsePattern\\n  AvoidBlocking -- No --\x3e HighThroughput{Extremely high throughput scenarios?}\\n  HighThroughput -- Yes --\x3e DoNotUsePattern[Do not use Sequential Convoy pattern]\\n  HighThroughput -- No --\x3e UsePattern\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Service Bus](https://learn.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview?WT.mc_id=AZ-MVP-5004796), [Azure Event Grid](https://learn.microsoft.com/azure/event-grid/overview?WT.mc_id=AZ-MVP-5004796), and [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\udccc Queue-Based Load Leveling\\n\\n> Use a buffer between a task and a service that it invokes in order to smooth intermittent heavy loads.\\n\\nUse a queue that acts as a buffer between a task and a service it invokes to smooth intermittent heavy loads that can cause the service to fail or the task to time out. This can help minimize the impact of peaks in demand on availability and responsiveness for both the task and the service.\\nContext and problem, many solutions in the cloud involve running tasks that invoke services. In this environment, if a service is subjected to intermittent heavy loads, it can cause performance or reliability issues.\\nA service could be part of the same solution as the tasks that use it, or it could be a third-party service providing access to frequently used resources such as a cache or a storage service. If the same service is used by a number of tasks running concurrently, it can be difficult to predict the volume of requests to the service at any time.\\nA service might experience peaks in demand that cause it to overload and be unable to respond to requests in a timely manner. Flooding a service with a large number of concurrent requests can also result in the service failing if it\'s unable to handle the contention these requests cause.\\nSolution, Refactor the solution and introduce a queue between the task and the service.\\n\\n![Cloud Design Patterns - Queue Based Levelling ](Blog_CloudDesignPatterns_QueueBasedLeveling.gif)\\n\\n##### Issues & considerations\\n\\nThe Queue-Based Load Leveling pattern is designed to handle intermittent heavy loads by using a queue as a buffer between a task and a service it invokes. Here are some issues and considerations associated with this pattern\\n\\n* Rate Control - It is necessary to implement application logic that controls the rate at which services handle messages to avoid overwhelming the target resource. This involves ensuring that spikes in demand are not passed to the next stage of the system\\n* Testing Under Load - The system should be tested under load to ensure that it provides the required leveling. Adjustments may be needed in the number of queues and the number of service instances that handle messages to achieve optimal performance\\n* Scalability - The pattern can help maximize scalability because both the number of queues and the number of services can be varied to meet demand. However, careful management is required to ensure that the system scales effectively without introducing bottlenecks\\n* Availability - This pattern can help maximize availability because delays in services won\'t have an immediate and direct impact on the application. The application can continue to post messages to the queue even when the service isn\'t available or isn\'t currently processing messages\\n* Cost Control - The number of service instances deployed only needs to be adequate to meet the average load rather than the peak load, which can help control costs. However, this requires careful monitoring and adjustment to ensure that the system remains cost-effective while meeting performance requirements\\n* Throttling - Some services implement throttling when demand reaches a threshold beyond which the system could fail. Implementing load-leveling with these services can ensure that this threshold isn\'t reached, thereby maintaining service functionality\\n* Temporal Decoupling - A message broker provides temporal decoupling, meaning the producer and consumer do not have to run concurrently. This allows the producer to send messages regardless of the consumer\'s availability, and the consumer can process messages at its own pace\\n* Asynchronous Processing \u2014 The pattern supports asynchronous message processing, which can help maintain the responsiveness of the user interface and distribute processing across multiple servers to improve throughput. By considering these issues and implementing appropriate strategies, the Queue-Based Load Leveling pattern can effectively manage load and improve a system\'s resilience and scalability.\\n\\n##### When to use this pattern\\n\\nThe Queue-Based Load Leveling pattern should be used in the following scenarios:\\n\\n* Handling Intermittent Heavy Loads - When a service is subjected to intermittent heavy loads that can cause performance or reliability issues, this pattern helps to smooth out these loads by using a queue as a buffer between the task and the service it invokes\\n* Decoupling Tasks from Services - When you need to decouple tasks from services to allow the service to handle messages at its own pace, regardless of the volume of requests from concurrent tasks. This ensures that the service is not overwhelmed by a sudden spike in demand\\n* Maximizing Availability - When you want to ensure that delays in services do not have an immediate and direct impact on the application. The application can continue to post messages to the queue even when the service isn\'t available or isn\'t currently processing messages\\n* Maximizing Scalability - When you need to scale both the number of queues and the number of services to meet demand. This pattern allows for flexible scaling to handle varying loads\\n* Cost Control - When you want to control costs by deploying a number of service instances that only need to be adequate to meet the average load rather than the peak load. This can help in reducing the overall cost of the system\\n* Throttling - When services implement throttling to prevent system failure due to high demand. The Queue-Based Load Leveling pattern can help ensure that the demand does not exceed the service\'s capacity, thereby maintaining functionality\\n* Asynchronous Processing \u2014 When you need to perform tasks asynchronously to maintain the responsiveness of the user interface and distribute processing across multiple servers to improve throughput, you can use the Queue-Based Load Leveling pattern in these scenarios to improve your system\'s resilience, scalability, and cost-effectiveness.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e IntermittentHeavyLoads{Handling Intermittent Heavy Loads?}\\n  IntermittentHeavyLoads -- Yes --\x3e UsePattern[Use Queue-Based Load Leveling pattern]\\n  IntermittentHeavyLoads -- No --\x3e DecouplingTasks{Decoupling Tasks from Services?}\\n  DecouplingTasks -- Yes --\x3e UsePattern\\n  DecouplingTasks -- No --\x3e MaximizingAvailability{Maximizing Availability?}\\n  MaximizingAvailability -- Yes --\x3e UsePattern\\n  MaximizingAvailability -- No --\x3e MaximizingScalability{Maximizing Scalability?}\\n  MaximizingScalability -- Yes --\x3e UsePattern\\n  MaximizingScalability -- No --\x3e CostControl{Cost Control?}\\n  CostControl -- Yes --\x3e UsePattern\\n  CostControl -- No --\x3e Throttling{Services implement Throttling?}\\n  Throttling -- Yes --\x3e UsePattern\\n  Throttling -- No --\x3e AsynchronousProcessing{Asynchronous Processing?}\\n  AsynchronousProcessing -- Yes --\x3e UsePattern\\n  AsynchronousProcessing -- No --\x3e DoNotUsePattern[Do not use Queue-Based Load Leveling pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Service Bus](https://learn.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview?WT.mc_id=AZ-MVP-5004796), [Azure Event Grid](https://learn.microsoft.com/azure/event-grid/overview?WT.mc_id=AZ-MVP-5004796), and [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\udccc Publisher-Subscriber\\n\\n> Enable an application to announce events to multiple interested consumers asynchronously, without coupling the senders to the receivers.\\n\\nPublisher-Subscriber, or PubSub for short, enables an application to announce events to multiple interested consumers asynchronously without coupling the senders to the receivers.\\n\\nIn cloud-based and distributed applications, system components often need to provide information to other components as events occur.\\nAsynchronous messaging is an effective way to decouple senders from consumers and avoid blocking the sender from waiting for a response. The sender uses an input messaging channel. The sender packages events into messages, using a known message format, and sends these messages via the input channel. The sender in this pattern is also called the\xa0publisher.\\n\\nOne output messaging channel per consumer. The consumers are known as\xa0subscribers.\\nA mechanism for copying each message from the input channel to the output channels for all subscribers interested in that message. This operation is typically handled by an intermediary such as a message broker or event bus.\\n\\n![Cloud Design Pattern - Publisher-Subscriber (PubSub)](Blog_CloudDesignPatterns_PubSub.gif)\\n\\n##### Issues & considerations\\n\\nThe Publisher-Subscriber pattern, also known as Pub-Sub, is a messaging pattern where an application (publisher) sends messages or events to multiple interested consumers (subscribers) asynchronously. Here are some issues and considerations associated with this pattern\\n\\n* Scalability - One of the main challenges is ensuring the system can scale efficiently as the number of subscribers increases. Each new subscriber typically requires additional resources and potentially changes to the application code to handle the new message queue\\n* Message Delivery - Ensuring that messages are delivered to all subscribers reliably and in a timely manner can be complex. Message queues generally aim to deliver messages in a first-in-first-out (FIFO) manner, but this order is not always guaranteed, especially under high-load conditions\\n* Monitoring and Alerts - Implementing effective monitoring and alerting mechanisms is crucial to tracking the performance of message queues, detecting stuck messages, and ensuring that subscribers are processing messages efficiently\\n* Security - Ensuring that messages are encrypted and only authorized subscribers can access them is essential. This involves implementing robust authentication and authorization mechanisms\\n* Handling Subscriptions - Deciding how to manage subscriptions is important. You need to determine whether subscribers can subscribe and unsubscribe at their own pace or if an approval mechanism is required\\n* Message Duplication - Handling duplicate messages is necessary because message queues guarantee at least one delivery, which means there is a chance that a message might be delivered more than once. Implementing deduplication mechanisms can help manage this issue\\n* Poison Messages - Managing poison messages, which are messages that repeatedly fail to be processed, is important. These messages should be moved to a separate poison queue to free up the main queue for other messages\\n* One-Way Communication - The Pub-Sub pattern is inherently one-way, meaning that subscribers cannot send acknowledgments or responses back to the publisher. If two-way communication is needed, another pattern like Request-Reply should be implemented\\n* Message Expiration - Defining how long messages should remain in the queue before they expire is important. This helps in managing the lifecycle of messages and ensuring that outdated messages do not clog the system \\n* Wildcard Subscriptions \u2014 Implementing wildcard subscriptions can allow subscribers to subscribe to all topics with a single action, simplifying the management of multiple topics. By addressing these issues and considerations, the Publisher-Subscriber pattern can be effectively implemented to enable scalable, reliable, and secure asynchronous communication between publishers and multiple subscribers.\\n\\n##### When to use this pattern\\n\\nThe Publisher-Subscriber (Pub-Sub) pattern should be used in the following scenarios.\\n\\n* Broadcasting Information to Multiple Consumers - When you need to send the same information to multiple consumers simultaneously, the Pub-Sub pattern is ideal. This allows a single publisher to broadcast messages to multiple subscribers without needing to send individual messages to each one\\n* Decoupling Senders and Receivers - When you want to decouple the senders of messages from the receivers, enabling them to operate independently. This is useful in systems where the publisher does not need to know about the subscribers and vice versa\\n* Asynchronous Communication - When you need to enable asynchronous communication between different parts of a system. This allows the publisher to send messages without waiting for the subscribers to process them, improving the overall responsiveness and scalability of the system \\n* Event-Driven Architectures - When implementing event-driven architectures where components need to react to events as they happen. The Pub-Sub pattern allows for real-time event propagation to multiple interested parties\\n* Handling Real-Time Notifications - When you need to provide real-time notifications to multiple subscribers, such as in monitoring systems, alerting mechanisms or live data feeds \\n* Scalability and Flexibility - When you need a scalable and flexible system where new subscribers can be added without modifying the publisher. This allows the system to grow and adapt to new requirements easily\\n* Filtering and Routing Messages \u2014 When you need to filter and route messages to specific subscribers based on certain criteria, you can use topics and content filtering mechanisms. By using the Publisher-Subscriber pattern in these scenarios, you can create a robust, scalable, and flexible messaging system that efficiently handles communication between different components.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e BroadcastingInfo{Broadcasting Information to Multiple Consumers?}\\n  BroadcastingInfo -- Yes --\x3e UsePattern[Use Pub-Sub pattern]\\n  BroadcastingInfo -- No --\x3e DecouplingSendersReceivers{Decoupling Senders and Receivers?}\\n  DecouplingSendersReceivers -- Yes --\x3e UsePattern\\n  DecouplingSendersReceivers -- No --\x3e AsynchronousCommunication{Asynchronous Communication?}\\n  AsynchronousCommunication -- Yes --\x3e UsePattern\\n  AsynchronousCommunication -- No --\x3e EventDrivenArchitectures{Event-Driven Architectures?}\\n  EventDrivenArchitectures -- Yes --\x3e UsePattern\\n  EventDrivenArchitectures -- No --\x3e RealTimeNotifications{Handling Real-Time Notifications?}\\n  RealTimeNotifications -- Yes --\x3e UsePattern\\n  RealTimeNotifications -- No --\x3e ScalabilityFlexibility{Scalability and Flexibility?}\\n  ScalabilityFlexibility -- Yes --\x3e UsePattern\\n  ScalabilityFlexibility -- No --\x3e FilteringRoutingMessages{Filtering and Routing Messages?}\\n  FilteringRoutingMessages -- Yes --\x3e UsePattern\\n  FilteringRoutingMessages -- No --\x3e DoNotUsePattern[Do not use Pub-Sub pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Service Bus](https://learn.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview?WT.mc_id=AZ-MVP-5004796), [Azure Event Grid](https://learn.microsoft.com/azure/event-grid/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\udd04 Asynchronous Request-Reply\\n\\n> Decouple backend processing from a frontend host, where backend processing needs to be asynchronous, but the frontend still needs a clear response.\\n\\nIn modern application development, it\'s normal for client applications \u2014 often code running in a web client (browser) \u2014 to depend on remote APIs to provide business logic and compose functionality. In most cases, APIs for a client network infrastructure are largely out of the control of the application developer. Most APIs can respond quickly enough for responses to arrive back over the same connection. Application code can make a synchronous API call in a non-blocking way, giving the appearance of asynchronous processing, which is recommended for I/O-bound operations.\\nIn some scenarios, however, the work done by the backend may be long-running, on the order of seconds, or might be a background process that is executed in minutes or even hours. In that case, it isn\'t feasible to wait for the work to complete before responding to the request. This situation is a potential problem for any synchronous request-reply pattern. Applications are designed to respond quickly, on the order of 100 ms or less.\xa0\\nOne solution to this problem is to use HTTP polling. Polling is useful to client-side code, as it can be hard to provide call-back endpoints or use long-running connections. Even when callbacks are possible, the extra libraries and services that are required can sometimes add too much extra complexity.\\n\\n![Cloud Design Pattern - Asyncronous Request-Reply](Blog_CloudDesignPatterns_AsyncronousRequestReply.gif)\\n\\n##### Issues & considerations\\n\\nThe Asynchronous Request-Reply pattern is used to decouple backend processing from a frontend host, where backend processing needs to be asynchronous, but the frontend still needs a clear response. Here are some issues and considerations associated with this pattern\\n\\n* Latency and Response Time - The pattern is designed to handle scenarios where backend processing is long-running. However, this introduces latency in the response time, as the client has to poll for the result of the long-running operation. This can affect the user experience if not managed properly\\n* Polling Mechanism - Implementing an efficient polling mechanism is crucial. The HTTP02 response should indicate the location and frequency that the client should poll for the response. This helps in managing the load on the server and ensures that the client does not overwhelm the server with frequent polling requests\\n* Handling Long-Running Operations - The backend must be capable of handling long-running operations efficiently. This includes offloading processing to another component, such as a message queue, and ensuring that the status endpoint can accurately reflect the progress of the operation\\n* Error Handling - Proper error handling mechanisms need to be in place. If an error occurs during processing, the error should persist at the resource URL described in the location header, and the appropriate response code (HTTP 4xx) should be returned. This ensures that the client is aware of any issues and can take appropriate action\\n* Status Codes and Redirects - The API should return the correct status codes based on the state of the operation. For example, upon successful processing, the API should return HTTP00 (OK), HTTP01 (Created), or HTTP04 (No Content). If the status endpoint redirects on completion, HTTP 302 or HTTP 303 should be used\\n* Client-Side Considerations - The client-side code must be designed to handle asynchronous responses. This includes managing the polling logic, handling different status codes, and updating the user interface based on the progress and completion of the operation\\n* Scalability - The pattern allows the client process and the backend API to scale independently. However, this separation also brings additional complexity when the client requires success notification, as this step needs to become asynchronous\\n* Security - Ensuring secure communication between the client and the server is essential. This includes validating both the request and the action to be performed before starting the long-running process and ensuring that sensitive data is protected during transmission\\n* Legacy Clients - Some legacy clients might not support this pattern. It is important to consider the compatibility of the pattern with existing clients and whether additional support or alternative solutions are needed for those clients\\n* Resource Management - Efficiently managing resources such as message queues and status endpoints is crucial to prevent resource exhaustion and ensure the system can handle many concurrent requests. By addressing these issues and considerations, the Asynchronous Request-Reply pattern can be effectively implemented to handle long-running operations while providing a clear response to the client.\\n\\n##### When to use this pattern\\n\\nThe Asynchronous Request-Reply pattern should be used in the following scenarios.\\n\\n* Long-Running Backend Operations - When the backend processing is long-running, taking seconds, minutes, or even hours, and it is not feasible to keep the client waiting for the operation to complete. This pattern allows the client to initiate the request and then poll for the result later\\n* Decoupling Frontend and Backend - When you need to decouple the frontend host from the backend processing. This is useful in scenarios where the frontend needs a clear response, but the backend processing needs to be asynchronous\\n* Scalability - When you want to enable the client process and the backend API to scale independently. By using a message broker to separate the request and response stages, you can achieve better scalability and manage load more effectively\\n* Handling High Latency - When factors such as network infrastructure, geographic location, or current load can add significant latency to the response. The pattern helps mitigate these issues by allowing the backend to process the request asynchronously and the client to poll for the result\\n* Microservices Architectures - When implementing microservices architectures where server-to-server REST API calls are common. The pattern helps manage long-running operations and decouples services, making the system more resilient and scalable\\n* Client-Side Polling - When it is difficult to provide call-back endpoints or use long-running connections on the client side. The pattern allows the client to poll for the result, which can be simpler and more efficient in certain scenariosBy using the Asynchronous Request-Reply pattern in these scenarios, you can effectively manage long-running operations, improve scalability, and decouple frontend and backend processing, leading to a more robust and responsive system.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e LongRunningBackendOperations{Long-Running Backend Operations?}\\n  LongRunningBackendOperations -- Yes --\x3e UsePattern[Use Asynchronous Request-Reply pattern]\\n  LongRunningBackendOperations -- No --\x3e DecouplingFrontendBackend{Decoupling Frontend and Backend?}\\n  DecouplingFrontendBackend -- Yes --\x3e UsePattern\\n  DecouplingFrontendBackend -- No --\x3e Scalability{Scalability?}\\n  Scalability -- Yes --\x3e UsePattern\\n  Scalability -- No --\x3e HandlingHighLatency{Handling High Latency?}\\n  HandlingHighLatency -- Yes --\x3e UsePattern\\n  HandlingHighLatency -- No --\x3e MicroservicesArchitectures{Microservices Architectures?}\\n  MicroservicesArchitectures -- Yes --\x3e UsePattern\\n  MicroservicesArchitectures -- No --\x3e ClientSidePolling{Client-Side Polling?}\\n  ClientSidePolling -- Yes --\x3e UsePattern\\n  ClientSidePolling -- No --\x3e DoNotUsePattern[Do not use Asynchronous Request-Reply pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAn application that uses Azure Functions to implement this pattern. There are three functions in the solution:\\n\\n* The asynchronous API endpoint.\\n* The status endpoint.\\n* A backend function that takes queued work items and executes them.\\n\\nAzure resources that can be used to implement this pattern include [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796), [Azure Service Bus](https://learn.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview?WT.mc_id=AZ-MVP-5004796), and [Azure Storage Queues](https://learn.microsoft.com/azure/storage/queues/storage-queues-introduction?WT.mc_id=AZ-MVP-5004796).\\n\\n### Reliability Patterns\\n\\n![Reliability Patterns](Blog_CloudDesignPatterns_Reliability.PNG)\\n\\n#### \u26a1\ufe0f Circuit Breaker\\n\\n> Handle faults that might take a variable amount of time to recover from when connecting to a remote service or resource. This can improve an application\'s stability and resiliency.\\n\\nIn a distributed environment, calls to remote resources and services can fail due to transient faults, such as slow network connections, timeouts, or the resources being overcommitted or temporarily unavailable. These faults typically correct themselves after a short period of time, and a robust cloud application should be prepared to handle them by using a strategy such as the\xa0[Retry pattern](https://learn.microsoft.com/dotnet/architecture/cloud-native/application-resiliency-patterns?WT.mc_id=AZ-MVP-5004796#retry-pattern).\\nHowever, there can also be situations where faults are due to unanticipated events, and that might take much longer to fix. These faults can range in severity from a partial loss of connectivity to the complete failure of a service. In these situations, it might be pointless for an application to retry an operation that is unlikely to succeed continually, and instead, the application should quickly accept that the operation has failed and handle this failure accordingly. Additionally, if a service is very busy, failure in one part of the system might lead to cascading failures. The purpose of the Circuit Breaker pattern is different than the Retry pattern. The Retry pattern enables an application to retry an operation in the expectation that it\'ll succeed. The Circuit Breaker pattern prevents an application from performing an operation that is likely to fail. An application can combine these two patterns by using the Retry pattern to invoke an operation through a circuit breaker. However, the retry logic should be sensitive to any exceptions returned by the circuit breaker and abandon retry attempts if the circuit breaker indicates that a fault is not transient. The failure counter used by the\xa0Closed\xa0state is time-based. It\'s automatically reset at periodic intervals. This helps to prevent the circuit breaker from entering the\xa0Open\xa0state if it experiences occasional failures.\\n\\n![Cloud Design Patterns - Circuit Breaker](Blog_CloudDesignPatterns_CircuitBreaker.gif)\\n\\n##### Issues & considerations\\n\\nThe Circuit Breaker pattern is designed to handle faults that might take a variable amount of time to recover from, especially when connecting to remote services or resources. Here are some issues and considerations associated with this pattern\\n\\n* State Management - The Circuit Breaker pattern typically involves three states: Closed, Open, and Half-Open. Managing these states correctly is crucial. The Closed state allows normal operation, the Open state immediately fails requests, and the Half-Open state allows a limited number of test requests to check if the issue has been resolved\\n* Thresholds and Timeouts - Setting appropriate thresholds for the number of failures and timeouts is essential. If the thresholds are too low, the circuit breaker might open too frequently, causing unnecessary failures. If they are too high, the system might not protect itself effectively from cascading failures\\n* Recovery Testing - In the Open state, the circuit breaker should periodically test the service to see if it has recovered. This can be done using a timer or by pinging the service. The interval for these tests should be carefully chosen based on the criticality of the operation and the nature of the service\xa0\\n* Manual Override - Providing a manual override option can be beneficial in systems where the recovery time for a failing operation is extremely variable. This allows administrators to manually reset the circuit breaker or force it into the Open state as needed\\n* Concurrency - The circuit breaker might be accessed by a large number of concurrent instances of an application. The implementation should ensure that it does not block concurrent requests or add excessive overhead to each call\\n* Resource Differentiation - Using a single circuit breaker for multiple underlying independent providers can be problematic. For example, in a data store with multiple shards, one shard might be accessible while another is experiencing issues. The circuit breaker should differentiate between these resources to avoid unnecessary failures\\n* Accelerated Circuit Breaking - Sometimes, a failure response can provide enough information for the circuit breaker to trip immediately and stay tripped for a minimum amount of time. For example, an overloaded shared resource might indicate that an immediate retry is not recommended\\n* Replaying Failed Requests - In the Open state, the circuit breaker could record the details of each failed request and arrange for these requests to be replayed when the service becomes available again. This ensures that important operations are not lost\\n* Inappropriate Timeouts - The circuit breaker might not fully protect applications from operations that fail in external services configured with lengthy timeouts. If the timeout is too long, a thread running the circuit breaker might be blocked for an extended period, tying up resources\\n* Impact on Shared Resources - Aggressive retry policies can cause an increasing number of transient faults for other users and applications sharing the same resources. It is important to consider the impact of the circuit breaker on shared resources and other tenants in a multi-tenant environment. Addressing these issues and considerations, the Circuit Breaker pattern can be effectively implemented to improve the stability and resiliency of an application, preventing cascading failures and managing long-lasting faults efficiently.\\n\\n##### When to use this pattern\\n\\nThe Circuit Breaker pattern should be used in the following scenarios.\\n\\n* Handling Persistent or Non-Transient Errors - When an application encounters persistent or non-transient errors, such as a service being down or a network failure, the Circuit Breaker pattern can prevent the application from continually trying to perform an operation that is likely to fail\\n* Preventing Resource Exhaustion - If a service is frequently unavailable or busy, it might be due to resource exhaustion. The Circuit Breaker pattern helps to prevent further strain on the service by stopping additional requests until the service has had time to recover\\n* Improving System Stability - The pattern provides stability while the system recovers from a failure and minimizes the impact on performance. It quickly rejects requests for operations that are likely to fail rather than waiting for them to time out or never return\\n* Managing Long-Lasting Faults - When faults are likely to be long-lasting or terminal, the Circuit Breaker pattern can handle these faults as exceptions. The application can log the exception and try to continue by invoking an alternative service or offering degraded functionality\\n* Avoiding Cascading Failures - The pattern helps mitigate failures and avoid cascading failures disrupting the entire application. By stopping requests to a failing service, it prevents the failure from propagating to other parts of the system\\n* Testing Service Recovery - The Circuit Breaker pattern allows the application to periodically test the service to detect when it becomes available again. This can be done on an intermittent basis with long intervals between requests, depending on the criticality of the operation and the nature of the service\\n* Logging and Monitoring\u2014The pattern can be used to log all failed requests and monitor the operation\'s health. This information can be used to alert administrators when a circuit breaker trips to the Open state, providing insights into the system\'s health. Using the Circuit Breaker pattern in these scenarios, you can improve the resiliency and stability of your application, prevent resource exhaustion, and manage long-lasting faults effectively.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e PersistentErrors{Handling Persistent or Non-Transient Errors?}\\n  PersistentErrors -- Yes --\x3e UsePattern[Use Circuit Breaker pattern]\\n  PersistentErrors -- No --\x3e ResourceExhaustion{Preventing Resource Exhaustion?}\\n  ResourceExhaustion -- Yes --\x3e UsePattern\\n  ResourceExhaustion -- No --\x3e SystemStability{Improving System Stability?}\\n  SystemStability -- Yes --\x3e UsePattern\\n  SystemStability -- No --\x3e LongLastingFaults{Managing Long-Lasting Faults?}\\n  LongLastingFaults -- Yes --\x3e UsePattern\\n  LongLastingFaults -- No --\x3e CascadingFailures{Avoiding Cascading Failures?}\\n  CascadingFailures -- Yes --\x3e UsePattern\\n  CascadingFailures -- No --\x3e ServiceRecovery{Testing Service Recovery?}\\n  ServiceRecovery -- Yes --\x3e UsePattern\\n  ServiceRecovery -- No --\x3e LoggingMonitoring{Logging and Monitoring?}\\n  LoggingMonitoring -- Yes --\x3e UsePattern\\n  LoggingMonitoring -- No --\x3e DoNotUsePattern[Do not use Circuit Breaker pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\udcee Deployment Stamps\\n\\n> The deployment stamp pattern involves provisioning, managing, and monitoring a heterogeneous group of resources to host and operate multiple workloads or tenants.\\n\\nThe deployment stamp pattern involves provisioning, managing, and monitoring a heterogeneous group of resources to host and operate multiple workloads or tenants. Each individual copy is called a\xa0stamp or sometimes a\xa0service unit,\xa0scale unit, or\xa0cell. In a multitenant environment, every stamp or scale unit can serve a predefined number of tenants. Multiple stamps can be deployed to scale the solution almost linearly and serve an increasing number of tenants. This approach can improve the scalability of your solution, allow you to deploy instances across multiple regions, and separate your customer data. consider grouping resources in\xa0scale units\xa0and provisioning multiple copies of your\xa0stamps. Each\xa0scale unit\xa0will host and serve a subset of your tenants. Stamps operate independently of each other and can be deployed and updated independently. A single geographical region might contain a single stamp or might contain multiple stamps to allow for horizontal scale-out within the region.\xa0\\n\\n![Cloud Design - Deployment Stamps](Blog_CloudDesignPatternsDeploymentStamps.gif)\\n\\n##### Issues & considerations\\n\\nSeveral issues and considerations need to be addressed when implementing the deployment stamp pattern to ensure a successful deployment.\\n\\n* Deployment Process - It\'s crucial to have automated and fully repeatable deployment processes. Tools like Bicep, JSON ARM templates, or Terraform modules can be used to declaratively define your stamps and keep the definitions consistent\\n* Cross-Stamp Operations - When your solution is deployed across multiple stamps, operations that span multiple stamps, such as aggregating customer data, can become complex. Queries might need to be executed against each stamp, and the results aggregated. Alternatively, consider having all stamps publish data into a centralized data warehouse for consolidated reporting\\n* Determining Scale-Out Policies - Stamps have a finite capacity, which might be defined using a proxy metric such as the number of tenants that can be deployed to the stamp. It\'s important to monitor the available and used capacity for each stamp and proactively deploy additional stamps to accommodate new tenants\\n* Minimum Number of Stamps - It\'s advisable to deploy at least two stamps of your solution to avoid hard-coding assumptions in your code or configuration that won\'t apply when you scale out\\n* Cost - Deploying multiple copies of your infrastructure components will likely involve a substantial increase in the cost of operating your solution. This includes the cost of additional memory, compute, and other resources required for each stamp2\xa0\\n* Moving Between Stamps - Each stamp is deployed and operated independently, so moving tenants between stamps can be difficult. Custom logic is needed to transmit customer information to a different stamp and remove it from the original stamp. This process might require a backplane for communication between stamps, further increasing the complexity of the solution\\n* Traffic Routing - Routing traffic to the correct stamp for a given request can require an additional component to resolve tenants to stamps. This component needs to be highly available to ensure reliable traffic routing\\n* Shared Components - Some components might be shared across stamps. For example, a shared single-page app for all tenants could be deployed in one region and replicated globally using Azure CDN\\n* Geographical Distribution - For multi-region applications, each tenant\'s data and traffic should be directed to a specific region. This ensures that requests are routed to the correct stamp based on the user\'s geographical location\\n* Resiliency During Outages - Stamps are independent of one another, so if an outage affects a single stamp, tenants deployed to other stamps should not be affected. This isolation helps to contain the \'blast radius\' of an incident or outage\\n* Handling Single and Multi-Tenant Instances - The pattern should support both single and multi-tenant instances, ensuring that each tenant\'s data is isolated and secure\\n* Update Frequency \u2014 Different stamps might need to be on different versions of your solution at the same time. This requires careful management of updates and version control across stamps. By considering these issues and implementing appropriate strategies, the Deployment Stamp pattern can effectively improve the scalability, resiliency, and manageability of your solution.\\n\\n##### When to use this pattern\\n\\nThe Deployment Stamp pattern is useful in the following scenarios\\n\\n* Natural Limits on Scalability - If certain components of your system cannot or should not scale beyond a specific number of customers or requests, the Deployment Stamp pattern allows you to scale out by deploying multiple instances (stamps) of your solution\\n* Tenant Isolation - When there is a requirement to separate certain tenants from others due to security concerns or compliance requirements, the Deployment Stamp pattern can be used to deploy these tenants onto their own isolated stamps\\n* Multi-Region Applications - For applications that need to be deployed across multiple regions, each tenant\'s data and traffic can be directed to a specific region. This ensures that requests are routed to the correct stamp based on the user\'s geographical location\\n* Resiliency During Outages - Stamps are independent of one another, so if an outage affects a single stamp, tenants deployed to other stamps should not be affected. This isolation helps to contain the \'blast radius\' of an incident or outage, enhancing the overall resiliency of the application\\n* Different Versions of the Solution - If there is a need to have some tenants on different versions of your solution at the same time, the Deployment Stamp pattern allows for this flexibility by deploying different stamps with different versions\\n* High-Scale Platforms \u2014 The pattern is suitable for implementing high-scale platforms with users distributed over a wide area. It helps manage the load and ensure that the system can handle a large number of users efficiently. By using the Deployment Stamp pattern in these scenarios, you can achieve better scalability, isolation, and resiliency for your application.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e NaturalLimitsOnScalability{Natural Limits on Scalability?}\\n  NaturalLimitsOnScalability -- Yes --\x3e UsePattern[Use Deployment Stamp pattern]\\n  NaturalLimitsOnScalability -- No --\x3e TenantIsolation{Tenant Isolation?}\\n  TenantIsolation -- Yes --\x3e UsePattern\\n  TenantIsolation -- No --\x3e MultiRegionApplications{Multi-Region Applications?}\\n  MultiRegionApplications -- Yes --\x3e UsePattern\\n  MultiRegionApplications -- No --\x3e ResiliencyDuringOutages{Resiliency During Outages?}\\n  ResiliencyDuringOutages -- Yes --\x3e UsePattern\\n  ResiliencyDuringOutages -- No --\x3e DifferentVersionsOfSolution{Different Versions of the Solution?}\\n  DifferentVersionsOfSolution -- Yes --\x3e UsePattern\\n  DifferentVersionsOfSolution -- No --\x3e HighScalePlatforms{High-Scale Platforms?}\\n  HighScalePlatforms -- Yes --\x3e UsePattern\\n  HighScalePlatforms -- No --\x3e DoNotUsePattern[Do not use Deployment Stamp pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Traffic Manager](https://learn.microsoft.com/azure/traffic-manager/traffic-manager-overview?WT.mc_id=AZ-MVP-5004796), [Azure Front Door](https://learn.microsoft.com/azure/frontdoor/front-door-overview?WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\udea2 Bulkhead\\n\\n> In a bulkhead architecture, elements of an application are isolated into pools so that if one fails, the others will continue to function.\\n\\nThe Bulkhead pattern is a type of application design that is tolerant of failure. In a bulkhead architecture, elements of an application are isolated into pools so that if one fails, the others will continue to function. It\'s named after the sectioned partitions (bulkheads) of a ship\'s hull. If the hull of a ship is compromised, only the damaged section fills with water, which prevents the ship from sinking. A cloud-based application may include multiple services, with each service having one or more consumers. Excessive load or failure in a service will impact all consumers of the service. Partition service instances into different groups, based on consumer load and availability requirements. This design helps to isolate failures and allows you to sustain service functionality for some consumers, even during a failure.\\n\\nThe Bulkhead pattern and the Deployment Stamp pattern are both architectural patterns used to improve the resiliency and scalability of applications, but they address different concerns and are implemented in different ways, for example, the Deployment Stamp isolates entire instances of applications, each with its own services, the bulkhead partner isolates different components or services within the same application or system and is primarily focuses on fault isolation and resource management within a single deployment. These patterns can work together for a highly resistant solution.\\n\\n![Cloud Design Patterns - Bulkhead](Blog_CloudDesignPatternsBulkhead.gif)\\n\\n##### Issues & considerations\\n\\nThe Bulkhead pattern is designed to improve the resiliency of an application by isolating different parts of the system to prevent a failure in one part from cascading to others. Here are the key issues and considerations when implementing the Bulkhead pattern\\nIssues\\n\\n* Resource Exhaustion - If a service or consumer fails or becomes misconfigured, it can exhaust resources such as connection pools, memory, or CPU. This can prevent other services or consumers from functioning properly.\\n* Cascading Failures - Without isolation, a failure in one service can lead to cascading failures, affecting the entire system. This can happen if a single service consumes all available resources, leaving none for other services.\\n* Complexity - Implementing the Bulkhead pattern adds complexity to the system. It requires careful planning and management of resource allocation and isolation.\\n* Less Efficient Use of Resources - Isolating resources for different services or consumers can lead to less efficient use of resources. For example, some resources might remain underutilized while others are overutilized\\nConsiderations\\n* Partitioning Services and Consumers \u2014 Services and consumers should be divided into different groups based on load and availability requirements. This helps isolate failures and maintain service functionality for some consumers even during a failure.\\n* Resource Allocation - Allocate separate resources (e.g., connection pools, memory) for each service or consumer. This ensures that a failure in one service does not affect the resources available to other services.\\n* Granularity of Bulkheads - Determine the appropriate level of granularity for bulkheads. For example, you could isolate resources at the service level, consumer level, or even at the tenant level.\\n* Combining with Other Patterns - Consider combining the Bulkhead pattern with other fault-handling patterns such as retry, circuit breaker, and throttling to provide more sophisticated fault handling.\\n* Technology Overhead - Evaluate the overhead in terms of cost, performance, and manageability when partitioning services or consumers into bulkheads. For example, using containers can offer a good balance of resource isolation with fairly low overhead.\\n* Monitoring and SLA - Monitor the performance and SLA of each partition to ensure that the isolation is effective and that resources are being used efficiently.\\n* Critical vs. Standard Consumers - Isolate critical consumers from standard consumers to ensure that critical operations can continue even if standard operations fail.\\n* Security\u2014The segmentation between components helps constrain security incidents to the compromised bulkhead, enhancing the overall security of the system. By addressing these issues and considerations, the Bulkhead pattern can help improve the resiliency and reliability of your application, ensuring that failures are contained and do not affect the entire system.\\n\\n##### When to use this pattern\\n\\nThe Bulkhead pattern is particularly useful in the following scenarios\\n\\n* Isolating Resources for Backend Services - Use the Bulkhead pattern to isolate resources used to consume a set of backend services. This is especially important if the application can provide some level of functionality even when one of the services is not responding\\n* Isolating Critical Consumers from Standard Consumers - Implement the Bulkhead pattern to isolate critical consumers from standard consumers. This ensures that critical operations can continue even if standard operations fail\\n* Protecting the Application from Cascading Failures - The Bulkhead pattern helps protect the application from cascading failures. By isolating different parts of the system, a failure in one part does not affect the entire system\\n* Handling Resource Exhaustion - When a service or consumer fails or becomes misconfigured, it can exhaust resources such as connection pools, memory, or CPU. The Bulkhead pattern helps to prevent this by isolating resources for different services or consumers\\n* Maintaining Service Functionality During Failures - The Bulkhead pattern allows you to sustain service functionality for some consumers even during a failure. By partitioning service instances into different groups based on consumer load and availability requirements, you can isolate failures and maintain service functionality\\n* Combining with Other Fault-Handling Patterns - Consider combining the Bulkhead pattern with other fault-handling patterns such as retry, circuit breaker, and throttling to provide more sophisticated fault-handling\\n* Deploying Services with Different Quality of Service - The Bulkhead pattern allows you to deploy services that offer a different quality of service for consuming applications. For example, a high-priority consumer pool can be configured to use high-priority services. By using the Bulkhead pattern in these scenarios, you can improve the resiliency and reliability of your application, ensuring that failures are contained and do not affect the entire system.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e IsolatingResources{Isolating Resources for Backend Services?}\\n  IsolatingResources -- Yes --\x3e UsePattern[Use Bulkhead pattern]\\n  IsolatingResources -- No --\x3e IsolatingCriticalConsumers{Isolating Critical Consumers from Standard Consumers?}\\n  IsolatingCriticalConsumers -- Yes --\x3e UsePattern\\n  IsolatingCriticalConsumers -- No --\x3e ProtectingFromCascadingFailures{Protecting the Application from Cascading Failures?}\\n  ProtectingFromCascadingFailures -- Yes --\x3e UsePattern\\n  ProtectingFromCascadingFailures -- No --\x3e HandlingResourceExhaustion{Handling Resource Exhaustion?}\\n  HandlingResourceExhaustion -- Yes --\x3e UsePattern\\n  HandlingResourceExhaustion -- No --\x3e MaintainingServiceFunctionality{Maintaining Service Functionality During Failures?}\\n  MaintainingServiceFunctionality -- Yes --\x3e UsePattern\\n  MaintainingServiceFunctionality -- No --\x3e CombiningFaultHandlingPatterns{Combining with Other Fault-Handling Patterns?}\\n  CombiningFaultHandlingPatterns -- Yes --\x3e UsePattern\\n  CombiningFaultHandlingPatterns -- No --\x3e DeployingServicesDifferentQoS{Deploying Services with Different Quality of Service?}\\n  DeployingServicesDifferentQoS -- Yes --\x3e UsePattern\\n  DeployingServicesDifferentQoS -- No --\x3e DoNotUsePattern[Do not use Bulkhead pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Kubernetes Service (AKS)](https://learn.microsoft.com/azure/aks/intro-kubernetes?WT.mc_id=AZ-MVP-5004796), [Azure Container Instances](https://learn.microsoft.com/azure/container-instances/container-instances-overview?WT.mc_id=AZ-MVP-5004796), and [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\ude0a Compensating Transaction\\n\\n> If one or more of the steps fail, you can use the Compensating Transaction pattern to undo the work that the steps performed.\\n\\nWhen you use an eventually consistent operation that consists of a series of steps, the Compensating Transaction pattern can be useful. Specifically, if one or more of the steps fail, you can use the Compensating Transaction pattern to undo the work that the steps performed. Typically, you find operations that follow the eventual consistency model in cloud-hosted applications that implement complex business processes and workflows. Applications that run in the cloud frequently modify data. This data is sometimes spread across various data sources in different geographic locations. To avoid contention and improve performance in a distributed environment, an application shouldn\'t try to provide strong transactional consistency. Rather, the application should implement eventual consistency. In the eventual consistency model, a typical business operation consists of a series of separate steps. While the operation performs these steps, the overall view of the system state might be inconsistent. But when the operation finishes and all the steps have run, the system should become consistent again. The data that are affected by an operation that implements eventual consistency isn\'t always held in a database. For example, consider a service-oriented architecture (SOA) environment. An SOA operation can invoke an action in a service and cause a change in the state that\'s held by that service. \\n\\nTo undo the operation, you also have to undo this state change. This process can involve invoking the service again and performing another action that reverses the effects of the first.\\n\\nThe solution is to implement a compensating transaction. The steps in a compensating transaction undo the effects of the steps in the original operation. An intuitive approach is to replace the current state with the state the system was in at the start of the operation. But a compensating transaction can\'t always take that approach, because it might overwrite changes that other concurrent instances of an application have made. Instead, a compensating transaction must be an intelligent process that takes into account any work that concurrent instances do. This process is usually application-specific, driven by the nature of the work that the original operation performs.\\n\\n\\n![Cloud Design Pattern - Compensating Transaction](Blog_CloudDesignPatterns_CompensatingTransaction.gif)\\n\\n##### Issues & considerations\\n\\nWhen implementing the Compensating Transaction pattern, there are several issues and considerations to keep in mind\\n\\n* Determining Failure - It might not be easy to determine when a step in an operation that implements eventual consistency fails. A step might not fail immediately but could get blocked. Implementing a time-out mechanism can help identify such failures\\n* Complexity of Compensation Logic - Compensation logic is often application-specific and relies on the application having sufficient information to undo the effects of each step in a failed operation. Generalizing this logic can be challenging\\n* Idempotency - Define the steps in a compensating transaction as idempotent commands. This ensures that the steps can be repeated if the compensating transaction itself fails, which is crucial for maintaining consistency\\n* Resilience and Reliability - The infrastructure handling the steps must be resilient both in the original operation and in the compensating transaction. It should not lose the information required to compensate for a failing step and must reliably monitor the progress of the compensation logic\\n* State Restoration - A compensating transaction doesn\'t necessarily return the system data to its state at the start of the original operation. Instead, it compensates for the work that the operation completed successfully before it failed\\n* Order of Steps - The order of the steps in the compensating transaction isn\'t necessarily the exact opposite of the steps in the original operation. For example, one data store might be more sensitive to inconsistencies than another, so the steps that undo changes to this store should occur first\\n* Concurrency and Parallelism - It might be possible to perform some of the undo steps in parallel, depending on how the compensating logic is designed for each step. This can help speed up the compensation process\\n* Manual Intervention - In some cases, manual intervention might be the only way to recover from a step that has failed. The system should raise an alert and provide as much information as possible about the reason for the failure\\n* Resource Locking - To increase the likelihood of overall activity success, you can place a short-term, time-out\u2013based lock on each resource required to complete an operation. Obtain these resources in advance and perform the work only after acquiring all the resources, finalizing all actions before the locks expire\\n* Retry Logic - Implement retry logic that is more forgiving than usual to minimize failures that trigger a compensating transaction. If a step in an operation fails, handle the failure as a transient exception and repeat the step. Stop the operation and initiate a compensating transaction only if a step fails repeatedly or can\'t be recovered. By considering these issues and implementing appropriate strategies, you can effectively manage the complexities associated with compensating transactions and ensure the reliability and consistency of your application.\\n\\n##### When to use this pattern\\n\\nThe Compensating Transaction pattern should be used in the following scenarios.\\n\\n* Undoing Operations in Eventual Consistency Models - Use this pattern to undo operations that implement the eventual consistency model. This is particularly useful when you need to maintain data consistency across distributed systems\\n* Handling Failures in Long-Running Transactions - When dealing with long-running transactions that involve multiple steps, the Compensating Transaction pattern can be used to undo the effects of each step if the transaction fails at any point. This ensures that the system can recover gracefully from failures\\n* Ensuring Reliability and Resilience - Implement this pattern to address malfunctions in critical workload paths by rolling back data changes, breaking transaction locks, or executing native system behavior to reverse the effect. This helps make the workload resilient to malfunctions and ensures it recovers to a fully functioning state after a failure\\n* Complex Business Processes and Workflows - Use the Compensating Transaction pattern in cloud-hosted applications that implement complex business processes and workflows. This pattern helps in managing eventual consistency and ensuring that the system can handle failures effectively\\n* Avoiding the Complexity of Distributed Transactions - When it is not feasible to use distributed transactions due to their complexity and overhead, the Compensating Transaction pattern can be an alternative. It allows you to break down a distributed transaction into separate, compensable tasks\\n* Handling State Changes in Service-Oriented Architectures (SOA) - In an SOA environment, where operations can cause changes in the state held by services, the Compensating Transaction pattern can be used to undo these state changes if necessary\\n* Manual Intervention and Recovery - In cases where manual intervention might be required to recover from a failed step, the Compensating Transaction pattern can be used to raise alerts and provide information about the failure, allowing for manual recovery actions using the Compensating Transaction pattern in these scenarios, you can ensure that your system can handle failures gracefully, maintain data consistency, and recover from errors effectively.\\n\\n##### Azure Solutions\\n\\nCustomers use a travel website to book itineraries. A single itinerary might consist of a series of flights and hotels. A customer who travels from Seattle to London and then on to Paris might perform the following steps when creating an itinerary:\\n\\n1. Book a seat on flight F1 from Seattle to London.\\n2. Book a seat on flight F2 from London to Paris.\\n3 Book a seat on flight F3 from Paris to Seattle.\\n4. Reserve a room at Hotel H1 in London.\\n5. Reserve a room at Hotel H2 in Paris.\\n\\nThese steps constitute an eventually consistent operation, although each step is a separate action. In addition to performing these steps, the system must also record the counter operations for undoing each step. This information is needed in case the customer cancels the itinerary. The steps necessary to perform the counter operations can then run as a compensating transaction.\\n\\nIn many business solutions, failure of a single step doesn\'t always necessitate rolling back the system by using a compensating transaction. For example, consider the travel website scenario. Suppose the customer books flights F1, F2, and F3 but can\'t reserve a room at Hotel H1. It\'s preferable to offer the customer a room at a different hotel in the same city rather than canceling the flights. The customer can still decide to cancel. In that case, the compensating transaction runs and undoes the bookings for flights F1, F2, and F3. But the customer should make this decision, not the system.\\n\\nAzure resources that can be used to implement this pattern include [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796), [Azure Logic Apps](https://learn.microsoft.com/azure/logic-apps/logic-apps-overview?WT.mc_id=AZ-MVP-5004796), and [Azure Service Bus](https://learn.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview?WT.mc_id=AZ-MVP-5004796).\\n\\n### Security Patterns\\n\\n![Security Patterns](Blog_CloudDesignPatterns_Security.PNG)\\n\\n#### \ud83d\ude0a Backends for Frontends\\n\\n> Create separate backend services to be consumed by specific frontend applications or interfaces. \\n\\nThe Backends for Frontends (BFF) pattern involves creating separate backend services tailored to specific frontend applications or interfaces. This pattern is particularly useful when you want to avoid customizing a single backend for multiple interfaces, which can lead to conflicting requirements and development bottlenecks. An application may initially target a desktop web UI, with a backend service developed to support it. As the user base grows, a mobile application might be developed that needs to interact with the same backend. However, the capabilities and requirements of mobile devices differ significantly from desktop browsers, leading to competing requirements for the backend. An application may initially target a desktop web UI, with a backend service developed to support it. As the user base grows, a mobile application might be developed that needs to interact with the same backend. However, the capabilities and requirements of mobile devices differ significantly from desktop browsers, leading to competing requirements for the backend\\n\\n![Cloud Design Pattern - Backends for Frontends](Blog_CloudDesignPatterns_BackendsForFrontEnds.gif)\\n\\n##### Issues & considerations\\n\\nWhen implementing the Backends for Frontends (BFF) pattern, there are several issues and considerations to keep in mind\\n\xa0\\n* Competing Requirements - Different frontends (e.g., desktop web UI and mobile applications) have varying requirements in terms of screen size, performance, and display limitations. These differences can lead to competing requirements for the backend, making it challenging to serve both interfaces effectively\\n* Development Bottlenecks - The backend can become a bottleneck in the development process due to the need to accommodate conflicting update requirements from different front-end teams. This can result in significant effort being spent on maintaining a single deployable resource that serves multiple frontends\\n* Complexity in Maintenance - Maintaining separate backends for each frontend can increase the complexity of the system. Each backend needs to be updated and maintained independently, which can lead to increased operational overhead \\nConsiderations\\n* Segmentation and Isolation - The BFF pattern introduces segmentation and isolation by creating separate backend services for each frontend. This reduces the surface area of the API and limits lateral movement among different backends, which might expose different capabilities\\n* Tailored Security and Authorization - The separation allows for tailored security and authorization mechanisms for each front end. This ensures that the security requirements for each frontend are addressed effectively, potentially reducing the surface area of an API\\n* Optimized Interfaces - By creating separate backends for each client, you can expose an optimal interface for that client. This allows for different payload sizes or interaction patterns that are specific to the needs of each front, improving performance and user experience\\n* Reduced Attack Surface - The BFF pattern reduces the attack surface by limiting the functionality exposed by each backend to only what is necessary for its corresponding front end. This minimizes the potential for security vulnerabilities \\n* Enhanced Monitoring and Control - The separation of backends allows for more granular monitoring and control over each service. Security incidents can be contained within the specific backend, reducing the blast radius of any potential compromise\\n* Compliance and Regulatory Requirements - Different frontends might have different compliance and regulatory requirements. The BFF pattern allows for these requirements to be addressed individually, ensuring that each backend complies with the necessary standards By considering these issues and implementing appropriate strategies, you can effectively manage the complexities associated with the Backends for Frontends pattern and ensure the reliability, security, and performance of your application.\\n\\n##### When to use this pattern\\n\\nThe Backends for Frontends (BFF) pattern should be used in the following scenarios.\\n\\n* Different Client Requirements - When different types of clients, such as mobile applications and desktop web browsers, require different payload sizes or interaction patterns, the BFF pattern is useful. It allows you to create separate backends for each client, which exposes an optimal interface for that client\\n* Avoiding Backend Bottlenecks - If the backend service becomes a bottleneck due to conflicting update requirements from different frontend teams, the BFF pattern can help. By creating separate backend services for each frontend, you can reduce the effort spent on maintaining a single deployable resource that serves multiple frontends\\n* Competing Requirements - When the capabilities of different frontends (e.g., mobile devices vs. desktop browsers) differ significantly, leading to competing requirements for the backend, the BFF pattern can address these differences. It allows each backend to be optimized for the specific needs of its corresponding frontend\\n* Security and Authorization - The BFF pattern is beneficial when you need to tailor security and authorization mechanisms for each front end. By individualizing the service layer for each front, you can reduce the surface area of the API and limit lateral movement among different backends, which might expose different capabilities\\n* Optimized Interfaces - When you need to expose an optimal interface for each client, the BFF pattern is appropriate. This allows for different payload sizes or interaction patterns that are specific to the needs of each frontend, improving performance and user experience. Using the Backends for Frontends pattern in these scenarios, you can effectively manage the complexities associated with serving multiple frontends, ensure optimal performance, and enhance the security of your application.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e DifferentClientRequirements{Different Client Requirements?}\\n  DifferentClientRequirements -- Yes --\x3e UsePattern[Use BFF pattern]\\n  DifferentClientRequirements -- No --\x3e AvoidingBackendBottlenecks{Avoiding Backend Bottlenecks?}\\n  AvoidingBackendBottlenecks -- Yes --\x3e UsePattern\\n  AvoidingBackendBottlenecks -- No --\x3e CompetingRequirements{Competing Requirements?}\\n  CompetingRequirements -- Yes --\x3e UsePattern\\n  CompetingRequirements -- No --\x3e SecurityAndAuthorization{Security and Authorization?}\\n  SecurityAndAuthorization -- Yes --\x3e UsePattern\\n  SecurityAndAuthorization -- No --\x3e OptimizedInterfaces{Optimized Interfaces?}\\n  OptimizedInterfaces -- Yes --\x3e UsePattern\\n  OptimizedInterfaces -- No --\x3e DoNotUsePattern[Do not use BFF pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts?WT.mc_id=AZ-MVP-5004796), [Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview?WT.mc_id=AZ-MVP-5004796), [App Service](https://learn.microsoft.com/azure/app-service/?WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\udea7 Gatekeeper\\n\\n> Protect applications and services by using a dedicated host instance to broker requests between clients and the application or service.\\n\\nCloud services expose endpoints that allow client applications to call their APIs. The code used to implement the APIs triggers or performs several tasks, including but not limited to authentication, authorization, parameter validation, and some or all request processing. The API code is likely to access storage and other services on behalf of the client.\\nIf a malicious user compromises the system and gains access to the application\'s hosting environment, its security mechanisms and access to data and other services are exposed. As a result, the malicious user can gain unrestricted access to credentials, storage keys, sensitive information, and other services.\\n\\nThe GateKeeper patterns allow you to protect applications and services by using a dedicated host instance to broker requests between clients and the application or service. The broker validates and sanitizes the requests, and can provide an additional layer of security and limit the system\'s attack surface.\\n\\n![Cloud Design Pattern - Gatekeeper](Blog_CloudDesignPatterns_Gatekeeper.gif)\\n\\n##### Issues & considerations\\n\\nThe Gatekeeper pattern is a design pattern used to protect applications and services by acting as an intermediary that validates and sanitizes requests before passing them to the trusted host. Here are the key issues and considerations when implementing this pattern\\n\xa0\\n* Single Point of Failure - The gatekeeper instance could become a single point of failure. If the gatekeeper fails, it can disrupt the entire system. To mitigate this, consider deploying redundant instances and using an autoscaling mechanism to ensure capacity and maintain availability\\n* Performance Impact - Adding the gatekeeper layer introduces additional processing and network communication, which can impact the overall performance of the system. This additional latency needs to be considered, especially for performance-critical applications\\n* Complexity in Implementation - Implementing the gatekeeper pattern adds complexity to the system architecture. It requires careful design to ensure that the gatekeeper effectively validates and sanitizes requests without becoming a bottleneck or introducing new vulnerabilities\\nConsiderations\\n\xa0\\n* Controlled Validation - The gatekeeper should validate all incoming requests and reject those that do not meet the validation requirements. This helps in ensuring that only legitimate requests are processed by the trusted host\\n* Limited Risk and Exposure - The gatekeeper should not have access to the credentials or keys used by the trusted host to access storage and services. This limits the risk and exposure in case the gatekeeper is compromised, as the attacker would not gain access to these sensitive credentials or keys\\n* Appropriate Security - The gatekeeper should run in a limited privilege mode, while the rest of the application runs in full trust mode required to access storage and services. This ensures that even if the gatekeeper is compromised, it cannot directly access the application services or data\\n* Secure Communication - Use secure communication channels (e.g., HTTPS, SSL, or TLS) between the gatekeeper and the trusted hosts or tasks. This helps protect the data in transit and ensures that the communication between the gatekeeper and the trusted hosts is secure\\n* Internal or Protected Endpoints - Ensure that the trusted hosts expose only internal or protected endpoints that are used exclusively by the gatekeeper. The trusted hosts should not expose any external endpoints or interfaces to minimize the attack surface\\n* Redundant Instances - To minimize the impact of a failure, deploy redundant instances of the gatekeeper and use an autoscaling mechanism to ensure that there is always sufficient capacity to handle incoming requests\\n* Separation of Concerns - The gatekeeper should not perform any processing related to the application or services or access any data. Its function should be purely to validate and sanitize requests. The trusted hosts might need to perform additional request validation, but the core validation should be done by the gatekeeper. By considering these issues and implementing appropriate strategies, you can effectively manage the complexities associated with the Gatekeeper pattern and ensure the reliability, security, and performance of your application.\\n\\n##### When to use this pattern\\n\\nThe Gatekeeper pattern should be used in the following scenarios.\\n\\n* Handling Sensitive Information - When your application handles sensitive information that requires a high degree of protection from malicious attacks, the Gatekeeper pattern is beneficial. It acts as an intermediary that validates and sanitizes requests before they reach the trusted host, thereby protecting sensitive data\\n* High-Security Requirements - If your application exposes services that require robust security measures, such as web application firewalls, DDoS protection, bot detection, and centralized authentication and authorization checks, the Gatekeeper pattern can centralize these security functionalities\\n* Mission-Critical Operations - For applications that perform mission-critical operations that cannot be disrupted, the Gatekeeper pattern ensures that only validated and sanitized requests are processed, reducing the risk of malicious attacks and system failures\\n* Separate Request Validation - When there is a need to perform request validation separately from the main tasks or to centralize this validation to simplify maintenance and administration, the Gatekeeper pattern is appropriate. It ensures that the core validation is done by the gatekeeper, while the trusted hosts might perform additional validation\\n* Reducing Attack Surface - The Gatekeeper pattern is useful when you want to reduce the attack surface of your system. By decoupling the public endpoints from the code that processes requests and accesses storage, the gatekeeper can provide an additional layer of security and limit the system\'s exposure to potential attacks\\n* Secure Communication - When secure communication between clients and the application or service is required, the Gatekeeper pattern can be implemented using secure communication channels (e.g., HTTPS, SSL, or TLS) between the gatekeeper and the trusted hosts or tasks\\n* Redundancy and Availability - In scenarios where high availability is crucial, deploying redundant instances of the gatekeeper and using an autoscaling mechanism can ensure that there is always sufficient capacity to handle incoming requests, minimizing the impact of a failure using the Gatekeeper pattern in these scenarios, you can enhance the security, reliability, and performance of your application, ensuring that it meets the necessary protection and operational requirements.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e HandlingSensitiveInformation{Handling Sensitive Information?}\\n  HandlingSensitiveInformation -- Yes --\x3e UsePattern[Use Gatekeeper pattern]\\n  HandlingSensitiveInformation -- No --\x3e HighSecurityRequirements{High Security Requirements?}\\n  HighSecurityRequirements -- Yes --\x3e UsePattern\\n  HighSecurityRequirements -- No --\x3e MissionCriticalOperations{Mission Critical Operations?}\\n  MissionCriticalOperations -- Yes --\x3e UsePattern\\n  MissionCriticalOperations -- No --\x3e SeparateRequestValidation{Separate Request Validation?}\\n  SeparateRequestValidation -- Yes --\x3e UsePattern\\n  SeparateRequestValidation -- No --\x3e ReducingAttackSurface{Reducing Attack Surface?}\\n  ReducingAttackSurface -- Yes --\x3e UsePattern\\n  ReducingAttackSurface -- No --\x3e SecureCommunication{Secure Communication?}\\n  SecureCommunication -- Yes --\x3e UsePattern\\n  SecureCommunication -- No --\x3e RedundancyAndAvailability{Redundancy and Availability?}\\n  RedundancyAndAvailability -- Yes --\x3e UsePattern\\n  RedundancyAndAvailability -- No --\x3e DoNotUsePattern[Do not use Gatekeeper pattern]\\n```\\n\\n##### Azure Solutions\\n\\nThe Azure resources that can be used to implement this pattern include [Azure Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview?WT.mc_id=AZ-MVP-5004796) and [Azure Firewall](https://learn.microsoft.com/azure/firewall/overview?WT.mc_id=AZ-MVP-5004796). \\n\\n#### \ud83e\udd1d Federated Identity\\n\\n> Delegate authentication to an external identity provider. This can simplify development, minimize the requirement for user administration, and improve the user experience of the application.\\n\\nUsers typically need to work with multiple applications provided and hosted by different organizations they have a business relationship with.\\n\\nImplement an authentication mechanism that can use federated identity. Separate user authentication from the application code and delegate authentication to a trusted identity provider.\\n\\nThis model is often called claims-based access control. Applications and services authorize access to features and functionality based on the claims contained in the token. The service that requires authentication must trust the IdP. The client application contacts the IdP, which performs the authentication.\\n\\nFederated authentication provides a standards-based solution to the issue of trusting identities across diverse domains,\\n\\nThis type of authentication is becoming more common across all types of applications, especially cloud-hosted applications because it supports single sign-on without requiring a direct network connection to identity providers. The user doesn\'t have to enter credentials for every application. This increases security because it prevents the creation of credentials required to access many different applications, and it also hides the user\'s credentials from all but the original identity provider. Applications see just the authenticated identity information contained within the token.\\n\\n![Cloud Design Patterns - Federated Identity](Blog_CloudDesignPatterns_FederatedIdentity.gif)\\n\\n##### Issues & considerations\\n\\nWhen implementing the Federated Identity pattern, there are several issues and considerations to keep in mind\\nIssues\\n\xa0\\n* Single Point of Failure - Authentication can become a single point of failure. If the identity provider (IdP) or the Security Token Service (STS) fails, it can disrupt access to all applications relying on it. To mitigate this, consider deploying your identity management mechanism across multiple data centers to maintain reliability and availability\\n* Complexity in Implementation - Implementing federated identity can be complex, especially if retrofitting it into existing applications that were built using different authentication mechanisms. This complexity can make it less cost-effective and more challenging to manage\\n* Performance Impact - The additional steps involved in federated authentication, such as token issuance and validation, can introduce latency. This performance impact needs to be considered, especially for applications requiring high responsiveness\\n* Security Vulnerabilities - Federated identity can expose security vulnerabilities if not properly managed. For example, if a user leaves the company and their account is not immediately deprovisioned, it can lead to unauthorized access. Ensuring timely de-provisioning and robust security practices is crucial\\n* Administrative Overhead - While federated identity can reduce the administrative overhead of managing user credentials within the application, it requires careful management of the relationship with the identity provider. This includes ensuring that the identity provider\'s security practices are robust and that the integration is maintained.\\n\xa0* Role-Based Access Control (RBAC) - Federated identity allows for the implementation of RBAC based on role claims contained in the authentication token. This provides a more granular level of control over access to features and resources within the application\\n* Home Realm Discovery - If there are multiple identity providers configured, the STS must determine which identity provider the user should be redirected to for authentication. This process, known as home realm discovery, can be automated based on various factors such as email address, subdomain, IP address, or cookies\\n* User Experience - Federated identity can improve the user experience by enabling single sign-on (SSO) across multiple applications. Users do not need to remember multiple credentials, which simplifies their interaction with the system and reduces the likelihood of forgotten passwords\\n* Security and Compliance - Offloading user management and authentication to a trusted identity provider can enhance security by leveraging the provider\'s advanced capabilities for identity-based threat detection and prevention. This also helps in meeting compliance requirements without implementing these capabilities in the application itself\\n* Business Continuity - Federated systems typically require a load-balanced array of servers to ensure high availability for authentication requests. This setup helps maintain business continuity by providing redundancy and failover capabilities\\n* Integration with Third-Party Services - Federated identity is useful for integrating with third-party services and applications, especially in scenarios where multiple organizations need to collaborate. It allows users from different organizations to authenticate using their own credentials, simplifying access management. By considering these issues and implementing appropriate strategies, you can effectively manage the complexities associated with the Federated Identity pattern and ensure the reliability, security, and performance of your application.\\n\\n##### When to use this pattern\\n\\nThe Federated Identity pattern should be used in the following scenarios.\\n\\n* Single Sign-On in the Enterprise - When you need to authenticate employees for corporate applications hosted in the cloud outside the corporate security boundary, without requiring them to sign in every time they visit an application. This provides a seamless user experience similar to on-premises applications where users authenticate once and gain access to all relevant applications\\n* Federated Identity with Multiple Partners - When you need to authenticate both corporate employees and business partners who do not have accounts in the corporate directory. This is common in business-to-business applications, applications that integrate with third-party services, and scenarios where companies with different IT systems have merged or shared resources\\n* SaaS Applications - For independent software vendors providing a ready-to-use service for multiple clients or tenants. Each tenant can authenticate using a suitable identity provider, such as corporate credentials for business users or social identity credentials for consumers and clients of the tenant\\n* Improving User Experience - When users typically need to work with multiple applications provided and hosted by different organizations, federated identity can simplify the user experience by allowing them to use the same credentials across all applications. This reduces the likelihood of forgotten passwords and provides a more cohesive user experience\\n* Reducing Administrative Overhead - When you want to minimize the administrative overhead of managing user credentials within the application. By delegating authentication to a trusted identity provider, you can simplify development and reduce the need for user administration tasks such as password reminders and account deprovisioning\\n* Enhancing Security - When you need to enhance security by offloading user management and authentication to a trusted identity provider. This can provide advanced capabilities for identity-based threat detection and prevention and ensure that user credentials are managed securely\\n* Supporting Claims-Based Access Control \u2014 When you need to implement role-based access control (RBAC) based on role claims contained in the authentication token, this allows for a more granular level of control over access to features and resources within the application.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e SingleSignOn{Single Sign-On in the Enterprise?}\\n  SingleSignOn -- Yes --\x3e UsePattern[Use Federated Identity pattern]\\n  SingleSignOn -- No --\x3e FederatedIdentityWithMultiplePartners{Federated Identity with Multiple Partners?}\\n  FederatedIdentityWithMultiplePartners -- Yes --\x3e UsePattern\\n  FederatedIdentityWithMultiplePartners -- No --\x3e SaaSApplications{SaaS Applications?}\\n  SaaSApplications -- Yes --\x3e UsePattern\\n  SaaSApplications -- No --\x3e ImprovingUserExperience{Improving User Experience?}\\n  ImprovingUserExperience -- Yes --\x3e UsePattern\\n  ImprovingUserExperience -- No --\x3e ReducingAdministrativeOverhead{Reducing Administrative Overhead?}\\n  ReducingAdministrativeOverhead -- Yes --\x3e UsePattern\\n  ReducingAdministrativeOverhead -- No --\x3e EnhancingSecurity{Enhancing Security?}\\n  EnhancingSecurity -- Yes --\x3e UsePattern\\n  EnhancingSecurity -- No --\x3e SupportingClaimsBasedAccessControl{Supporting Claims-Based Access Control?}\\n  SupportingClaimsBasedAccessControl -- Yes --\x3e UsePattern\\n  SupportingClaimsBasedAccessControl -- No --\x3e DoNotUsePattern[Do not use Federated Identity pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure services that can be used to implement this pattern include [Microsoft Entra ID](https://learn.microsoft.com/entra/fundamentals/whatis?WT.mc_id=AZ-MVP-5004796) and [Microsoft Entra ID External](https://learn.microsoft.com/entra/external-id/external-identities-overview?WT.mc_id=AZ-MVP-5004796).\\n\\n#### \ud83d\udee1\ufe0f Gateway Offloading\\n\\n> Offload shared or specialized service functionality to a gateway proxy. \\n\\nThis pattern can simplify application development by moving shared service functionality, such as the use of SSL certificates, from other parts of the application into the gateway. Offload some features into a gateway, particularly cross-cutting concerns such as certificate management, authentication, SSL termination, monitoring, protocol translation, or throttling.\\n\\nIt is similar to the Gatekeeper pattern, which focuses on protecting applications by validating and sanitizing requests, decoupling public endpoints from backend processing, and centralizing security functionalities to limit the system\'s attack surface.\\n\\nThe Gateway Offloading\xa0focuses on offloading cross-cutting concerns and shared functionalities to a gateway to simplify application development and maintenance, enhance security, and ensure consistency in logging and monitoring.\\n\\n![Cloud Design - Gateway Offloading](Blog_CloudDesignPatterns_GatewayOffloading.gif)\\n\\n##### Issues & considerations\\n\\nWhen implementing the Gateway Offloading pattern, there are several issues and considerations to keep in mind\\n\\n* Coupling Across Services - Introducing a gateway can create coupling across services, which might not be suitable for all architectures. This coupling can lead to dependencies that complicate the deployment and scaling of individual services\\n* Single Point of Failure - The gateway itself can become a single point of failure if not properly managed. It is crucial to ensure that the gateway is highly available and resilient to failure by running multiple instances and designing it for the capacity and scaling requirements of the application\\n* Performance Overhead - Offloading functionality to a gateway can introduce performance overhead. The gateway must be designed to handle the additional load without becoming a bottleneck for the application. This includes ensuring that the gateway can scale appropriately to meet the demands of the application\\n* Security Management - Properly handling security issues such as token validation, encryption, and SSL certificate management requires specialized skills. The gateway must be configured and managed by a team with the necessary expertise to ensure that security is not compromised\\n* Administrative Overhead - While offloading shared functionalities to a gateway can reduce administrative overhead in some areas, it can also introduce new administrative tasks. For example, managing and updating SSL certificates or other shared resources at the gateway level requires careful coordination and planning\\n* Consistency in Logging and Monitoring - The gateway can provide consistency in request and response logging and monitoring. However, it is essential to ensure that the gateway is correctly instrumented to provide the necessary level of monitoring and logging, even if individual services are not\\n* Specialized Features - Offloading specialized features such as authentication, authorization, logging, monitoring, or throttling to the gateway allows dedicated teams to manage these features. This can simplify the development and maintenance of the core application but requires coordination between teams to ensure that the offloaded features are correctly implemented and maintained\\n* Business Logic - Business logic should never be offloaded to the gateway. The gateway should only handle cross-cutting concerns that are used by the entire application, such as security or data transfer. Offloading business logic can lead to a tightly coupled architecture that is difficult to maintain and scale. Considering these issues and carefully planning the implementation, you can effectively use the Gateway Offloading pattern to simplify application development, enhance security, and ensure consistency in logging and monitoring.\\n\\n##### When to use this pattern\\n\\nThe Gateway Offloading pattern should be used in the following scenarios.\\n\\n* Shared or Specialized Service Functionality - When you need to offload shared or specialized service functionality, such as SSL certificate management, authentication, authorization, logging, monitoring, or throttling, to a gateway proxy. This can simplify application development by moving these cross-cutting concerns from other parts of the application into the gateway\\n* Reducing Administrative Overhead - When managing shared features across multiple services increases administrative overhead and the likelihood of deployment errors. Offloading these features to a gateway can reduce the need to configure, manage, and maintain them across all services, simplifying updates and reducing the risk of errors\\n* Centralized Security Management - When you need to handle complex security tasks such as token validation, encryption, and SSL certificate management. Offloading these tasks to a gateway allows dedicated teams with specialized skills to manage them, ensuring that security is not compromised\\n* Consistency in Logging and Monitoring - When you want to ensure consistency in request and response logging and monitoring across services. The gateway can be configured to provide a minimum level of monitoring and logging, even if individual services are not correctly instrumented\\n* High Availability and Resilience - When you need to ensure that the gateway is highly available and resilient to failure. Running multiple instances of the gateway and designing it for the capacity and scaling requirements of the application can help avoid single points of failure and ensure that the gateway does not become a bottleneck\\n* Simplifying Configuration and Management - When you want to simplify the configuration and management of supporting resources, such as web server certificates and secure website configurations. Offloading these responsibilities to a gateway can make service upgrades simpler and more manageable\\n* Specialized Team Management - When you want to allow dedicated teams to implement features that require specialized expertise, such as security. This allows your core team to focus on the application functionality, leaving these specialized but cross-cutting concerns to the relevant experts\\n* Network Security and Attack Mitigation - When you need to mitigate attacks at the edge ensure that internet traffic routes only through the gateway, not directly to the workloads or PaaS services. The gateway should have the ability to mitigate exploits and provide network security. By considering these scenarios, you can determine whether the Gateway Offloading pattern is suitable for your application and ensure that it meets the necessary security, user experience, and administrative requirements.\\n\\n```mermaid\\ngraph TB\\n  Start(Start) --\x3e SharedOrSpecializedServiceFunctionality{Shared or Specialized Service Functionality?}\\n  SharedOrSpecializedServiceFunctionality -- Yes --\x3e UsePattern[Use Gateway Offloading pattern]\\n  SharedOrSpecializedServiceFunctionality -- No --\x3e ReducingAdministrativeOverhead{Reducing Administrative Overhead?}\\n  ReducingAdministrativeOverhead -- Yes --\x3e UsePattern\\n  ReducingAdministrativeOverhead -- No --\x3e CentralizedSecurityManagement{Centralized Security Management?}\\n  CentralizedSecurityManagement -- Yes --\x3e UsePattern\\n  CentralizedSecurityManagement -- No --\x3e ConsistencyInLoggingAndMonitoring{Consistency in Logging and Monitoring?}\\n  ConsistencyInLoggingAndMonitoring -- Yes --\x3e UsePattern\\n  ConsistencyInLoggingAndMonitoring -- No --\x3e HighAvailabilityAndResilience{High Availability and Resilience?}\\n  HighAvailabilityAndResilience -- Yes --\x3e UsePattern\\n  HighAvailabilityAndResilience -- No --\x3e SimplifyingConfigurationAndManagement{Simplifying Configuration and Management?}\\n  SimplifyingConfigurationAndManagement -- Yes --\x3e UsePattern\\n  SimplifyingConfigurationAndManagement -- No --\x3e SpecializedTeamManagement{Specialized Team Management?}\\n  SpecializedTeamManagement -- Yes --\x3e UsePattern\\n  SpecializedTeamManagement -- No --\x3e NetworkSecurityAndAttackMitigation{Network Security and Attack Mitigation?}\\n  NetworkSecurityAndAttackMitigation -- Yes --\x3e UsePattern\\n  NetworkSecurityAndAttackMitigation -- No --\x3e DoNotUsePattern[Do not use Gateway Offloading pattern]\\n```\\n\\n##### Azure Solutions\\n\\nAzure resources that can be used to implement this pattern include [Azure Front Door with WAF](https://learn.microsoft.com/azure/frontdoor/front-door-overview?WT.mc_id=AZ-MVP-5004796), [Azure Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview?WT.mc_id=AZ-MVP-5004796) and [Azure Firewall](https://learn.microsoft.com/azure/firewall/overview?WT.mc_id=AZ-MVP-5004796)\\n\\n### \ud83c\udfd7\ufe0f Architecting for Azure\\n\\nThe following Frameworks can help guide you, from the business requirements to the technical implementation of your solution:\\n\\n1. [Cloud Adoption Framework](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) - Proven guidance and best practices that help you confidently adopt the cloud and achieve business outcomes.\\n2. [Azure Architecture Center](https://learn.microsoft.com/azure/architecture/?WT.mc_id=AZ-MVP-5004796) - Guidance for architecting solutions on Azure using established patterns and practices\\n3. [Cloud Design Patterns (part of Architecture Center)](https://learn.microsoft.com/azure/architecture/patterns/?WT.mc_id=AZ-MVP-5004796) - Design patterns are useful for building reliable, scalable, secure applications in the cloud.\\n4. [Well-Architected Framework](https://learn.microsoft.com/azure/well-architected/?WT.mc_id=AZ-MVP-5004796) - A set of quality-driven tenets, architectural decision points, and review tools intended to help solution architects build a technical foundation for their workloads.\\n\\n:::info\\n\\"Design patterns are important for reusability, efficiency, maintainability, consistency, performance, and modularity \\n\\nCloud design patterns are important for scalability, resilience, operational excellence, performance efficiency, and security \\nCategories of patterns include data management, security, reliability, messaging, and design & implementation\\nImplementing patterns in Azure involves using the Cloud Adoption Framework, Well-Architected Framework, and Azure Architecture Center \\n\\nRecap: Design patterns are important in modern cloud architecture and can be implemented in Azure using established frameworks and guidance to ensure scalability, resilience, and security.\\"\\n:::\\n\\n![The time is now, go build!The time is now, go build!](Blog_CloudDesignPatterns_TimettoBuild.PNG)"},{"id":"azure/export-azure-devops-repos-azure-storage-account","metadata":{"permalink":"/azure/export-azure-devops-repos-azure-storage-account","source":"@site/blog/2024-05-13-ado-export-stgaccount/index.mdx","title":"Export Azure DevOps Repositories to Azure Storage Account","description":"This is a step-by-step guide on exporting Azure DevOps repositories to an Azure Storage Account for backup and recovery.","date":"2024-05-13T08:21:56.906Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":15.42,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Export Azure DevOps Repositories to Azure Storage Account","metaDescription":"Learn how to export Azure DevOps repositories to an Azure Storage Account for backup and disaster recovery.","description":"This is a step-by-step guide on exporting Azure DevOps repositories to an Azure Storage Account for backup and recovery.","date":"2024-05-13T08:21:56.906Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/export-azure-devops-repos-azure-storage-account","keywords":["Azure","Azure DevOps","Repositories","Export","Azure Storage Account","Backup","Disaster Recovery"]},"unlisted":false,"prevItem":{"title":"Cloud Design Patterns","permalink":"/azure/cloud-design-patterns"},"nextItem":{"title":"Deploying Large Language Models on AKS with Kaito","permalink":"/azure/run-local-llm-aks"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\nWhen recently deleted, Azure DevOps code repositories go into a soft-delete state, any repositories deleted are retained for 30 days before they are permanently deleted.\\n\\nFor Disaster Recovery scenarios or scenarios where you may not realize something has been deleted until after Microsoft retains the backup, you may want to export the repositories at a single point in time to an Azure storage account.\\n\\nThe Azure DevOps service is highly redundant and built on core Azure Platform infrastructure components, such as Availability Zones and is built using Azure Cloud native Storage and Azure SQL services, which are highly redundant and backed-up. \\n\\nHowever, mistakes can happen, and there may be organizational requirements to retain backups of the repositories. \\n\\nToday, we will examine exporting your repositories to an Azure Storage Account using a DevOps pipeline. This pipeline will capture the repositories at their current state and run nightly. \\n\\n\x3c!-- truncate --\x3e\\n\\n## \ud83d\udcda Overview\\n\\n:::info\\n\\"You can recover deleted organizations or projects within the 28-day window following deletion. But, once this period elapses, these entities are permanently deleted and can\'t be restored. While these backups serve as a crucial component for disaster recovery, customers need to practice appropriate data management and backup strategies to ensure comprehensive data protection.\\"\\n\\n\\"Accidental deletion here refers to scenarios that arise as a result of an incident on our services. It doesn\'t include customers\' accidental deletion of assets (for example, repositories, work items, attachments, or artifacts).\\n\\n**We don\'t support restoring assets that customers accidentally delete. These backups are meant only for business continuity and to aid recovery from outages or disaster scenarios.**\\"\\n\\nReference: [Data protection - Mistakes Happen](https://learn.microsoft.com/en-us/azure/devops/organizations/security/data-protection?view=azure-devops&WT.mc_id=AZ-MVP-5004796#mistakes-happen)\\n:::\\n\\n:::warning\\nPlease remember that this backup will only export the repositories and pipelines, not the work items, pipeline variables, or other artifacts. This will also backup any code stored in Azure DevOps, so if secrets or other sensitive information are stored in the code, please ensure that these are not stored in the code or are encrypted, or they may be readable by someone with access to the storage account.\\n:::\\n\\nToday, we will run a PowerShell script with an Azure DevOps pipeline. The script will connect to the Azure DevOps API and grab the zip file of each Repository in all Projects into an artifact, which then gets exported and copied to an Azure Storage Account.\\n\\n```mermaid\\ngraph TD\\n  A[Trigger Pipeline] --\x3e B[Checkout Code]\\n  B --\x3e C[Read Azure DevOps Repositories]\\n  C --\x3e G[Export Azure DevOps Repositories]\\n  G --\x3e D[Allow Public Access to Azure DevOps Export Storage Account for upload]\\n  D --\x3e E[Check Storage Network Access]\\n  E --\x3e F[Download Build Artifacts]\\n  F --\x3e H[Upload Repositories to Azure Storage Account]\\n```\\n\\n## \ud83d\ude80 Deployment\\n\\n### \u2601\ufe0f Azure Storage Account\\n\\nWe will need to create an Azure Storage Account to store the exported repositories. We can point towards an already existing Storage Account and Container. \\n\\nWe will deploy the Storage account fresh using [Azure Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796) to our existing Resource Group for this article.\\n\\n:::tip\\nI will use the [Deployment Pane](https://luke.geek.nz/azure/Azure-Bicep-Deploy-Pane/) to deploy the Storage Account straight from Visual Studio Code, running in a [Codespace](https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/).\\n:::\\n\\nWe will use [Lifecycle Management policies](https://learn.microsoft.com/azure/storage/blobs/lifecycle-management-policy-configure?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796) to help control the retention of the exported repositories. We will move the repositories to Cool Storage after 30 days, Archive Storage after 90 days, and delete them after 120 days.\\n\\n![Export ADO - Create Storage Account](Export_ADO_CreateStgAccount.gif)\\n\\n```bicep\\n// Parameters for the script\\nparam storageAccountName string // Name of the storage account\\nparam location string = resourceGroup().location // Location of the storage account, default to the location of the resource group\\nparam containerName string = \'adoexport\' // Name of the container\\n\\n// Resource definition for the storage account\\nresource storageAccount \'Microsoft.Storage/storageAccounts@2023-01-01\' = {\\n  name: storageAccountName // Set the name of the storage account\\n  location: location // Set the location of the storage account\\n  sku: {\\n    name: \'Standard_LRS\' // Set the SKU to Standard_LRS\\n  }\\n  kind: \'StorageV2\' // Set the kind to StorageV2\\n  properties: {\\n    accessTier: \'Hot\' // Set the access tier to Hot\\n    allowBlobPublicAccess: false // Disable public access to blobs\\n    publicNetworkAccess: \'Disabled\' // Disable public network access\\n  }\\n}\\n\\n// Resource definition for the blob service\\nresource blobService \'Microsoft.Storage/storageAccounts/blobServices@2023-01-01\' = {\\n  name: \'default\' // Set the name of the blob service to default\\n  parent: storageAccount // Set the parent to the storage account\\n  properties: {\\n    lastAccessTimeTrackingPolicy: {\\n      enable: true // Enable last access time tracking\\n    }\\n  }\\n}\\n\\n// Resource definition for the storage container\\nresource storageContainer \'Microsoft.Storage/storageAccounts/blobServices/containers@2023-01-01\' = {\\n  name: containerName // Set the name of the container\\n  parent: blobService // Set the parent to the blob service\\n  properties: {\\n    publicAccess: \'None\' // Set public access to None\\n  }\\n}\\n\\n// Define a new resource of type \'Microsoft.Storage/storageAccounts/managementPolicies\'\\nresource lifecyclePolicy \'Microsoft.Storage/storageAccounts/managementPolicies@2023-01-01\' = {\\n  // The name of the management policy. \'default\' is a reserved name for the policy.\\n  name: \'default\'\\n  \\n  // The parent resource that this policy is associated with, which is the storage account.\\n  parent: storageAccount\\n  \\n  properties: {\\n    policy: {\\n      rules: [\\n        {\\n          // This rule is enabled\\n          enabled: true\\n          // The name of the rule\\n          name: \'MoveToCoolStorageAfter30Days\'\\n          // The type of rule, in this case, it\'s a lifecycle rule\\n          type: \'Lifecycle\'\\n          definition: {\\n            actions: {\\n              // The actions to take on base blobs (i.e., non-versioned blobs)\\n              baseBlob: {\\n                // Move the blob to cool storage after it hasn\'t been modified for 30 days\\n                tierToCool: {\\n                  daysAfterModificationGreaterThan: 30\\n                }\\n              }\\n            }\\n            // The filters that determine which blobs the rule applies to\\n            filters: {\\n              // The types of blobs that the rule applies to. In this case, the rule applies to block blobs\\n              blobTypes: [\\n                \'blockBlob\'\\n              ]\\n            }\\n          }\\n        }\\n        {\\n          enabled: true\\n          name: \'MoveToArchiveStorageAfter90Days\'\\n          type: \'Lifecycle\'\\n          definition: {\\n            actions: {\\n              baseBlob: {\\n                // Move the blob to archive storage after it hasn\'t been modified for 90 days\\n                tierToArchive: {\\n                  daysAfterModificationGreaterThan: 90\\n                }\\n              }\\n            }\\n            filters: {\\n              blobTypes: [\\n                \'blockBlob\'\\n              ]\\n            }\\n          }\\n        }\\n        {\\n          enabled: true\\n          name: \'DeleteBlobsAfter120Days\'\\n          type: \'Lifecycle\'\\n          definition: {\\n            actions: {\\n              baseBlob: {\\n                // Delete the blob after it hasn\'t been modified for 120 days\\n                delete: {\\n                  daysAfterModificationGreaterThan: 120\\n                }\\n              }\\n            }\\n            filters: {\\n              blobTypes: [\\n                \'blockBlob\'\\n              ]\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n\\noutput storageAccountName string = storageAccount.name // Output the name of the storage account\\noutput containerName string = storageContainer.name // Output the name of the storage account container\\n\\n```\\n\\nPlease note the Storage Account name and Container Name; we will need them for Azure DevOps pipeline variables.\\n\\n### \ud83d\udee0\ufe0f Azure DevOps Pipeline\\n\\nNow, it\'s time to set up your project and repository. If you don\'t know how to do that, you can follow the official Microsoft Learn documentation here: [Create a new Git repo in your project](https://learn.microsoft.com/en-us/azure/devops/repos/git/create-new-repo?view=azure-devops&WT.mc_id=AZ-MVP-5004796). \\n\\nOnce your repository has been set up and initialized, we need to add the following files.\\n\\n# \ud83d\udcbb PowerShell Script\\n\\nThe PowerShell script, `Export-AzDevOpsRepos.ps1`, is responsible for cloning all repositories in a given Azure DevOps organization and downloading them as ZIP files. It uses the Azure DevOps REST API to fetch all projects and their respective repositories. For each repository, it generates a URL to download the repository as a ZIP file and saves it to a local directory. If the script is run in an Azure Pipelines environment, it also uploads the directory containing the downloaded repositories as a pipeline artifact.\\n\\n:::tip\\nThe PowerShell script can also be run locally. If it detects an Azure DevOps agent, it will automatically collect the variables from the pipeline variables; otherwise, those variables can be added manually to the script to run it locally.\\n\\n```powershell\\nif ($env:AGENT_ID) {\\n    # Running in Azure DevOps\\n    $personalAccessToken = \\"$env:pat\\" # Assuming PAT is stored as a secret variable in the pipeline\\n    $organization = \\"$env:AzDevOpsOrg\\"\\n\\n}\\nelse {\\n    # Running on a local PC\\n    $personalAccessToken = \'\'\\n    $organization = \'\'\\n}\\n```\\n:::\\n\\n# \ud83d\ude80 Azure Pipelines\\n\\nThe Azure Pipelines configuration file, `.azure-pipelines/pipeline.ci.adoexport.yml` sets up a CI pipeline that runs the PowerShell script on a schedule. The pipeline is configured to run on the latest Windows agent. It uses the Azure CLI task to run the PowerShell script and to manage access to an Azure storage account. After the script has run, the pipeline downloads any ZIP files produced as artifacts and copies them to the Azure storage account. The pipeline is scheduled to run daily at midnight.\\n\\n<Tabs>\\n<TabItem value=\\"pipeline\\" label=\\"pipeline.ci.adoexport.yml\\">\\n\\n```yaml\\nname: \\"Export Azure DevOps repositories\\" # Name of the pipeline\\n\\ntrigger: none # Pipeline is not triggered automatically\\n\\nschedules: # Define schedules for pipeline runs\\n  - cron: \\"0 12 * * *\\" # Run daily at midnight\\n    displayName: Daily midnight run # Display name for the schedule\\n    branches:\\n      include:\\n        - main # Only run on the \'main\' branch\\n    always: true # Run even if there are no code changes\\n\\npool:\\n  vmImage: \\"windows-latest\\" # Use the latest Windows agent\\n\\nsteps:\\n  - checkout: self # Checkout the source code from the repository\\n    persistCredentials: true # Persist credentials for subsequent steps\\n    displayName: \\"Checkout code\\" # Display name for the step\\n\\n  - task: AzureCLI@2 # Use Azure CLI task\\n    displayName: \\"Clone Azure DevOps Repositories\\" # Display name for the step\\n    inputs:\\n      azureSubscription: \\"$(azServiceConnection)\\" # Azure subscription to use\\n      scriptType: \\"ps\\" # Use PowerShell script\\n      scriptLocation: \\"scriptPath\\" # Script location is a file path\\n      scriptPath: \\"$(Build.Repository.LocalPath)/Export-AzDevOpsRepos.ps1\\" # Path to the PowerShell script\\n    env:\\n      PAT: $(PAT) # Personal Access Token for authentication\\n      AzDevOpsOrg: $(AzDevOpsOrg) # Azure DevOps organization\\n\\n  - task: AzureCLI@2 # Use Azure CLI task\\n    displayName: Allow Public Access to Azure DevOps Export Storage Account for upload # Display name for the step\\n    condition: succeeded() # Run only if the previous step succeeded\\n    inputs:\\n      azureSubscription: \\"$(azServiceConnection)\\" # Azure subscription to use\\n      scriptType: bash # Use Bash script\\n      scriptLocation: inlineScript # Script location is inline\\n      inlineScript: |\\n        az storage account update --name \\"${storageAccount}\\"  --resource-group \\"${storageAccountRG}\\" --default-action Allow\\n    env:\\n      storageAccount: $(storageAccount) # Personal Access Token for authentication\\n      storageAccountRG: $(storageAccountRG) # Azure DevOps organization\\n\\n  - task: AzureCLI@2 # Use Azure CLI task\\n    displayName: \\"Check Storage Network Access\\" # Display name for the step\\n    condition: succeeded() # Run only if the previous step succeeded\\n    timeoutInMinutes: 10 # Timeout after 10 minutes\\n    continueOnError: true # Continue even if the step fails\\n    name: check_storage_access # Name of the step\\n    inputs:\\n      azureSubscription: \\"$(azServiceConnection)\\" # Azure subscription to use\\n      scriptType: bash # Use Bash script\\n      scriptLocation: inlineScript # Script location is inline\\n      inlineScript: |\\n        set -x\\n        echo -e \\"Setting up authentication...\\"\\n        AZURE_STORAGE_ACCOUNT=${storageAccount}\\n        AZURE_STORAGE_KEY=$(az storage account keys list --account-name ${storageAccount} --query \'[0].value\' --output tsv)\\n        echo -e \\"Checking storage account access every 60 seconds...\\"\\n        sleep 10\\n        for i in {1..60}; do\\n          if az storage container list --output none; then\\n            echo \\"Access granted\\"\\n            break\\n          else\\n            echo \\"Access denied, retrying in 60 seconds...\\"\\n            sleep 60\\n          fi\\n        done\\n\\n  - task: DownloadPipelineArtifact@2 # Download pipeline artifacts\\n    displayName: \\"Download Build Artifacts\\" # Display name for the step\\n    inputs:\\n      patterns: \\"**/*.zip\\" # Include all ZIP files\\n      path: \\"$(Build.ArtifactStagingDirectory)\\" # Download artifacts to the staging directory\\n\\n  #Storage account needs the SPN to have a Storage Blob Data Contributor role to allow blob upload.\\n\\n  - task: AzureFileCopy@6 # Use Azure File Copy task\\n    displayName: \\"Copy artifacts to $(storageAccount)\\" # Display name for the step\\n    inputs:\\n      azureSubscription: \\"$(azServiceConnection)\\" # Azure subscription to use\\n      blobPrefix: \\"$(Build.DefinitionName)/$(Build.BuildId)\\" # Prefix for the blob names\\n      containerName: $(stgAccContainer) # Name of the storage container\\n      destination: \\"AzureBlob\\" # Copy to Azure Blob storage\\n      sourcePath: \\"$(Build.ArtifactStagingDirectory)/*\\" # Source path for the artifacts\\n      storage: $(storageAccount) # Storage account to copy the artifacts to\\n    # Log the status of artifact download and storage account operations\\n\\n\\n  - task: AzureCLI@2 # Use Azure CLI task\\n    displayName: Remove Public Access from Azure DevOps Export Storage Account # Display name for the step\\n    condition: succeededOrFailed() # Run whether the previous step succeeded or failed\\n    inputs:\\n      azureSubscription: \\"$(azServiceConnection)\\" # Azure subscription to use\\n      scriptType: bash # Use Bash script\\n      scriptLocation: inlineScript # Script location is inline\\n      inlineScript: |\\n        az storage account update --name \\"${storageAccount}\\" --resource-group \\"${storageAccountRG}\\" --default-action Deny\\n    env:\\n      storageAccount: $(storageAccount) # Personal Access Token for authentication\\n      storageAccountRG: $(storageAccountRG) # Azure DevOps organization\\n\\n```\\n</TabItem>\\n<TabItem value=\\"script\\" label=\\"Export-AzDevOpsRepos.ps1\\">\\n\\n```powershell\\n$apiVersion = \'7.1\' # Update API version to 6.0\\n\\nif ($env:AGENT_ID) {\\n    # Running in Azure DevOps\\n    $personalAccessToken = \\"$env:pat\\" # Assuming PAT is stored as a secret variable in the pipeline\\n    $organization = \\"$env:AzDevOpsOrg\\"\\n\\n}\\nelse {\\n    # Running on a local PC\\n    $personalAccessToken = \'\'\\n    $organization = \'\'\\n}\\n\\n$base64AuthInfo = [System.Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes(\\":$($personalAccessToken)\\"))\\n$headers = @{Authorization = (\\"Basic {0}\\" -f $base64AuthInfo) }\\n\\n# Get all projects\\n$projects = Invoke-RestMethod -Uri \\"https://dev.azure.com/$organization/_apis/projects?api-version=$apiVersion\\" -Method Get -Headers $headers -Verbose\\n\\n# Output the count and names of the projects\\nWrite-Host \\"Number of projects: $($projects.value.Count)\\"\\nWrite-Host \\"Project names: $($projects.value | ForEach-Object { $_.name })\\"\\n\\n# For each project, get all repositories and download them as zip\\n\\n# Ensure the repositories directory exists before starting the loop\\n$repositoriesPath = \\"$env:SYSTEM_DEFAULTWORKINGDIRECTORY/repositories\\"\\nif (-not (Test-Path -Path $repositoriesPath)) {\\n    Write-Host \\"Creating repositories directory: $repositoriesPath\\"\\n    New-Item -ItemType Directory -Path $repositoriesPath | Out-Null\\n}\\n\\n$projects.value | ForEach-Object {\\n    $projectName = $_.name\\n\\n    if (-not [string]::IsNullOrWhiteSpace($projectName)) {\\n        $projectName = $projectName.Replace(\' \', \'%20\')\\n        $result = Invoke-RestMethod -Uri \\"https://dev.azure.com/$organization/$projectName/_apis/git/repositories?api-version=$apiVersion\\" -Method Get -Headers $headers -Verbose\\n\\n        $result.value | ForEach-Object {\\n            $repoName = $_.name\\n            Write-Host \\"Attempting to clone the repository: $repoName\\"\\n\\n            if (-not [string]::IsNullOrWhiteSpace($repoName)) {\\n                $repoId = $_.id\\n                $zipUrl = \\"https://dev.azure.com/$organization/$projectName/_apis/git/repositories/$repoId/items?scopePath=/&recursionLevel=Full&api-version=$apiVersion&`$format=zip\\"\\n                $outputPath = \\"repositories/$repoName.zip\\"\\n                Write-Host \\"Output path: $outputPath\\"\\n                # Ensure the directory exists before trying to download the file\\n                $directoryPath = Split-Path -Path $outputPath -Parent\\n                if (-not (Test-Path -Path $directoryPath)) {\\n                    Write-Host \\"Creating directory: $directoryPath\\"\\n                    New-Item -ItemType Directory -Path $directoryPath | Out-Null\\n                }\\n                try {\\n                    Write-Host \\"Starting download for $repoName from $zipUrl\\"\\n                    Invoke-WebRequest -Uri $zipUrl -OutFile $outputPath -Headers $headers\\n                    Write-Host \\"Download completed for $repoName\\"\\n                }\\n                catch {\\n                    Write-Host \\"Failed to download $repoName from $zipUrl\\"\\n                    Write-Host $_.Exception.Message\\n                }\\n            }\\n        }\\n    }\\n}\\n\\n# Check if the repositories directory exists before trying to upload it\\nif (Test-Path -Path \\"$env:SYSTEM_DEFAULTWORKINGDIRECTORY/repositories\\") {\\n    Write-Host \\"Repositories directory exists, uploading as an artifact.\\"\\n    Write-Host \\"##vso[artifact.upload containerfolder=repositories;artifactname=AzureDevOpsExportedRepositories;]$env:SYSTEM_DEFAULTWORKINGDIRECTORY/repositories\\"\\n}\\nelse {\\n    Write-Host \\"The repositories directory does not exist, so you cannot upload it as an artifact.\\"\\n}\\n\\n```\\n</TabItem>\\n</Tabs>\\n\\nYou can also find the latest files in the [lukemurraynz/ADO_Export](https://github.com/lukemurraynz/ADO_Export) GitHub repository. Feel free to open a Pull Request, Suggest changes, or improve.\\n\\nI am going to be lazy and just drag the files into the repo and commit them.\\n\\n![Export_ADO_CommitFiles](Export_ADO_CommitFiles.gif)\\n\\nOnce committed, it is time to set up our pipeline.\\n\\nBut first, we need to setup our [PAT (Personal Access Token)](https://learn.microsoft.com/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&tabs=Windows&WT.mc_id=AZ-MVP-5004796) and [Service Connection](https://learn.microsoft.com/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796). Both I have completed.\\n\\nFor your PAT token, you will need the following permissions:\\n\\n- Code (Read)\\n- Project and Team (Read)\\n\\nFor your Service Connection, you will need the following permissions:\\n\\nReader on the Subscription that Contains your Storage Account *(the subscription scoping is required for the Azure CLI commanders to check and access your Storage account; if you are using a Self-Managed DevOps agent, you may be able to avoid this)*, and Storage Blob Data Contributor on the Storage Account.\\n\\nThen, we need to add to our pipeline the following:\\n\\n| Variable            | Example                                        | Notes                                                                                         |\\n| ------------------- | ---------------------------------------------- | --------------------------------------------------------------------------------------------- |\\n| azDevOpsOrg         | Contoso                                        | Your Azure DevOps Org - matching your URL ie (https://dev.azure.com/contoso)                 |\\n| azServiceConnection | AzureServiceConnection                         | Your Service Connection name for Azure Resource Provider access to your Azure Resource Group |\\n| pat                 | 2xj2jv4yf2h6xvqjgk7y2n4d2iucxltlzyxuhcdjxibnrf | Your PAT (Personal Access Token), used to authenticate to Azure DevOps. Set this as a secure string.                       |\\n| stgAccContainer     | adoexport                                      | The Container that will contain your ADO Exports inside of the Storage Account               |\\n| storageAccount      | adoexport23                                    | Storage account that your ADO exports will be going into.                                     |\\n| storageAccountRG    | adoexport-rg                                   | Resource Group, containing your Azure Storage Account                                         |\\n\\nSo, let\'s import our pipeline and configure our variables!\\n\\n1. To import the Pipeline, navigate to your Azure DevOps Project, click on Pipelines, and then click on the \\"New Pipeline\\" button.\\n2. Click Azure Repos Git and select your repository.\\n3. Select Existing Azure Pipelines YAML file\\n4. Select your Pipeline and click Continue\\n5. Now, we can add our Variables by clicking Variables and then Add. If you have already imported the Pipeline, you can navigate to the Pipeline, click Edit, and then Variables.\\n\\n![Import Azure DevOps Pipeline & Configure Variables](Export_ADO_ImportPipelineVariables.gif)\\n\\n:::info\\nBy default, the pipeline has a cronjob configured to set it for midnight, but you can change this to any time you like by editing the pipeline file and changing the [cron syntax](https://learn.microsoft.com/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796#cron-syntax).\\n\\n```yaml\\nschedules: # Define schedules for pipeline runs\\n  - cron: \\"0 12 * * *\\" # Run daily at midnight\\n    displayName: Daily midnight run # Display name for the schedule\\n    branches:\\n      include:\\n        - main # Only run on the \'main\' branch\\n    always: true # Run even if there are no code changes\\n```\\n:::\\n\\n## \ud83c\udfc3\u200d\u2642\ufe0f Run\\n\\nNow that you have configured your Storage Account and Pipeline, it\'s time to run the pipeline manually to ensure there are no problems before your scheduled time runs.\\n\\n:::warning\\nBefore you run:\\n\\nDouble-check the permissions you have granted the Service Principal and your variable naming and values; else you may get an error similar to the following when attempting to copy the artifacts to the Storage Account:\\n\\n```plaintext\\nERROR: (InvalidApiVersionParameter) The api-version \'2023-01-01\' is invalid. The supported versions are \'2024-03-01,2023-07-01,2023-07-01-preview,2023-03-01-preview,2022-12-01,2022-11-01-preview,2022-09-01,2022-06-01,2022-05-01,2022-03-01-preview,2022-01-01,2021-04-01,2021-01-01,2020-10-01,2020-09-01,2020-08-01,2020-07-01,2020-06-01,2020-05-01,2020-01-01,2019-11-01,2019-10-01,2019-09-01,2019-08-01,2019-07-01,2019-06-01,2019-05-10,2019-05-01,2019-03-01,2018-11-01,2018-09-01,2018-08-01,2018-07-01,2018-06-01,2018-05-01,2018-02-01,2018-01-01,2017-12-01,2017-08-01,2017-06-01,2017-05-10,2017-05-01,2017-03-01,2016-09-01,2016-07-01,2016-06-01,2016-02-01,2015-11-01,2015-01-01,2014-04-01-preview,2014-04-01,2014-01-01,2013-03-01,2014-02-26,2014-04\'.\\nCode: InvalidApiVersionParameter\\n```\\nIt means, that your variables are not parsing correctly, to the Azure CLI, make sure they are correct.\\n:::\\n\\n*You may need to permit your Pipeline to use your Service Connection by clicking on the \\"Allow\\" button when the pipeline runs.*\\n\\nYour pipeline may take a while to run, depending on how many repositories you have and how large they are. You can monitor the progress of the pipeline by clicking on the pipeline, then clicking on the Job, and then the Task to see the output of the PowerShell script. This is why I would recommend running this on a schedule during a time that does not impact users, as it can hold up an agent while it runs; also be aware that depending on your repositories, running this every night can quickly use up any [build minutes you may have, if running from a Microsoft hosted agent](https://azure.microsoft.com/pricing/details/devops/azure-devops-services/?WT.mc_id=AZ-MVP-5004796).\\n\\n:::info\\nIf a Repository is only used for Azure Boards and not for code, it will not be exported, as the API will not return it *, i.e., you will use a (404) Not Found*.\\n:::\\n\\n![Azure DevOps - Artifact_Export](Artifact_Export.png)\\n\\nOnce that\'s completed - you will be able to see your exported repositories in your storage account!\\n\\n![Azure DevOps - Storage Account](Export_ADO_CheckAzureDevOpsExportStfAcc.gif)"},{"id":"azure/run-local-llm-aks","metadata":{"permalink":"/azure/run-local-llm-aks","source":"@site/blog/2024-05-09-run-local-slm-aks/index.mdx","title":"Deploying Large Language Models on AKS with Kaito","description":"Deploy large language models on AKS using Kaito, an operator that simplifies the deployment of AI/ML inference models in a Kubernetes cluster.","date":"2024-05-08T21:49:34.752Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":15.855,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deploying Large Language Models on AKS with Kaito","metaDescription":"Learn how to deploy large language models (or Small Language Models) on Azure Kubernetes Service (AKS) using Kaito, an operator that automates the deployment of AI/ML inference models in a Kubernetes cluster.","description":"Deploy large language models on AKS using Kaito, an operator that simplifies the deployment of AI/ML inference models in a Kubernetes cluster.","date":"2024-05-08T21:49:34.752Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":"kaito-arch.png","slug":"azure/run-local-llm-aks","keywords":["azure","large language models","AKS","Kaito","Kubernetes","operator","AI/ML inference models","GPU-enabled nodes","deployment","model provisioning","model configuration","model tuning","model deployment","model management","open-source LLMs","cost reduction","data security","AI Shared responsibility model","AKS cluster","quota","GPU compute nodes","Azure CLI","GitHub Codespace","infrastructure as code","Terraform","Azure Developer CLI","deployment","AKS cluster deployment","region","environment configuration","deployment completion","microservices","Pets app","AKS demos","experiments","polyglot architecture","event-driven design","open-source back-end services","RabbitMQ","MongoDB","OpenAI\'s GPT-3 models","generative text","graphics creation","MongoDB instance","RabbitMQ","prerequisites","Azure subscription","GPU compute workload","Standard NCSv3","Azure Developer CLI","Terraform","AKS cluster deployment"]},"unlisted":false,"prevItem":{"title":"Export Azure DevOps Repositories to Azure Storage Account","permalink":"/azure/export-azure-devops-repos-azure-storage-account"},"nextItem":{"title":"Authorization Permission Mismatch error with Azure Storage","permalink":"/azure/authorization-permission-mismatch-error-azure-storage"}},"content":"Today, we are going to look at deploying a large language model *(LLM)* directly into your AKS *(Azure Kubernetes Service)* cluster, running on GPU-enabled nodes, using [Kaito *(Kubernetes AI Toolchain Operator)*](https://github.com/Azure/kaito).\\n\\n> KAITO is an open-source operator that transforms how you deploy AI models on Kubernetes. It streamlines the process, automating critical tasks like infrastructure provisioning and resource optimization. It intelligently selects the optimal hardware configuration for your specific model, using available CPU and GPU resources on AKS. KAITO eliminates the manual setup complexities, accelerating your deployment time and reducing associated costs.\\n\\n\x3c!-- truncate --\x3e\\n\\n## \ud83d\udc40 Overview\\n\\n:::info\\n[Kaito](https://github.com/Azure/kaito) is an operator that automates the deployment of the AI/ML inference model in a Kubernetes cluster. The target models are popular open-sourced inference models such as Falcon and llama2. Kaito has the following critical differentiations compared to most of the mainstream model deployment methodologies built on top of virtual machine infrastructures:\\n\\n* Manage large model files using container images. An HTTP server is provided to perform inference calls using the model library.\\n* Avoid tuning deployment parameters to fit GPU hardware by providing preset configurations.\\n* Auto-provision GPU nodes based on model requirements.\\n* Host large model images in the public Microsoft Container Registry (MCR) if the license allows.\\n\\nKaito follows the classic Kubernetes Custom Resource Definition(CRD)/controller design pattern. The user manages a workspace custom resource that describes the GPU requirements and the inference specification. Kaito controllers automate the deployment by reconciling the workspace custom resource.\\n\\n![Kaito - Architecture](kaito-arch.png)\\n\\n* Workspace controller: It reconciles the workspace custom resource, creates machine custom resources to trigger node auto-provisioning, and creates the inference workload (deployment or stateful set) based on the model preset configurations.\\n* Node provisioner controller: The controller\'s name is gpu-provisioner in [Kaito helm chart](https://github.com/Azure/kaito/tree/main/charts/kaito/gpu-provisioner). It uses the machine CRD originated from Karpenter to interact with the workspace controller. It integrates with Azure Kubernetes Service(AKS) APIs to add new GPU nodes to the AKS cluster. Note that the gpu-provisioner is an open-sourced component. It can be replaced by other controllers if they support Karpenter-core APIs.\\n\\n> At the time of this article, [Kaito is in preview](https://azure.microsoft.com/updates/public-preview-kubernetes-ai-toolchain-operator-kaito-addon-for-aks/?WT.mc_id=AZ-MVP-5004796) and **is not recommended for production use**.\\n\\nThere are some significant benefits of running open-source LLMs with Kaito. Some advantages include:\\n\\n* Automated GPU node provisioning and configuration: Kaito will automatically provision and configure GPU nodes for you. This can help reduce the operational burden of managing GPU nodes, configuring them for Kubernetes, and tuning model deployment parameters to fit GPU profiles.\\n* Reduced cost: Kaito can help you save money by splitting inferencing across lower-end GPU nodes, which may also be more readily available and cost less than high-end GPU nodes.\\n* Support for popular open-source LLMs: Kaito offers preset configurations for popular open-source LLMs. This can help you deploy and manage open-source LLMs on AKS and integrate them with your intelligent applications.\\n* Fine-grained control: You can fully control data security and privacy, model development and configuration transparency, and fine-tune the model to fit your specific use case.\\n* Network and data security: You can ensure these models are ring-fenced within your organization\'s network, and the data never leaves the Kubernetes cluster.\\n:::\\n\\n:::warning\\nA word of warning: when looking at running your model in your own AKS cluster, be aware of the [AI Shared responsibility model](https://learn.microsoft.com/en-us/azure/security/fundamentals/shared-responsibility-ai?WT.mc_id=AZ-MVP-5004796). There is a difference in the Azure AI PaaS *(Platform as a Service)* offerings, i.e., Model safety, compute, versioning, etc. and running your model in your own AKS cluster. So, although the Kaito operator helps you run some SLM models in your AKS cluster, you are responsible for the security, compliance, and governance of the entire system versus consuming a Microsoft-provided model as a service offering.\\n:::\\n\\n:::info\\nIf you want to follow the guide and set up Kaito, make sure you have approved [quota](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?WT.mc_id=AZ-MVP-5004796) in your region for the GPU compute nodes. In this example, I am using a 12 vCPU Standard NCSv3 instance.\\n\\nYou can use the following Azure CLI command to check your use and limit:\\n\\n~~~bash\\naz vm list-usage \\\\\\n  --location ${AZURE_LOCATION} \\\\\\n  --query \\"[? contains(localName, \'Standard NCSv3\')]\\" \\\\\\n  -o table\\n~~~\\n\\nYou can follow this Microsoft guide for requesting a quota increase: [Request a quota increase](https://learn.microsoft.com/azure/quotas/regional-quota-requests?WT.mc_id=AZ-MVP-5004796).\\n:::\\n\\n### \ud83d\udc36 Example App - Pets\\n\\nToday, we will use the [aks store demo\u2014Pets](https://github.com/Azure-Samples/aks-store-demo) sample microservices app, commonly used for AKS demos, tutorials, and experiments. We will deploy it to our AKS cluster, update it to use a locally hosted mistral model to generate the local product descriptions, and then call the externally hosted Azure OpenAI endpoint.\\n\\n> This sample demo app consists of containerized microservices that can be easily deployed into an Azure Kubernetes Service (AKS) cluster. It is meant to show a realistic scenario using a polyglot architecture, event-driven design, and common open-source back-end services *(e.g., RabbitMQ, MongoDB)*. The application also leverages OpenAI\'s GPT-3 models to generate product descriptions. This can be done using either Azure OpenAI or OpenAI.\\n\\n![Demo Pets - Architecture with Azure OpenAI](demo-arch-with-openai.png)\\n\\nThe application has the following services: \\n\\n| Service | Description |\\n| --- | --- |\\n| `makeline-service` | This service handles processing orders from the queue and completing them (Golang) |\\n| `order-service` | This service is used for placing orders (Javascript) |\\n| `product-service` | This service is used to perform CRUD operations on products (Rust) |\\n| `store-front` | Web app for customers to place orders (Vue.js) |\\n| `store-admin` | Web app used by store employees to view orders in queue and manage products (Vue.js) | \\n| `virtual-customer` | Simulates order creation on a scheduled basis (Rust) |\\n| `virtual-worker` | Simulates order completion on a scheduled basis (Rust) |\\n| `ai-service` | Optional service for adding generative text and graphics creation (Python) |\\n| `mongodb` | MongoDB instance for persisted data |\\n| `rabbitmq` | RabbitMQ for an order queue |\\n\\n### \ud83d\ude80 Overview\\n\\n### \ud83d\udce6 Prerequisites\\n\\n* [GitHub account](https://github.com/) *(we will use a GitHub Codespace to deploy the App, and requirements)*\\n* [Azure subscription](https://azure.microsoft.com/en-us/free/open-source?WT.mc_id=AZ-MVP-5004796) *(we will need an Azure subscription, to install the cluster the model to)*\\n* [Request a quota increase](https://learn.microsoft.com/azure/quotas/regional-quota-requests?WT.mc_id=AZ-MVP-5004796) for a GPU compute workload *(ie Standard NCSv3)* in your region. I am based in New Zealand so that I will be deploying to Australia East.\\n\\n### \ud83d\udc1d Deploy AKS cluster\\n\\n1. Let us deploy our Cluster and application using a pre-created Azure Developer CLI infrastructure as code deployment by connecting to the Codespace *(which already has the dependencies of Azure Developer CLI, Kubectl, Helm, Azure Bicep, and Terraform Azure CLI pre-installed)* at [aks store demo\u2014Pets](https://github.com/Azure-Samples/aks-store-demo).\\n![Open Codespace](Kaito-AKS_Codespace_Open.gif)\\n2. Now, we need to prep for the Azure Developer CLI deployment, which will use Terarform (a random pet name module for a unique name) to deploy our AKS cluster in our specified region. So, let\'s log in to Azure.\\n\\n```bash\\n# login to Azure CLI\\naz login --use-device-code\\n\\n# login to Azure Developer CLI\\nazd auth login\\n```\\n\\n![Codespace Azure Login](Kaito-AKS_Codespace_AzLogin.gif)\\n\\n3. Once logged in, we can configure a new Azure Developer CLI environment in preparation for our deployment.\\n\\n```bash\\nazd env new\\n```\\n\\n4. Enter an environment name *(i.e., Kaito)*; the Azure Developer CLI uses this to store the environment settings for the deployment and won\'t reflect any of your deployed resources.\\n5. Now, we need to set a few environment variables for our deployment, which will be used by the Terraform deployment to deploy the Azure Open AI instance that Pets will use *(and we will replace it with Falcon later on; however, I recommend including it in the deployment to confirm that the OpenAI components are confirmed working first)*.\\n\\n```bash\\nazd env set DEPLOY_AZURE_OPENAI true\\nazd env set DEPLOY_AZURE_WORKLOAD_IDENTITY true\\n```\\n\\nTo confirm variables, you can run the following:\\n\\n```bash\\nazd env get-values\\n```\\n\\n![Check Azure Developer CLI variables](Kaito-AKS_Codespace_CheckAzdEnvVariables.gif)\\n\\n6. Finally, it\'s time to deploy. Run the following command and select the Subscription you want to deploy the AKS cluster into and the region *(remember this region is the region for which you have requested the GPU quota increase)*.\\n\\n```bash\\nazd up\\n```\\n\\n![Azure Developer CLI -Up](Kaito-AKS_Codespace_AZDUp.gif)\\n\\n7. Once the deployment is complete, you will see the output (AKS and helm deployments for the PET microservices), including the AKS cluster name, the Azure OpenAI endpoint, and the OIDC issuer URL.\\n\\nAfter you have provided the information, the azd up command will start by registering Azure providers and features and installing Azure CLI extensions. From there, it will invoke the terraform apply command and execute \\"azd-hook\\" scripts, which is a neat way for you to \\"hook\\" into the deployment process and add any customizations. We will invoke a helm install command in our deployment to apply our Kubernetes manifests.\\n\\nThis will take a few minutes to complete. Once it\'s done, you will see the terraform apply command output and the created resources. You can also view the resources in the Azure portal.\\n\\n![AKS Pet store Azure resource deployments](Kaito-AKS-rg-funnyrhino17.png)\\n\\n7. Once the deployment is complete, run the following command to load all the AZD environment variables into your shell to be used by the Kaito deployment.\\n\\n```bash\\neval \\"$(azd env get-values)\\"\\n```\\n### \ud83e\uddea Test the Pet-store app\\n\\nNow, lets test that its working, by taking a look at the pet store admin public endpoint.\\n\\n1. Open your favorite browser and navigate to the [Azure Portal](https://portal.azure.com/#home)\\n2. Search for Kubernetes services\\n3. Find your AKS cluster and click on the resource\\n4. Click on Services and Ingresses\\n5. Find the store-admin service and click on the public endpoint to open the store-admin page.\\n6. You can test the current *(external to the AKS cluster)* Azure OpenAI endpoint by clicking on Products, Add Product, typing in a Keyword, clicking on the \\"Ask AI Assistant\\" button, and seeing the generated text.\\n\\n![AKS - Test Pet store admin OpenAI endpoint](Kaito-AKS_PetStoreAdminAzOpenAITest.gif)\\n\\n### \ud83d\udc26 Implement Kaito and Falcon LLM\\n\\nNow that the AKS cluster is deployed and the Pets store app is running, we can implement Kaito and run the Falcon 7b-instruct large language model.\\n\\n> You can continue to set this up in the Codespace; I am going to switch to an [Azure CloudShell](https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796) to complete the rest of the implementation, mainly to come at the implementation, from the point of view of installing Kaito in an existing cluster, not related to the Pet store example *(i.e., make sure that there are no variables, etc. that we might be reliant on, without knowing why)*. \\n\\n:::info\\nResearchers from Technology Innovation Institute, Abu Dhabi, introduced the [Falcon series](https://arxiv.org/pdf/2311.16867.pdf), which includes models with 7 billion, 40 billion, and 180 billion parameters. These models, intended to be causal decoder-only models, were trained on a high-quality, varied corpus mostly obtained from online data. Falcon-180B, the largest model in the series, is the only publicly available pretraining run ever, having been trained on a dataset of more than 3.5 trillion text tokens.\\n\\nThe researchers discovered that Falcon-180B shows great advancements over other models, including PaLM or Chinchilla. It outperforms models being developed concurrently, such as LLaMA 2 or Inflection-1. Falcon-180B achieves performance close to PaLM-2-Large, which is noteworthy given its lower pretraining and inference costs. With this ranking, Falcon-180B joins GPT-4 and PaLM-2-Large as the leading language models in the world. For more information, see the following resources:\\n\\n* [The Falcon Series of Open Language Models](https://arxiv.org/pdf/2311.16867.pdf)\\n* [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)\\n* [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B)\\n* [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)\\n* [Falcon-7B-Instruct](https://github.com/Azure-Samples/aks-kaito-terraform/blob/main/alcon-7B-Instruct)\\n\\nReference: [Deploy Kaito on AKS using Terraform](https://techcommunity.microsoft.com/t5/azure-for-isv-and-startups/deploy-kaito-on-aks-using-terraform/ba-p/4108930?WT.mc_id=AZ-MVP-5004796)\\n:::\\n\\n1. To do this, we need to make sure that we have the preview version of the AKS CLI extension installed. You can check this by running the following command:\\n\\n```bash\\naz extension list --query \\"[?name==\'aks-preview\']\\"\\n```\\n\\nIf you don\'t have it, you can install it:\\n\\n```bash\\naz extension add --name aks-preview --allow-preview true\\n```\\n\\nAnd if needed, update it:\\n\\n```bash\\naz extension update --name aks-preview --allow-preview true\\n```\\n2. Re-login to Azure, if needed *(you don\'t need to login to the Azure Developer CLI; it was only required for previous steps)*. Next, we need to register the AIToolchainProvider resource provider by running the following command:\\n\\n```bash\\naz feature register --namespace \\"Microsoft.ContainerService\\" --name \\"AIToolchainOperatorPreview\\"\\n```\\n\\nThis will allow the Kaito (AI Toolchain Operator) APIs on your Azure subscription to be deployed into your AKS cluster. It could take a few minutes to register.\\n\\n![Register AI Tool Chain Operator](Kaito-AKS_RegisterAIToolChainOperator.gif)\\n\\n3. Once the registration is complete, we can prepare to deploy the workload identity, workspace, and GPU-enabled node pool for Kaito; as part of this, we need to define the environment variables (if you are still using the Codespace, these should be carried over from the previous deployment; if not, we can set them)*. Adding the variables helps us avoid having to enter the values each time we run a command.\\n\\n```bash\\nexport AZURE_SUBSCRIPTION_ID=\\"9b06e177-a5bb-468b-9fd6-fc58e5b67239\\"\\nexport AZURE_RESOURCE_GROUP=\\"rg-funnyrhino17\\"\\nexport AZURE_LOCATION=\\"australiaeast\\"\\nexport CLUSTER_NAME=\\"aks-funnyrhino17\\"\\n```\\n\\n![Set Kaito environment variables](Kaito-AKS_Variables.gif)\\n\\n4. We are now going to update our AKS cluster to enable the Kaito operator and OpenID Connect issuer *(required to enable connectivity to a Managed identity to create the Node pools)* by running the following command:\\n\\n```bash\\naz aks update --name ${CLUSTER_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --enable-oidc-issuer --enable-ai-toolchain-operator\\n```\\n\\n![Update AI Toolchain Operator](Kaito-AKS_EnableAIToolChainOperatorExistingCluster.gif)\\n\\n5. Once updated, we can then connect directly to the cluster using the Kubernetes command line tool, Kubectl, by running the following command:\\n\\n```bash\\naz aks get-credentials --resource-group ${AZURE_RESOURCE_GROUP} --name ${CLUSTER_NAME}\\n```\\n\\n![Connect to AKS cluster with Kubectl](Kaito-AKS_ConnectAKSClusterKubectl.gif)\\n\\n6. Then verify the connection:\\n\\n```bash\\nkubectl get nodes\\n```\\n![Kubectl get nodes](Kaito-AKSKubectlgetnodes.gif)\\n\\n:::tip\\n`kubectl` is a command line tool for interacting with Kubernetes clusters. It\'s part of the Kubernetes project and is used to deploy and manage applications on Kubernetes. It can also be used to inspect and debug cluster and application resources.\\n\\nKubectl is the Kubernetes command-line tool that allows you to run commands against Kubernetes clusters. You can use Kubectl to deploy applications, inspect and manage cluster resources, and view logs. It\'s a crucial component for interacting with Kubernetes and is used in various operations, from creating, updating, and deleting Kubernetes resources to debugging running applications.\\n:::\\n\\n8. Lets assign the managed identity, Contributor rights over the Resource Group, that contains the Cluster by running the following command:\\n\\n```bash\\nexport AKS_OIDC_ISSUER=$(az aks show --name \\"${CLUSTER_NAME}\\" --resource-group \\"${AZURE_RESOURCE_GROUP}\\" --query \\"oidcIssuerProfile.issuerUrl\\" -o tsv)\\nexport MC_RESOURCE_GROUP=$(az aks show --resource-group ${AZURE_RESOURCE_GROUP} --name ${CLUSTER_NAME} --query nodeResourceGroup -o tsv)\\nKAITO_IDENTITY_PRINCIPAL_ID=$(az identity show --name ai-toolchain-operator-${CLUSTER_NAME} --resource-group ${MC_RESOURCE_GROUP} --query principalId --output tsv)\\nKAITO_IDENTITY_CLIENT_ID=$(az identity show  --name ai-toolchain-operator-${CLUSTER_NAME} --resource-group ${MC_RESOURCE_GROUP} --query clientId --output tsv)\\naz role assignment create --assignee $KAITO_IDENTITY_PRINCIPAL_ID --scope \\"/subscriptions/${AZURE_SUBSCRIPTION_ID}/resourcegroups/${AZURE_RESOURCE_GROUP}\\" --role Contributor\\naz identity federated-credential create --name \\"Kaito-federated-identity\\" --identity-name ai-toolchain-operator-${CLUSTER_NAME} --resource-group ${MC_RESOURCE_GROUP} --issuer ${AKS_OIDC_ISSUER} --subject system:serviceaccount:kube-system:kaito-gpu-provisioner --audience api://AzureADTokenExchange\\n```\\n\\n![Assign Kaito Managed Identity federated credentials](Kaito-AKSAssignKaitoMI.gif)\\n\\n:::warning\\n\\nIf you get \'Failed to connect to MSI. Please make sure MSI is configured correctly.\' Make sure you relogin to Azure *(ie az login --use-device-code)*, you may need to assign Contributor rights manually as well. Reference: [Error from CloudShell - Failed to connect to MSI. Please make sure MSI is configured correctly. #17695](https://github.com/Azure/azure-cli/issues/17695)\\n\\n:::\\n\\n:::tip\\n\\nYou can check what entries are in the environment variables by running using echo to output the variable:\\n\\n```bash\\necho $KAITO_IDENTITY_CLIENT_ID\\n```\\n\\nIf the gpu-provisioner is not running, you can check the logs by running the following command:\\n\\n```bash\\nkubectl get pods --all-namespaces -l app=ai-toolchain-operator\\n```\\nFind the name of the gpu-provisioner pod, and then run the following command to get the logs:\\n\\n```bash\\nkubectl logs -n kube-system <gpu-provisioner-pod-name>\\n```\\n\\nI had issues where the gpu-provisioner pod was not running and constantly restarted. The logs showed that the identity was not found, so I had to re-run the command to assign the correct managed identity and use the echo command to reveal the managed identity, highlighting the wrong one was selected.\\n\\n:::\\n\\n![kaito-gpu-provisioner | Azure Portal overview](kaito-gpu-provisoner-deployment-azportal.png)\\n\\n9. Let\'s now deploy the Falcon 7b instruct model to our AKS cluster.\\n\\n:::warning\\n\\nAs soon as the models are grabbed from the remote registry, the GPU nodes will spin up, and you will start paying for their use, so just be aware.\\n\\n:::\\n\\nWe are going to use one of the **supported models to create our interference endpoint in AKS, the Falcon 7b instruct model, as per their examples in the [Kaito GitHub repository](https://github.com/Azure/kaito/tree/main/examples/inference)**.\\n\\n```bash\\nkubectl apply -f https://raw.githubusercontent.com/Azure/kaito/main/examples/inference/kaito_workspace_falcon_7b-instruct.yaml\\n```\\n\\n:::tip\\n\\nYou can monitor the progress of the workspaces and model deployment by running the following command:\\n\\n```bash\\nkubectl get workspace workspace-falcon-7b-instruct -w\\n```\\n\\n> The deployment can take a few minutes *(ie 20-50 minutes, the Resources because ready, before the workspace is ready)*, so be patient. It needs to pull the Falcon model from the Microsoft public container registry *(ie, Pulling image \\"mcr.microsoft.com/aks/kaito/kaito-falcon-7b-instruct:0.0.4\\")*.  It took me 45 minutes when I tested with the Mistral 7b model\\n\\nWait for the Workspace to be ready before continuing.\\n\\n![Deploy workspace-falcon-7b-instruct](Kaito-DeployFalcon7bInstruct.gif)\\n\\n:::\\n\\nOnce the workspaces and pods/nodes have been provisioned, let us test the deployment by running the following command:\\n\\n```bash\\nkubectl get svc workspace-falcon-7b-instruct -o jsonpath=\'{.spec.clusterIP}\'\\nexport SERVICE_IP=$(kubectl get svc workspace-falcon-7b-instruct -o jsonpath=\'{.spec.clusterIP}\')\\nkubectl run -it --rm --restart=Never curl --image=curlimages/curl -- curl -X POST http://$SERVICE_IP/chat -H \\"accept: application/json\\" -H \\"Content-Type: application/json\\" -d \\"{\\\\\\"prompt\\\\\\":\\\\\\"What is your favorite ice cream flavor?\\\\\\"}\\"\\n```\\n\\n![Test Falcon 7b instruct model](Kaito-TestFalcon7bInstruct.gif)\\n\\n### \ud83d\ude80 Update the Pet-store app\\n\\nAlmost there! Now that we know that our local chat endpoint is working with the Falcon 7b instruct model, we can update the Pet Store app to use the local Falcon 7b model and remove the Azure OpenAI to generate the product descriptions.\\n\\nWe will continue in our Terminal to do that, but like the previous steps, it can be done in the Codespace. The Pet Store application already has some great examples of doing this, so we will create a new manifest and repoint the application to use the local Falcon workspace instead of the public Azure Open AI endpoint that was created initially:\\n\\n1. First, we need to create a new config map to deploy the Pet Store app with the Falcon 7b instruct model by running the following command:\\n\\n```bash\\nkubectl apply -n pets -f - <<EOF\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: ai-service-configmap\\ndata:\\n  USE_LOCAL_LLM: \\"True\\"\\n  AI_ENDPOINT: \\"http://workspace-falcon-7b-instruct/chat\\"\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: ai-service\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: ai-service\\n  template:\\n    metadata:\\n      labels:\\n        app: ai-service\\n    spec:\\n      nodeSelector:\\n        \\"kubernetes.io/os\\": linux\\n      containers:\\n      - name: order-service\\n        image: ghcr.io/azure-samples/aks-store-demo/ai-service:latest\\n        ports:\\n        - containerPort: 5001\\n        envFrom:\\n        - configMapRef:\\n            name: ai-service-configmap\\n        resources:\\n          requests:\\n            cpu: 20m\\n            memory: 50Mi\\n          limits:\\n            cpu: 30m\\n            memory: 85Mi\\n        startupProbe:\\n          httpGet:\\n            path: /health\\n            port: 5001\\n          initialDelaySeconds: 60\\n          failureThreshold: 3\\n          timeoutSeconds: 3\\n          periodSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            path: /health\\n            port: 5001\\n          initialDelaySeconds: 3\\n          failureThreshold: 3\\n          timeoutSeconds: 3\\n          periodSeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            path: /health\\n            port: 5001\\n          failureThreshold: 3\\n          initialDelaySeconds: 3\\n          timeoutSeconds: 3\\n          periodSeconds: 3\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: ai-service\\nspec:\\n  type: ClusterIP\\n  ports:\\n  - name: http\\n    port: 5001\\n    targetPort: 5001\\n  selector:\\n    app: ai-service\\nEOF\\n```\\n\\nNow, let\'s test it! We will remove the Azure OpenAI instance first and then test it!\\n\\n![Test Pet Store Local LLM](Kaito-TestFalcon7bInstruct-PetStore.gif)\\n\\nWe can confirm within our AI Service ConfigMap now that the local endpoint is being used:\\n\\n![Falcon 7b Kaito Local LLM](Falconb_Kaito_LocalLLM_ConfigMap.png)"},{"id":"azure/authorization-permission-mismatch-error-azure-storage","metadata":{"permalink":"/azure/authorization-permission-mismatch-error-azure-storage","source":"@site/blog/2024-05-06-authorizationpermissionmismatchstorage/index.mdx","title":"Authorization Permission Mismatch error with Azure Storage","description":"Resolve the \\"AuthorizationPermissionMismatch\\" error when copying files to Azure Storage in Azure DevOps pipeline.","date":"2024-05-06T08:23:54.576Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.745,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Authorization Permission Mismatch error with Azure Storage","metaDescription":"Learn how to fix the authorization permission mismatch error when copying files to an Azure Storage account in Azure DevOps pipeline.","description":"Resolve the \\"AuthorizationPermissionMismatch\\" error when copying files to Azure Storage in Azure DevOps pipeline.","date":"2024-05-06T08:23:54.576Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"slug":"azure/authorization-permission-mismatch-error-azure-storage","keywords":["Azure","Authorization Permission Mismatch","Azure Storage","Azure DevOps pipeline","DevOps"]},"unlisted":false,"prevItem":{"title":"Deploying Large Language Models on AKS with Kaito","permalink":"/azure/run-local-llm-aks"},"nextItem":{"title":"Federated Credentials to AKS Managed Identity","permalink":"/azure/federated-credentials-aks"}},"content":"When attempting to copy files in an Azure DevOps pipeline to an Azure Storage account, I received the following error:\\n\\n:::warning\\n##[error]Upload to container: \'StorageContainer\' in storage account: \'ContosoStorageAccount\' with blob prefix: \'736\' failed with error: \'AzCopy.exe exited with non-zero exit code while uploading files to blob storage.\' For more info please refer to [https://aka.ms/azurefilecopyreadme](https://aka.ms/azurefilecopyreadme).\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\nAfter reviewing the Azure DevOps raw pipeline logs I noticed the following:\\n\\n:::warning\\n2024-05-06T06:25:34.1051117Z --------------------------------------------------------------------------------\\n2024-05-06T06:25:34.1052461Z RESPONSE 403: 403 This request is not authorized to perform this operation using this permission.\\n2024-05-06T06:25:34.1053291Z ERROR CODE: AuthorizationPermissionMismatch\\n2024-05-06T06:25:34.1055333Z --------------------------------------------------------------------------------\\n:::\\n\\n\\n>The fix was to make sure that the Service Principal that Azure DevOps is using to read/write to the Azure Storage account has the correct permissions to the Data plane *not just the Management plane. \\n\\nIn my case, I needed to assign the  **[Storage Blob Data Contributor](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles?WT.mc_id=AZ-MVP-5004796#storage) role to the Service Principal**."},{"id":"azure/federated-credentials-aks","metadata":{"permalink":"/azure/federated-credentials-aks","source":"@site/blog/2024-05-05-aks-federatedidentity/index.mdx","title":"Federated Credentials to AKS Managed Identity","description":"Workloads deployed on an Azure Kubernetes Services (AKS) cluster require Microsoft Entra application credentials or managed identities to access Microsoft Entra-protected resources, such as Azure Key Vault and Microsoft Graph. Microsoft Entra Workload ID integrates with the capabilities native to Kubernetes to federate with external identity providers.","date":"2024-05-05T08:39:07.238Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.295,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Federated Credentials to AKS Managed Identity","metaDescription":"Learn how to add federated credentials to a user-assigned managed identity in Azure Kubernetes Service (AKS) to access protected resources.","Description":"Explore configuring federated credentials for a user-assigned managed identity in AKS, enabling secure access to Microsoft Entra-protected resources such as Azure Key Vault and Microsoft Graph.","date":"2024-05-05T08:39:07.238Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":"aks-workload-identity-model.png","slug":"azure/federated-credentials-aks","keywords":["azure","portal","Adding Federated Credentials to AKS Managed Identity","AKS","Azure Kubernetes Service","federated credentials","user-assigned managed identity","Microsoft Entra","Azure Key Vault","Microsoft Graph"]},"unlisted":false,"prevItem":{"title":"Authorization Permission Mismatch error with Azure Storage","permalink":"/azure/authorization-permission-mismatch-error-azure-storage"},"nextItem":{"title":"Uncollapsing the Azure Portal Blade Menu","permalink":"/azure/uncollapsing-azure-portal-blade-menu"}},"content":"Workloads deployed on an Azure Kubernetes Services (AKS) cluster require Microsoft Entra application credentials or managed identities to access Microsoft Entra-protected resources, such as Azure Key Vault and Microsoft Graph. Microsoft Entra Workload ID integrates with the capabilities native to Kubernetes to federate with external identity providers.\\n\\nLet\'s look at how that might be set up for Managed Identity for AKS (Azure Kubernetes Service) in Azure.\\n\\n![AKS Workload Identity](aks-workload-identity-model.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n:::tip\\nThere is a lot of detail around identity, federated identity, and workload identity that I won\'t be going into today.\\n\\nIf this is something you want to dig into, I recommend the following:\\n\\n* [Use Microsoft Entra Workload ID with Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview?tabs=dotnet&WT.mc_id=AZ-MVP-5004796)\\n* [Create an OpenID Connect provider on Azure Kubernetes Service (AKS)](https://learn.microsoft.com/azure/aks/use-oidc-issuer?WT.mc_id=AZ-MVP-5004796)\\n* [AKS Workload Identity - Quick Tutorial](https://www.youtube.com/watch?v=i2GobU0Wg48)\\n:::\\n\\nSo, let us look at adding a Federated Credential to a User Assigned Managed identity.\\n\\nClick on **Settings** and **Federated credentials**\\n\\nWe will look at one I set up earlier. How this is used is application-specific, but in this example, the Federated credentials are used by an AI Service running on an AKS cluster to talk to an Azure OpenAI instance that the User Assigned Managed identity has access to.\\n\\n![Federated Credentials](UserManagedIdentityEditFederatedCredential.gif)\\n\\n:::warning\\nIt is worth knowing that to use Federated Credentials and open the OIC APIs on your cluster, you will need to either create a new cluster or update it. The OIC APIs are not open by default on the cluster. See [Create an OpenID Connect provider on Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/use-oidc-issuer?WT.mc_id=AZ-MVP-5004796) for more information.\\n:::\\n\\nFirst up is:\\n\\n**Cluster Issue URL**, in my example, I have:\\n  \\n  ```plaintext\\n  https://australiaeast.oic.prod-aks.azure.com/00000000-0000-0000-0000-000000000000/00000000-0000-0000-0000-000000000000/\\n  ```\\n  So what is this? This is the URL of the OpenID Connect provider with which the AKS cluster is federating. It is also the URL that the AKS cluster will use to authenticate the User Assigned Managed identity.\\n\\n  To get the URL, you can use the Azure CLI and run the following query:\\n\\n  ```bash\\n  az aks show -n myAKScluster -g myResourceGroup --query \\"oidcIssuerProfile.issuerUrl\\" -otsv\\n  ```\\n  \\nTo test that it is responding and open, you can navigate to the URL discovery endpoint \'/openid/v1/jwks\', for example:\\n\\nhttps://australiaeast.oic.prod-aks.azure.com/00000000-0000-0000-0000-000000000000/00000000-0000-0000-0000-000000000000/openid/v1/jwks\\n\\nYou will get an HTTP response back similar to the following:\\n\\n  ```plaintext\\n{\\n  \\"keys\\": [\\n    {\\n      \\"use\\": \\"sig\\",\\n      \\"kty\\": \\"RSA\\",\\n      \\"kid\\": \\"4gZVAbix5cXrHnMF9D6rp-EOpB92gfamRWVG873e9s5\\",\\n      \\"alg\\": \\"RS256\\",\\n      \\"n\\": \\"wCZChNTi7ymXkmLeMP95PUl-9b2Fnzm1xLlslLw4Cc_1oMVbvNUbJS6r4_bgylTYSowUCqQLZUPH_DClav4aJC-G3OYlCkQrG3wFnB7b70Q8qJrwsVr2XaYh8lwfah_BceiDEJNc2yjjEBQoHs3zbi3qOVhYp3_BYHVoXuBJnhEgMb5bpxzgN-ZrM2QwmLzLSv9BB_iq034B3P0Exn8n0yWvXPFe_pWPzwyC_l4dgMN_TpNMLjaBYoJ-wdfim7eEJuOgVfuV1O8smMeXhUNUoRKOvFNw9H2qjZM243JhVBnLX1C7Oj_8L2AW7wSptuThl-gIqy9nrcSPT_oHhVUwAgO-gwFCRSgilFiBhsG9-wkflSdELeJCRpLi1j_8OOON0LTX6vZrHjwGzGa0bC5zgouKCFE1iNtVc3HoNKczZL-wiERUIgo8YKey_uOdeOtdBPOLVO0igfN4xtDZnxb_CzSUJ3fJHGAg03k3MBEpr0KAMmhsTSo-KUqfcgnclHkOywGdwnX-oT43SvcItEDbUiKyFV-i0CabAVebzF4O6r_ovpK4N2QJ-4s8_bHx4d3RMstML1D_yEpy40PeFjQq5ZPQQt1AUfkT1jP5-siCdJgm8HTc0zIPwfZfwKdKefYMRmamNHwk3TNV9gdOX0fnKP07R9FBKCgypqeDcLYZYgMt\\",\\n      \\"e\\": \\"AQAB\\"\\n    },\\n    {\\n      \\"use\\": \\"sig\\",\\n      \\"kty\\": \\"RSA\\",\\n      \\"kid\\": \\"wQ4TGh1obxJ6l8nxEiV-hf-lccLnJwXxx7Rw8E-UDBo\\",\\n      \\"alg\\": \\"RS256\\",\\n      \\"n\\": \\"ydxfJlP6Qmh_dfkhWkrGQrrad-o0ShvZCShIUz4-k8bhH98xAhSgC52EmXnbCvkbDxHpwQIZ8GQHFGjbFC-Z-ZbdIpETd-MVwp70HXyoejuFNmdghTeLZaN5Q-lf1U8gjnxfzUo5tL_Vwcd1RhGi3YXwQmiEGJoZ5ZWhQs75O8iAnvyOzDqWt0CxraIAVPoY9aJ1IBTPlA0bIdDtuCTiynCMQiozO4dx72BDMjJ3ZcEwKrFZBFAu71SPr4xs7ErUYqZFtZxmcDxKkhTNCPd9dUkmo6uknMuVSHj5OqeXexDdKhZKcJhTFBGF89oWtIjx0ix9G8VTqTJsDZp3_LBcs6Ob0hpfyaGePhibZTqhDXD_Nr3p-yw78lXO0ceD1xz8ZJyY4EzxtY2yobAGGNsg_wU23XJsIkJLOGiCAjRg81PxbtlEPzClXkxbrZTjKI9OVJxujGxt01PmgiLN38piThL3zsvGF_4N-ApX1fpdKH9Owl2xsmVd-zzzoJ1l8y9FKZpgnJKQ-VECIbVNwzO6KEm2f7O6MKpqB5hLi-QPH-ZLk3zbi_55zb8h1PZxmuam4rS3MHi98IxxmVz8fbMVirosQhzP7oYuM-L1ZgZLzwKvDenmoUYIfzwUl6QCsM3DxZs_07sxUrZk0isNvBHU3wXKsCtSXJyf0CJyq6TAYCD1\\",\\n      \\"e\\": \\"AQAB\\"\\n    }\\n  ]\\n}\\n  ```\\n\\nHowever, for the Federated credential configuration for AKS, the OIC URL from the Azure CLI `az aks show` command is enough.\\n\\nSecond is the **Namespace**. This is the Namespace in which the User Assigned Managed identity will be used. In my example, I have \'pets\' as this is my Application namespace. Be as specific as you can.\\n\\nIn Kubernetes, a namespace organizes and isolates resources within a cluster. It acts as a virtual cluster within the main cluster. Namespaces allow you to group related objects (such as pods, services, and deployments) together and provide a scope for resource names.\\n\\nThird is the **Service Account**. This Service Account will be linked to the Federated credential inside of AKS.\\n\\nA service account is an identity used by pods (containers) running within a namespace to authenticate with other services or APIs. Each namespace can have multiple service accounts, and they are associated with specific permissions. When a pod is created, it can be assigned a service account. The pod then uses the credentials associated with that service account to access other resources (e.g., secrets, APIs). Service accounts are useful for controlling access and ensuring that pods have the appropriate permissions.\\n\\nThat leaves the **Subject identifier**. This is the Subject identifier.\\n\\nIn AKS Federated credentials, the subject identifier refers to the identity the managed identity will trust. When using workload identity, the subject identifier is typically the service account associated with a pod. The subject identifier establishes trust between the managed identity (related to the AKS cluster) and the service account (associated with the pod). Once this trust is established, the managed identity can securely use the service account\u2019s credentials to access other Azure services.\\n\\nNamespaces provide isolation, service accounts represent identities for pods, and subject identifiers (usually service accounts) are used in federated credentials to establish trust between the managed identity and Kubernetes workloads.\\n\\nUnder the Credentials details, we have **Name** and **Audience**.\\n\\nThe **name** refers to the identifier associated with a managed identity or service principal in Azure. In AKS (Azure Kubernetes Service), when configuring federated credentials using managed identities, you create a managed identity (or use an existing one) to represent your AKS cluster. This managed identity is assigned a unique name, which is used to identify it within Entra ID. For example, creating a managed identity named \u201cmy-aks-cluster-identity\u201d becomes the name associated with the identity.\\n\\nThe **audience** (also known as the \u201caud\u201d claim) is a critical part of security tokens (such as JWTs) issued by Azure AD. In the context of AKS federated credentials, the audience represents the intended recipient of the token. When a pod in AKS requests an access token (e.g., to access Azure Key Vault secrets), the audience is set to the specific Azure resource (e.g., Key Vault) the pod wants to access. The audience ensures that the token is valid only for the specified resource. For example, if a pod seeks to access a secret stored in Azure Key Vault, the audience in the token will be set to the Key Vault\u2019s endpoint.\\n\\nThe `api://AzureADTokenExchange` audience is typically used when a service needs to obtain a token to act on behalf of a user or another service. For example:\\n\\n- A user logs in to a web application (front-end), and the application needs to call an API (back-end) on behalf of the user. The front-end application requests an on-behalf-of token with the `api://AzureADTokenExchange` audience.\\n- A service (acting as a middle-tier) receives an access token from a client and needs to call another downstream service.\\n\\n*(e.g., a database or an API)*. It exchanges the token for an on-behalf-of token using the same audience.\\n\\nThe api://AzureADTokenExchange audience is used explicitly for token exchange scenarios, allowing services to obtain tokens on behalf of users or other services.\\n\\nName: The unique identifier for a managed identity or service principal.\\nAudience: The intended recipient (Azure resource) for the security token."},{"id":"azure/uncollapsing-azure-portal-blade-menu","metadata":{"permalink":"/azure/uncollapsing-azure-portal-blade-menu","source":"@site/blog/2024-05-05-uncollapse-azure-portal/index.mdx","title":"Uncollapsing the Azure Portal Blade Menu","description":"There have been some changes to the Azure Portal that have people scratching their heads; I am talking about the infamous collapsable Blade menu.","date":"2024-05-05T07:53:07.238Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.915,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Uncollapsing the Azure Portal Blade Menu","metaDescription":"Learn how to keep the Azure Portal Blade Menu always open for easy access to new services and options.","Description":"Discover how to configure your Azure Portal to have the Blade menu always open, ensuring you never miss out on new services and options.","date":"2024-05-05T07:53:07.238Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":"Azure_Portal_CollapseableMenu.png","slug":"azure/uncollapsing-azure-portal-blade-menu","keywords":["azure","portal"]},"unlisted":false,"prevItem":{"title":"Federated Credentials to AKS Managed Identity","permalink":"/azure/federated-credentials-aks"},"nextItem":{"title":"Troubleshooting scenarios for Azure Open AI","permalink":"/azure/troubleshooting-scenarios-azure-open-ai"}},"content":"There have been some changes to the Azure Portal that have people scratching their heads; I am talking about the infamous collapsable Blade menu.\\n\\n![Azure Portal - Collapse Blade Menu](Azure_Portal_CollapseableMenu.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nI actually like it because it gives the flexibility to hide areas that you may be potentially distracted by, especially as more and more service functionality is brought into the Azure Portal. However, a concern I have is not being able to see new services or options offered in the blade of a service, so for that reason alone, at the moment, I have configured my Azure Portal to have the Blade menu always open.\\n\\nYou can do this fairly easily per user: click on Settings *(top right)*, click Appearance + Startup views, select Expanded Service menu behavior, and then click Apply.\\n\\n![Azure Expanded Menu](AzurePortal_ExpandedMenuSetting.gif)\\n\\nYou can also leave it collapsed, then expand the Blade menu of a resource, by clicking on the *little* expand icon at the top of the Blade menu, to expand or collapse all headers of the resource pane.\\n\\n![Azure Portal - Collapse Resource Blade Menu](AzurePortal_CollapseableResource.png)"},{"id":"azure/troubleshooting-scenarios-azure-open-ai","metadata":{"permalink":"/azure/troubleshooting-scenarios-azure-open-ai","source":"@site/blog/2024-04-26-AzureOpenAI_TroubleshootingScenarios/index.mdx","title":"Troubleshooting scenarios for Azure Open AI","description":"Troubleshooting Azure Open AI and API calls to OpenAI can be challenging, especially when you may not know where to start!","date":"2024-04-26T01:36:46.434Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Service Management","permalink":"/tags/service-management"}],"readingTime":7.315,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Troubleshooting scenarios for Azure Open AI","metaDescription":"This article provides some scenarios for troubleshooting Azure OpenAI using the Kepner-Tregoe problem analysis methodology.","Description":"This article provides some scenarios for troubleshooting Azure OpenAI using the Kepner-Tregoe problem analysis methodology.","date":"2024-04-26T01:36:46.434Z","tags":["Azure","Misc","Service Management"],"categories":["Azure"],"authors":["Luke"],"header":"KT_Method.png","slug":"azure/troubleshooting-scenarios-azure-open-ai","keywords":["azure","kt","openai","problem","troubleshooting","scenarios","API calls","Kepner-Tregoe","problem analysis","methodology","accuracy","high-volume transactions","chunking process","token limit checks","API call failures","token sizes","region usage","latency","service disruptions","model versioning","encoding issues","performance issues","streaming response","user experience issues","token volume expectancy","overload","underperformance","logging practices","diagnostic effectiveness","API versioning","production readiness","system message control","system messages"]},"unlisted":false,"prevItem":{"title":"Uncollapsing the Azure Portal Blade Menu","permalink":"/azure/uncollapsing-azure-portal-blade-menu"},"nextItem":{"title":"Resolving 404 Error with OpenAI Azure Function Binding","permalink":"/azure/404-attempting-query-openai-azure-function-binding"}},"content":"Troubleshooting Azure Open AI and API calls to OpenAI can be challenging, especially when you may not know where to start!\\n\\nThe idea of this article is to give you not only a place to start with some common scenarios you may run into but also a way of thinking - to help you troubleshoot. This is not a technical \'get-your-hands dirty, delve into those logs\' type article.\\n\\nI am a big fan of the KT, or [Kepner-Tregoe](https://kepner-tregoe.com/) problem analysis methodology, and I have used it in many scenarios throughout my career to help discover and test the root cause of various problems. So, we will use the base of this problem analysis methodology to help us troubleshoot the scenarios we will discuss in this article.\\n\\n![Kepner-Tregoe Method](KT_Method.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## \u2753 The Kepner-Tregoe (KT) \\n\\n:::info\\nThe Kepner-Tregoe (KT) Problem Management methodology incorporates elements such as:\\n\\n* \\"Is\\" *(what we know to be true about the problem)*\\n* \\"Is Not\\" *(what we know to be false or different from the problem)*\\n* \\"Could Be\\" *(possible causes)*\\n* \\"Could Not Be\\" *(what is not a possible cause)*\\n* \\"Distinctive Clarity\\" *(what sets this problem apart from others)*\\n\\nand \\"Next Steps\\" *(actions to further diagnose or solve the problem)*\\n:::\\n\\nYou may find that with some issues, we instinctively do a lot of this. Still, this method helps, gives you context, allows you to check any bias you may have in trying to find the root cause, and gives you some great tools to rule out and test any theories, given it has been a few years since I have been through the formal training. Still, it\'s one of those methodologies that have stuck with me, and I always keep it in mind when troubleshooting issues. Obviously, you can use this methodology for more than just troubleshooting OpenAI issues; this is the scenario we are going to cover today.\\n\\n> The key to successful troubleshooting, if any IT *(Information Technology)* issue - is having a clear problem statement, and in the \'real world\' really concentrating on the one single problem statement to remain effective for this article; however, we will be covering a mix of common scenarios problems, to help give you high-level ideas and context when troubleshooting issues.\\n\\n## \u2757 Problem Statements\\n\\nToday, we are going to look at the following statements:\\n\\n| Type of Problem Statement | Problem Statement |  \\n|---------------------------|-------------------|  \\n| Chunking Control | \\"Inconsistent accuracy during high-volume transactions suggests that the chunking process is not fully controlled. The problem manifests where API consumption occurs and is especially prominent when handling complex inputs of varying sizes.\\" |  \\n| Token Limit Checks | \\"Unexpected API call failures, which are confirmed to occur before the API request is made, indicate that the token limit pre-check may not be accurately estimating token sizes, particularly at the time of calling and in cases of complex requests.\\" |  \\n| Region Usage | \\"Increased latency and occasional service disruptions are observed in a specific default region during peak usage times, suggesting that network latency or regional service performance may not be optimized for the workload, impacting certain regions more than others.\\" |  \\n| Model Versioning | \\"Encoding and performance issues have arisen across all API endpoints following model or API updates, which are more pronounced in certain versions, indicating that using outdated model versions or incompatibilities between model versions and API might be the underlying cause.\\" |  \\n| Streaming Response | \\"User experience issues with the streaming implementation on the front-end application during real-time interactions suggest that there might be backend streaming service limitations or insufficient front-end optimization, affecting certain user interactions.\\" |  \\n| Token Volume Expectancy | \\"The system occasionally experiences overload or underperformance across all API endpoints during 24/7 operation, implying that token volume expectancy might not be accurately predicted or that the system\'s scaling and load balancing are not adequately configured.\\" |  \\n| Logging Practices | \\"Issues in diagnostic effectiveness within the logging system arise during error occurrences, which may be due to incomplete logging data or incorrect logging configurations, affecting the resolution of problems by not capturing comprehensive data.\\" |  \\n| API Versioning | \\"Persisting aborted issues and doubts regarding production readiness after using a preview API version suggest that stability might be compromised due to the continued use of a less stable preview version rather than the GA version across the API service.\\" |  \\n\\n## \ud83c\udfaf IS and Is Nots\\n\\nLet\'s look at each problem and work out various Is and Nots and potential causes at a high level.\\n\\n| Troubleshooting Criteria       | Is (What is True)                        | Is Not (What is False)                  | Could Be (Possible Causes)                       | Could Not Be (What is not a cause) | Distinctive Clarity (What sets this apart)             | Next Steps                                             |  \\n|--------------------------------|------------------------------------------|-----------------------------------------|--------------------------------------------------|------------------------------------|--------------------------------------------------------|--------------------------------------------------------|  \\n| Control of Chunking            | Chunking process is implemented          | Perfect control over chunking           | Inadequate chunk size management                 | A problem with the API itself      | How chunking affects the accuracy of responses         | [Evaluate and refine the semantic chunking process](https://learn.microsoft.com/azure/search/vector-search-how-to-chunk-documents?WT.mc_id=AZ-MVP-5004796)       |  \\n| Token Limit Check              | Pre-call token limit check is in place   | Always under the token limit            | Misestimation of token size                      | A network connectivity issue       | Instances when token limits are exceeded               | [Implement stricter checks and alerts for token limits](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/strategies-for-optimizing-high-volume-token-usage-with-azure/ba-p/4007751?WT.mc_id=AZ-MVP-5004796)   |  \\n| Region Selection               | Using a specific default region          | Region is the cause of all issues       | Network latency or regional service disruption   | Model versioning issues           | The impact of region selection on response times       | [Test performance in different regions; consider geo-redundancy](https://learn.microsoft.com/azure/ai-services/openai/quotas-limits?WT.mc_id=AZ-MVP-5004796#regional-quota-limits) |  \\n| Model Version                  | Using a specified model version          | All versions have the same performance  | Outdated model causing issues                    | A problem with user input          | Differences in encoding and performance between versions | [Upgrade to the latest GA or recommended version of the model](https://learn.microsoft.com/azure/ai-services/openai/how-to/working-with-models?tabs=powershell&WT.mc_id=AZ-MVP-5004796) |  \\n| Streaming Response             | Streaming is being used                  | Streaming is flawless                   | Streaming implementation affecting user experience | Token limit issues                | User experience with streaming vs. non-streaming responses | [Optimize streaming experience based on user feedback](https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/11-streaming-completions.ipynb)   |  \\n| Token Volume                   | Expected volume is known                 | The system can handle any volume        | Insufficient API rate limiting or scaling        | An issue with the model\'s capabilities | Peak token volume times or patterns                   | [Plan for scaling and load balancing based on expected volume](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/smart-load-balancing-for-openai-endpoints-and-azure-api/ba-p/3991616?WT.mc_id=AZ-MVP-5004796) |  \\n| Logging                        | Logging practices are in place           | All necessary data is being logged      | Incomplete or incorrect logging                  | An issue with the model\'s accuracy | The level of detail and usefulness of logs            | [Enhance logging for better diagnostics and problem resolution](https://learn.microsoft.com/azure/architecture/ai-ml/openai/architecture/log-monitor-azure-openai?WT.mc_id=AZ-MVP-5004796) |  \\n| API Version                    | Currently using a non-GA API version     | GA version has been tested              | Use of preview API versions causing issues       | An issue unrelated to the API version | The stability and features of GA vs. preview API versions | [Test and migrate to the GA version of the API](https://learn.microsoft.com/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796)          |  \\n| System Message Control         | System messages are being used           | Full control over system messages       | System message limits not being respected by the model | An issue with the API call structure | How system messages guide user interactions          | [Ensure system message limits are enforced and informative](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message?WT.mc_id=AZ-MVP-5004796) |  \\n| System Message Compliance      | Checks for model adherence to system messages | The model always adheres to system message limits | Oversights in system message enforcement       | An issue with user expectations     | Instances of non-compliance impacting user experience | [Regularly verify model compliance with system message limits](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems?WT.mc_id=AZ-MVP-5004796) |  \\n| Hallucinations and Accuracy    | Occurrences of hallucinations and inaccuracies | The model is always accurate          | Model training or complexity of queries          | An issue with the input data quality | Specific scenarios where inaccuracies are more frequent | [Investigate model choice and implement LLM OPS for evaluation](https://semaphoreci.com/blog/openai-models) |  \\n| Request and Timeout Issues     | Experiencing request or timeout issues   | All requests are processed promptly | High server load, inadequate resources, or network latency | Complete system failure            | Frequency and conditions of timeouts and request failures | [Analyze server and network logs, monitor system performance, and optimize configuration](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency?WT.mc_id=AZ-MVP-5004796) |  \\n\\n![KT - Dimensions of a Problem](KT_DimensionsofaProblem.jpeg)\\n\\nAlthough this is a very theoretical article, hopefully, it points you in the right direction and *thinking* when needing to troubleshoot your issues with Azure OpenAI. As a Consultant or Engineer, it can be very quick to jump straight into solutions without fully knowing the extent of the issue or the problem you are actually trying to solve.\\n\\n## \ud83d\udd17 References\\n\\n* [Azure Open AI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796)\\n* [Azure OpenAI Service frequently asked questions](https://learn.microsoft.com/azure/ai-services/openai/faq?WT.mc_id=AZ-MVP-5004796)\\n* [Kepner-Tregoe](https://kepner-tregoe.com/)\\n* [OpenAI Tokenizier](https://platform.openai.com/tokenizer)\\n* [What\'s new in Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/whats-new?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/404-attempting-query-openai-azure-function-binding","metadata":{"permalink":"/azure/404-attempting-query-openai-azure-function-binding","source":"@site/blog/2024-04-21-AzureOpenAI-AzureFunctionBinding/index.mdx","title":"Resolving 404 Error with OpenAI Azure Function Binding","description":"Troubleshooting a 404 error when querying OpenAI using Azure Function binding in Azure.","date":"2024-04-21T05:06:54.374Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.52,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Resolving 404 Error with OpenAI Azure Function Binding","metaDescription":"This article provides a guide on troubleshooting a 404 error when querying OpenAI using Azure Function binding in Azure.","date":"2024-04-21T05:06:54.374Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":"AzureFunction_OpenAIBinding_NotFound.png","slug":"azure/404-attempting-query-openai-azure-function-binding","keywords":["azure","function","cloudnative","openai","ExtensionBundle"],"description":"Troubleshooting a 404 error when querying OpenAI using Azure Function binding in Azure."},"unlisted":false,"prevItem":{"title":"Troubleshooting scenarios for Azure Open AI","permalink":"/azure/troubleshooting-scenarios-azure-open-ai"},"nextItem":{"title":"Implementing a Sandbox Environment in Microsoft Azure","permalink":"/azure/implementing-sandbox-vending"}},"content":"When attempting to work with [Azure Open AI](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=AZ-MVP-5004796), you may want to work with an [Azure Functions](https://learn.microsoft.com/azure/azure-functions/functions-overview?pivots=programming-language-csharp&WT.mc_id=AZ-MVP-5004796) to adjust or massage the response, to do so you can use the [Azure Functions bindings for OpenAI\'s GPT engine\\n](https://github.com/Azure/azure-functions-openai-extension) to query the OpenAI API. However, you may encounter a 404 Not Found error when attempting to query OpenAI using the Azure Function binding. \\n\\n![Azure Function - 404 Not Found](AzureFunction_OpenAIBinding_NotFound.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nIf you encounter this error, it could be likely that you are using the Azure Function ExtensionBundle, which does not contain the necessary dependencies to query the OpenAI API. \\n\\nThe binding requires at least the following:\\n\\n* Microsoft.Azure.WebJobs.Extensions.OpenAI: 0.12.0-alpha\\n* Microsoft.Azure.WebJobs.Extensions.OpenAI.Kusto: 0.12.0-alpha\\n\\n![Nuget Webjob extensions](AzureFunction_OpenAIBinding_NugetPackageRequirements.png)\\n\\n:::warning\\nThe binding type(s) \'textCompletion\' were not found in the configured extension bundle. Please ensure the correct version of the extension bundle is configured.\\n:::\\n\\nBoth packages are not integrated into the [4.13.2](https://github.com/Azure/azure-functions-extension-bundles/releases/tag/4.13.2) version of the ExtensionBundle, which, at the time of writing, is not available in the production ExtensionBundle shipped with new Azure Functions *(which is 4.13.2)*. \\n\\n**However**, it is integrated into the Extension Bundle Preview. For example, Extension Bundle Preview for [4.18.0](https://github.com/Azure/azure-functions-extension-bundles/releases/tag/4.18.0-Preview) features:\\n\\n* Microsoft.Azure.WebJobs.Extensions.OpenAI: 0.13.0-alpha\\n* Microsoft.Azure.WebJobs.Extensions.OpenAI.Kusto: 0.13.0-alpha\\n\\n![Extensions Bundles Preview release4.18.0](AzureFunction_OpenAIBinding_ExtensionsBundlePreview4.18.0.png)\\n\\n\ud83d\udd27 To use the latest Preview Bundle and enable the OpenAI binding extension, you need to update your function apps: host.json file:\\n\\n```json\\n{\\n  \\"version\\": \\"2.0\\",\\n  \\"managedDependency\\": {\\n    \\"Enabled\\": true\\n  },\\n   \\"extensionBundle\\": {\\n        \\"id\\": \\"Microsoft.Azure.Functions.ExtensionBundle.Preview\\",\\n        \\"version\\": \\"[4.*, 5.0.0)\\"\\n    }\\n}\\n```\\n\\nUpdate the id of `Microsoft.Azure.Functions.ExtensionBundle.Preview` and the version to `[4.*, 5.0.0)` in accordance with the latest [bundle releases](https://github.com/Azure/azure-functions-extension-bundles/releases) to ensure you are using the latest Preview Bundle. The wildcard `*` will ensure you use the latest Extension Bundle Preview version within that major release.\\n\\nYou should now be able to query the OpenAI API using the Azure Function binding.\\n\\n![Run After Binding](AzureFunction_OpenAIBinding_SuccessRunAfterBindingUpdate.png)"},{"id":"azure/implementing-sandbox-vending","metadata":{"permalink":"/azure/implementing-sandbox-vending","source":"@site/blog/2024-04-14-SandboxVending/index.mdx","title":"Implementing a Sandbox Environment in Microsoft Azure","description":"This article highlights reference implementation considerations for implementing a Sandbox environment in Microsoft Azure.","date":"2024-04-14T00:26:42.797Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":19.51,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Implementing a Sandbox Environment in Microsoft Azure","metaDescription":"This article provides a comprehensive guide on implementing a Sandbox environment within the Microsoft Azure platform. It includes considerations for reference implementation, resource consistency, enterprise policy as code, and more.","date":"2024-04-14T00:26:42.797Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":"teaser:Azure_Environment_Concepts.png","slug":"azure/implementing-sandbox-vending","keywords":["azure","sandbox","subscription vending","sandbox vending","Terraform","Infrastructure as Code","Azure DevOps","GitHub","Symphony"],"description":"This article highlights reference implementation considerations for implementing a Sandbox environment in Microsoft Azure."},"unlisted":false,"prevItem":{"title":"Resolving 404 Error with OpenAI Azure Function Binding","permalink":"/azure/404-attempting-query-openai-azure-function-binding"},"nextItem":{"title":"Change the Timezone of your Azure Container App","permalink":"/azure/change-timezone-azure-container-app"}},"content":"When working with Microsoft Azure, you may want an environment for learning, whether for an individual or a team.\\n\\nThis article aims to highlight some reference implementation considerations for implementing a Sandbox environment within the Microsoft Azure platform.\\n\\n![Azure Environment Concepts](Azure_Environment_Concepts.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n## \ud83d\udcdd Overview\\n\\nWhen working with Microsoft Azure, you may want an environment for learning, whether for an individual or a team.\\n\\nCloud Sandboxes are contained, isolated environments that allow the evaluation of new Cloud services and features (without impacting production environments).\\n\\n:::info\\nThis follows from a previous article about [Sandbox Design](https://luke.geek.nz/azure/microsoft-azure-sandbox-design-considerations/) considerations but focuses on the implementation elements. This article aims to give you some ideas on how you can achieve Sandbox vending and is opinionated based on my experience. However, it\'s purely intended to show one (of many) possible ways. Just make sure you understand the business requirements and what you need to achieve.\\n:::\\n\\nA design area of the Ready phase of the [Cloud Adoption Framework](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) is the design and implementation of the [Azure Landing Zone](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796), it would be asmiss of me not to bring up [Subscription vending](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/landing-zone/design-area/subscription-vending?WT.mc_id=AZ-MVP-5004796).\\n\\n:::info\\n\\"Subscription vending provides a platform mechanism for programmatically issuing subscriptions to application teams that need to deploy workloads.\\"\\n:::\\n\\nSubscription Vending is the foundation of what we will discuss today: Sandbox vending.\\n\\nI will base this article on Unmanaged Sandboxes _(Subscription-scoped Sandboxes)_; however, much of the same information can be used across all Sandbox types.\\n\\n![Sandbox Types](Azure_Sandbox_Types.png)\\n\\n```mermaid\\nmindmap\\n  Azure Sandbox Environment Creation\\n    Understanding Azure Sandbox\\n      Definition\\n      Purpose\\n      Limitations\\n    Prerequisites\\n      Account Setup\\n      Subscription Types\\n      Roles Permissions\\n      CLI PowerShell\\n    Planning\\n      Objectives\\n      Required Services\\n      Costs\\n      Security Compliance\\n    Sandbox Setup\\n      Resource Groups\\n      Naming Conventions\\n      Networking Setup\\n      Security Setup\\n      Data Compliance\\n      Monitoring Diagnostics\\n      Disaster Recovery Plan\\n    Services Deployment\\n      VMs\\n      Storage Accounts\\n      Database Services\\n      Self-service\\n      Serverless Computing\\n      Container Services\\n      Microservices Architecture\\n    Dev Tools\\n      Dev Tools Selection\\n      Dev Environment Setup\\n      Version Control\\n    Access Management\\n      Identity Management\\n      RBAC\\n      Managed Identities\\n      Access Logging\\n    Automation DevOps\\n      IaC\\n      CI CD\\n      Scripting\\n      Configuration Management\\n    Testing Experimentation\\n      Load Testing Tools\\n      Security Testing\\n      A B Testing\\n      Service Scalability\\n    Monitoring Management\\n      Azure Monitor\\n      Cost Management\\n      Policy Management\\n      Backup Recovery\\n      Performance Monitoring\\n    Operational Excellence\\n      Incident Response\\n      Change Management\\n      Service Level Agreements\\n      Training\\n      Provision and decommissioning\\n      Antipatterns\\n```\\n\\n## \ud83c\udfac Scenario\\n\\nThe scenario we are going to run through today involves creating an [Unmanaged](https://luke.geek.nz/azure/microsoft-azure-sandbox-design-considerations/) _(i.e., subscription-scoped)_ Sandbox _(Sandbox vending)_ per user or team, which could be a method of implementing it.\\n\\nTo go through this scenario, we will use the following Disciplines of [Cloud Adoption](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) to help separate elements.\\n\\n![Disciplines of Cloud Governance](Azure_Sandbox_CloudGovernance_Disciplines.png)\\n\\n| Discipline              | Description                                                                                                                  |\\n| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\\n| Cost Management         | Cost is a primary concern for cloud users. Develop policies for cost control for all cloud platforms.                        |\\n| Security Baseline       | Policies and enforcement apply those requirements across network, data, and asset configurations.                            |\\n| Resource Consistency    | Resources can be configured consistently to manage risks related to onboarding, drift, discoverability, and recovery.        |\\n| Identity Baseline       | Identity Baseline discipline focuses on ensuring that identity is consistently applied across cloud adoption efforts.        |\\n| Deployment Acceleration | Centralization, standardization, and consistency in approaches to deployment and configuration improve governance practices. |\\n\\n## \ud83d\udcb0 Cost Management\\n\\n### \ud83d\udcb0 FinOps\\n\\nWhen working with a Sandbox environment, you need to be aware of the costs associated with it. [FinOps principles](https://www.finops.org/framework/principles/) can be key.\\n\\n[Tags](https://learn.microsoft.com/azure/azure-resource-manager/management/tag-resources?WT.mc_id=AZ-MVP-5004796) can be key to help you showback/chargeback costs and help you assign resource owners.\\n\\nRecommended Tags for a Sandbox environment could be:\\n\\n- \\"costCenter\\": \\"sandbox\\"\\n- \\"costModel\\": \\"show-back\\"\\n- \\"environment\\": \\"sandbox\\"\\n- \\"resourceowner\\": \\"your_name\\"\\n- \\"project\\": \\"sandbox_project\\"\\n\\nAlthough the Sandbox environment is for learning, you still need to be aware of its associated costs and keep these as lean as possible.\\n\\nYou can also implement Cost Management and Governance workbooks that allow Sandbox users access to interactive dashboards to help them understand their costs and usage.\\n\\n- [Governance workbook](https://microsoft.github.io/finops-toolkit/governance-workbook) - Monitor the governance posture of your Azure environment. Leverage recommendations to address compliance issues.\\n- [Cost optimization workbook](https://microsoft.github.io/finops-toolkit/optimization-workbook) - Give your engineers a single pane of glass for cost optimization with this handy Azure Monitor workbook.\\n\\n### \ud83d\udcb0 Budgets\\n\\nImplement [Budgets](https://learn.microsoft.com/azure/cost-management-billing/costs/tutorial-acm-create-budgets?tabs=psbudget&WT.mc_id=AZ-MVP-5004796) for each Sandbox, assigned to the Sandbox owner.\\n\\nMonthly resource spending should be forecast initially and amended as the footprint changes.\\n\\nBudget alerts are set up to highlight unplanned spending, not to prevent it _(i.e., they are alerting thresholds, not limits)_.\\n\\nEach Sandbox environment could start with a consistent Budget and can be adjusted IF required as an exception.\\nBudgets are intended to drive Sandbox owners to keep their costs under control by having the information on hand.\\n\\n## \ud83d\udd12 Security Baseline\\n\\nSecurity is a key concern for any environment, and the Sandbox environment is no different; however, there are some tradeoffs. The key to a successful Sandbox environment is that it\'s an environment for learning, so the level of security you would adopt should be less. However, there are some stop gaps that should be implemented.\\n\\n### \ud83d\udee1\ufe0f Defender for Cloud\\n\\n[Defender for Cloud](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-cloud-introduction?WT.mc_id=AZ-MVP-5004796) is a cloud-native application protection platform (CNAPP) that consists of security measures and practices designed to protect cloud-based applications from various cyber threats and vulnerabilities.\\n\\n![Defender for Cloud](defender-for-cloud-pillars.png)\\n\\nDefender for Cloud should be enabled on all Sandboxes to help protect against threats and increase visibility. It is a great learning tool.\\n\\nAs people learn how to use Azure technologies, they may not necessarily know how to secure them or how their services might be adapted for a more secure environment. Defender for Cloud helps increase the knowledge around resources in alignment with [current security best practices](https://learn.microsoft.com/azure/defender-for-cloud/alerts-overview?WT.mc_id=AZ-MVP-5004796).\\n\\n> \\"While the security team is responsible for improving the security posture, team members might not actually implement security recommendations.\\n> Using governance rules driven by the security team helps you to drive accountability and an SLA around the remediation process.\\"\\n\\n:::tip\\n[Governance rules](https://learn.microsoft.com/en-us/azure/defender-for-cloud/governance-rules?WT.mc_id=AZ-MVP-5004796) are key.\\n:::\\n\\nFor example, using Tags, you could assign resource owners to resources and help drive accountability, alerts, and remediation. Defender for Cloud can be a great learning tool by informing your Sandbox users of potential security issues.\\n\\nYou should also consider possible scenarios, such as the Sandbox environment being one method to exfiltrate data, so make sure you look at [Purview](https://learn.microsoft.com/purview/?WT.mc_id=AZ-MVP-5004796) and [data sensitivity](https://learn.microsoft.com/en-us/azure/defender-for-cloud/defender-for-storage-data-sensitivity?WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\udcdc Azure Policy\\n\\n[Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796) is a service in Azure that you use to create, assign, and manage policies. These policies enforce different rules and effects over your resources so those resources stay compliant with your corporate standards and service level agreements.\\n\\n:::tip\\nI highly recommend that you deploy your policies and initiatives using [Azure Enterprise Policy as Code](https://azure.github.io/enterprise-azure-policy-as-code/).\\n:::\\n\\nThere are tradeoffs with the type of Azure policies you deploy in a Sandbox environment vs what you would deploy in your Production or even Dev/Test environment; recommended policies as a base I would recommend are:\\n\\nReference: [ALZ policies](https://github.com/Azure/Enterprise-Scale/blob/main/docs/wiki/ALZ-Policies.md)\\n\\n| Assignment Name                    | Definition Name                                  | Policy Type                         | Description                                                                                                                                                                                                                                      | Effect(s) |\\n| ---------------------------------- | ------------------------------------------------ | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------- |\\n| **Enforce ALZ Sandbox Guardrails** | **Enforce policies in the Sandbox Landing Zone** | `Policy Definition Set,` **Custom** | This initiative will help enforce and govern subscriptions that are placed within the Sandbox Management Group. Policies included: <ul><li>Deny vNET peering across subscriptions</li><li>Deny the deployment of vWAN/ER/VPN gateways.</li></ul> | Enforce   |\\n\\n| Assignment Name                           | Definition Name                                         | Policy Type                         | Description                                                                                                                                                                                                                                                                                                  | Effect(s) |\\n| ----------------------------------------- | ------------------------------------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------- |\\n| **Enforce ALZ Decommissioned Guardrails** | **Enforce policies in the Decommissioned Landing Zone** | `Policy Definition Set,` **Custom** | This initiative will help enforce and govern subscriptions that are placed within the decommissioned Management Group as part of your Subscription decommissioning process. Policies included: <ul><li>Deny the deployment of new resources</li><li>Deploy an auto VM shutdown policy at UTC 00:00</li></ul> | Enforce   |\\n\\nThe Decommissioned Management group is where you would move your Sandbox subscriptions once they are no longer required and you want to decommission them.\\n\\n![Azure Policy - Azure Landing Zones - Initiative](AzurePolicy_ALZSandboxIniiative.png)\\n\\nKey policies are part of the ALZ Sandbox Guardrails, which are essentially the policies to restrict the deployment of network resources. These policies would allow you to link virtual networks to on-premises, peer-to-peer virtual networks and allow Sandbox users to connect to production virtual networks or virtual networks outside your organization\'s control.\\n\\n### \ud83d\udcdc Terms of use\\n\\nMake sure you don\'t forget about the People/Processes of your sandbox. You should implement Terms of Use for the Sandbox environment so that users understand the rules and regulations of the environment, such as \'Do not store Production data\'.\\n\\nYou can combine this directly into the Azure Portal login experience by using [Microsoft Entra terms of use](https://learn.microsoft.com/en-us/entra/identity/conditional-access/terms-of-use?WT.mc_id=AZ-MVP-5004796).\\n\\n![Microsoft Entra - Terms of use](user-tou.png)\\n\\n## \ud83d\udd04 Resource Consistency\\n\\nResource Consistency is key. It ensures that resources are configured consistently to manage onboarding, drift, discoverability, and recovery risks.\\n\\nThis is where a lot of automation comes into play to prevent drift and inconsistencies in creating (and decommissioning) a Sandbox environment.\\n\\n![Azure DevOps Pipeline - Sandbox](ResourceConsistency_SBX_ADOPipelines.png)\\n\\nIdeally, you would have a user interface where a user could request a Sandbox environment _(I will talk more about this in the deployment acceleration section)_, which would trigger an Azure DevOps pipeline or GitHub action, as an example, to create the environment.\\n\\nUsing Infrastructure as Code _(IaC)_, you can ensure that the environment is consistent and repeatable and can be provisioned and decommissioned.\\n\\n### \ud83d\udcda Enterprise Policy as Code\\n\\n[Enterprise Policy as Code or EPAC](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/policy-management/enterprise-policy-as-code?WT.mc_id=AZ-MVP-5004796) for short comprises a number of scripts which can be used in CI/CD based system or a semi-automated use to deploy Policies, Policy Sets, Assignments, Policy Exemptions and Role Assignments.\\n\\n:::info\\nThis is a solution created by Microsoft employees but without Microsoft\'s official support as an open-source initiative.\\n:::\\n\\nEPAC is designed for organizations with a large number of Policies, Policy Sets, and Assignments. It is also designed for organizations with multiple tenants and/or environments. The solution primarily consists of PowerShell scripts intended to run operations such as policy planning, deployment, and sync of external policies, such as Enterprise Landing Zone policies, into the EPAC repo.\\n\\nEnterprise Policy as Code can also be used to control exemptions. An example is running this nightly; any policies or exemptions not in the repository are removed.\\n\\nEnterprise Policy as Code could be used for Sandbox environments, providing an opportunity to clean up outdated policies, assignments, and exemptions and maintain consistency.\\n\\n![Enterprise Policy as Code - Sandbox](SandboxPolicy_EPAC.gif)\\n\\nFeel free to refer a blog post, I did previously on [Enterprise Policy as Code, with Azure DevOps](https://luke.geek.nz/azure/enterprise-policy-code-azure-devops/).\\n\\n### \ud83d\udee0\ufe0f Azure DevOps/GitHub\\n\\n[Azure DevOps](https://azure.microsoft.com/products/devops?WT.mc_id=AZ-MVP-5004796) or [GitHub](https://github.com/), both offer features, such as Boards/Discussions, Code Repos, Pipelines _(or Actions)_ which can be used as a platform, for IaC _(Infrastructure as Code)_ and CI/CD _(Continous Integration and Continous Deployment)_, and is definitely recommended to be factored in your Sandbox design, when it comes to automation and desired state of your Sandbox environment.\\n\\nThe following automation recommendations assume you have access to Azure DevOps or GitHub _(or similar)_.\\n\\n### \ud83c\udfbc Symphony\\n\\nA large part of the Sandbox vending is a DevOps pipeline and connectivity to Azure that allows Terraform and the pipeline in Azure DevOps to connect to make changes in Azure. Terraform is the car, and Symphony is the road that it travels on.\\n\\n[Symphony](https://github.com/microsoft/symphony) is a framework and set of patterns and best practices for developing, testing, and deploying infrastructure on Azure using Infrastructure as Code _(IAC)_ It includes modern DevOps practices for IAC such as Main and Pull Request workflows, IaC Code Validation & Linting, Automated Testing, Security Scanning, Multi-environment deployments, modules dependencies and more. A mature workflow for IAC not only automates the deployment of the IAC resources but also incorporates engineering fundamentals, resource validation, dependency management, test execution, security scanning, and more.\\n\\nThe Symphony pipelines for Azure DevOps and Terraform, for example, are:\\n\\n| Pipeline Name                        | Comments                                                                                                                                                                                                                                                                                                                                                                                                             |\\n| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| pipeline.ci.terraform.yml            | This YAML file defines a Continuous Integration (CI) pipeline for Azure DevOps, which triggers changes to the main branch, sets up certain variables, and executes several stages, including validation of configuration and preview deployment using Terraform templates. This file is the command file, triggering the other pipelines.                                                                            |\\n| pipeline.destroy.terraform.yml       | This YAML file defines a pipeline in Azure DevOps that uses a specific agent image to run a job based on a template for destroying a Terraform environment, with the environment details and key vault information passed as parameters. The pipeline does not trigger automatically on any branch or pull request changes.                                                                                          |\\n| template.terraform.previewdeploy.yml | This YAML file defines a job in an Azure DevOps pipeline that sets up a clean workspace, installs Terraform and Go, retrieves secrets from an Azure Key Vault, updates network rules and storage account settings, and executes a Terraform plan and apply operation. It also cleans up by removing the agent IP from the Key Vault and denying public access to the Terraform DevOps Storage after the job is done. |\\n| template.terraform.report.yml        | This YAML file defines a job in an Azure DevOps pipeline that, depending on a condition, cleans the workspace, retrieves secrets from an Azure Key Vault, and executes a bash script to backup the state files of a specified environment using Terraform.                                                                                                                                                           |\\n| template.terraform.test.yml          | This YAML file defines an end-to-end test job in an Azure DevOps pipeline that sets up a clean workspace, installs Terraform and Go, retrieves secrets from an Azure Key Vault, adds the agent IP to the Key Vault and storage account, runs tests, publishes the test results, and finally removes the agent IP from the Key Vault and storage account.                                                             |\\n| template.terraform.validate.yml      | This YAML file defines a validation job in an Azure DevOps pipeline that sets up a clean workspace, installs necessary tools, runs GitLeaks for code analysis, adds the agent IP to a Key Vault, runs Terraform lint and validate, optionally runs layer tests, publishes test results, and finally removes the agent IP from the Key Vault and denies public access to the Terraform DevOps Storage.                |\\n\\nUsing a framework such as Symphony gives you a good starting point for your deployments.\\n\\n![Symphony - Prestep](Azure_Sandbox_Symphony_Prestep.png)\\n![Symphony - Preview & Deploy](Azure_Sandbox_Symphony_PreviewandDeploy.png)\\n\\n### \ud83c\udfd7\ufe0f Terraform\\n\\n[Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796) and [Terraform](https://www.terraform.io/) are both tools used for infrastructure-as-code (IaC), which means they allow you to define and manage your cloud infrastructure using code. Both are declarative, meaning you define the desired state of your infrastructure and the tool figures out how to make it so. Bicep is more focused on Microsoft Azure, while Terraform is more focused on multi-cloud.\\n\\nFor this article, I will focus on Terraform, based purely on work done with it around Sandbox provisioning and decommissioning. The Terraform state file helps with a lot of the Sandbox decommissioning aspect.\\n\\nTerraform modules that can be used during Sandbox provisioning are:\\n\\n| Name                     | Link                                                                                                                                           |\\n| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------- |\\n| azuread                  | [https://registry.terraform.io/providers/hashicorp/azuread/latest](https://registry.terraform.io/providers/hashicorp/azuread/latest)           |\\n| azurerm                  | [https://registry.terraform.io/providers/hashicorp/azurerm/latest](https://registry.terraform.io/providers/hashicorp/azurerm/latest)           |\\n| random                   | [https://registry.terraform.io/providers/hashicorp/random/latest](https://registry.terraform.io/providers/hashicorp/random/latest)             |\\n| time                     | [https://registry.terraform.io/providers/hashicorp/time/latest](https://registry.terraform.io/providers/hashicorp/time/latest)                 |\\n| Azure/lz-vending/azurerm | [https://registry.terraform.io/modules/Azure/lz-vending/azurerm/latest](https://registry.terraform.io/modules/Azure/lz-vending/azurerm/latest) |\\n\\nThe real foundation of Sandbox vending is the [Azure/terraform-azurerm-lz-vending](https://github.com/Azure/terraform-azurerm-lz-vending) Terraform module.\\n\\nIt can be used to create Azure resources according to the landing zone _(Sandbox)_ vending pattern. It can be used to provision resources such as subscriptions, management group associations, role assignments, and virtual networks.\\n\\nHere is an example _(partial snippet of an overall solution)_ of how you COULD use the module by having multiple YAML files per Sandbox and using a for_each loop to create each Sandbox environment:\\n\\n```hcl\\n# This module is used to create resources in Azure according to the landing zone vending pattern.\\n\\nmodule \\"lz_vending\\" {\\n  # The source and version of the module.\\n  source  = \\"Azure/lz-vending/azurerm\\"\\n  version = \\"4.0.2\\"\\n\\n  # The for_each loop allows us to create multiple instances of this module, one for each item in the landing_zone_data_map.\\n  for_each = local.landing_zone_data_map\\n\\n  # The location where the resources will be created.\\n  location = var.location\\n\\n  # Subscription variables.\\n  # Enables the resource provider creation\\n  subscription_register_resource_providers_enabled = true\\n  # The map of resource providers to register.\\n  subscription_register_resource_providers_and_features = var.subscription_register_resource_providers_and_features\\n  # Enables the creation of a subscription alias.\\n  subscription_alias_enabled = true\\n  # The billing scope for the subscription.\\n  subscription_billing_scope = \\"/providers/Microsoft.Billing/billingAccounts/0000000/enrollmentAccounts/${each.value.billing_enrollment_account}\\"\\n  # The display name for the subscription. It is converted to lowercase to maintain consistency.\\n  subscription_display_name = (\\"SUB-Contoso-${each.value.name}_${var.env}-001\\")\\n  # The alias name for the subscription. It is also converted to lowercase.\\n  subscription_alias_name = (\\"SUB-Contoso${each.value.name}_${var.env}-001\\")\\n  # The workload type for the subscription.\\n  subscription_workload = each.value.workload\\n\\n  # Management group association variables.\\n  # Enables the association of the subscription with a management group.\\n  subscription_management_group_association_enabled = true\\n  # The ID of the management group to associate with the subscription.\\n  subscription_management_group_id = each.value.management_group_id\\n  # Tags to apply to the subscription. The Owner tags are set to the owner of the workload.\\n  subscription_tags = merge(\\n    var.tags,\\n    {\\n      \\"resource-owner\\" = each.value.owner,\\n      \\"expiry_date\\"    = each.value.expiry_date\\n    },\\n\\n  )\\n\\n  # Role assignment variables.\\n\\n  role_assignment_enabled = true\\n\\n  role_assignments = {\\n    # using role definition name, created at subscription scope\\n    contrib_user_sub = {\\n      principal_id   = azuread_group.contributor[each.key].id\\n      definition     = \\"Contributor\\"\\n      relative_scope = \\"\\"\\n    },\\n\\n  }\\n  # Virtual network variables.\\n  # Enables the creation of a virtual network.\\n  virtual_network_enabled = true\\n  # Enables the creation of a network watcher resource group.\\n  network_watcher_resource_group_enabled = true\\n  # The details of the virtual network to create.\\n  virtual_networks = {\\n\\n    one = {\\n      # The name of the virtual network. It is converted to lowercase.\\n      name                        = lower(\\"vnet-${var.env}-${each.value.name}\\")\\n      resource_group_lock_enabled = false\\n\\n      # The address space for the virtual network.\\n      address_space = [\\"192.168.1.0/24\\"]\\n      # The name of the resource group where the virtual network will be created. It is converted to lowercase.\\n      resource_group_name = lower(\\"rg-contoso-networks-${var.env}-aue-001\\")\\n      # The location where the virtual network will be created.\\n      location = var.location\\n\\n    }\\n\\n  }\\n\\n}\\n```\\n\\nThis will create a Subscription, Resource Group, and define Contributor access, based on data from a data map, that looks at each YAML file in a folder.\\n\\n\\"for_each = local.landing_zone_data_map\\" - This is the key part, as it allows you to create multiple instances of the module, one for each item in the landing_zone_data_map.\\n\\n```hcl\\nlocals {\\n  # landing_zone_data_dir is the directory containing the YAML files for the landing zones.\\n  # It is set to the root directory of the current Terraform project.\\n  landing_zone_data_dir = path.root\\n  # landing_zone_data_map is a map that stores the decoded YAML data from each landing zone file.\\n  # It uses a for loop to iterate over each file in the landing_zone_files list.\\n  # For each file, it reads the contents of the file using the file function and decodes the YAML data using the yamldecode function.\\n  # The file path is constructed by concatenating the landing_zone_data_dir and the file name.\\n  # The decoded YAML data is then stored in the landing_zone_data_map with the file name as the key.\\n  landing_zone_data_map = {\\n    for f in local.landing_zone_files :\\n    f => yamldecode(file(\\"${local.landing_zone_data_dir}/${f}\\"))\\n  }\\n  # landing_zone_files is the list of landing zone YAML files to be processed.\\n  # It uses the fileset function to find all files in the landing_zone_data_dir that match the pattern \\"landing_zone_*.yaml\\".\\n  landing_zone_files = fileset(local.landing_zone_data_dir, \\"sbx_landing_zone_*.yaml\\")\\n}\\n```\\n\\nHere is an example YAML file:\\n\\n```yaml\\nname: SandboxOwner1 # Name of the landing zone. No spaces.\\nworkload: DevTest # Type of subscription for the landing zone (DevTest vs Production (PAY AS YOU GO)). Most Sandboxes will be Production (Pay As You Go), unless all users have a Visual Studio Enterprise subscription.\\nowner: admin@contoso.com # Owner of the landing zone\\nbilling_enrollment_account: 123456 # Billing enrollment account for the landing zone. Limit of 5000 API requests for new subscriptions. Later Subscriptions may need a new billing enrollment.\\nmanagement_group_id: Unmanaged-Sandbox # Management group ID for the landing zone\\nbudget_amount: 1000 # Budget amount for the landing zone, numerical value\\nexpiry_date: 2024-10-16 # The date when the landing zone will expire, the default should be 3 months from the date of provisioning. After this date, resources may be automatically cleaned up.\\ncost-centre: Contoso (Cloud) # Cost center for the landing zone\\ncost-model: show-back # Cost model for the landing zone\\n```\\n\\nThis is a very basic example, but it gives you an idea of how you could use the module to create a Subscription and Resource Group and define Contributor access based on data from a data map that looks at each YAML file in a folder. The Expiry date is used as part of the Sandbox decommissioning process and also added as a Tag to the Subscription, along with the Owner, which can also be used as an email recipient of a Budget.\\n\\n:::tip\\nThere is a [limit](https://learn.microsoft.com/azure/cost-management-billing/manage/programmatically-create-subscription-enterprise-agreement?tabs=rest&WT.mc_id=AZ-MVP-5004796#limitations-of-azure-enterprise-subscription-creation-api) to the number of API requests you can make to create a new subscription, so you may need to create a new billing enrollment account if you hit this limit, which is why its included in the yaml.\\n:::\\n\\nUsing this method, you can create a new Sandbox environment by creating a new YAML file, running the Terraform apply command, and decommissioning the environment by removing the YAML file and running the Terraform apply command again.\\n\\nFor the decommissioning, you can run a separate pipeline to check the expiry of the YAML file, then delete it if it\'s past its due date, which could then trigger the removal.\\n\\n## \ud83d\udc64 Identity Baseline\\n\\nTo connect to your Sandbox environment, you need to ensure that you have the correct Identity Baseline in place.\\n\\nI recommend you merge your Sandbox identity access, i.e. Entra ID Groups, Access packages, etc., into your IaC pipelines so that access is granted automatically when a new Sandbox environment is created and removed when no longer required.\\n\\n### \ud83d\udc65 Entitlement Management\\n\\n[Entitlement management](https://learn.microsoft.com/en-us/entra/id-governance/entitlement-management-overview?WT.mc_id=AZ-MVP-5004796) is an identity governance feature that enables organizations to manage identity and access lifecycle at scale, by automating access request workflows, access assignments, reviews, and expiration.\\n\\n![Entitlement Management](Azure_Sandbox_IAM_EntitlementManagement.png)\\n\\n### \ud83d\udce6 Access packages\\n\\nWith an access package, an administrator or delegated access package manager lists the resources (groups, apps, and sites), and the roles the users need for those resources.\\nAccess packages also include one or more policies. A policy defines the rules or guardrails for assigning access to an access package. Each policy can be used to ensure that only the appropriate users can have access to assignments. Access is time-limited and will expire if not renewed.\\n\\n![Access Packages](Azure_Sandbox_IAM_AccessPackages.png)\\n\\nYou can use Access packages to grant access to your Sandbox environment and then remove access when the environment is no longer required.\\n\\nAccess Packages can be used to delegate access to each Sandbox environment to its owner. This allows Sandbox owners, who may have financial delegation, to control who has access to their environment and remove that access when no longer required.\\n\\n### \ud83d\udd12 Conditional Access\\n\\nMake sure you factor Conditional Access into your Sandbox design by making sure that only the users who may be using compliant devices or from trusted networks can access the environment.\\n\\n## \ud83d\ude80 Deployment Acceleration\\n\\n### \ud83d\udda5\ufe0f Onboarding self-service\\n\\nOnboarding of a new Sandbox environment could be automated from start to end, using a self-service portal. An example is:\\n\\nA [PowerApp](https://learn.microsoft.com/power-apps/powerapps-overview?WT.mc_id=AZ-MVP-5004796) that allows a user to request a new Sandbox environment, by filling in a form, that then triggers a [PowerAutomate](https://learn.microsoft.com/power-automate/?WT.mc_id=AZ-MVP-5004796) flow, to gain approval, and then creates a new YAML file, in a folder, that then triggers a Terraform apply command, to create the new Sandbox environment.\\n\\n![Cloud Landing Zone Request - PowerApp](Azure_SandboxPowerApps.png)\\n\\n## \ud83d\udcd6Reference Links\\n\\n* [Azure Sandbox Environment - Turbo360 Podcast](https://www.youtube.com/watch?v=dkNAnw1InA0)"},{"id":"azure/change-timezone-azure-container-app","metadata":{"permalink":"/azure/change-timezone-azure-container-app","source":"@site/blog/2024-04-06-Update-Timezone-AzureContainerApp/index.mdx","title":"Change the Timezone of your Azure Container App","description":"This article provides a guide on how to change the timezone of your Azure Container App. It includes reasons why you might need to do this, a step-by-step process, and references for further reading.","date":"2024-04-06T08:52:05.548Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.065,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Change the Timezone of your Azure Container App","metaDescription":"This article provides a guide on how to change the timezone of your Azure Container App. It includes reasons why you might need to do this, a step-by-step process, and references for further reading.","date":"2024-04-06T08:52:05.548Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"mariushostingDockerTimeZoneNZ.png"},"slug":"azure/change-timezone-azure-container-app","keywords":["azure","cloud","containers","ContainerApps","CloudNative","timezone"],"description":"This article provides a guide on how to change the timezone of your Azure Container App. It includes reasons why you might need to do this, a step-by-step process, and references for further reading."},"unlisted":false,"prevItem":{"title":"Implementing a Sandbox Environment in Microsoft Azure","permalink":"/azure/implementing-sandbox-vending"},"nextItem":{"title":"Azure Container Apps - Overview","permalink":"/azure/azure-container-apps-overview"}},"content":"Updating the timezone of your Azure Container Apps containers might not be a common operation, but there are scenarios where it could be necessary.\\n\\n\x3c!-- truncate --\x3e\\n\\nExample scenarios on why you may want to update the Timezone (from UTC) to another timezone may be because of the following:\\n\\n* Application requirements\\n* Logging and debugging concerns\\n* Compliance and regulations\\nOr general application testing and development\\n\\nYou could have this built into the container itself, which would be preferred to keep a consistent experience and to work with any time offset that may be needed (daylight savings, etc), but you can update the container environment.\\n\\n\ud83d\udcfa To demo this, I am using a Container App Environment and a Container App using the \'Simple hello world container\' quickstart image.\\n\\nMy revision mode is set to Single; however, the same steps can be applied with multiple revision modes. Just ensure you test and update the right revision or the date change may not be applied.\\n\\nTo update the timezone, we will be the \'TZ\' Environment variable, and this change will trigger a new Revision.\\n\\nFirst, let us check the current timezone of the container app by running the following command *(on a running Container App)* from the Console:\\n\\n```PowerShell\\n    date\\n```\\n\\n![Check Time/date](Update_AzContainerAppTimeZone-Before.gif)\\n\\nThis will return the current date and time in UTC.\\n\\nNow, let us update the Timezone. To do that, we need to specify the timezone in the TZ database format; to find this format, we can use sites like [Wikipedia](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones) or [mariushosting Docker Time Zone (TZ)](https://timezone.mariushosting.com/), as I am based in New Zealand, I will be using: Pacific/Auckland.\\n\\n![Timezone - Pacific/Auckland](mariushostingDockerTimeZoneNZ.png)\\n\\nTo do this:\\n\\n:::danger\\n\u26a0\ufe0f If you\'re using Single Revision mode: This may trigger an outage, during the creation of a new revision, make sure you test this in a safe environment.\\n:::\\n\\n1. **Navigate** to your **Container App** in the Azure Portal\\n2. Click on **Containers**\\n3. Click **Edit and Deploy**\\n4. Click your **Container image**\\n5. Navigate down to **Environment variables**\\n6. **Add**: **TZ** with the value of your **timezone** *(Manual Entry, although you could also have this as a Secret in a KeyVault)*.\\n7. Click **Save**\\n8. Click **Create** to create your Revision\\n9. You can then navigate back to the Console and type in Date to confirm the timezone has been updated.\\n\\n![Update Azure Container Apps - Timezone](Update_AzContainerAppTimeZone.gif)\\n\\n\ud83d\udcd6 References: \\n\\n* [Default timezone in Docker containers](https://support.circleci.com/hc/en-us/articles/115015771347-How-do-I-set-the-timezones-in-Docker-images)\\n* [How do I change timezone in a docker container?](https://stackoverflow.com/questions/57607381/how-do-i-change-timezone-in-a-docker-container)"},{"id":"azure/azure-container-apps-overview","metadata":{"permalink":"/azure/azure-container-apps-overview","source":"@site/blog/2024-04-02-AzContainerApps-Overview/index.mdx","title":"Azure Container Apps - Overview","description":"Overview of Azure Container Apps and some of the Cloud Native ecosystem.","date":"2024-04-02T09:15:09.104Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":21.58,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Container Apps - Overview","metaDescription":"Overview of Azure Container Apps and some of the Cloud Native ecosystem.","date":"2024-04-02T09:15:09.104Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"ACA_BlogHeading_Overview.png"},"slug":"azure/azure-container-apps-overview","keywords":["azure","cloud","containers","ContainerApps","CloudNative","dapr","keda"],"description":"Overview of Azure Container Apps and some of the Cloud Native ecosystem."},"unlisted":false,"prevItem":{"title":"Change the Timezone of your Azure Container App","permalink":"/azure/change-timezone-azure-container-app"},"nextItem":{"title":"Book Review Cloud Solution Architects Career Master Plan","permalink":"/azure/book-review-cloud-solution-architects-career-master-plan"}},"content":"[Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796), [KEDA](https://keda.sh/), and [Dapr](https://dapr.io/) \u2014 what are these, and how do they work together? This article aims to give a high-level insight into Azure Container Apps and some of the [Cloud Native ecosystem](https://landscape.cncf.io/) that surrounds this container orchestrator.\\n\\n![Azure Container Apps - Overview](ACA_BlogHeading_Overview.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n:::info\\n\u201c[Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) is a fully managed [Kubernetes-based](https://kubernetes.io/) application platform that helps you deploy apps from code or containers without orchestrating complex infrastructure.\u201d\\n:::\\n\\n## \u2601\ufe0f Understanding Azure Container Apps\\n\\nIn this section, we will take a look at the following:\\n\\n* Overview of Microservices\\n* Getting started with Container APPS\\n* Differences between ACA and other Azure container solutions\\n\\n![Understanding Azure Container Apps](ACA_Overview_UnderstandingAzContainerApps.png)\\n\\n:::info\\n[Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) is a fully managed serverless container service for building and deploying modern apps at scale\xa0- without managing infrastructure. It is built on top of Kubernetes and provides a simple, fully managed experience for deploying containerized applications.\\n:::\\n\\n### \ud83e\udd14 Monoliths, Microservices and Containers? Huh?\\n\\nTo understand why we might need Container Apps, we need to go back to some of the elements of software development. \\n\\nContainers can be a valuable tool for deploying and managing microservices, but they are not always necessary. Let\'s explore why you may not need containers for microservices and how they can generally help.\\n\\nMicroservices architecture is a software development approach where an application is built as a collection of small, loosely coupled services that can be developed, deployed, and scaled independently. Each microservice focuses on a specific business capability and communicates with other microservices through well-defined APIs.\\n\\n:::info\\nWhile containers are not always necessary for deploying microservices, they offer benefits such as isolation, portability, scalability, and deployment consistency. Evaluating your specific requirements, existing infrastructure, and resource constraints will help determine whether containers are the right choice for your microservices architecture, and is intended to help you understand how it could be used, especially around some of the Cloud Native integration components.\\n:::\\n\\n**Why you may not need containers for microservices:**\\n\\n* Simplicity: If your microservices are developed using a single programming language or framework and can be easily deployed on traditional servers or virtual machines, containers may not be necessary. In such cases, deploying microservices directly on servers or virtual machines can be simpler and more straightforward.\\n* Existing Infrastructure: If you already have a well-established infrastructure with efficient deployment processes and tools in place, containers may not be required. Leveraging your existing infrastructure can save time and effort in adopting and managing container technologies.\\n* Resource Constraints: Containers introduce additional overhead in terms of resource utilization. If you have limited resources or strict resource constraints, deploying microservices directly on servers or virtual machines may be more efficient.\\n\\n**However, how containers can generally help with microservices:**\\n\\n* Isolation: Containers provide a lightweight and isolated runtime environment for each microservice. This isolation ensures that changes or issues in one microservice do not affect others, improving overall system stability.\\n* Portability: Containers encapsulate the dependencies and runtime environment required by a microservice, making it highly portable. Microservices packaged as containers can be easily deployed and run on different platforms, such as local development machines, cloud environments, or on-premises servers.\\n* Scalability: Containers enable easy scaling of microservices. With container orchestration platforms like Kubernetes, you can dynamically scale the number of containers running a specific microservice based on demand, ensuring optimal resource utilization.\\n* Deployment Consistency: Containers provide a consistent deployment model, ensuring that the microservice runs the same way across different environments. This consistency simplifies the deployment process and reduces the chances of configuration-related issues.\\n\\n\\nFirst, let\'s take a look at Monoliths.\\n\\n#### \ud83c\udfe2 Monoliths\\n\\n![Azure Container Apps - What is a Monolith?](ACA_Overview_UnderstandingWhatIsAMonolith.PNG)\\n\\n\\"In software engineering, a monolithic application is a single unified software application that is self-contained and independent from other applications but typically lacks flexibility\\"\u2014Wikipedia.\\n\\n![Monolith shortcomings](ACA_Overview_Monolith_Shortcomings.PNG)\\n\\nGenerally, monolithic architectures suffer from drawbacks that can delay application development and deployment. These drawbacks become especially significant when the product\'s complexity increases or when the development team grows in size.\\n\\nThe codebase of monolithic applications can be difficult to understand because it may be extensive. This can make it difficult for new developers to modify the code to meet changing business or technical requirements. \\n\\nAs requirements evolve or become more complex, it becomes difficult to correctly implement changes without hampering the code\'s quality and affecting the application\'s overall operation.\\n\\n\ud83d\udcd6 References: \\n\\n* [monolithic architecture](https://www.techtarget.com/whatis/definition/monolithic-architecture)\\n* [Challenges and patterns for modernizing a monolithic application into microservices](https://developer.ibm.com/articles/challenges-and-patterns-for-modernizing-a-monolithic-application-into-microservices/)\\n* [Microservices with Azure Container Apps](https://learn.microsoft.com/azure/container-apps/microservices?WT.mc_id=AZ-MVP-5004796)\\n\\nNext up is Microservices, a more modular approach to software development.\\n\\n#### \ud83c\udfd7\ufe0f Microservices\\n\\nAt its core, the concept of the microservices architecture is an approach to application development in which a large application is built as a collection of modular and cooperating services. \\n\\nHowever, in order to successfully achieve a robust microservices architecture, the underlying infrastructure must also be correctly designed. In fact, due to the distributed nature of the microservices architecture, the line between what used to be separate application details and implementation details grows blurrier.\\n\\nContainers simplify the continuous deployment of microservices.\\n\\n![What are Microservices](ACA_Overview_UnderstandingWhatIsMicroservices.PNG)\\n\\n\ud83d\udcd6 References: \\n\\n* [Of Microservices & Containers](https://hackernoon.com/https-medium-com-spruha-pandya-of-microservices-containers-6f0ea25dac3)\\n\\n#### \ud83d\ude80 Deployment modes\\n\\nThe deployment of microservices and traditional monolith applications is usually done separately, which is an oversimplification.\\n\\n![Monolith shortcomings](ACA_Overview_UnderstandingMonolithMicroservices.gif)\\n\\n| Aspect | Microservices | Monolith Application |\\n|--------|---------------|----------------------|\\n| Service instantiation | Instantiate services instead of spinning up new machines => faster | Spinning up new machines for each instance => slower |\\n| Hardware utilization | Better utilization of hardware | Less efficient utilization of hardware |\\n| Cost | Optimized cost due to efficient resource usage | Potentially higher cost due to inefficient resource usage |\\n| Time to market | Lower time to market: more confident when it comes to upgrading a service | Higher time to market: less confident when it comes to upgrading a service |\\n| Independence | Services are independently implemented, deployed, scaled, and versioned | All components are tightly coupled and scaled together |\\n\\n**So, let\'s take a look at how Microservices and Containers can work together.**\\n\\nLet\u2019s delve into the differences between\xa0microservices\xa0and\xa0containers:\\n\\n| Aspect | Microservices | Containers |\\n|--------|---------------|------------|\\n| Definition | Microservices are an architectural style that breaks down an application into small, autonomous services. These services communicate via well-defined interfaces using lightweight APIs. | A container is a technology that bundles an application along with all its dependencies into a package. This package allows the application to be deployed consistently across different environments, abstracting away differences in operating systems and underlying infrastructure. |\\n| Structure | Microservices are self-contained and encapsulate their logic. They interact through well-defined interfaces, allowing independent development and deployment. | Containers ride on a host operating system, similar to virtual machines (VMs), but they share the OS kernel. This makes them lighter and faster to boot than VMs. Containers are hosted on a container runtime, which enables multiple containers (each several megabytes in size) to run on a single server. |\\n| Popular Tools | Microservices can be developed with various programming languages and frameworks. | Docker is a well-known commercial container management solution, while Kubernetes (often referred to as K8s) is a widely used free and open-source container management system. |\\n| Pros | Enhance maintainability, testability, and scalability. Organized around business capabilities and are typically owned by small teams. | Excellent for packaging and deploying applications consistently, regardless of the environment. More lightweight and faster to boot than VMs. |\\n| Cons | Complexity in managing multiple services. Need for coordination and communication between services. | Requires knowledge of container management and orchestration tools. Potential security risks if not properly isolated. |\\n| Use Case | Suitable for building modular, scalable applications. | Provides the infrastructure for running microservices, ensuring consistent deployment and management across different environments. |\\n\\n![microservices\xa0and\xa0containers](ACA_Overview_UnderstandingMicroservicesandContainers.PNG)\\n\\nContainers provide the infrastructure for running microservices, ensuring consistent deployment and management across different environments.\xa0Microservices, on the other hand, define the architectural approach for building modular, scalable applications.\\n\\n### \ud83c\udf9b\ufe0f Container orchestration\\n\\n:::info\\nContainer orchestration is required to transition from deploying containers individually on a single host, to deploying complex multi-container apps on many machines. It requires a distributed platform, independent from infrastructure, that stays online through the entire lifetime of your application, surviving hardware failure and software updates.\\n:::\\n\\n![Container orchastration](ACA_Overview_UnderstandingContainerOrchastration.PNG)\\n\\nExisting Container orchestration solutions include the following:\\n\\n| Solution | Description | Pros | Cons |\\n|----------|-------------|------|------|\\n| Kubernetes | An open-source platform designed to automate deploying, scaling, and operating application containers. | Highly flexible and customizable. Large community support. | Complexity in setup and management. |\\n| Docker Swarm | Docker\'s own native container orchestration. | Easy to set up. Integrated into Docker CLI. | Limited functionality compared to Kubernetes. |\\n| Amazon ECS | A highly scalable, high-performance container orchestration service that supports Docker containers. | Deep integration with AWS services. | Only available on AWS. |\\n| Azure Kubernetes Service (AKS) | Managed Kubernetes service provided by Azure. | Deep integration with Azure services. Managed Kubernetes with less operational complexity. | Only available on Azure. |\\n| Azure Container Apps | A serverless container service that enables you to run containerized applications at scale. | Serverless, event-driven, and supports Linux containers. Deep integration with Azure services. | Only available on Azure. |\\n\\n**So, what is Container orchestration?**\\n\\n![Container orchastration](ACA_Overview_WhatisContainerOrchastration.gif)\\n\\nContainer orchestration\xa0plays a crucial role in managing and scaling containerized applications. Here are some reasons why it\u2019s essential:\\n\\n| Feature | Description |\\n|---------|-------------|\\n| Scaling and Load Balancing | Container orchestration tools like Kubernetes allow you to dynamically scale your application by adding or removing containers based on demand. Load balancing ensures that incoming requests are distributed evenly across containers, preventing overload on any single instance. |\\n| High Availability | Orchestration ensures that your application remains available even if individual containers fail. It automatically restarts failed containers or replaces them with healthy ones. |\\n| Service Discovery and DNS | Containers come and go, making it challenging to keep track of their IP addresses. Orchestration tools provide service discovery and DNS resolution, allowing containers to find each other by name. |\\n| Rolling Updates and Rollbacks | When deploying new versions of your application, orchestration allows for rolling updates. You can gradually replace old containers with new ones, minimizing downtime. In case of issues, you can easily roll back to a previous version. |\\n| Configuration Management | Orchestration tools manage environment variables, secrets, and other configuration settings for containers. This centralizes configuration management and ensures consistency. |\\n| Resource Optimization | Orchestration optimizes resource utilization by scheduling containers efficiently across nodes. It balances CPU, memory, and storage requirements. |\\n| Networking and Security | Orchestration handles networking, including creating virtual networks for communication between containers. It also manages security policies, access controls, and network segmentation. |\\n| Monitoring and Logging | Container orchestration platforms provide monitoring dashboards and logs. You can track performance, troubleshoot issues, and gain insights into your application. |\\n\\nContainer orchestration simplifies deployment, enhances reliability, and ensures efficient management of containerized applications.\\n\\n## \ud83d\udc51 Container Apps\\n\\n[Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796)\xa0is a\xa0serverless platform\xa0that simplifies running containerized applications\\n\\n1. Azure Container Apps allows you to focus on your application logic without worrying about server configuration, container orchestration, or deployment details.\\n2. It reduces operational complexity and saves costs by providing up-to-date server resources.\\n3. Common use cases include deploying API endpoints, handling background processing jobs, event-driven processing, and running microservices.\\n\\n![Azure Container Apps](ACA_Overview_AzContainerApps.gif)\\n\\n**You may be wondering what sort of applications you can build with Azure Container Apps.**\\n\\nHere are some common ones including:\\n* Deploying API endpoints\\n* Hosting background processing applications\\n* Handling event-driven processing\\n* Running microservices\\n\\nWith each of those applications, you can dynamically scale based on:\\n* HTTP traffic\\n* Event-driven processing\\n* CPU or memory load\\n* Any\xa0KEDA-supported scaler\\n\\n### \ud83e\udd14 What are the differences between Azure Container Apps and other Container solutions?\\n\\n:::info\\nAzure customers can easily deploy microservices using either Azure Kubernetes Service for flexible customized container solutions or Azure Container Apps for productivity-optimized, fully-managed serverless container solutions. Azure delivers app portability to support business growth powered by open source in the Kubernetes ecosystem. \\n:::\\n\\nAzure Container Apps is built on a foundation of powerful open-source technology to deliver the same microservices benefits as Kubernetes as a fully managed and serverless app hosting option. Behind the scenes, every container app runs on Azure Kubernetes Service, with Kubernetes Event Driven Autoscaling (KEDA), Distributed Application Runtime (Dapr), and Envoy deeply integrated in the hosting service. However, developers and operators are not exposed to this underlying Kubernetes infrastructure. The open-source-centric approach enables a path for teams to build microservices without having to overcome the concept and operational overhead of working with Kubernetes directly and enables continued app portability by leveraging open standards and APIs\\n\\n![Azure Conter Apps and AKS](ACA_Overview_UnderstandingAKSACA.PNG)\\n\\n![Azure Conter Apps and AKS](ACA_Overview_UnderstandingAKS.PNG)\\n\\n![Azure Conter Apps and AKS](ACA_Overview_UnderstandingAKSCompareACA.PNG)\\n\\n## \u26a1Building Azure Container Apps\\n\\nNow that we know where Azure Container Apps fits in the Cloud Native ecosystem let\'s take a look at potential tools for building Azure Container Apps.\\n\\n![Building Azure Container Apps](ACA_Overview_BuildingContainerAppsApps.PNG)\\n\\n### \ud83d\ude80 Azure Container App Deployment options\\n\\nAlthough anything that can connect to the Azure APIs can essentially create a [Container App Environment](https://learn.microsoft.com/azure/container-apps/environment?WT.mc_id=AZ-MVP-5004796), common tools are:\\n\\n* [Azure Portal Deployment](https://learn.microsoft.com/azure/container-apps/quickstart-portal?WT.mc_id=AZ-MVP-5004796)\\n* [Azure CLI deployment](https://learn.microsoft.com/azure/container-apps/containerapp-up?WT.mc_id=AZ-MVP-5004796)\\n* [ARM template deployment](https://learn.microsoft.com/azure/container-apps/azure-resource-manager-api-spec?tabs=arm-template&WT.mc_id=AZ-MVP-5004796)\\n* [Bicep template deployment](https://learn.microsoft.com/azure/templates/microsoft.app/containerapps?pivots=deployment-language-bicep&WT.mc_id=AZ-MVP-5004796)\\n* [Terraform template deployment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/container_app)\\n* [Pulumi](https://www.pulumi.com/registry/packages/azure-native/api-docs/app/containerapp/) or other API tools\\n\\n### \ud83c\udf10 Container App environments\\n\\nBut before you jump into the tools to create, you need to understand what you will be deploying and building, so let us take a look at [Container App Environments](https://learn.microsoft.com/azure/container-apps/environment?WT.mc_id=AZ-MVP-5004796).\\n\\n:::info\\nA Container app environment is a secure boundary around groups of container apps that share the same virtual network and write logs to the same logging destination.\\n\\nContainer app environments are fully managed, where Azure handles OS upgrades, scale operations, failover procedures, and resource balancing.\\n:::\\n\\n| Reasons to deploy container apps to the same environment | Reasons to deploy container apps to different environments |\\n|-----------------------------------------------------------|-----------------------------------------------------------|\\n| Manage related services                                   | Two applications never share the same compute resources   |\\n| Deploy different applications to the same virtual network | Two Dapr applications can\'t communicate via the Dapr service invocation API |\\n| Instrument Dapr applications that communicate via the Dapr service invocation API | |\\n| Have applications to share the same Dapr configuration | |\\n| Have applications share the same log analytics workspace | |\\n| Provide an existing virtual network when you create an environment | |\\n\\n![Container App Environment](ACA_Overview_ContainerAppEnvironments.gif)\\n\\n#### \ud83d\udce6 Container Types\\n\\nAzure Container Apps supports two types of Containers.\\n\\nApp Container *(Sidecar)* - The container that runs your application code\\nInit Containers - Containers that run before the app container starts\\n\\n![Azure Container Apps - Container Types](ACA_Overview_BuildingContainerAppContainerTypes.PNG)\\n\\n#### \ud83d\udce6 Container registry\\n\\nAzure Container Apps supports multiple container registries, including:\\n\\n* [Azure Container Registry](https://azure.microsoft.com/products/container-registry?WT.mc_id=AZ-MVP-5004796)\\n* [Docker Hub](https://hub.docker.com/) and other registries.\\n\\nPublic and private registries, i.e., Azure Container Registry, are on a private endpoint.\\n\\n![Azure Container Apps - Container Registries](ACA_Overview_BuildingContainerAppContainerRegistry.PNG)\\n\\n#### \ud83d\udd04 Container Revisions\\n\\n:::info\\nAzure Container Apps implements container app versioning by creating [revisions](https://learn.microsoft.com/azure/container-apps/revisions?WT.mc_id=AZ-MVP-5004796). A revision is an immutable snapshot of a container app version.\\n:::\\n\\nThe first revision is automatically provisioned when you deploy your container app.\\nNew revisions are automatically provisioned when you make a\xa0revision-scope\xa0change to your container app.\\nWhile revisions are immutable, they\'re affected by\xa0application-scope\xa0changes, which apply to all revisions.\\nYou can create new revisions by updating a previous revision.\\nYou can retain up to 100 revisions, giving you a historical record of your container app updates.\\nYou can run multiple revisions concurrently.\\nYou can split external HTTP traffic between active revisions.\\n\\n![Container App Revisions](ACA_Overview_BuildingContainerAppRevisions.PNG)\\n\\n![Container App Revisions](ACA_Overview_ContainerAppRevisions.gif)\\n\\n#### \u23f9\ufe0f Container shutdowns\\n\\nIt\'s worth noting how your Containers will shut down as Azure Container Apps scales your Apps or you deactivate a revision, as this can affect your application.\\n\\nThe containers are shut down in the following situations:\\n\\n* As a container app scales in\\n* As a container app is being deleted\\n* As a revision is being deactivated\\n\\nWhen a shutdown is initiated, the container host sends a SIGTERM message to your container. The code implemented in the container can respond to this operating system-level message to handle termination.\\n\\nIf your application doesn\'t respond within 30 seconds to the SIGTERM message, then SIGKILL terminates your container.\\n\\nAdditionally, make sure your application can gracefully handle shutdowns. Containers restart regularly, so don\'t expect the state to persist inside a container. Instead, use external caches for expensive in-memory cache requirements.\\n\\n\ud83d\udcd6 References: \\n\\n* [Application lifecycle management in Azure Container Apps](https://learn.microsoft.com/azure/container-apps/application-lifecycle-management?WT.mc_id=AZ-MVP-5004796)\\n\\n#### \ud83d\udd12 Secret Management\\n\\nSecurely store sensitive configuration elements that are then available to containers through environment variables, scale rules, and Dapr.\\n\\n![Azure Container Apps - Secret Management](ACA_Overview_BuildingContainerAppSecretManagement.PNG)\\n\\n![Azure Container Apps - Secrets](ACA_Overview_Secrets.gif)\\n\\nThose environment variables can also reference an [Azure Key Vault](https://azure.microsoft.com/products/key-vault?WT.mc_id=AZ-MVP-5004796). For further information, refer to the following Microsoft documentation: [Manage secrets in Azure Container Apps](https://learn.microsoft.com/azure/container-apps/manage-secrets?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796).\\n\\n## \u2601\ufe0f Cloud Native Ecosystem\\n\\nAzure Container Apps integrates several Cloud Native technologies, including [KEDA](https://keda.sh/) and [Dapr](https://dapr.io/), to provide a fully managed serverless container service.\\n\\nThese technologies are part of the [Cloud Native Computing Foundation](https://www.cncf.io/) and designed to help developers build and deploy modern applications at scale. \\n\\n### \ud83d\udc55 Dapr\\n\\n[Dapr (Distributed Application Runtime)](https://dapr.io/) is a portable, event-driven runtime for building distributed applications across the cloud and edge. \\n\\n![Azure Container Apps - Dapr](ACA_Overview_UnderstandingDaprIntegration.PNG)\\n\\n:::info\\n[Dapr](https://techcommunity.microsoft.com/t5/microsoft-developer-community/open-at-microsoft-dapr/ba-p/3857064?WT.mc_id=AZ-MVP-5004796) is a new way to build modern distributed applications. Born at Microsoft in 2019 as an incubation project, it was donated to the Cloud Native Computing Foundation in 2021 and is now a highly successful open-source project. Dapr helps you write flexible and secure microservices that leverage industry-proven best practices to build connected distributed applications faster. By letting Dapr\u2019s sidecar take care of the complex challenges such as service discovery, message broker integration, encryption, observability, and secret management, you can focus on business logic and keep your code simple.\\n:::\\n\\nDapr is a set of integrated APIs with built-in best practices and patterns to build distributed applications. Dapr increases your developer productivity by 20-40% with out-of-the-box features such as workflow, pub/sub, state management, secret stores, external configuration, bindings, actors, distributed lock, and cryptography. You benefit from the built-in security, reliability, and observability capabilities, so you don\'t need to write boilerplate code to achieve production-ready applications.\\nDapr\'s component model decouples the integrated API with the underlying resources. For instance, when you\'re using the Dapr publish-subscribe API, you can change the message broker by swapping out a yaml component file to switch from RabbitMQ to Kafka (or any other supported broker) without changing your application code.\\n\\n![Dapr High-level](ACA_Overview_UnderstandingDaprIntegrationHigh_Level.PNG)\\n\\n![Dapr Building Blocks](ACA_Overview_UnderstandingDaprIntegrationBuildingBlocks.PNG)\\n\\n![Dapr Sidecar architecture](ACA_Overview_UnderstandingDaprIntegrationSidecar.PNG)\\n\\n![Dapr Sidecar architecture - Container Apps](ACA_Overview_UnderstandingDaprIntegrationSidecarContainerApps.PNG)\\n\\nDapr integration is built into Azure Container Apps, so you can focus on building your application without worrying about the underlying infrastructure. Dapr operates as a sidecar alongside your application, providing features only where needed.\\n\\nDapr integration is available straight from the Azure Portal for both the Envirionment and Container App itself.\\n\\n![Azure Container Apps - Dapr Portal Integration](ACA_Overview_UnderstandingDaprIntegrationContainerAppPortalIntegration.gif)\\n\\nAlso, make sure to check out [Dapr component resiliency](https://learn.microsoft.com/azure/container-apps/dapr-component-resiliency?tabs=bicep&WT.mc_id=AZ-MVP-5004796).\\n\\n### \ud83d\udcda KEDA\\n\\n[KEDA (Kubernetes-based Event-Driven Autoscaling)](https://keda.sh/) is a Kubernetes-based event-driven autoscaling component.\\n\\n:::info\\nKEDA is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of any container in Kubernetes based on the number of events needing to be processed.\\n:::\\n\\n![Azure Container Apps - KEDA](ACA_Overview_UnderstandingKEDAlntegration.png)\\n\\nKEDA (Kubernetes-based Event Driven Autoscaling) is an open-source project that provides event-driven autoscaling for Kubernetes workloads. KEDA can scale any container in response to events from various sources such as Azure Service Bus, Azure Event Hubs, Azure Storage Queues, Azure Storage Blobs, RabbitMQ, Kafka, and more.\\n\\nAn example of KEDA event scaling is being able to scale your Containers based on KEDA scalers, such as Azure DevOps pipelines. You can refer to a previous blog post I did on leveraging KEDA scalers with [Container App Jobs to create Self-Hosted Azure DevOps Agents](https://luke.geek.nz/azure/hosted-agents-container-apps-job/).\\n\\nKEDA really opens up the possibilities of scaling your Containers, based on events, and is a great addition to Azure Container Apps, and like DAPR KEDA is built-in to Azure Container Apps and managed by Microsoft.\\n\\n![KEDA](ACA_Overview_KEDA.gif)\\n\\n## \ud83d\udcbb Azure Container Apps Networking & Storage\\n\\nNow, let us talk about Networking and Storage.\\n\\n### \ud83c\udf10 Networking\\n\\n![Azure Container Apps - Networking Overview](ACA_Overview_NetworkArchitecture_HighLevel.PNG)\\n\\n:::info\\n[Azure Container Apps run in the context of an environment](https://learn.microsoft.com/azure/container-apps/networking?tabs=workload-profiles-env%2Cazure-cli&WT.mc_id=AZ-MVP-5004796), which is supported by a virtual network (VNET). \\n\\nWhen you create an environment, you can provide a custom VNET, otherwise a VNET is automatically generated for you. Generated VNETs are inaccessible to you as they\'re created in Microsoft\'s tenant.\\n\\nTo take full control over your VNET, provide an existing VNET to Container Apps as you create your environment.\\n:::\\n\\n![Azure Container Apps - Networking Overview](ACA_Overview_NetworkArchitecture_HighLevel2.PNG)\\n\\n**Accessibility levels**\\n\\nUsing internal and external accessibility levels, you can configure whether your container app allows public ingress or ingress only from within your VNet at the environment level.\\n\\n![Azure Container Apps - Accessibility levels](ACA_Overview_NetworkAccessibilitylevels.PNG)\\n\\nCustom VNET (Virtual Network) integration\\n\\nIf you want your container app to restrict all outside access, create an\xa0internal Container app environment.\\n\\nWhen you provide your own VNet, you need to provide a subnet that is dedicated to the Container App environment you deploy. This subnet isn\'t available to other services.\\n\\nNetwork addresses are assigned from a subnet range you define as the environment is created.\\n\\nYou can define the subnet range used by the Container Apps environment.\\n\\nYou can restrict inbound requests to the environment exclusively to the VNet by deploying the environment internally.\\n\\n**User-defined routes**\\n\\nUser-defined routes (UDR) and controlled egress through NAT Gateway are supported in the workload profiles environment. These features aren\'t supported in the consumption-only environment.\\n\\n:::info\\nWhen using UDR with Azure Firewall in Azure Container Apps, you need to add certain FQDNs and service tags to the firewall\'s allowlist. To learn more, see [configuring UDR with Azure Firewall](https://learn.microsoft.com/azure/container-apps/networking?WT.mc_id=AZ-MVP-5004796#configuring-udr-with-azure-firewall).\\n:::\\n\\n**Envoy**\\n\\nAzure Container Apps uses [Envoy proxy](https://www.envoyproxy.io/) *(also part of the Cloud Native Computing Foundation)* as an edge HTTP proxy. TLS is terminated on the edge, and requests are routed based on their traffic splitting rules, which route traffic to the correct application.\\n\\nAzure Container Apps uses Envoy as a network proxy for all HTTP requests. Envoy provides some capabilities such as:\\n\\n* Scaling to zero: Envoy allows your container apps to scale to zero instances when there is no traffic and scale up when there is demand. Envoy acts as a buffer between the clients and the container apps and handles the cold start latency.\\n* HTTPS redirection: Envoy automatically redirects all HTTP requests to HTTPS, ensuring secure communication between the clients and the container apps.\\n* TLS termination: Envoy terminates the transport layer security (TLS) at the edge of the container app environment and forwards the requests to the container apps over plain HTTP. This reduces the overhead of encryption and decryption for the container apps.\\n* Mutual TLS: Envoy supports mutual TLS (mTLS) when you use Dapr as a sidecar for your container apps. mTLS provides an additional layer of security by requiring both the client and the server to present valid certificates for authentication.\\n* Ingress controls: Envoy allows you to configure some ingress controls for your container apps, such as IP restrictions, CORS, WAF, etc.\\n\\n![Azure Container Apps - Envoy](ACA_Overview_NetworkEnvoy.gif)\\n\\n![Azure Container Apps - Network Lockdown](ACA_Overview_Networklockdown.PNG)\\n\\n\ud83d\udcd6 References: \\n\\n* [Networking in Azure Container Apps environment](https://learn.microsoft.com/azure/container-apps/networking?tabs=workload-profiles-env%2Cazure-cli&WT.mc_id=AZ-MVP-5004796)\\n* [From Chaos to Clarity: Simplifying Your Networking with Azure Container Apps](https://techcommunity.microsoft.com/t5/startups-at-microsoft/from-chaos-to-clarity-simplifying-your-networking-with-azure/ba-p/4034625?WT.mc_id=AZ-MVP-5004796)\\n* [User defined routes (UDR)](https://learn.microsoft.com/azure/container-apps/networking?tabs=consumption-only-env%2Cazure-cli&WT.mc_id=AZ-MVP-5004796#user-defined-routes-udr)\\n\\n### \ud83d\udcbe Storage\\n\\nA container app has access to different types of storage. A single app can take advantage of more than one type of storage if necessary.\\n\\n![Azure Container Apps - Storage](ACA_Overview_Storage.gif)\\n\\n\ud83d\udcd6 References: \\n\\n* [Use storage mounts in Azure Container Apps](https://learn.microsoft.com/azure/container-apps/storage-mounts?pivots=azure-cli&WT.mc_id=AZ-MVP-5004796)\\n\\n## \ud83d\udc40 Observability\\n\\nAzure Container Apps provides several built-in observability features that together give you a holistic view of your container app\u2019s health throughout its application lifecycle\\n\\nThese features help in monitoring and diagnosing the state of the application to improve performance and respond to trends and critical problems.\\n\\n### \ud83d\udc41\ufe0f Application lifecycle observability\\n\\n Phase | Features | Description |\\n| --- | --- | --- |\\n| Development and Test | Log streaming, Container console | Real-time access to containers\' application logs and console for debugging issues |\\n| Deployment | Azure Monitor metrics, Azure Monitor alerts, Azure Monitor Log Analytics | Continuous monitoring after deployment for identifying problems around error rates, performance, and resource consumption |\\n| Maintenance | Azure Monitor metrics, Azure Monitor alerts, Azure Monitor Log Analytics | Manages updates to your container app by creating revisions. Allows running multiple revisions concurrently for blue green deployments or A/B testing. Helps in monitoring applications across revisions |\\n\\n## \ud83c\udfe1 Azure Landing Zone for Container Apps\\n\\nLast but not least, now that we have investigated and examined some of the finer points of Azure Container Apps, we need somewhere to put it. Following the Enterprise Scale landing zone approach, the Well-architected and Ready phase of the Cloud Adoption Framework has an Application Landing zone reference architecture for Azure Container Apps.\\n\\nAzure landing zone accelerators provide architectural guidance, reference architectures, reference implementations, and automation to deploy workload platforms on Azure at scale. They are aligned with industry proven practices, such as those presented in [Azure landing zones guidance](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796) in the Cloud Adoption Framework.\\n\\n![Azure Container Apps - Landing Zone](ACA_Overview_LandingZoneContainerApps.gif)\\n\\nThis reference architecture can be found on GitHub directly: [Azure/aca-landing-zone-accelerator](https://aka.ms/aca-lza).\\n\\nThank you for sticking with me until the end! I hope you found this article useful. As usual, feel free to reach out to me on social media if there\'s anything you want covered that I may have missed, if you have any queries, or if you want me to cover anything else.\\n\\n**[Get started with Azure Container Apps today](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796)!**"},{"id":"azure/book-review-cloud-solution-architects-career-master-plan","metadata":{"permalink":"/azure/book-review-cloud-solution-architects-career-master-plan","source":"@site/blog/2024-03-22-Book-Review-csa-career-master-plan/index.mdx","title":"Book Review Cloud Solution Architects Career Master Plan","description":"This is a review of the book \'Cloud Solution Architect\'s Career Master Plan\'. The book provides valuable insights for aspiring Cloud Solution Architects.","date":"2024-03-22T05:40:29.756Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":10.565,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Book Review Cloud Solution Architects Career Master Plan","metaDescription":"This is a review of the book titled \'Cloud Solution Architect\'s Career Master Plan\'. The book provides valuable insights for aspiring Cloud Solution Architects.","date":"2024-03-22T05:40:29.756Z","tags":["Azure","Misc"],"categories":["Azure","Misc"],"authors":["Luke"],"header":{"teaser":"csa-career-master-plan-kindle-book.jpg"},"slug":"azure/book-review-cloud-solution-architects-career-master-plan","keywords":["azure","cloud","architect","solutions","csa","Microsoft"],"description":"This is a review of the book \'Cloud Solution Architect\'s Career Master Plan\'. The book provides valuable insights for aspiring Cloud Solution Architects."},"unlisted":false,"prevItem":{"title":"Azure Container Apps - Overview","permalink":"/azure/azure-container-apps-overview"},"nextItem":{"title":"Microsoft Azure - Sandbox Design Considerations","permalink":"/azure/microsoft-azure-sandbox-design-considerations"}},"content":"Book review of [Cloud Solution Architect\'s Career Master Plan: Proven techniques and practical tips to help you become a successful solution architect](https://www.amazon.com/Cloud-Solution-Architects-Career-Master/dp/1805129716)\\n\\n![Cloud Solution Architect\'s Career Master Plan](csa-career-master-plan-kindle-book.jpg)\\n\\n\x3c!-- truncate --\x3e\\n\\n:::info\\nAlthough this book was given to me to review _(for free, I have no formal relationship with either Packt Publishing or the authors, although I thank them for allowing me to review this book (Early Access), I am reviewing this book from an entirely independent view)_, as someone who architects and builds Azure solutions, this is the type of book, I usually would read, based on the title and synopsis, so let us open the page...\\n:::\\n\\nBook synopsis:\\n\\n>  Proven techniques and effective tips to help you become a successful solution architect.\\n\\n## Introduction\\n\\n[Cloud Solution Architect\'s Career Master Plan](https://www.amazon.com/Cloud-Solution-Architects-Career-Master/dp/1805129716) is a Packtpub published (in March 2024) book written by the following authors:\\n\\n* [Rick Weyenberg](https://www.linkedin.com/in/rickweyenberg/)\\n* [Kyle Burns](https://www.linkedin.com/in/kylemburns/)\\n\\nFirst up - let\'s take a look at who this book is for. According to the authors, this book is for aspiring Cloud Solution Architects:\\n\\n> If you are a self-motivated IT professional and would like to pursue a career as a solution architect, then \\nthis book is for you. You should have a solid base of traditional software architecture understanding, but not in-depth cloud concepts and design considerations.\\n> This book will also be valuable for anyone who is considering a solution architect role as a potential career field but doesn\u2019t know where to get started. No experience in the cloud architect role is needed to get started.\\n\\nIn order to set the context of my review, I need to give a brief blurb about my own IT Career to give the context of what lense I am reviewing from and what I am looking for in a book like this, my background is Systems/Infrastructure Engineer, then moving into Cloud-based roles, from project delivery/implementation to technical assurance roles, working with third-party service partners to deliver services, and now as a Senior Consultant/Architect *(the New Zealand IT industry, is very much generalised, one day you will be architecting strategic solutions, the next you will be doing infrastructure as code, as an engineer, we need to adapt to fit what needs doing at the time)*, so I am looking for a book that can help me in my career and  role, and also help me to mentor and guide others in their career.\\n\\nThere are 8 Chapters in this book, covering Introduction to the Cloud Solution Architect Role, Pursuing the Cloud Architect Role, Preparing for and after the Offer, and is 124 pages long.\\n\\nI am going to break my review into several areas and look at answering questions such as:\\n\\n1. Does the book provide a clear understanding of the CSA\u2019s role? How well does it explain the evolution of cloud computing?\\n2. Are the educational recommendations practical and accessible? Does the book cover a range of certifications?\\n3. Does the book offer actionable advice for gaining experience? Are the examples and suggestions relevant to current industry standards?\\n4. Is the book offer advice that is up-to-date and industry-specific? How helpful are the resume and interview tips?\\n\\nSo lets take a look and see if the book answers these questions. I don\'t want to give away too much of the book, but hopefully, this review will give you a good idea of what to expect from the book.\\n\\n## Role overview\\n\\nFor the role Overview section, we are going to take a look at the book and see whether it discusses the responsibilities and evolution of a Cloud Solution Architect (CSA).\\n\\n> Question: Does the book provide a clear understanding of the CSA\u2019s role? How well does it explain the evolution of cloud computing?\\n\\nChapters 1 and 2, are dedicated to answering this very question, with Chapter 1: understanding the responsibilities of a Cloud Solution Architect, and Chapter 2: Types of Cloud Solution Architect roles.\\n\\nChapter 1 outlines the evolution of Cloud computing, from the first concepts of what became the internet in the 1960s to the modern-day Cloud within the 2000s. The authors do a very good job of being concise on these details - and vendor agnostic, which is clear throughout the entire book. It\'s worth noting that although the authors have ties to Microsoft as part of their day-to-day roles, they cover Azure, AWS, and Google concepts as well. Cloud Solution Architect is not specific to one Cloud or hyper scaler, and in my opinion, the authors concentrate on that very view - it\'s purely about being able to fill the gap that Cloud architecture has as an Architect.\\n\\nNot only does the Chapter outline the history, but gives insights into core Cloud concepts as well, along with high-level points around the [Microsoft Cloud Adoption](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) and [Well-Architected Framework](https://learn.microsoft.com/azure/well-architected/?WT.mc_id=AZ-MVP-5004796).\\n\\nRegarding a CSA (Cloud Solution Architect) role, the book outlines the upsides and downsides of the role and some of the responsibilities that come with it, across elements from on-premises, IaaS *(Infrastructure as a Service)*, PaaS *(Platform as a Service)* and SaaS *(Software as a Service)*, including various architectural patterns that CSAs need to know.\\n\\nOne of the highlights of the role overview Chapter is that the authors have distilled the role of \'Cloud Solution Architect\', which could mean different things to different people - a very open-ended job title, into the various sub-categories that exist for CSAs and organisations such as Microsoft adhere to, such as Cloud Solution Architect (Infrastructure), Cloud Solution Architect (Applications), Cloud Solution Architect (Data and AI), and Cloud Solution Architect (Security) etc., highlighting the differences in these particular roles, including tools and relevant certifications for each Architect function.\\n\\n**Overall, these Chapters do a good job of setting the scene for the rest of the book, give a good overview of the role and the responsibilities that come with it, and, in my opinion, clearly answer the question of What a CSA is, and what they do, and even covers industry-specific *(such as Healthcare or Utilities)*.**\\n\\n## Educational Pathways\\n\\nFor this section, we will examine the various educational paths and certifications available for aspiring CSAs by answering the following question:\\n\\n> Question: Are the educational recommendations practical and accessible? Does the book cover a range of certifications?\\n\\n![Cloud Architect](Cloud_Architect_MSDesigner.jpeg)\\n\\n**I alluded to this in the previous section (Role overview); the book authors have included a list of relevant certifications for each CSA (Cloud Solution Architect) function, ie AWS Certified Machine Learning, AI Engineer Associate, for AI/ML CSAs (Artificial Intelligence/Machine Learning Cloud Solution Architects), including a draft of fundamentals certifications across multiple platforms. It is my opinion, that the book covers educational pathways pretty well, with a vendor-neutral focus, from technical certification recommendations to even soft-skills and growth-mindset recommended reading, some great content and pathways to get started on your journey as a Cloud Solution Architect.**\\n\\n## Real-World Application\\n\\nIn this section, we will evaluate the book\'s practical advice for gaining experience and industry-specific knowledge - outside of the book.\\n\\nWe will consider how the book addresses gaining real experience through side projects, open-source contributions, and hackathons.\\n\\n![Cloud Architect](Cloud_Architect_MSDesigner1.jpeg)\\n\\n> Question: Does the book offer actionable advice for gaining experience? Are the examples and suggestions relevant to current industry standards?\\n\\n**When looking at a role, such as a Cloud Solution Architect, gaining experience is key, and the book does a good job of outlining the various ways to gain experience, from side projects, to open source contributions, to hackathons, and even to the importance of networking, and building relationships with other professionals in the industry, and the importance of mentorship, and how to find a mentor.**\\n**As a community advocate and someone who has learned by helping the community *(outside of my day-to-day role)*, by contributing to open-source projects, Microsoft Learn documentation editing and writing, and helping on Microsoft Q&A etc., public speaking, etc. - the guidance this book offers is key and even touches on the high-level roadmap of, being a consumer of an open-source project and moving to more of a Contributor role. The path to a Cloud Solution Architect is not always a straight line, and the book does a good job of outlining this: the importance of gaining experience and the various ways to do so, which are not necessarily your traditional, become an engineer, then either go into architecture or management.**\\n\\nTo quote from the book itself:\\n\\n> \\"In 1975, Steve Wozniak started working on a side project to design a general-purpose computing device suitable for hobbyist use. This device became the Apple I and helped start Apple Computer down the path toward becoming a household name. In May 2023, Apple announced a revenue of $94.8 billion for the second quarter of 2023.\\"\\n\\n## Job Pursuit Strategy\\n\\nIn this section, we will review the guidance provided for pursuing a job, crafting a resume, and preparing for interviews.\\n\\n> Question: Is the job pursuit advice up-to-date and industry-specific? How helpful are the resume and interview tips?\\n\\nNow that the book has covered alot of theory about what a CSA (Cloud Solution Architect) is and how to get experience and education *(in my opinion, covered successfully)*, now lets see how whether the guidance can help us with the next step, of preparing for interviews, and crafting a resume.\\n\\n![Cloud Architect](Cloud_Architect_MSDesigner2.jpeg)\\n\\nFirst, we need to keep in mind that this book came out in March 2024, around the same time as this review. So, the information on hand is relevant and up-to-date at the time of this review, and the job pursuit advice is relevant for the current job market. However, different industries, companies, and geographical locations may have different requirements for an Architect, so make sure to read the fine print of that role.\\n\\n**Going back to the book - the book has already touched on a Cloud Solution Architect, what that is, and how to get experience and industry-specific concerns, but what the book hasn\'t touched on YET - is the different levels, for example, Junior, Senior, Principal Cloud Solution Architect, I question I had before reading this book was:**\\n\\n**What was the difference between a Senior Cloud Solution Architect and a Principal Architect? This book answered that.**\\n\\n**The book also covered, average salary *(again company, geographical specific)*,  and recommendations on resume tips and skills. These sections were very high-level but straight to the point - the authors allowed room for you to adjust and make their recommendations your own and adjustable for the actual role you are applying for. I took the recommendations as a way to structure the sentences and my achievements on my resume, and skills (both technical and soft skills) that I may need or didn\'t think to add.**\\n\\n**Alongside the resume tips are tips about leveraging Social Media and also some relevant interview questions.**\\n\\n## Author\'s Expertise\\n\\nI believe the authors have the correct expertise to make this book viable *(and based on my own knowledge, reading this book)* and accurate.\\n\\n* [Kyle Burns](https://www.linkedin.com/in/kylemburns/) is a Microsoft Principal Cloud Solution Architect - concentrating on Azure App Dev.\\n* [Rick Weyenberg](https://www.linkedin.com/in/rickweyenberg/) is a Microsoft Principal Azure Cloud Solution Architect with an industry focus for automotive.\\n\\n**It is my opinion that this duo is indeed suited to write this type of book, and this can be seen from the actionable and valid output of the book itself.**\\n\\n## Conclusion\\n\\n**Cloud Solution Architect\'s Career Master Plan (1st Edition) is a book containing 124 pages of actionable and useful content for those interested in what a Cloud Solution Architect is and how to start your journey to becoming one. Key traits needed, in my opinion, for a Cloud Solution Architect are a desire to learn and stay up to date with emerging technologies; this is reflected in the guidance of the book. Overall, I had no pre-conceptions when I was asked to review this book, but it was very timely for me personally, where I am in my own career, and the questions that I had.**\\n\\n**I would like to point out that this book goes beyond by dedicating a section to giving back to the Community and the importance of mentorship, networking, soft skills and a growth mindset, which are key to being a successful Cloud Solution Architect and highly recommend picking up to read. The only issue I had was that there was missing an \'s\' to an image for on-premises! My OCD kicked in! But it was a privilege to review, definitely one I will refer to!**\\n\\nIf interested, feel free to checkout another Book review I did on [Book Review Azure Architecture Explained](https://luke.geek.nz/azure/misc/Book-Review-Azure-architecture-explained/), another great book on the more technical and governance aspects of architecting solutions in Azure."},{"id":"azure/microsoft-azure-sandbox-design-considerations","metadata":{"permalink":"/azure/microsoft-azure-sandbox-design-considerations","source":"@site/blog/2024-03-21-Azure-Pattern-Sandbox/index.mdx","title":"Microsoft Azure - Sandbox Design Considerations","description":"Sandbox Design Considerations for Microsoft Azure","date":"2024-03-21T04:40:28.030Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":16.325,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Azure - Sandbox Design Considerations","metaDescription":"Sandbox design considerations for Mirosoft Azure.","date":"2024-03-21T04:40:28.030Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"BlogHeading_MicrosoftAzure_SandboxDesignConsiderations.png"},"slug":"azure/microsoft-azure-sandbox-design-considerations","keywords":["azure","platform","referencearchitecture","landingzone","sandbox"],"description":"Sandbox Design Considerations for Microsoft Azure"},"unlisted":false,"prevItem":{"title":"Book Review Cloud Solution Architects Career Master Plan","permalink":"/azure/book-review-cloud-solution-architects-career-master-plan"},"nextItem":{"title":"Tag Azure Resources with Owner using Azure Automation","permalink":"/azure/tag-azure-resources-owner-azure-automation-runbook"}},"content":"When working with Microsoft Azure, you may want an environment for learning, whether for an individual or a team. This article aims to highlight some architectural considerations when implementing a Sandbox environment within the Microsoft Azure platform.\\n\\n:::tip\\nYour Sandbox isn\u2019t a pathway to production; it\u2019s a pathway to \u201cCLICK, CLICK, OOPS.\u201d\\n:::\\n\\n![Microsoft Azure - Sandbox Design Considerations](BlogHeading_MicrosoftAzure_SandboxDesignConsiderations.png)\\n\\n{/*truncate*/}\\n\\n## \u2601\ufe0f What is a Cloud Sandbox environment?\\n\\nWhat am I talking about when talking about a Sandbox environment?\\n\\n:::info\\nCloud Sandboxes are contained, isolated environments that allow the evaluation of new Cloud services and features _(without impacting production environments)_.\\n\\n* Sandbox is a pathway to learning, not to development\\n* Build and experiment with capabilities with an open platform\\n* Co-innovate with trust and safeguards\\n\\nThe general principles for a Sandbox environment are:\\n\\n* Cannot store Production data\\n* Secure identity with production controls\\n* Environment for learning, not production\\n\\n:::\\n\\n## \u2754Why would I want a Sandbox environment?\\n\\nHaving an environment for learning is key to driving the adoption of Cloud technologies by giving consumers the ability to leverage and test Azure resources in an isolated environment from Production; users using the Sandbox could be users new to Microsoft Azure or those who may already have Azure knowledge but want to trial and test preview features, that may not be compliant with Production controls.\\n\\n![Gartner Top 10 - Strategic Technology Trends for 2024](Gartner_Top_10_StrategicTechTrends_2024.png)\\n\\n:::info\\nPlatform Engineering is the discipline of building and operating self-service internal platforms \u2014 each platform is a layer created and maintained by a dedicated product team, designed to support the needs of its users by interfacing with tools and processes.\\n:::\\n\\nAccording to the Gartner Top 10 technology trends for 2024, Platform Engineering is a key element to enabling developers and application teams to remain competitive, not only from a business standpoint but also from talent retention, empowering employees to do more, the premise of Platform Engineering is for a \'Platform Team\' to manage the governance of the Cloud platforms _(following key well-architected framework guidelines, such as Operational Excellence, Security, Performance and Reliability)_ and also allowing the Application owners to deploy and manage their applications, without having to worry about the necessary organisational guardrails.\\n\\n## \ud83d\ude37What are the antipatterns to avoid?\\n\\nThe word \'anti-patterns\' can be quite negative terminology to use. However, it\'s really a trade-off between having an environment protected, i.e., with regulatory requirements, etc., like you would in Production, which in most cases prevents your Sandbox consumers from being able to learn without running into the various guardrails and having an environment on the opposite end of the scale, where there are no guardrails in place.\\n\\nWhen we look at anti-patterns, we need to remember what we are trying to achieve with the Cloud Sandboxes, i.e. give Sandbox consumers an environment for learning Cloud technologies.\\n\\nSo, let us take a look at some patterns and areas to avoid.\\n\\n* Regional restrictions - the reason why you might want to restrict Regional Restrictions is due to the fact that not all Azure services are available in all regions.\\n* Over extensive blocklists\\n* Trying to build the entire platform at once\\n\\n## \ud83d\udccbSandbox Patterns\\n\\nOk, let\'s talk Patterns! Every organisation may have a different Sandbox requirement, but before we discuss the particular of patterns, let\'s first talk about Platform Landing Zones at a high level so I can make sure we are all talking the same language.\\n\\nIn the context of Platform Landing Zones, I am referring to an Azure environment setup like the Enterprise Scale Landing Zone Reference architecture (ESLZ), where the Landing Zone features common components for the Platform, such as Security and Connectivity, and is usually the base of your networking infrastructure, such as Virtual WAN (Wide Area Network) connecting to your on-premises environment or a Virtual Network connecting different Applications and dependencies together.\\n\\nThe Platform Landing Zone design aims to keep Platform management consistent, centralised, and secure, allowing your application teams to leverage the shared services supplied by the Platform Landing Zone design. An organisation may have requirements to move from this reference pattern - and that is ok - but the premise here is that the Platform Landing Zone is looked after by a Platform or Infrastructure team _(in most cases)_ and the environment is architected in a way that does not impede application teams, to deliver business value, while making sure that their solutions are secure and where necessary, consistent.\\n\\n![Azure Landing Zone - Sandbox - Reference Architecture](Azure_Landing_Zone-Sandbox-ReferenceArchitecture.png)\\n\\nA sandbox usually contains:\\n\\n* Data isolation\\n* Network security (segregated)\\n* Identity and access controls\\n* Compliance and regulatory considerations\\n\\nSo, let us discuss Sandbox Types.\\n\\n![Azure Sandbox Types](Azure_Sandbox_Types.png)\\n\\n![Azure Sandbox Types](Azure_Sandbox_Types2.png)\\n\\nThe term: Managed, below - is in the context of the Azure Cloud Adoption Framework, where the environment is managed by the organisation and is part of the organisation\'s governance, identity and compliance controls, and Platform team managed.\\n\\n### Sandbox Type - Personal\\n\\nWhen referring to a Personal Sandbox, I am referring to a Free, Pay As You Go, or Azure Sponsorship, [Visual Studio Enterprise subscription(s)](https://visualstudio.microsoft.com/subscriptions/?WT.mc_id=AZ-MVP-5004796) that are used by an individual, an example is a Cloud Engineer, who has access to a Visual Studio Enterprise subscription and then uses the free credit, to spin up an Azure subscription - that is unmanaged and not part of the organisation\'s network, identity or compliance controls. These subscriptions usually have a hard limit on the type of resources you can deploy or Budget.\\n\\nPersonal subscriptions are outside of the organisation\'s control and are not managed by the organisation. They are usually used for learning and not for production workloads. For individuals with these licenses, these subscriptions can be key to preparing for certification or individual upskilling.\\n\\n### Sandbox Type - Unmanaged\\n\\nWhen I refer to an Unmanaged Sandbox, I am referring to an Azure subscription that is managed by the organisation but is not part of the organisation\'s network, identity, or production compliance controls (although it would be auditable). \\nThese subscriptions are usually used for learning, not for production workloads. For individuals or teams, these subscriptions can be key to preparing for certification or team upskilling with Cloud technologies and preview features.\\n\\n### Sandbox Type - Lightly Managed\\n\\nWhen referring to Lightly Managed Sandboxes, these are more managed than an Unmanaged subscription because they are Resource Group limited; for example, a team of 10 may have access to a Lightly Managed Sandbox, where they can deploy resources into a specific Resource Group, or Resource Groups that they have access to, however, the overall governance of the Subscription is done by the Platform team. This is useful when you have a lot of individuals wanting a specific regulated Resource Group to stand up resources into, vs giving them a Subscription each.\\n\\nResource Group tags can be key to [FinOps](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/the-azure-finops-guide/ba-p/3704132?WT.mc_id=AZ-MVP-5004796) cost management and tracking.\\n\\nLightly Managed sandboxes can be useful for smaller organisations that do not want Subscription sprawl.\\n\\n### Sandbox Type - Highly Managed\\n\\nHighly Managed Sandboxes are usually used for testing the same set of services repeatedly, where the users (particularly Developers) don\'t necessarily care about the underly Azure platform and resources and are more focussed on the application and services they are deploying, for example, a developer may want to test a new version of a service, and they can deploy the service into a Highly Managed Sandbox, and then run their tests, and then destroy the environment. \\nHighly Managed Sandboxes are useful for standing up consistant infrastructure, such as an Azure Kubernetes Service or App Service, that is used every single time.\\n\\n\\n:::info\\nThe main difference is that managed sandboxes have pre-determined services that can be deployed as needed, which is useful for testing the same set of services repeatedly.\\nLightly managed sandboxes are useful when many individuals want a specific regulated Resource Group to allocate resources to.\\n\\nIn contrast, the unmanaged sandbox is more open and less restrictive, which makes it ideal for learning and trying new technologies for a small to large team.\\n:::\\n\\n### Sandbox Type decisions\\n\\nWhen deciding on the type of Sandbox, you need to consider the following:\\n\\n* **The purpose of the Sandbox**. What are you trying to achieve with the Sandbox? Are you trying to give users an environment to learn, or are you trying to give users an environment to test a specific set of services?\\n* **The scope of the Sandbox**. Are you trying to give users an environment to learn, or are you trying to give users an environment to test a specific set of services?\\n* **The lifecycle of the Sandbox**. How long do you want it to be available? Do you want it to be available for a specific period of time?\\n* **The Cost of the Sandbox**. How much are you willing to spend on it? Who has the financial authority to approve Cloud spending?\\n* **Access to the Sandbox**. Who has access to the Sandbox, and how do you manage access to the Sandbox?\\n* **Management of the Sandbox**. Who is responsible for managing the Sandbox? How do you manage the lifecycle of the Sandbox?\\n* **Governance of the Sandbox**. How do you manage the governance of the Sandbox? How do you manage the compliance of the Sandbox? \\n* **Migration of the Sandbox**. How do you migrate the Sandbox to a different environment? How do you migrate the Sandbox to a different subscription? ie what would migration of a proof of technology from a Sandbox environment into Dev/Test or Production look like?\\n* **Security of the Sandbox**. How do you manage the security of the Sandbox? How do you manage the identity and access to a Sandbox? How do you manage the compliance of the Sandbox?\\n* **Monitoring of the Sandbox** How do you monitor the Sandbox? How do you manage the cost of the Sandbox? How do you manage the performance of resources in the Sandbox (and do you need to)?\\n* **Integration of the Sandbox**. How do you integrate the Sandbox with other environments? How do you integrate the Sandbox with other subscriptions? How do you integrate the Sandbox with other services?\\n\\nConsideration of all of the above will help you decide on the type of Sandbox that is right for your organisation.\\n\\n![Azure Sandbox Decisions](Azure_Sandbox_TypeDecisions.png)\\n\\n| Criteria | Personal | Unmanaged | Lightly Managed | Highly Managed |\\n|----------|----------|-----------|-----------------|----------------|\\n| Purpose | Learning, individual upskilling | Learning, team upskilling | Regulated resource group for team | Testing specific services repeatedly |\\n| Scope | Individual | Small to large team | Specific team | Specific services |\\n| Lifecycle | Determined by individual | Determined by team | Managed by platform team | Managed by platform team |\\n| Cost | Limited by subscription type | Auditable, managed by organisation | Managed by platform team | Managed by platform team |\\n| Access | Individual | Team | Regulated by platform team | Regulated by platform team |\\n| Management | Individual | Team | Platform team | Platform team |\\n| Governance | Individual | Auditable, managed by organisation | Managed by platform team | Managed by platform team |\\n| Migration | Individual responsibility | Team responsibility | Managed by platform team | Managed by platform team |\\n| Security | Individual responsibility | Auditable, managed by organisation | Managed by platform team | Managed by platform team |\\n| Monitoring | Individual responsibility | Team responsibility | Managed by platform team | Managed by platform team |\\n| Integration | Individual responsibility | Team responsibility | Managed by platform team | Managed by platform team |\\n\\n\\nLet\'s look at a scenario:\\n\\nWe have a Data Scientist named Samuel.\\n\\n![Samuel - Data Scientist](Azure_Sandbox_Scenario_Samuel.png)\\n\\nSamuel\'s goals are to:\\n\\n1. **Deliver high-quality and robust machine learning solutions that can enhance customer satisfaction and loyalty**x, as well as increase the revenue and profitability of the company. \\n2. **Learn the latest and most advanced techniques and methods in the field of artificial intelligence, such as natural language processing**, computer vision, and deep learning, to create innovative and cutting-edge models.\\n3. To **collaborate and learn from other data scientists and experts in the field, both within and outside his company**, to improve his knowledge.\\n4. **Self-management and accountability of cost management** and access to the environment for Data/AI projects across multiple responsible team members**.\\n\\nIf we also take a look at the [Microsoft AI Shared responsibility model](https://learn.microsoft.com/azure/security/fundamentals/shared-responsibility-ai?WT.mc_id=AZ-MVP-5004796), we can see that the Data Scientist, is responsible for the Data, the AI model, and the AI application, and the Platform team is responsible for the underlying infrastructure, and the AI platform.\\n\\n![Microsoft AI Shared Responsibility Model](Azure_Sandbox_Scenario_Samuel_AI_SharedResponsibility.png)\\n\\nBecause of Samuel\'s requirements and the shared responsibility model, we can see that Samuel would benefit from an Unmanaged Sandbox, where he can deploy resources into a specific Subscription he has access to, alongside members of his team and approved external identities, while also being isolated from Production. He has also accepted responsibility for Cost.\\n\\nThe reference architecture of Samuel\'s Sandbox could look like the following:\\n\\n![Azure Sandbox - Reference Architecture - Samuel](Sandbox_Reference_Architecture.png)\\n\\n## \ud83d\ude99Pathway to Production\\n\\n:::info\\nRemember: The Sandbox is not a pathway to production; it is a pathway to \\"CLICK, CLICK, OOPS\\".\\n:::\\n\\n**However**, there will be times when users have learnt something in the Sandbox, and they want to move their solution into a Dev/Test or Production environment, and this is where the [Cloud Adoption Framework](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) comes into play, and the [Well-Architected Framework](https://learn.microsoft.com/azure/well-architected/?WT.mc_id=AZ-MVP-5004796) can be used to help guide the migration of the solution from the Sandbox into a Dev/Test or Production environment.\\n\\nMake sure to consider the following:\\n\\n1. **Identify and document the requirements and specifications of the product after your learning**: After experimenting and learning in the Sandbox, it\'s crucial to identify and document the requirements and specifications of the product. This includes understanding the technical requirements, the business needs, and the user expectations. This step is in line with the Strategy phase of the Cloud Adoption Framework.\\n2. **Security considerations (RBAC, Access, CISO review)**: Security is a paramount concern when moving from a Sandbox to a Production environment. Considerations should include Role-Based Access Control (RBAC) to ensure only authorized individuals have access, a thorough review of access permissions, and a review by the Chief Information Security Officer (CISO) or equivalent. This aligns with the Well-Architected Framework\'s security pillar.\\n3. **Ensure that the data, code, models, and outputs are suitable for Production; change**: Before moving to Production, ensure that all data, code, models, and outputs are production-ready. This means they should be thoroughly tested, reliable, and meet all necessary regulations and standards. This is part of the Plan and Ready phases of the Cloud Adoption Framework.\\n4. **Deploy the production using CI/CD where possible**: Continuous Integration/Continuous Deployment (CI/CD) is a best practice for deploying applications. It allows for frequent code changes, automated testing, and quick deployment to production. This is part of the Migrate and Innovate phases of the Cloud Adoption Framework.\\n5. **Test and debug**: Rigorous testing and debugging are essential to ensure the product works as expected and any issues are identified and fixed before going live. This aligns with the Well-Architected Framework\'s reliability pillar.\\n6. **Monitor and manage**: Once in production, the application should be continuously monitored and managed to ensure it remains secure, performs well, and meets user needs. This includes monitoring for any issues, managing updates and changes, and regularly reviewing performance. This is part of the Govern and Manage phases of the Cloud Adoption Framework.\\n\\nAlso, make sure you make sure of Cloud Adoption Framework Landing Zone guidance, such as [Dev/Test Management Group and subscription Landing Zone guidance, such as a Canary Management Group structure](https://learn.microsoft.com/en-gb/azure/cloud-adoption-framework/ready/enterprise-scale/testing-approach?WT.mc_id=AZ-MVP-5004796).\\n\\nAlthough not mandatory, the canary environment management group hierarchy can be used to simplify testing of the following resource types without ending up with a range of exemptions and overprivileged access to Production resources, where it is not required:\\n\\n* **Management groups**: Management groups in Azure provide a level of scope above subscriptions. They give you enterprise-grade management at a large scale no matter what type of subscriptions you might have. All subscriptions within a management group automatically inherit the conditions applied at the management group level.\\n* **Subscription placement**: The placement of subscriptions within the management group hierarchy is crucial. It allows for the application of governance conditions at different levels, providing flexibility and control over the environment.\\n* **Roles (built-in and custom)**: Azure provides several built-in roles that you can assign to users, groups, and services. You can also create custom roles if the built-in roles don\'t meet your specific needs.\\n* **Assignments**: Assignments in Azure RBAC are the links between roles and security principals (user, group, service principal, or managed identity). They define the access a security principal has.\\n* **Azure Policy**: Azure Policy is a service in Azure that you use to create, assign, and manage policies. These policies enforce different rules and effects over your resources, helping to ensure your resources stay compliant with your corporate standards and service level agreements.\\n* **Definitions (built-in and custom)**: Azure Policy uses policy definitions to express what to evaluate and what action to take. There are built-in definitions provided by Azure and you can also create custom definitions.\\n* **Initiatives, also known as set definitions**: An initiative definition is a set or group of policy definitions to help track your compliance state for a larger goal. Initiatives bring together one or more policies as a group to accomplish things like enabling real-time policy enforcement and more comprehensive coverage.\\n\\nThe Canary Landing Zone architecture is useful as it provides a controlled environment for testing changes and updates before they are applied to the production resources.\\n\\n![Azure Landing Zones - Canary](canary-mgmt-groups.png)\\n\\n## \ud83d\udea2Products and tools\\n\\nWhen considering a Sandbox environment, you may want to consider the following products and tools to assist.\\n\\n* [Azure Deployment Environments](https://learn.microsoft.com/azure/deployment-environments/overview-what-is-azure-deployment-environments?WT.mc_id=AZ-MVP-5004796) - useful for \'Highly Managed Sandboxes\', Azure Deployment Environments, allows you to pre-define a set of services through infrastructure as code, and offer it up to developers or Sandbox consumers, as a Service, that they can then deploy and tear down as required.\\n* [Azure Dev/Test Labs](https://learn.microsoft.com/azure/devtest-labs/devtest-lab-overview?WT.mc_id=AZ-MVP-5004796) - Azure DevTest Labs is a service for easily creating, using, and managing infrastructure-as-a-service (IaaS) virtual machines (VMs) and platform-as-a-service (PaaS) environments in labs. Labs offer preconfigured bases and artifacts for creating VMs, and Azure Resource Manager (ARM) templates for creating environments like Azure Web Apps or SharePoint farms. Azure DevTest Labs is useful for Lightly Managed Sandboxes, where you want to give a team access to a specific Resource Group.\\n* [Entra ID Access Packages](https://learn.microsoft.com/entra/id-governance/entitlement-management-access-package-create?WT.mc_id=AZ-MVP-5004796) - Access packages are great for giving delegated access to a specific set of resources, for a specific period of time, and can be useful for Lightly and Unmanaged  Sandboxes, where you want to give a team access to a specific Resource Group or Subscription.\\n* [Microsoft Purview](https://learn.microsoft.com/purview/purview?WT.mc_id=AZ-MVP-5004796) - Microsoft Purview is a unified data governance service that helps you manage and govern your on-premises, multi-cloud, and software-as-a-service (SaaS) data. You can use Purview to understand your data, manage data policies, and ensure data privacy and compliance. Microsoft Purview is useful for understanding the data that is being used in the Sandbox.\\n\\n## \ud83d\udcd6Reference Links\\n\\n1. [Festive Tech Calender - 2023 - Azure Sandbox Design](https://youtu.be/WR9zRbzYfTQ)\\n2. [Gartner Top 10 Strategic Technology Trends for 2024](https://www.gartner.com/en/articles/gartner-top-10-strategic-technology-trends-for-2024)\\n3. [Azure Sandbox (Azure Architecture Center)](https://learn.microsoft.com/en-us/azure/architecture/guide/azure-sandbox/azure-sandbox?WT.mc_id=AZ-MVP-5004796)\\n4. [Microsoft Azure Cloud Adoption Framework](https://azure.microsoft.com/en-us/solutions/cloud-enablement/cloud-adoption-framework?WT.mc_id=AZ-MVP-5004796)\\n5. [Azure subscription and service limits, quotas and constraints](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?WT.mc_id=AZ-MVP-5004796)\\n6. [BevanSin/AzureSandbox](https://github.com/BevanSin/AzureSandbox)\\n7. [dazzlejim/AzureSandbox](https://github.com/dazzlejim/AzureSandbox)\\n8. [Azure Sandbox Environment - Turbo360 Podcast](https://www.youtube.com/watch?v=dkNAnw1InA0)"},{"id":"azure/tag-azure-resources-owner-azure-automation-runbook","metadata":{"permalink":"/azure/tag-azure-resources-owner-azure-automation-runbook","source":"@site/blog/2024-03-06-tag-owner-azresource/index.mdx","title":"Tag Azure Resources with Owner using Azure Automation","description":"Tag Azure Resources with Owner using Azure Automation runbook","date":"2024-03-07T07:10:25.200Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.035,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Tag Azure Resources with Owner using Azure Automation","metaDescription":"Tag Azure Resources with Owner using Azure Automation runbook","date":"2024-03-07T07:10:25.200Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"BlogHeading_TagAzureResourceswithOwnerAzureAutomation.png"},"slug":"azure/tag-azure-resources-owner-azure-automation-runbook","keywords":["azure","runbook","tag","azureresource","azureautomation","powershell"],"description":"Tag Azure Resources with Owner using Azure Automation runbook"},"unlisted":false,"prevItem":{"title":"Microsoft Azure - Sandbox Design Considerations","permalink":"/azure/microsoft-azure-sandbox-design-considerations"},"nextItem":{"title":"Enterprise Policy as Code with Azure DevOps","permalink":"/azure/enterprise-policy-code-azure-devops"}},"content":"Inspired by [Tagging Azure Resources with a Creator](https://techcommunity.microsoft.com/t5/core-infrastructure-and-security/tagging-azure-resources-with-a-creator/ba-p/1479819?WT.mc_id=AZ-MVP-5004796) an Azure function + event grid solution, that will tag resources with the creator of the resource. I wanted to see if I could do the same thing using Azure Automation runbooks, instead of using event grid but a schedule instead, to make use of an already existing [Azure Automation](https://learn.microsoft.com/azure/automation/overview?WT.mc_id=AZ-MVP-5004796) account.\\n\\nIt turns out you can, so let\'s take a look.\\n\\n![Blog Heading - Tag Azure Resources with Owner using Azure Automation runbook](BlogHeading_TagAzureResourceswithOwnerAzureAutomation.png)\\n\\n{/*truncate*/}\\n\\nMake sure to change the ManagementGroupID variable to match your own environment.\\n\\nThis runbook uses a System Managed Identity from the Azure Automation account, so make sure this has Contributor rights to the subscription or management group you want to tag resources in. \\n\\n:::info\\nThis PowerShell script is designed to tag Azure resources with the user\'s email address who last modified them. It\'s particularly useful for tracking who is responsible for each resource in your Azure environment. Here\'s a step-by-step breakdown of what the script does:\\n1. **Disable AzContext Autosave**: The script starts by disabling the autosave feature for the Azure context. This ensures the script doesn\'t inherit any Azure context from previous sessions.\\n2. **Suppress Warnings**: The script suppresses warnings related to breaking changes in Azure PowerShell. This is to prevent these warnings from cluttering the output.\\n3. **Import Modules**: The script imports the Az.Accounts and Az.Resources modules, which provide the cmdlets needed to interact with Azure.\\n4. **Define Variables**: The script defines a variable for the tag name (`$tagName`) and the management group ID (`$ManagementGroupID`).\\n5. **Connect to Azure**: The script uses a Managed Service Identity.\\n6. **Get Subscriptions**: The script defines a function (`Get-AzSubscriptionsFromManagementGroup`) that retrieves all the subscription IDs under a specified management group, including subscriptions under child management groups. It then calls this function to get the subscription IDs under the management group specified by `$ManagementGroupID`.\\n7. **Process Each Subscription**: For each subscription ID retrieved, the script sets the Azure context to that subscription and retrieves all resources in the subscription.\\n8. **Process Each Resource**: For each resource in the subscription, the script checks if the resource has a tag with the name specified by `$tagName`.\\n9. **Add Tag If Not Present**: If the resource does not have a tag with the name specified by `$tagName`, the script retrieves the Azure activity logs for the resource for the past seven days and finds the user\'s email address who last modified the resource. It then adds a tag to the resource with the name specified by `$tagName` and the value set to the user\'s email address.\\nThis script is designed to be run on a schedule, such as once a day, to ensure that all resources are tagged with the user\'s email address who last modified them.\\n\\n```mermaid\\ngraph TD\\n  A[Start] --\x3e B[Disable AzContext Autosave]\\n  B --\x3e C[Suppress Warnings]\\n  C --\x3e D[Import Modules]\\n  D --\x3e E[Define Variables]\\n  E --\x3e F[Connect to Azure]\\n  F --\x3e G[Get Subscriptions]\\n  G --\x3e H{For Each Subscription}\\n  H --\x3e I[Set Azure Context]\\n  I --\x3e J[Get All Resources]\\n  J --\x3e K{For Each Resource}\\n  K --\x3e L[Check If Tag Exists]\\n  L --\x3e |No|M[Get Azure Activity Logs]\\n  M --\x3e N[Find Last Modified User]\\n  N --\x3e O[Add Tag]\\n  O --\x3e P[End]\\n  L --\x3e |Yes|P\\n  K --\x3e H\\n  ```\\n:::\\n\\n![Tag Azure Resources with Owner](Tag_Owner-RunbookRun.gif)\\n\\nThe Runbook is as follows:\\n\\n```powershell\\n<# Ensures you do not inherit an AzContext in your runbook #>\\nDisable-AzContextAutosave -Scope Process | Out-Null;\\n\\n#Toggle to stop warnings with regard to Breaking Changes in Azure PowerShell\\nSet-Item -Path Env:\\\\SuppressAzurePowerShellBreakingChangeWarnings -Value $true\\n\\n# Import the required modules\\nImport-Module Az.Accounts\\nImport-Module Az.Resources\\n\\n# Define the tag name as a variable\\n$tagName = \\"Createdby\\"\\n\\n#Adjust to suit your management group; this is the top scope that the Script will run under\\n$ManagementGroupID = \'mg-landingzones\'\\n\\n\\n<# Connect using a Managed Service Identity #>\\n\\nConnect-AzAccount -Identity\\n\\n\\n\\n# Get the subscription IDs under the specified management group AND child management groups\\nfunction Get-AzSubscriptionsFromManagementGroup {\\n    param($ManagementGroupName)\\n    $mg = Get-AzManagementGroup -GroupId $ManagementGroupName -Expand\\n    foreach ($child in $mg.Children) {\\n        if ($child.Type -match \'/managementGroups$\') {\\n            Get-AzSubscriptionsFromManagementGroup -ManagementGroupName $child.Name\\n        }\\n        else {\\n            $child | Select-Object @{N = \'Name\'; E = { $_.DisplayName } }, @{N = \'Id\'; E = { $_.Name } }\\n        }\\n    }\\n}\\n\\n\\nWrite-Output \\"Setting ManagementGroupID to $($mgid.DisplayName)\'...\\"\\n\\nWrite-Output \\"Retrieving management group with ID \'$ManagementGroupID\'...\\"\\n$mgid = Get-AzManagementGroup -GroupId $ManagementGroupID -Expand\\n\\nWrite-Output \\"Successfully retrieved management group with ID \'$ManagementGroupID\'.\\"\\n\\nWrite-Output \\"Retrieving subscription IDs from management group \'$($mgid.DisplayName)\'...\\"\\n\\n$subIds = Get-AzSubscriptionsFromManagementGroup -ManagementGroupName $ManagementGroupID \\n\\nforeach ($subId in $subIds) {\\n    Write-Output \\"Setting subscription context for subscription $subId...\\"\\n    Set-AzContext -Subscription $subId.Id\\n\\n    $resources = Get-AzResource \\n\\n    Write-Output \\"Found resources in subscription $subId.\\"\\n\\n    foreach ($resource in $resources) {\\n        Write-Output \\"Processing resource $($resource.Name)...\\"\\n        $tags = $resource.Tags\\n        if ($null -eq $tags -or -not $tags.ContainsKey($tagName)) {        \\n            Write-Output \\"Resource $($resource.Name) does not have \'resource-owner\' and  tags. Adding tags...\\"\\n\\n            $endTime = Get-Date\\n            $startTime = $endTime.AddDays(-7)\\n            $owners = Get-AzLog -ResourceId $resource.ResourceId -StartTime $startTime -EndTime $endTime |\\n            Where-Object { $_.Authorization.Action -like \\"*/write*\\" } |\\n            Select-Object -ExpandProperty Caller \\n            $owner = $owners | Where-Object { $_ -match \\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\\" } | Select-Object -First 1\\n\\n            #Objects created by a Service Principal will tag the objects with a GUID instead of a name by default. You can fix this behavior by giving the Managed Identity the Application Developer role in Entra ID. \\n\\n            # If owner is null, stop the script\\n            if ($null -eq $owner) {\\n                Write-Output \\"No owner found that matches an email address.\\"\\n            }\\n            \\n            # Output owners\\n            Write-Output \\"Owners: $owners, selected owner: $owner\\"\\n            $existingTags = $resource.Tags\\n            $modifiedTags = @{\\n                $tagName = $owner\\n            }\\n            # Merge existing tags with new tags\\n            $allTags = $existingTags + $modifiedTags\\n\\n            $resource | Set-AzResource -Tag $allTags -Force\\n        }\\n    }\\n}\\n```\\n\\n:::info\\nThe script is also held on [GitHub](https://github.com/lukemurraynz/Azure/blob/main/Azure%20Automation/Tag-ResourceOwner.ps1), so feel free to raise an Issue or Pull Request if you have any improvements.\\nIf there are no logged users due to the resource being written to or created outside of the Log retention, the resource will be skipped.\\n:::\\n\\nThis script could be extended with the [Change Actor feed](https://techcommunity.microsoft.com/t5/azure-governance-and-management/announcing-the-public-preview-of-change-actor/ba-p/4076626?WT.mc_id=AZ-MVP-5004796#:~:text=Identifying%20who%20made%20a%20change,all%20your%20tenants%20and%20subscriptions) to determine who made a recent change to the resource."},{"id":"azure/enterprise-policy-code-azure-devops","metadata":{"permalink":"/azure/enterprise-policy-code-azure-devops","source":"@site/blog/2024-02-19-epac/index.mdx","title":"Enterprise Policy as Code with Azure DevOps","description":"Enterprise Azure Policy as Code, or EPAC for short, comprises a number of scripts which can be used in a CI/CD-based system or a semi-automated use to deploy Policies, Policy Sets, Assignments, Policy Exemptions and Role Assignments. Let us take a look at its use.","date":"2024-02-18T23:25:11.692Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":18.97,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Enterprise Policy as Code with Azure DevOps","metaDescription":"Enterprise Azure Policy as Code or EPAC for short comprises a number of scripts which can be used in CI/CD based system or a semi-automated use to deploy Policies, Policy Sets, Assignments, Policy Exemptions and Role Assignments. Lets take a look at its use.","date":"2024-02-18T23:25:11.692Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"BlogHeading_EPAC_ADO.png"},"slug":"azure/enterprise-policy-code-azure-devops","keywords":["azure","policy","azurepolicy","security","iac"],"description":"Enterprise Azure Policy as Code, or EPAC for short, comprises a number of scripts which can be used in a CI/CD-based system or a semi-automated use to deploy Policies, Policy Sets, Assignments, Policy Exemptions and Role Assignments. Let us take a look at its use."},"unlisted":false,"prevItem":{"title":"Tag Azure Resources with Owner using Azure Automation","permalink":"/azure/tag-azure-resources-owner-azure-automation-runbook"},"nextItem":{"title":"Accessing KeyVault from Azure DevOps","permalink":"/azure/accessing-keyvault-azure-devops"}},"content":"Enterprise Azure Policy as Code (EPAC) comprises a number of scripts which can be used in a CI/CD-based system or a semi-automated use to deploy Azure Policies, Policy Sets, Assignments, Policy Exemptions and Role Assignments! This is a great way to ensure that your Azure environment complies with your company\'s policies and standards, so let us look at it!\\n\\n![Blog Heading - EPAC ADO](BlogHeading_EPAC_ADO.png)\\n\\n{/*truncate*/}\\n\\n[Enterprise Azure Policy as Code (EPAC)](https://azure.github.io/enterprise-azure-policy-as-code/) allows you to define and deploy your Azure policies and exemptions as code. This is a great way to ensure consistency of one or more Azure environments, including managing your exemptions in a way that is auditable and a way that can be integrated into CI/CD pull request approval processes.\\n\\nSo, let\'s delve a little further into it.\\n\\n## What is Enterprise Azure Policy as Code?\\n\\n:::info\\nEnterprise Azure Policy as Code, or EPAC for short, comprises a number of scripts which can be used in a CI/CD-based system or a semi-automated use to deploy Policies, Policy Sets, Assignments, Policy Exceptions and Role Assignments.\\n\\nMain features include:\\n\\n* Multi-tenant/environment policy deployment\\n* Easy CI/CD Integration\\n* Extract existing policy objects from an environment\\n* Support JSON and CSV inputs for large, complex policies\\n* PowerShell Module\\n* Integration with Azure Landing Zone recommended policies\\n* Starter Kit with examples\\n* Schema to provide Intellisense for VS Code development\\n:::\\n\\nEnterprise Policy as Code, runs primarily on PowerShell scripts, at the time of writing, there is 114 PowerShell scripts that make up this solution *(not all are in use for day-to-day operations)*, from the actual deployment to the plan, importing existing policies and initiatives, creating GitHub or Azure DevOps issues, for policy remediation tasks and even importing Azure Landing Zone policies into the solution, this solution is jam-packed with an ever-expanding toolset.\\n\\n:::tip\\nIt is worth noting that although this solution was designed and implemented by Microsoft employees, this is an [Open Source initiative](https://opensource.microsoft.com/codeofconduct/) and is not officially supported by Microsoft, but the community is very active, and the solution is constantly being updated and improved upon. All [Issues](https://github.com/Azure/enterprise-azure-policy-as-code/issues) can be raised directly on GitHub, and if you have any queries, concerns or issues, I suggest you look here first.\\n:::\\n\\n## Why would you use Enterprise Azure Policy as Code?\\n\\nSo, why would you use EPAC? Before we go into that - let\'s go back to basics - Azure Policy.\\n\\n![What is Azure Policy](WhatIsAzurePolicy.png)\\n\\n[Azure policies](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796) are essential to Azure governance; they are the \'make it so\' of the Azure environment *(and beyond)*.\\n\\nThe policies exist to help across areas such as:\\n\\n* Maintain regulatory compliance with industry standards\\n* Security and performance consistencies\\n* Enterprise-wide design principles\\n* Controlling cost\\n\\nBut how do they do that? Let us take a look at the definitions that make them sing!\\n\\nFor this, we are to look at some of the key components of a definition for the \'Function apps should authenticate to Azure Container Registry using a managed identity\' policy.\\n\\n![Azure Policy definition](PolicyDefinition.png)\\n\\n* Metadata\\n* Parameters\\n* Rules\\n\\nMetadata includes items such as: DisplayName, Mode *(Indexed vs All)*, version and categories. \\nParameters give you the flexibility to adjust your policy to your environment, in this case, the policy is looking for a specific Azure Container Registry, but you could have a parameter for the resource group, the subscription, or even the identity that it uses, which can be adjusted to be unique between environments.\\nRules are the actual policy; in this case, the policy is looking for a specific identity to be used to authenticate to the Azure Container Registry but will contain the type of resources that the policy will affect, the aliases or resource properties that it needs to look for to evaluate against, and even change.\\n\\nSo, why would you use EPAC? Imagine you have a large number of policies, a large number of environments, or even a large number of policy exemptions, managing your policies as code is a great way to manage them all in a consistent and auditable way, especially when you are working with different versions of policies, and policies deployed to different scopes as well.\\n\\n![Azure Policy Scopes](EnterprisePolicyAsCode_PolicyAssignmentScope.png)\\n\\n## How do you use Enterprise Azure Policy as Code?\\n\\nNow that we have looked into some of the complexity of Azure policies, especially at scale. Let us take a look into how we can get going with [Enterprise Policy as Code](https://azure.github.io/enterprise-azure-policy-as-code/).\\n\\nIn this article, I am going to assume a few things:\\n\\n1. You have permission to deploy policies and policy assignments in your Azure environment\\n2. You have an Azure DevOps environment (we will use Azure DevOps pipelines in this article, however there is also GitHub action workflows that can be used if you are using GitHub as well) and [Service Connection configured](https://azure.github.io/enterprise-azure-policy-as-code/ci-cd-app-registrations/).\\n\\nWe are going to leverage the EPAC [StarterKit](https://github.com/Azure/enterprise-azure-policy-as-code/tree/main/StarterKit), for the pipelines and scripts.\\n\\nBefore we get CI/CD pipelines set up, we need to do a few things locally first.\\n\\n### Step 1: Import existing policies\\n\\nSo, let\'s import our existing policies and set up our environment! I highly recommend importing your existing environment first, as this will give you a good starting point for your policies, policy sets and exemptions.\\n\\n:::info\\nThis requires a computer running: [PowerShell](https://learn.microsoft.com/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.4&WT.mc_id=AZ-MVP-5004796) 7.3.1 or later, 7.3.4 *(latest)* recommended, and the [Azure PowerShell](https://learn.microsoft.com/powershell/azure/install-azure-powershell?view=azps-11.3.0&WT.mc_id=AZ-MVP-5004796) module installed (Az required 9.3.0 or later).\\n:::\\n\\nIn my environment, I have a range of Custom and Builtin policies and initiatives, that are mostly deployed to at the top level, including Sandbox policies, and am I also running an NZISM 3.5 initiative, which is a builtin iniative but not the most up-to-date, so as part of the EPAC implementation, I want to replace the policy assignment with 3.6 custom and assign my Sandbox policies to the Sandbox Management Group.\\n\\nSo let us get cracking!\\n\\n1. First, we need to install the EPAC PowerShell module and the Az module and connect to our Azure environment.\\n\\n```powershell\\n    Install-Module EnterprisePolicyAsCode -Scope CurrentUser\\n    Install-Module Az -Scope CurrentUser\\n    Connect-AzAccount\\n```\\n![Install PowerShell modules](EPAC_InstallPowerShellModules.gif)\\n\\n2. Once that\'s installed and you are connected to the Azure environment that you want to export your policies from, now we can do the export. To do the export, you have to generate a BuildDefinition folder to contain your policies, assignments and exemptions.\\n\\n\\n```powershell\\n    New-EPACDefinitionFolder -DefinitionsRootFolder Definitions\\n```\\n\\n![Create Definitions folder](EPAC_CreateDefinitionsFolder.gif)\\n\\nThat will create the following folders and files, giving us the base scaffolding for Enterprise Policy as Code:\\n\\n| Folder & Files        | Type   | Notes                                                                      |\\n| --------------------- | ------ | -------------------------------------------------------------------------- |\\n| policyAssignments     | Folder | Contains your Assignments (for both Policy and Initiatives)                |\\n| policyDefinitions     | Folder | Contains your Policy Definitions                                           |\\n| policyDocumentations  | Folder | Contains documentation about your policies                                 |\\n| policySetDefinitions  | Folder | Contains the definitions for your initiatives (or PolicySets)              |\\n| global-settings.jsonc | File   | Main configuration file, containing your environments and deployment paths |\\n\\n3. Now that we have our base, scaffold - before we can import our existing Definitions and Assignments, we need to edit the \'global-settings.jsonc\' file, to add in environment context.\\n\\n:::tip\\nThe file is a JSONC file, which is a JSON file with comments, so you can add comments to the file, which is great for documentation. The same jsonc format is also used for PolicyDefinitions and assignments, allowing you to add more context and notes to your policies and assignments! I encourage you to use this functionality to add notes and comments to your assignments!\\n:::\\n\\nSo open the global-settings.jsonc file, and paste the following example:\\n\\n```jsonc\\n    {\\n        \\"$schema\\": \\"https://raw.githubusercontent.com/Azure/enterprise-azure-policy-as-code/main/Schemas/global-settings-schema.json\\",\\n        \\"pacOwnerId\\": \\"f2ce1aea-944e-4517-94fb-edada00633ae\\", // Generate a guid using New-Guid and place it here\\n        \\"managedIdentityLocations\\": {\\n            \\"*\\": \\"australiaeast\\" // Update the default location for managed identities\\n        },\\n        \\"globalNotScopes\\": {\\n            \\"*\\": [\\n                \\"/resourceGroupPatterns/excluded-rg*\\"\\n            ]\\n        },\\n        \\"pacEnvironments\\": [\\n            {\\n                \\"pacSelector\\": \\"quick-start\\",\\n                \\"cloud\\": \\"AzureCloud\\",\\n                \\"tenantId\\": \\"bdb8ea1c-17da-4423-8895-6b79af002b4e\\", // Replace this with your tenant Id\\n                \\"deploymentRootScope\\": \\"/providers/Microsoft.Management/managementGroups/root\\" // Replace this with a management group that represents the functional root in your environment. \\n            }\\n        ]\\n    }\\n```\\n\\n![Create global-settings.jsconc](EPAC_Create-global-settings.jsconc.gif)\\n\\n**Reference: [EPAC Quick Start](https://azure.github.io/enterprise-azure-policy-as-code/quick-start/)**\\n\\nWe now need to define our overall environment, and we can start by using PowerShell to generate our own unique pacOwnerId.\\n\\n```powershell\\n    New-Guid\\n```\\nThe pacOwner helps to identify who or what owns an Assignment or Policy definition deployment and needs to be unique to your EPAC environment. The pacOwnerId is used to identity policy resources that are deployed by your EPAC repository, or another EPAC isntance, legacy or another solution entirely.\\n\\nAnother thing you may need to adjust is the managedIdentityLocations, this is the default location for managed identities that are used for policies that are DeployIfNotExist and Modify. This is the location that the managed identity will be created in, if it does not exist. I will keep this as the example, AustraliaEast which is the primary region that my resources are deployed into.\\n\\nIf you run multiple regions, you can remove the wildcard and replace it with the pacEnvironments name that represents your other regions.\\n\\nAlthough you can add exclusions to your individual Policy and PolicySetAssignments, the globalNotScopes is a way to exclude resources from all Policy and PolicySetAssignments, without having to add it to every single assignment, in this case, the example excludes all resources that are in a resource group that starts with \'excluded-rg\'. You can exclude resources at different scopes, ie Resource Group, Subscriptions, Resouce Groups.\\n\\nNow, lets take a look at pacEnvironments. pacEnvironments is a way to define your environments, and the deploymentRootScope is the scope that represents the functional root in your environment. This is the top scope that all your Policy and PolicySetAssignments will be deployed to, and is the scope that you will be deploying your policies definitions to.\\n\\nThe pacSelector will be used in your assignments to select what environment and scope you are deploying to, as you can have multiple environments, so give it a name that you can understand - ie \'epac-prod\', \'epac-dev\', \'epac-qa\' etc. This will be reused in your assignments and pipelines.\\n\\nMake sure the tenantId matches your Entra ID and the deploymentRootScope is the top-level scope that you want to deploy your policies to.\\n\\nSave the file.\\n\\n4. The next step is to import your existing policies and assignments, the commands will use the information you just defined in the \'global-settings.jsonc\' file, to review of the scope and the environment of the export.\\n\\nTo do that, we will go back to PowerShell:\\n\\n  ```powershell\\n    Export-AzPolicyResources -DefinitionsRootFolder .\\\\Definitions -OutputFolder Output\\n  ```\\n\\n![Export-AzPolicyResources.gif](Export-AzPolicyResources.gif)\\n\\n:::tip\\nIf you want to target a specific environment, ie you have a Management Group, where you want to deploy your policies and test using the Azure Portal first **(this can be useful for quickly importing assignments and policies in a format that\'s ready to go by EPAC)**, then you can add the \'-InputPacSelector\' parameter and target a specific environment for export, else it will do all environments listed in the global-settings.\\n:::\\n\\nDepending on the complexity of your environment, it could take a while, but once it is done, you will have a new folder called \'Output\', which will contain an export of your policy-ownerships, definitions, assignments and exemptions in an Enterprise Policy as Code useable format *(JSON)*.\\n\\nYour Output folder will contain the policy exemptions, assignments, and definitions of your existing environment.\\n\\nYou can copy the files and folders from the Output folder, into the original Definitions folder, originally created, to bring in your existing policies and assignments into your EPAC environment.\\n\\n:::tip\\nNow is a great time to look at those exemptions and policy assignments and see if they are still relevant, and if they are, add comments to the files to give context to the assignments and exemptions and to help with the deployment of the policies and assignments in the future.\\n:::\\n\\n### Step 2: Configure Azure DevOps\\n\\nNow that we have our policies and assignments imported, we can now configure our Azure DevOps environment, to automatically deploy them. I am going to go through the process from the beginning but feel free to skip to the relevant sections if you already have a project and repository setup.\\n\\n1. Login to Azure DevOps\\n2. Create a new project\\n3. Give the project a name, and a description, and select the visibility of the project **(ie Private)**\\n4. Click \'Create\'\\n5. Navigate to Repos\\n6. Click on Files\\n6. Initialize the repository with a README or add a new file\\n\\n![Create EPAC ADO Project](Create-ADO_EPACProject.gif)\\n\\nNow that the repository has been initialized. We can now add the files from the EPAC StarterKit to the repository.\\n\\nClone the repository to your local computer copy the Definitions folder to the repository and commit. \\n\\n:::info\\nYou can Clone the Git repository using [Visual Studio Code](https://azuredevopslabs.com/labs/azuredevops/git/?T.mc_id=AZ-MVP-5004796), or Git CLI or even [GitHub Desktop](https://luke.geek.nz/2021/12/30/git-using-github-desktop-on-windows-for-sysadmins/).\\n:::\\n\\nOnce cloned, copy the Definitions folder from the Output folder, into the repository, and commit the changes.\\n\\nNow that we have our repository setup, we can now create a pipeline to deploy our policies and assignments, to get started we will grab the Azure Pipelines from the [StarterKit](https://github.com/Azure/enterprise-azure-policy-as-code/tree/main/StarterKit/Pipelines) folder, and copy them into the repository, along with the Scripts folder.\\n\\nFor this article, we will use the single-tenant-pipeline.yml file, as we are only deploying to a single environment, but if you are deploying to multiple environments, you can use the multi-tenant-pipeline.yml file.\\n\\n![Create ADO Pipeline](Create_ADO-PipelineTemplate.gif)\\n\\nOnce downloaded, make sure you commit it to your repository. We will have to edit the pipeline, but before we do that - we need a Service Connection for Azure DevOps to connect to Azure to deploy the policies and assignments.\\n\\n1. Open your EPAC Azure DevOps project\\n2. Click on Project Settings\\n3. Select Service Connections\\n4. Click on Create Service Connection\\n5. Click Azure Resource Manager\\n6. Click Next\\n7. Select Workload Identity Federation (Automatic) and click Next\\n8. Select Management Group\\n9. Select your top-level management group, that matches your Enterprise Policy as Code environment (ie where the definitions will be deployed to)\\n10. Type in a name and description and click Save **(make sure this is a name that is easy to understand, and what its function is, we will reuse the name in our pipelines)**\\n\\n![Create Service Principal](Create_ADO-SPN.gif)\\n\\n:::info\\nEnterprise Policy as Code (EPAC) suggests the least privileged and just enough permissions to deploy the policies, assignment and RBAC **(Role Based Access Control)** where they need to.\\n\\nThe pipeline itself will look for the following separate Service Connections:\\n\\n* devServiceConnection - This service connection is used for development purposes, allowing the pipeline to connect to the development environment and deploy policies and assignments.\\n* tenantPlanServiceConnection - This service connection is used for managing the policy plan, which includes creating and updating policy definitions, policy sets, and policy initiatives.\\n* tenantDeployServiceConnection - This service connection is used for deploying policy assignments and exemptions to the target environment.\\n* tenantRolesServiceConnection - This service connection is used for managing RBAC (Role-Based Access Control) roles and permissions in the target environment.\\n\\nSo consider whether you have separate scopes and service principals for your environments.\\n:::\\n\\nFor the purposes of this article, I am going to use the same service principal to deploy all the EPAC functions, so let\'s edit the pipeline to use it.\\n\\n1. You can edit the Pipeline, either locally with Visual Studio Code and recommit it, or edit it directly in Azure DevOps, update the following variables with the relevant service connection names:\\n\\n* devServiceConnection \\n* tenantPlanServiceConnection\\n* tenantDeployServiceConnection                                                                               \\n* tenantRolesServiceConnection \\n\\n![Edit ADO Pipeline](Edit_ADO-Pipeline_SPN.gif)\\n\\nThe single-tenant Azure DevOps pipeline has 5 stages:\\n\\n| Stage                   | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                |\\n| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| devStage                | This stage is used for planning, deploying, and assigning roles in the development environment. It uses the\xa0devServiceConnection\xa0to connect to Azure. The stage is only executed if the build reason is \'Manual\', \'IndividualCI\', or \'BatchedCI\' and the source branch is not \'main\'.                                                                                                                                                |\\n| tenantPlanFeatureStage  | This stage is used for planning feature branches in the tenant environment. It uses the tenantPlanServiceConnection to connect to Azure. This stage depends on the devStage and is only executed if the devStage has not failed or been canceled, the build reason is \'Manual\', \'IndividualCI\', or \'BatchedCI\', and the source branch is not \'main\'.                                                                             |\\n| tenantPlanMainStage     | This stage is used for planning the main branch in the tenant environment. It uses the tenantPlanServiceConnection to connect to Azure. This stage depends on the tenantPlanFeatureStage and is only executed if the tenantPlanFeatureStage has not failed or been canceled, the build reason is \'Manual\', \'IndividualCI\', or \'BatchedCI\', and the source branch is \'main\'.                                                      |\\n| tenantDeployPolicyStage | This stage is used for deploying policies in the tenant environment. It uses the tenantDeployServiceConnection to connect to Azure. This stage depends on the tenantPlanMainStage and is only executed if the tenantPlanMainStage has not failed or been canceled, the tenantPlanMainStage has policy changes to deploy, the build reason is \'Manual\', \'IndividualCI\', or \'BatchedCI\', and the source branch is \'main\'.              |\\n| tenantRolesStage        | This stage is used for deploying role assignments in the tenant environment. It uses the tenantRolesServiceConnection to connect to Azure. This stage depends on the tenantDeployPolicyStage and is only executed if the tenantDeployPolicyStage has not failed or been canceled, the tenantPlanMainStage has role changes to deploy, the build reason is \'Manual\', \'IndividualCI\', or \'BatchedCI\', and the source branch is \'main\'. |\\n\\nThe name \'Tenant\' is used to represent the environment that the policies and assignments are being deployed to. Tenant is the default name of the EPAC environment, so we will need to change this, to reflect our own environments. This is the pacSelector name, in the global-settings.jsonc file, that we created earlier.\\n\\nI will go back and add a new Service Connection, for Development, and add another development environment to the global-settings.jsonc. You don\'t need to do this, but it does give you the option to test your policies and assignments before deploying them to your main environment.\\n\\nNow we need to edit the pipeline again, to align to our pacEnvironments.\\n\\n:::info\\nYou could do a a full Find and Replace, of the Word Tenant on the pipeline and replace it with your Production pacEnvironment name, just be wary that the stage name is used as a depedency for other steps, so you will need to make sure all references have been updated.\\n:::\\n\\nOnce you have made the changes, commit the pipeline to your repository.\\n\\n![Edit ADO Pipeline - pacEnvironment](Edit_ADO-Pipeline_pacEnvironments.gif)\\n\\nBefore we look at actually running the pipeline and redeploying the policies under Enterprise Policy as Code, we need to be aware of another environment option - and that is desiredState.\\n\\ndesiredState allows you to control, how much control EPAC has over your environment, ie how much of a \'make it so\'.\\n\\ndesiredState options are:\\n\\n* full\\n* ownedOnly\\n\\nIf the desired state strategy is \'Full\', then EPAC will manage all the policies and assignments in the environment and will remove any policies and assignments that are not in the EPAC repository.\\nIf the desiredState strategy is \'ownedOnly\', then EPAC will only manage the policies and assignments that are in the EPAC repository, and will not remove any policies and assignments that are not in the EPAC repository.\\n\\nFull is the default desiredState, so if you want to use ownedOnly, you will need to add the desiredState to the global-settings.jsonc file for the environment.\\n\\n  ```jsonc\\n // desiredState is an optional object that specifies the desired state of the environment.\\n            \\"desiredState\\": { // [optional]\\n                // strategy specifies the strategy to achieve the desired state. The default is \\"full\\".\\n                \\"ownedOnly\\": \\"full\\" // default full\\n            }\\n  ```\\n\\nAnother environment option is related to Defender for Cloud. [Defender for Cloud](https://learn.microsoft.com/azure/defender-for-cloud/policy-reference?WT.mc_id=AZ-MVP-5004796) uses Azure Policy Assignments to enable and configure the various capabilities.\\nEnterprise Policy as Code could remove the Defender for Cloud assignments if they are not in the EPAC repository, so you can add the following to the global-settings.jsonc file to prevent this from happening.\\n\\n  ```jsonc\\n   \\"desiredState\\": { // [optional]\\n                // strategy specifies the strategy to achieve the desired state. The default is \\"full\\".\\n                \\"strategy\\": \\"full\\" ,// default full\\n                \\"keepDfcSecurityAssignments\\": true // default false\\n            }\\n  ```\\n\\nIf set to true or strategy is ownedOnly, EPAC will not remove Security Policy assignments created by Defender for Cloud.\\nIf omitted or set to false and strategy is full, EPAC will remove Security Policy Set Assignments created by Defender for Cloud.\\n\\nEven though these settings are managed by the default behaviour of Enterprise Policy as Code, I prefer to make sure they are added in the configuration for awareness. Security Policies should be managed by EPAC at the Management Group level; this is the recommended approach for managing Security Policies instead of relying on the auto-assignments, but as usual, it comes down to business requirements.\\n\\n### Step 3: Run the Pipeline\\n\\n:::danger\\nMake sure you test your deployment in a development environment before deploying to your main environment, especially if you are using the \'full\' desiredState strategy.\\n:::\\n\\nNow it\'s time to import the pipeline.\\n\\n1. Login to Azure DevOps\\n2. Navigate to your EPAC project\\n3. Navigate to Pipelines\\n4. Click on Create Pipeline\\n5. Click on Azure Repos Git\\n6. Select your repository\\n7. Select Existing Azure pipelines YAML file\\n8. Under path, select single-tenant-pipeline.yml\\n9. Click Save\\n\\n![Import ADO Pipeline](Import_ADO-Pipeline.gif)\\n\\nNow that it\'s imported, we can run the pipeline. The first pipeline deployment is aimed at making the state of Azure match the EPAC environment and also giving access to the pipelines to the Service Connections.\\n\\n![Run ADO Pipeline](Run_ADO-Pipeline.gif)\\n\\nIf this is the first time the pipeline has been run, there may be delays, as you need to approve the pipeline to be able to use the Service Connections.\\n\\nCongratulations! You have deployed your Azure Policy as Code!\\n\\nIf you login to the Azure Portal and navigate to your [Policies and assignments](https://portal.azure.com/#view/Microsoft_Azure_Policy/PolicyMenuBlade/~/Assignments), they should match.\\n\\n![Azure Policy Assignments](AzurePortal_PolicyAssignments.png)\\n\\n## Policy configuration\\n\\nNow that the policies and assignments are deployed, you can start to configure your policies and assignments to match your environment.\\n\\n### Change assignment scope\\n\\nIn my scenario, my Sandbox policies are assigned to my Lukegeeknz Management Group, and I want them to be assigned to my Sandbox Management Group, so I will need to update the policy assignments to reflect this.\\n\\nI will test this by deploying it into a New Branch, which will trigger the dev Plan.\\n\\n![Change assignment scope](Change_EPAC-AssignmentScopeSandbox.gif)\\n\\nAnd we can see in the plan that the assignment will change successfully. \\n\\n![Azure DevOps epac-dev plan](Run_ADO-Pipeline_Dev_Plan.png)\\n\\nNow, let\'s merge or change into Production by opening up a Pull Request; once approved and merged, the Sandbox assignment will be updated in Production.\\n\\n![EPAC Pull Request](Change_EPAC-AssignmentScopeSandboxPR.gif)\\n\\n![Azure Policy Assignment](AzurePortal_PolicyAssignments.pngAfterChange.png)\\n\\n## Reference\\n\\nYou can view my EPAC code for reference directly on GitHub: [lukemurraynz/EPAC_ADO](https://github.com/lukemurraynz/EPAC_ADO)."},{"id":"azure/accessing-keyvault-azure-devops","metadata":{"permalink":"/azure/accessing-keyvault-azure-devops","source":"@site/blog/2024-02-04-azure-devops-keyvault/index.mdx","title":"Accessing KeyVault from Azure DevOps","description":"Access an Azure Key vault from a Microsoft-hosted agent in Azure DevOps","date":"2024-02-03T10:56:08.514Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.225,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Accessing KeyVault from Azure DevOps","metaDescription":"Access an Azure Key vault from a Microsoft-hosted agent in Azure DevOps","date":"2024-02-03T10:56:08.514Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"BlogHeadingAccessingKeyVaultfromAzureDevOps.gif"},"slug":"azure/accessing-keyvault-azure-devops","keywords":["azure","cloudnative","keyvault","security","devops"],"description":"Access an Azure Key vault from a Microsoft-hosted agent in Azure DevOps"},"unlisted":false,"prevItem":{"title":"Enterprise Policy as Code with Azure DevOps","permalink":"/azure/enterprise-policy-code-azure-devops"},"nextItem":{"title":"Using the Azure Naming Tool API to name your Bicep resources","permalink":"/azure/azure-naming-tool-api-bicep-resources"}},"content":"If you are running a [Microsoft-hosted Azure DevOps agent](https://learn.microsoft.com/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser&WT.mc_id=AZ-MVP-5004796#microsoft-hosted-agents), you may need to access a [KeyVault](https://azure.microsoft.com/products/key-vault?WT.mc_id=AZ-MVP-5004796) to retrieve secrets. This is a common scenario when deploying resources to Azure. \\n\\nIn this post, I will show you how to access a KeyVault from an Azure DevOps pipeline by adding the IP of the Azure DevOps agent directly into your Azure Keyvault and removing it after it retrieves the secrets.\\n\\n![Accessing KeyVault from a Azurne DevOps\xa0Microsoft\xa0Hosted\xa0Agent](BlogHeadingAccessingKeyVaultfromAzureDevOps.gif \\"Accessing KeyVault from a Azurne DevOps\xa0Microsoft\xa0Hosted\xa0Agent\\")\\n\\n{/*truncate*/}\\n\\nWhen attempting to retrieve secrets from a KeyVault using Azure DevOps, you may get an error such as:\\n\\n:::warning\\n##[error]Get secrets failed. Error: Client address is not authorized and caller is not a trusted service.\\n:::\\n\\nThis is due to the Firewall of the Keyvault being enabled, which is best practice.\\n\\nYou have a few options here:\\n\\n* Option 1: Enable private endpoint for the KeyVault and use a [self-hosted agent](https://learn.microsoft.com/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser&WT.mc_id=AZ-MVP-5004796#self-hosted-agents). This is the most secure option, but it requires more setup and configuration, and a self-hosted agent, running on a Virtual Machine or [Container Apps](https://luke.geek.nz/azure/hosted-agents-container-apps-job/).\\n* Option 2: Add the IP of the Azure DevOps agent to the KeyVault firewall rules. This is the easiest option, but it\'s not the most secure, as the IP address of the Microsoft hosted agents can change *(the Microsoft hosted agents have a dynamic IP address)*.\\n\\nNot all organisations have the resources to support a self-hosted agent, so we will look at Option 2.\\n\\n![Azure KeyVault - Networking Blade (Azure Portal)](Azure_KeyVault_NetworkingPane.png \\"Azure KeyVault - Networking Blade (Azure Portal)\\")\\n\\nAs a status, the Microsoft Hosted agents can come from any number of [Microsoft-owned IP addresses](https://www.microsoft.com/en-us/download/details.aspx?id=56519?WT.mc_id=AZ-MVP-5004796) from the region, that your Azure DevOps organisation is hosted.\\n\\nYou may immediately want to check the **Allow trusted Microsoft services to bypass this firewall** as an instant thought, but [ Azure DevOps](https://learn.microsoft.com/en-us/azure/key-vault/general/network-security?WT.mc_id=AZ-MVP-5004796#key-vault-firewall-enabled-trusted-services-only) is not an assumed trusted Microsoft service, and its not hosted within your subscription boundary.\\n\\nSo, that leaves us with enabling public access to the key vault, right? Because the IP will be different each time? Not quite! What we can do is:\\n\\n1. Add the IP of the Azure DevOps agent to the KeyVault firewall rules.\\n2. Do the task, required from Azure DevOps (i.e. retrieve the secrets)\\n3. Remove the IP of the Azure DevOps agent from the KeyVault firewall rules.\\n\\nSo, let\'s take a look at the Tasks you can use in your Azure DevOps pipelines to achieve this.\\n\\nThe following tasks use an Azure CLI command to grab the IP address of the Azure DevOps agent and then add it to the KeyVault firewall rules, and then remove it after the secrets have been retrieved, tasks below. References to the KeyVaultArmSvcConnectionName is the Service Connection that is used to authenticate to the Azure Subscription, so make sure this is adjusted for your environment.\\nThe keyVaultName is the name of the keyVault you want to access.\\n\\nNote: The condition criteria is set to succeed or fail so that the task will run regardless of whether previous tasks have succeeded or failed.\\n\\n# Add Agent IP to KeyVault Firewall\\n\\n```yaml title=\\"AzureCLI@2.yml\\"\\n# This task uses Azure CLI to add the IP address of the Azure DevOps agent to the firewall of an Azure Key Vault.\\n- task: AzureCLI@2  # Specifies the task to use Azure CLI version 2.\\n  displayName: Add Agent IP From Key Vault  # The name displayed for this task in the Azure DevOps pipeline.\\n  condition: succeededOrFailed()  # This task will run regardless of whether previous tasks have succeeded or failed.\\n  inputs:  # The section where you specify input parameters for the task.\\n    azureSubscription: ${{ parameters.keyVaultArmSvcConnectionName }}  # The Azure subscription that the task should use. The value is taken from the keyVaultArmSvcConnectionName parameter.\\n    scriptType: bash  # Specifies that the script to be run is a Bash script.\\n    scriptLocation: inlineScript  # Specifies that the script will be provided directly in the pipeline file, rather than being located in an external file.\\n    inlineScript: |  # Begins the section where the actual script is provided. The | character indicates that a multi-line string will follow.\\n      agentIP=$(curl -s https://checkip.amazonaws.com)  # Uses the curl command to get the public IP address of the Azure DevOps agent and stores it in the agentIP variable.\\n      echo \\"Agent IP: $agentIP\\"  # Prints the IP address to the console.\\n      az keyvault network-rule add --name \\"${{ parameters.keyVaultName }}\\" --ip-address \\"$agentIP\\" --only-show-errors  # Uses the Azure CLI to add the IP address of the Azure DevOps agent to the firewall of the Azure Key Vault specified by the keyVaultName parameter. The --only-show-errors option means that the command will only output information if an error occurs.\\n```\\n\\n# Remove Agent IP to KeyVault Firewall after your other tasks\\n\\n```yaml title=\\"AzureCLI@2.yml\\"\\n# This task uses Azure CLI to remove the IP address of the Azure DevOps agent from the firewall of an Azure Key Vault.\\n- task: AzureCLI@2  # Specifies the task to use Azure CLI version 2.\\n  displayName: Remove Agent IP From Key Vault  # The name displayed for this task in the Azure DevOps pipeline.\\n  condition: succeededOrFailed()  # This task will run regardless of whether previous tasks have succeeded or failed.\\n  inputs:  # The section where you specify input parameters for the task.\\n    azureSubscription: ${{ parameters.keyVaultArmSvcConnectionName }}  # The Azure subscription that the task should use. The value is taken from the keyVaultArmSvcConnectionName parameter.\\n    scriptType: bash  # Specifies that the script to be run is a Bash script.\\n    scriptLocation: inlineScript  # Specifies that the script will be provided directly in the pipeline file, rather than being located in an external file.\\n    inlineScript: |  # Begins the section where the actual script is provided. The | character indicates that a multi-line string will follow.\\n      agentIP=$(curl -s https://checkip.amazonaws.com)  # Uses the curl command to get the public IP address of the Azure DevOps agent and stores it in the agentIP variable.\\n      echo \\"Agent IP: $agentIP\\"  # Prints the IP address to the console.\\n      az keyvault network-rule remove --name \\"${{ parameters.keyVaultName }}\\" --ip-address \\"$agentIP\\" --only-show-errors  # Uses the Azure CLI to remove the IP address of the Azure DevOps agent from the firewall of the Azure Key Vault specified by the keyVaultName parameter. The --only-show-errors option means that the command will only output information if an error occurs.\\n ```"},{"id":"azure/azure-naming-tool-api-bicep-resources","metadata":{"permalink":"/azure/azure-naming-tool-api-bicep-resources","source":"@site/blog/2024-01-16-azure-naming-tool-api-bicep/index.mdx","title":"Using the Azure Naming Tool API to name your Bicep resources","description":"Deployment of Azure Naming Tool into an Azure WebApp and testing the API.","date":"2024-01-16T03:44:02.021Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.985,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Using the Azure Naming Tool API to name your Bicep resources","metaDescription":"Deployment of Azure Naming Tool into an Azure WebApp and testing the API.","date":"2024-01-16T03:44:02.021Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"/uploads/azurenamingtoollogo.png"},"slug":"azure/azure-naming-tool-api-bicep-resources","keywords":["api","azure","cloudnative","NamingTool","WebApp","bicep"],"description":"Deployment of Azure Naming Tool into an Azure WebApp and testing the API."},"unlisted":false,"prevItem":{"title":"Accessing KeyVault from Azure DevOps","permalink":"/azure/accessing-keyvault-azure-devops"},"nextItem":{"title":"Add existing Azure resource to a Deployment Stack","permalink":"/azure/existing-resource-deploymentstack"}},"content":"The [Azure Naming Tool](https://github.com/mspnp/AzureNamingTool) was created to help administrators define and manage their naming conventions for Azure resources while providing a simple interface for users to generate a compliant name. \\nThe tool was developed using a naming pattern based on [Microsoft\'s best practices](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/azure-best-practices/naming-and-tagging?WT.mc_id=AZ-MVP-5004796). Once an administrator has defined the organizational components, users can use the tool to generate a name for the desired Azure resource.\\n\\nToday, we will use the Azure Naming Tool API to generate a name for our storage account bicep resource.\\n\\n![Azure Naming Tool Logo](/uploads/azurenamingtoollogo.png \\"Azure Naming Tool Logo\\")\\n\\n{/*truncate*/}\\n\\n## Overview\\n\\n:::note\\nI had a previous Blog article for [Deploying Azure Naming Tool into an Azure WebApp as a container](https://luke.geek.nz/2022/07/11/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container/) this was a previous version, as of June 2022, but check it out, if you run into issues with your deployment. Today, we will deploy the latest version at the time of this article straight to a WebApp.\\n:::\\n\\n:::tip\\nThe process is reasonably well documented directly on [Wiki of GitHub repository](https://github.com/mspnp/AzureNamingTool/wiki) for Azure Naming Tool, and it is what I followed, but to trigger the Publishing Profile button (if it greyed out) in the Azure Portal, for a brand new web app, I was able to go into Configuration/General Settings and Turn Basic Auth Publishing Off, Save, then back on, Save, and the button was enabled.\\n:::\\n\\nSo, now we have our Azure Naming Tool set up and deployed; we can test it out by using the API to generate a name for our storage account bicep resource.\\n\\n## Scripts\\n\\n> To do this, we will have three scripts:\\n\\n* **main.bicep** - This is our main bicep file, which will deploy our storage account and use the Azure Naming Tool API to generate a name for it.\\n* **command.ps1** - This will be our PowerShell script, which will call the Azure Naming Tool API, generate a name for our storage account, and then trigger the Bicep deployment.\\n* **Request-ResourceName.ps1** - This is our PowerShell function that will call the Azure Naming Tool API and return the generated name value.\\n\\nCommand.ps1 will call the other scripts.\\n\\nSo let us take a look at those:\\n\\n```powershell title=\\"command.ps1\\"\\n# Check if the Az.Accounts module is installed\\n$module = Get-Module -ListAvailable -Name Az.Accounts\\n\\n# If the module is not installed, install it\\nif ($null -eq $module) {\\n    Install-Module -Name Az.Accounts -Scope CurrentUser -Force\\n}\\n\\n# Check if the Az.Resources module is installed\\n$module = Get-Module -ListAvailable -Name Az.Resources\\n\\n# If the module is not installed, install it\\nif ($null -eq $module) {\\n    Install-Module -Name Az.Resources -Scope CurrentUser -Force\\n}\\n\\n# Connect-AzAccount -UseDeviceAuthentication\\n\\n# Import the Request-ResourceName function\\n. .\\\\Request-ResourceName.ps1\\n\\n# Define the API key and the Azure Naming Tool site name\\n$API = \'\'\\n$AzNamingToolSiteName = \'\'\\n\\n# Call the function with some example parameters\\n$storageAccountName = Request-ResourceName -API $API -AzNamingToolSiteName $AzNamingToolSiteName -resourceEnvironment \'dev\'  -resourceInstance \'1\' -resourceLocation \'aue\'  -resourceType \'st\' -createdBy \'me\' -resourceProjAppSvc \'spa\' | Select-Object -ExpandProperty resourceName\\n\\n# Define the resource group and location\\n$resourceGroupName = \'stgaccount_rg\'\\n\\n# Create the resource group if it doesn\'t exist\\n$resourceGroup = Get-AzResourceGroup -Name $resourceGroupName -ErrorAction SilentlyContinue\\nif ($null -eq $resourceGroup) {\\n    New-AzResourceGroup -Name $resourceGroupName -Location \'australiaeast\'\\n}\\n\\n# Deploy the Bicep file\\nNew-AzResourceGroupDeployment -ResourceGroupName $resourceGroupName -TemplateFile \'./main.bicep\' -storageAccountName $storageAccountName\\n```\\n\\n```bicep title=\\"main.bicep\\"\\nparam storageAccountName string \\nparam location string = resourceGroup().location\\n\\nresource storageacc \'Microsoft.Storage/storageAccounts@2023-01-01\' = {\\n  name: storageAccountName\\n  location: location\\n  properties: {\\n    accessTier: \'Hot\'\\n  }\\n  sku: {\\n  \\n    name:  \'Standard_LRS\'\\n  }\\n  kind:  \'BlobStorage\'\\n}\\n```\\n```powershell title=\\"Request-ResourceName.ps1\\"\\n\\n# Define the function that requests a resource name\\nfunction Request-ResourceName {\\n    # Define the parameters that the function accepts\\n    param (\\n        [Parameter(Mandatory=$true)] [string] $API,  # The API key\\n        [Parameter(Mandatory=$true)] [string] $AzNamingToolSiteName,  # The Azure Naming Tool site name\\n        [Parameter(Mandatory=$true)]\\n        [ValidateSet(\'dev\', \'prd\', \'sbx\', \'shd\', \'stg\', \'tst\', \'uat\')]\\n        [ValidateLength(1,5)]\\n        [string] $resourceEnvironment,  # The environment of the resource\\n        [Parameter(Mandatory=$false)] [string] $resourceFunction,  # The function of the resource\\n        [Parameter(Mandatory=$true)] [string] $resourceInstance,  # The instance of the resource\\n        [Parameter(Mandatory=$true)]\\n        [ValidateSet(\'aue\', \'aus\', \'nzn\', \'\\tusw\')]\\n        [ValidateLength(1,10)]\\n        [string] $resourceLocation,  # The location of the resource\\n        [Parameter(Mandatory=$false)] [string] $resourceOrg,  # The organization of the resource\\n        [Parameter(Mandatory=$false, HelpMessage=\\"The shortcode of the project. This needs to match an existing Project in Azure Naming Tool.\\")]\\n        [ValidateLength(1,3)]\\n        [string] $resourceProjAppSvc,  # The project or application service of the resource\\n        [Parameter(Mandatory=$true)] [string] $resourceType,  # The type of the resource\\n        [Parameter(Mandatory=$false)]\\n        [ValidateLength(1,5)]\\n        [string] $resourceUnitDept,  # The unit or department of the resource\\n        [Parameter(Mandatory=$false)] [Hashtable] $customComponents,  # Any custom components for the resource\\n        [Parameter(Mandatory=$false)] [string] $createdBy  # The creator of the resource\\n    )\\n\\n    # Define the headers for the API request\\n    $headers = @{\\n        \'accept\' = \'*/*\'\\n        \'APIKey\' = $API\\n        \'Content-Type\' = \'application/json\'\\n    }\\n\\n    # Define the body of the API request\\n    $body = @{\\n        \'resourceEnvironment\' = $resourceEnvironment\\n        \'resourceFunction\' = $resourceFunction\\n        \'resourceInstance\' = $resourceInstance\\n        \'resourceLocation\' = $resourceLocation\\n        \'resourceOrg\' = $resourceOrg\\n        \'resourceProjAppSvc\' = $resourceProjAppSvc\\n        \'resourceType\' = $resourceType\\n        \'resourceUnitDept\' = $resourceUnitDept\\n        \'customComponents\' = $customComponents\\n        \'createdBy\' = $createdBy\\n    } | ConvertTo-Json -Depth 6  # Convert the body to JSON format\\n\\n    # Send the API request and store the response\\n    try {\\n        $response = Invoke-RestMethod -Uri \\"https://$AzNamingToolSiteName.azurewebsites.net/api/ResourceNamingRequests/RequestName\\" -Method Post -Headers $headers -Body $body\\n        if ($null -eq $response) {\\n            Write-Host \\"The API returned a successful response with no body.\\"\\n        } else {\\n            Write-Host \\"The API returned a successful response, with the name generated as: $($response.resourceName)\\"\\n        }\\n    } catch {\\n        Write-Host \\"An error occurred: $_\\"\\n    }\\n    # Return the response\\n    return $response\\n}\\n\\n\\n```\\n\\n## Deployment\\n\\nNow that we have the scripts, it\'s time to run it. I am going to run this from a GitHub Codespace that already has the latest version of PowerShell and Bicep installed. I have whitelisted the IP of the Codespace to allow it to access the Azure Naming Tool WebApp.\\n\\n:::warning\\nYour Azure Naming Tool configuration is key! It needs to align with your organisational needs, naming conventions, projects and departments where necessary. Different resource types, will require different configurations, so make sure you plan this out, the Azure Naming Tool reference page can be key to review for successful name generation.\\n:::\\n\\nFirst, we will need the API Key, so within the Azure Naming Tool, click on Admin, enter your password and Copy the Full Access API Key. This key is needed as the generated API resource name is a POST operation.\\n\\n![Copy Azure Naming Tool API](AzureNamingTool_CopyAPIKey.gif \\"Azure Naming Tool API Key\\")\\n\\nYou will also need the name of your Azure Naming Tool WebApp, the Request-ResourceName script does make an assumption that this is an Azure hosted website *(ie https://aznamingtoolgeeknz.azurewebsites.net/)*. If you are hosting it elsewhere, you will need to update the script to reflect the domain name.\\n\\nEnter the API and WebApp name into the command.ps1 script, these are parameters for the script, along with resourceType, resourceInstance, resourceLocation, etc. Different resource types and configurations will require different parameters.\\n\\nNow it\'s time to create our Storage Account using a Name generated from the Azure Naming Tool.\\n\\nThe command script will trigger a REST API call to Azure Naming Tool to generate a name for our storage account. Then we will use that name as a parameter for the storageAccountName Bicep deployment.\\n\\n![Generate Name using Azure Naming Tool API](AzureNamingTool_RunAPIGenerateName.gif \\"Azure Naming Tool generate name\\")\\n\\nOur Storage account is now created:\\n\\n![Azure Storage account created](Created_StorageAccount.png \\"Created Azure Storage account\\")"},{"id":"azure/existing-resource-deploymentstack","metadata":{"permalink":"/azure/existing-resource-deploymentstack","source":"@site/blog/2024-01-09-add-existingresource-deploymentstack/index.mdx","title":"Add existing Azure resource to a Deployment Stack","description":"Add existing Azure resources to a Deployment Stack using Bicep.","date":"2024-01-09T06:05:43.214Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.78,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Add existing Azure resource to a Deployment Stack","metaDescription":"This guide shows you how to add existing Azure resources to a Deployment Stack using Bicep.","date":"2024-01-09T06:05:43.214Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"BlogHeading_DeploymentStackMissingResource.gif"},"slug":"azure/existing-resource-deploymentstack","keywords":["azure","bicep","deployment","deploymentstack"],"description":"Add existing Azure resources to a Deployment Stack using Bicep."},"unlisted":false,"prevItem":{"title":"Using the Azure Naming Tool API to name your Bicep resources","permalink":"/azure/azure-naming-tool-api-bicep-resources"},"nextItem":{"title":"The specified service namespace is invalid EventHub","permalink":"/azure/service-namespace-invalid-eventhub"}},"content":"An Azure [deployment stack](https://learn.microsoft.com/azure/azure-resource-manager/bicep/deployment-stacks?tabs=azure-powershell&WT.mc_id=AZ-MVP-5004796) is a type of Azure resource that enables the management of a group of Azure resources as an atomic unit.\\n\\nIf you deploy a new Deployment Stack, all resources in your ARM/Bicep template will be included as Managed resources, but what if you want to include a resource deployed outside of Bicep into your Deployment Stack?\\n\\n![Blog Heading ](BlogHeading_DeploymentStackMissingResource.gif)\\n\\nA community member approached me to ask how they could do this, so let us take a look.\\n\\n{/*truncate*/}\\n\\n:::info\\nAn Azure deployment stack is a type of Azure resource that enables the management of a group of Azure resources as an atomic unit. When a Bicep file or an ARM JSON template is submitted to a deployment stack, it defines the resources that are managed by the stack. If a resource that was previously included in the template is removed, it will either be detached or deleted based on the specified actionOnUnmanage behaviour of the deployment stack. Similar to other Azure resources, access to the deployment stack can be restricted using Azure role-based access control (Azure RBAC). To create and update a deployment stack, you can utilize Azure CLI, Azure PowerShell, or the Azure portal, along with Bicep files. These Bicep files are compiled into ARM JSON templates, which are then deployed as a deployment object by the stack.\\n:::\\n\\n:::tip\\nCheck out a previous post I did on [Deployment Stacks](https://luke.geek.nz/azure/Azure-Bicep-Deployment-with-Deployment-Stacks/) for a bit more detail on how they can be used and setup.\\n:::\\n\\nIn our demo, we will have a Resource Group, with an existing Storage account already pre created *(using the Azure Portal)* and a Deployment Stack, consisting of an Azure KeyVault deployed using Bicep.\\n\\nThe PowerShell command used to deploy the Bicep is:\\n\\n```powershell\\nNew-AzResourceGroupDeploymentStack `\\n  -Name \\"MyDeploymentStack\\" `\\n  -ResourceGroupName \\"deploymentstacks-rg\\" `\\n  -TemplateFile \\"main.bicep\\" `\\n  -DenySettingsMode \\"none\\"\\n```\\n\\nAnd the Bicep we initially deployed *(where the Storage account isn\'t part of the Deployment Stack, and the KeyVault is a dummy resource for something to deploy)* is:\\n\\n```bicep\\n\\nresource storage \'Microsoft.Storage/storageAccounts@2023-01-01\' existing = {\\n  name: \'stfacc1337\'\\n}\\n\\nresource keyvault \'Microsoft.KeyVault/vaults@2023-07-01\' = {\\n  name: \'keyvaulstest13345\'\\n  location:  resourceGroup().location\\n  \\n  properties: {\\n    accessPolicies: [\\n      {\\n        tenantId: subscription().tenantId\\n        objectId: \'00000000-0000-0000-0000-000000000000\'\\n        permissions: {\\n          keys: [\'all\']\\n          secrets: [\'all\']\\n          certificates: [\'all\']\\n        }\\n      }\\n    ]\\n    \\n    sku: {\\n      family:   \'A\'\\n      name:  \'standard\'\\n    }\\ntenantId: subscription().tenantId\\n  }\\n}\\n\\n```\\n\\n![Deployment Stack - Missing Storage account](DeploymentStack_LackingStgAccount.png)\\n\\nAs you can see, even though the \'stfacc1337\' storage account is tested as a referenceable object in Bicep by using the [existing](https://learn.microsoft.com/azure/azure-resource-manager/bicep/existing-resource?WT.mc_id=AZ-MVP-5004796) flag, it hasn\'t been brought into the Deployment Stack.\\n\\nBeing able to reference the module in Bicep isn\'t enough to bring it into the Deployment Stack, and this is by design *([Issue #146](https://github.com/Azure/deployment-stacks/issues/146))*.\\n\\n> So, if you can\'t reference an existing resource, how can you bring it into the deployment stack?\\n\\n:::warning[Answer!]\\n**The answer is: You need to import it into the Bicep!**\\nFrom this moment forward, it would be wise to manage that imported resource in Bicep. Any changes to that resource outside of the Bicep will conflict with the code and state, and you may find your changes being reversed on the next deployment.\\n:::\\n\\nA resource needs to exist in Infrastructure as Code, in order to be added to a Deployment Stack, the easiest way is to import it.\\n\\nI did a separate [blog article](https://luke.geek.nz/azure/azure-bicep-and-insert-resource/) on using the Bicep Visual Studio extension to import an existing resource, so refer to that, but in essence, it can import the resource from ARM, and compile it into an identical resource into Bicep, matching what you have deployed.\\n\\nThe Bicep now looks like this:\\n\\n```bicep\\n@description(\'Generated from /subscriptions/a42e282a-c1e4-4abc-9fa1-1dd1ecbd6bf9/resourceGroups/deploymentstacks-rg/providers/Microsoft.Storage/storageAccounts/stfacc1337\')\\nresource stfacc \'Microsoft.Storage/storageAccounts@2023-01-01\' = {\\n  sku: {\\n    name: \'Standard_RAGRS\'\\n  }\\n  kind: \'StorageV2\'\\n  name: \'stfacc1337\'\\n  location: \'australiaeast\'\\n  tags: {}\\n  properties: {\\n    dnsEndpointType: \'Standard\'\\n    defaultToOAuthAuthentication: false\\n    publicNetworkAccess: \'Enabled\'\\n    allowCrossTenantReplication: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowBlobPublicAccess: false\\n    allowSharedKeyAccess: true\\n    networkAcls: {\\n      ipv6Rules: []\\n      bypass: \'AzureServices\'\\n      virtualNetworkRules: []\\n      ipRules: []\\n      defaultAction: \'Allow\'\\n    }\\n    supportsHttpsTrafficOnly: true\\n    encryption: {\\n      requireInfrastructureEncryption: false\\n      services: {\\n        file: {\\n          keyType: \'Account\'\\n          enabled: true\\n        }\\n        blob: {\\n          keyType: \'Account\'\\n          enabled: true\\n        }\\n      }\\n      keySource: \'Microsoft.Storage\'\\n    }\\n    accessTier: \'Hot\'\\n  }\\n}\\n\\nresource keyvault \'Microsoft.KeyVault/vaults@2023-07-01\' = {\\n  name: \'keyvaulstest13345\'\\n  location:  resourceGroup().location\\n  \\n  properties: {\\n    accessPolicies: [\\n      {\\n        tenantId: subscription().tenantId\\n        objectId: \'00000000-0000-0000-0000-000000000000\'\\n        permissions: {\\n          keys: [\'all\']\\n          secrets: [\'all\']\\n          certificates: [\'all\']\\n        }\\n      }\\n    ]\\n    \\n    sku: {\\n      family:   \'A\'\\n      name:  \'standard\'\\n    }\\ntenantId: subscription().tenantId\\n  }\\n}\\n\\n```\\n\\nNow redeploy the Deployment Stack.\\n\\n:::tip\\nThe create-stack commands can also be used to update deployment stacks!\\n:::\\n\\n> Once your Deployment Stack has been deployed, your resource is now imported.\\n\\n![Deployment Stacks - Including Storage account](DeploymentStack_IncludingStgAccount.png)"},{"id":"azure/service-namespace-invalid-eventhub","metadata":{"permalink":"/azure/service-namespace-invalid-eventhub","source":"@site/blog/2024-01-05-ServiceNamespaceInvalidEventHub/index.mdx","title":"The specified service namespace is invalid EventHub","description":"The specified service namespace is invalid when attempting to deploy an Azure EventHub using Bicep","date":"2024-01-05T07:52:10.785Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.81,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"The specified service namespace is invalid EventHub","description":"The specified service namespace is invalid when attempting to deploy an Azure EventHub using Bicep","metaTitle":"The specified service namespace is invalid EventHub","metaDescription":"The specified service namespace is invalid when attempting to deploy an Azure EventHub using Bicep","date":"2024-01-05T07:52:10.785Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"header":{"teaser":"EventHub_ServiceNamespaceisInvalid.png"},"slug":"azure/service-namespace-invalid-eventhub","keywords":["azure","bicep","eventhub"]},"unlisted":false,"prevItem":{"title":"Add existing Azure resource to a Deployment Stack","permalink":"/azure/existing-resource-deploymentstack"},"nextItem":{"title":"Deploying and Testing Azure Email Communication Services","permalink":"/azure/azure-email-communication-services"}},"content":"When deploying an Event Hub using Azure Bicep, you may get the following error:\\n\\n    \\"code\\": \\"BadRequest\\",\\n    \\"message\\": \\"The specified service namespace is invalid. CorrelationId: 652cc73c-1fa7-450a-9788-b73ad6a818df\\"\\n\\n![The specified service namespace is invalid](EventHub_ServiceNamespaceisInvalid.png)\\n\\nThis could be caused by the name of your namespace needing to meet the naming requirements.\\n\\n{/* truncate */}\\n\\nFor example:\\n\\nThe namespace identifier should adhere to the following naming conventions:\\n\\n* The name must be unique across Azure. The system immediately checks to see if the name is available.\\n* The name length is at least 6 and at most 50 characters.\\n* The name can contain only letters, numbers, and hyphens \u201c-\u201c.\\n* The name must start with a letter and end with a letter or number.\\n* The name doesn\'t end with \u201c-sb\u201c or \u201c-mgmt\u201c.\\n\\nReference: \\n* [Create Namespace](https://learn.microsoft.com/rest/api/servicebus/create-namespace?WT.mc_id=AZ-MVP-5004796), even though this article dates back to an API from 2021, the naming standards are still valid in the API for 2023-01-01-preview.\\n* [Create Service Bus Namespace in Portal](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-portal#:~:text=Enter%20a%20name,sb%E2%80%9C%20or%20%E2%80%9C-mgmt?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/azure-email-communication-services","metadata":{"permalink":"/azure/azure-email-communication-services","source":"@site/blog/2024-01-02-azureemailcommunicationservice/index.mdx","title":"Deploying and Testing Azure Email Communication Services","description":"Use Azure Bicep to create Email Communication Services, then REST API to send an email.","date":"2024-01-02T06:54:06.568Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":9.08,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deploying and Testing Azure Email Communication Services","description":"Use Azure Bicep to create Email Communication Services, then REST API to send an email.","metaTitle":"Deploying and Testing Azure Email Communication Services","metaDescription":"This guide shows you how to use Azure Bicep to create Email Communication Services, and then use the REST API to send an email.","date":"2024-01-02T06:54:06.568Z","tags":["Azure"],"categories":["Azure"],"authors":["Luke"],"toc_min_heading_level":2,"toc_max_heading_level":5,"header":{"teaser":"BlogHeading_DeployandTestACSE.gif"},"slug":"azure/azure-email-communication-services","keywords":["azure","bicep","communication","email"]},"unlisted":false,"prevItem":{"title":"The specified service namespace is invalid EventHub","permalink":"/azure/service-namespace-invalid-eventhub"},"nextItem":{"title":"Microsoft Copilot for Azure - Preview","permalink":"/azure/microsoft-copilot-azure-preview"}},"content":"[Azure Communication Services](https://learn.microsoft.com/azure/communication-services/overview?WT.mc_id=AZ-MVP-5004796) brings rich communication APIs to all of your apps across any device on any platform, using the same reliable and secure infrastructure that powers Microsoft Teams.\\n\\nToday, we will look into using Email as part of Azure Communication Services using the REST API and PowerShell to send an email.\\n\\n![Azure Communication Services](BlogHeading_DeployandTestACSE.gif)\\n\\n{/* truncate */}\\n\\n## Overview\\n\\nAzure Communication Services is a platform of products and services that enable you to create custom communication applications and solutions. Microsoft has taken the same technologies that power Skype and Microsoft Teams and made it available to developers as an Azure product, allowing easy integration with other Microsoft developer services for additional functionality. \\n\\n:::info\\nAzure Communication Services supports various communication formats:\\n\\n* [Voice and Video Calling](https://learn.microsoft.com/en-us/azure/communication-services/concepts/voice-video-calling/calling-sdk-features?WT.mc_id=AZ-MVP-5004796)\\n* [Rich Text Chat](https://learn.microsoft.com/azure/communication-services/concepts/chat/concepts?WT.mc_id=AZ-MVP-5004796)\\n* [SMS](https://learn.microsoft.com/en-us/azure/communication-services/concepts/sms/concepts?WT.mc_id=AZ-MVP-5004796)\\n* [Email](https://learn.microsoft.com/en-us/azure/communication-services/concepts/email/email-overview?WT.mc_id=AZ-MVP-5004796)\\n:::\\n\\n:::note\\n[SMS](https://learn.microsoft.com/en-us/azure/communication-services/concepts/sms/concepts?WT.mc_id=AZ-MVP-5004796) services are not available on Azure Sponsorship or Visual Studio Enterprise subscriptions.\\n:::\\n\\nToday we will be looking at the **[Email Communication Service](https://learn.microsoft.com/azure/communication-services/concepts/email/email-overview?WT.mc_id=AZ-MVP-5004796) by deploying Email and Azure Communication Services with [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796), and Send a test email using the API and PowerShell RestMethod**.\\n\\n```mermaid\\nflowchart TD\\n    PowerShell_Script--\x3e|Uses Azure RestAPI|Azure_Communication_Services\\n    Azure_Communication_Services--\x3e|Talks to|Azure_Communication_Services_Email_Application\\n    Azure_Communication_Services_Email_Application--\x3e|Sends back to|Azure_Communication_Services\\n    Azure_Communication_Services--\x3e|Sends to|Email_Processing_Service\\n    Email_Processing_Service--\x3e|Delivers to|Email_Recipient\\n```\\n\\n### Deployment\\n\\nI am going to make some assumptions here that you know how to deploy with [Azure Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796), and have it [installed](https://learn.microsoft.com/azure/azure-resource-manager/bicep/install?WT.mc_id=AZ-MVP-5004796), and that you have an Azure subscription and the ability to create new resources.\\n\\n:::tip\\nIf you are doing Bicep deployment, I do recommend looking at the [Azure Bicep Deploy Pane Visual Studio Code extension](https://luke.geek.nz/azure/Azure-Bicep-Deploy-Pane/).\\n:::\\n\\nIn order to use the Email Communication Service, we need to create it and then create Azure Communication Services, whose endpoint we will then use to send an email.\\nTo authenticate with Azure Communication Services, we will use a Service Principal, which we will create, then use that Service Principal with a custom role to send an email using PowerShell and the Azure APIs.\\n\\nFor email services, you can use your own [Custom Domain](https://learn.microsoft.com/azure/communication-services/quickstarts/email/add-custom-verified-domains?WT.mc_id=AZ-MVP-5004796), and in most Production based scenarios you would, this requires some records being added to your DNS server, for domain ownership and to configure the DKIM/SFP records, but for the purposes of this article, we will use an Azure Managed *(and autogenerated domain)*.\\n\\n#### Bicep Deployment\\n\\nDeployment of Azure Communication Services via Bicep can be done using the [PowerShell cmdlet or the Azure CLI](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/deploy-cli?WT.mc_id=AZ-MVP-5004796). Still, today I will use the Deployment Pane and deploy from Visual Studio Code for my demo.\\n\\nThe Azure Bicep will deploy:\\n\\n* Azure Communication services\\n* Email Communication Services\\n\\nAnd configure 2 FROM email addresses:\\n\\n* DoNotReply\\n* itservicedesk\\n\\nHowever, you can modify this to your requirements; I put these in a variable array to make it easier to add and remove sender addresses.\\n\\nDefault tags have been added. However, these can easily be modified or removed if needed.\\n\\n3 Parameter values will need to be defined:\\n\\n| Parameters                | Notes                                                                                  |\\n| ------------------------- | -------------------------------------------------------------------------------------- |\\n| emailServicesName         | Azure Communication Services Email resource name.                                      |\\n| communicationServicesName | Azure Communication Services Email name.                                              |\\n| emailServicesLocation     | This is a global service; however, the data at rest (Data Location) needs to be specified. |\\n\\nI am based in New Zealand, so my Region and Data location is Australia and Australia East.\\n\\n:::info\\nThe Azure Communication Service is a global resource and will need a unique name. You can only have 1 Azure managed domain per Email Communication Service, but you can also add a Custom Domain.\\n:::\\n\\nLet us deploy and configure Azure Communication Services to talk to the Email service!\\n\\n![Azure Communication Services - Bicep Deployment](ACS_ProvisionEmailService.gif)\\n\\n```bicep title=\\"main.bicep\\"\\n// Scope\\ntargetScope = \'resourceGroup\'\\n\\n// Parameters & Variables\\n\\nparam emailServicesName string \\nparam communicationServicesName string\\n\\n\\n@description(\'The location where the email service stores its data at rest.\')\\nparam emailServicesLocation string = \'Australia\'\\n\\nvar emailServicesTags = {\\n  environment: \'production\'\\n  department: \'IT\'\\n  project: \'emailServices\'\\n}\\n\\nvar senderUsernames = [\\n  {\\n    name: \'donotreply\'\\n    username: \'DoNotReply\'\\n    displayName: \'DoNotReply\'\\n  }\\n  {\\n    name: \'itservicedesk\'\\n    username: \'itservicedesk\'\\n    displayName: \'itservicedesk\'\\n  }\\n  // Add more sender usernames here\\n]\\n\\n\\n// Bicep\\n\\nresource emailServices \'Microsoft.Communication/emailServices@2023-04-01-preview\' = {\\n  name: emailServicesName\\n  location: \'global\'\\n  tags: emailServicesTags\\n  properties: {\\n    dataLocation: emailServicesLocation\\n  }\\n}\\n\\nresource emailServicesAzureDomain \'Microsoft.Communication/emailServices/domains@2023-06-01-preview\' = {\\n  name: \'AzureManagedDomain\'\\n  parent: emailServices\\n  location: \'global\'\\n\\n  properties: {\\n    domainManagement: \'AzureManaged\'\\n    userEngagementTracking: \'Disabled\'\\n  }\\n}\\n\\nresource emailServicesSendAddresses \'Microsoft.Communication/emailServices/domains/senderUsernames@2023-06-01-preview\' = [for senderUsername in senderUsernames: {\\n  parent: emailServicesAzureDomain\\n  name: senderUsername.name\\n  properties: {\\n    username: senderUsername.username\\n    displayName: senderUsername.displayName\\n  }\\n}]\\n\\n\\nresource communicationServices \'Microsoft.Communication/communicationServices@2023-06-01-preview\' = {\\n  name: communicationServicesName\\n  location: \'global\'\\n  tags: emailServicesTags\\n  properties: {\\n    dataLocation: emailServicesLocation\\n    linkedDomains: [\\n    emailServicesAzureDomain.id\\n    ]\\n  }  \\n}\\n\\noutput emailAddresses array = [for senderUsername in senderUsernames: {\\n  email: \'${senderUsername.username}@${emailServicesAzureDomain.name}\'\\n}]\\n\\noutput domainName string = emailServicesAzureDomain.properties.mailFromSenderDomain\\n\\noutput communicationServicesuri string = communicationServices.properties.hostName\\n\\n```\\n\\n#### Setup and configure Authentication\\n\\nNow, that we have our Azure Communications Service and Email Communication service, next we need to create a Service Principal and Custom Role that we will use in the script.\\n\\nTo do that, we will, set up a new Service Principal and then add a custom role, which has the least amount of privileges to send an email, then add it to our Resource Group, containing the Azure Communication Services resources.\\n\\n:::info\\nUse this opportunity to copy your Application ID and generate and copy a new Secret; these will be used by the test PowerShell script in the next step, along with your Entra ID tenant ID. In a Production scenario, the secret should be stored in an [Azure Key Vault](https://azure.microsoft.com/products/key-vault?WT.mc_id=AZ-MVP-5004796) as a Secret and not referenced as plain text anywhere.\\n:::\\n\\n![Azure Communication Services - Create SPN and assign custom role](ACS_ProvisionEmailService_SPN.gif)\\n\\n```json title=\\"CommunicationServiceMailSender.json\\"\\n{\\n  \\"properties\\": {\\n    \\"roleName\\": \\"Communication Service Mail Sender\\",\\n    \\"description\\": \\"Minimal set of permissions required to send mail with Azure Communication Service.\\",\\n    \\"assignableScopes\\": [\\n      \\"\\" \\n    ],\\n    \\"permissions\\": [\\n      {\\n        \\"actions\\": [\\n          \\"Microsoft.Communication/CommunicationServices/Write\\",\\n          \\"Microsoft.Communication/CommunicationServices/Read\\",\\n          \\"Microsoft.Communication/EmailServices/read\\"\\n        ],\\n        \\"notActions\\": [],\\n        \\"dataActions\\": [],\\n        \\"notDataActions\\": []\\n      }\\n    ]\\n  }\\n}\\n\\n```\\n\\n### Test outgoing email using API\\n\\nNow that we have our Communication Services and email Services and configured our Service Principal, it is time to run our test.\\n\\n:::warning\\nThe script I will run below was written purely for my testing purposes and stores credentials as plain text; when looking to do this in Production, make sure these credentials are protected and the Service Principal is locked down from accepting requests outside of approved networks. \\n:::\\n\\nTo do our test, we need to update a few variables.\\n\\n| Parameters               | Notes                                                                                                             |\\n| ------------------------ | ----------------------------------------------------------------------------------------------------------------- |\\n| SPNAppId                 | Your Service Principal Application ID                                                                             |\\n| SPNSecretValue           | Your Service Principal Client Secret                                                                              |\\n| SPNTenantId              | Your Entra ID Tenant ID                                                                                           |\\n| senderAddress            | The email address Azure Communication Services is sending FROM                                                    |\\n| recipientAddress         | The email address you re sending TO                                                                               |\\n| communicationendpointurl | The endpoint of your Azure Communication Services (not Email Service). Https will be added further in the script. |\\n\\nThe senderAddress must match a valid email address configured in Azure Email Communication Service. I also added in some random Contoso email addresses into the recipient\'s body for testing, the idea is to give you a scaffold to adjust as needed; feel free to amend and remove what you don\'t need.\\n\\n![Azure Communication Services - Test sending an email](ACS_ProvisionEmailService_Test.gif)\\n\\n```powershell title=\\"AzureEmailCommunicationSend.ps1\\"\\n# Define the Service Principal credentials and email addresses\\n\\n# The Service Principal\'s Application (client) ID\\n# This is a unique identifier for the app, assigned by Entra ID. Replace to match your environment.\\n$SPNAppId = \'069893df-a389-4a2a-99aa-d035265cfbb7\'\\n\\n# The Service Principal\'s secret\\n# This is like a password for the app, used for authentication. Replace to match your environment.\\n$SPNSecretValue = \'fc6652c483d4475a9c59cc1d81b6d45a\'\\n\\n# The Entra ID tenant ID\\n# This is the unique identifier for the Entra ID tenancy instance where the app is registered. Replace to match your environment.\\n$SPNTenantId = \'49e37426-fba3-4995-b563-0355b5d6fc60\'\\n\\n# The sender\'s email address\\n# This is the email address that will appear in the \\"From\\" field of the email\\n$senderAddress = \'itservicedesk@959df321-6092-41e7-8414-e7b4ea05da2b.azurecomm.net\'\\n\\n# The recipient\'s email address\\n# This is the email address where the email will be sent\\n$recipientAddress = \'recipientemail@test.com\'\\n\\n# The URI for the Azure Communication Services API\\n$communicationendpointurl = \\"azcommservices1.australia.communication.azure.com\\"\\n\\n# Function to get the access token from Entra ID\\nfunction Get-AccessToken {    \\n    # Define the parameters for the REST API call\\n    $params = @{\\n        Uri    = \\"https://login.microsoftonline.com/$($SPNTenantId)/oauth2/v2.0/token\\"\\n        Method = \\"POST\\"\\n        Body   = @{\\n            client_id     = $SPNAppId\\n            client_secret = $SPNSecretValue\\n            grant_type    = \\"client_credentials\\"\\n            scope         = \\"https://communication.azure.com/.default\\"\\n        }\\n    }\\n\\n    # Call the REST API and get the access token\\n    $token = Invoke-RestMethod @params\\n    return $token.access_token\\n}\\n\\n# Define the URI for the Azure Communication Services API\\n\\n$uri = \\"https://$communicationendpointurl/emails:send?api-version=2023-03-31\\"\\n\\n# Define the headers for the REST API call\\n$headers = @{\\n    \\"Content-Type\\"  = \\"application/json\\"\\n    \\"Authorization\\" = \\"Bearer $(Get-AccessToken)\\"\\n}\\n\\n# Define the body for the REST API call\\n# Define the body for the REST API call\\n# This includes the email headers, sender address, content, recipients, attachments, reply-to addresses, and tracking settings\\n\\n$apiResponse = @{\\n    # The headers of the email, including a unique ID generated by New-Guid\\n    headers                        = @{\\n        id = (New-Guid).Guid\\n    }\\n    # The sender\'s email address\\n    senderAddress                  = $senderAddress \\n    # The content of the email, including the subject, plain text body, and HTML body\\n    content                        = @{\\n        subject   = \\"Contoso Email Test\\"\\n        plainText = \\"This is a test email from Contoso. If you received this, our test was successful.\\"\\n        html      = \\"<html><head><title>Contoso Email Test</title></head><body><h1>This is a test email from Contoso.</h1><p>If you received this, our test was successful.</p></body></html>\\"\\n    }\\n    # The recipients of the email, including the \\"to\\", \\"cc\\", and \\"bcc\\" addresses\\n    recipients                     = @{\\n        to  = @(\\n            @{\\n                address     = $recipientAddress\\n                displayName = $recipientAddress\\n            },\\n            @{\\n                address     = \\"Jane.Doe@contoso.com\\"\\n                displayName = \\"Jane Doe\\"\\n            }\\n        )\\n        cc  = @(\\n            @{\\n                address     = \'wendy.smith@contoso.com\'\\n                displayName = \'Wendy Smith\'\\n            },\\n            @{\\n                address     = \\"jimmy.johns@contoso.com\\"\\n                displayName = \\"Jimmy Johns\\"\\n            }\\n        )\\n        bcc = @(\\n            @{\\n                address     = \\"bob.jones@contoso.com\\"\\n                displayName = \\"Bob Jones\\"\\n            },\\n            @{\\n                address     = \\"alice.johnson@contoso.com\\"\\n                displayName = \\"Alice Johnson\\"\\n            }\\n        )\\n    }\\n    # The attachments to the email, including the name, content type, and content in Base64\\n    attachments                    = @(\\n        @{\\n            name            = \\"Attachment.txt\\"\\n            contentType     = \\"application/txt\\"\\n            contentInBase64 = \\"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\\"\\n        }\\n\\n    )\\n    # The reply-to addresses for the email\\n    replyTo                        = @(\\n        @{\\n            address     = \\"contoso-support@contoso.com\\"\\n            displayName = \\"Contoso Support\\"\\n        }\\n    )\\n    # A flag to disable user engagement tracking\\n    userEngagementTrackingDisabled = $true\\n}\\n\\n# Convert the PowerShell object to JSON\\n# The -Depth parameter is set to 10 to ensure all levels of the object are converted\\n$body = $apiResponse | ConvertTo-Json -Depth 10\\n\\n# Send the email\\ntry {\\n    $response = Invoke-RestMethod -Uri $uri -Method Post -Headers $headers -Body $body -UseBasicParsing\\n    $response\\n}\\ncatch {\\n    Write-Error $_.Exception.Message\\n}\\n\\n```\\n\\n> Congratulations, you have now stood up Azure Communications Services and successfully sent and received an email using the Azure Communication Service API.\\n\\n## Additional Reading\\n\\n* [Azure Communication Services pricing](https://azure.microsoft.com/contact/pricing/?WT.mc_id=AZ-MVP-5004796)\\n* [Introduction to Azure Communication Services](https://learn.microsoft.com/en-us/training/modules/intro-azure-communication-services/?source=recommendations&WT.mc_id=AZ-MVP-5004796)\\n* [Quick-start: Handle Email events](https://learn.microsoft.com/en-us/azure/communication-services/quickstarts/email/handle-email-events?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/microsoft-copilot-azure-preview","metadata":{"permalink":"/azure/microsoft-copilot-azure-preview","source":"@site/blog/2023-12-26-Microsoft-Copilot-for-Azure-Preview.md","title":"Microsoft Copilot for Azure - Preview","description":"Microsoft Copilot for Azure is designed to generate the best possible responses within the context it can access. Lets take a look at the Public Preview.","date":"2023-12-26T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":11.03,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Copilot for Azure - Preview","authors":["Luke"],"tags":["Azure"],"toc":true,"date":"2023-12-26T00:00:00.000Z","slug":"azure/microsoft-copilot-azure-preview","keywords":["azure","copilot","Microsoft Copilot"],"description":"Microsoft Copilot for Azure is designed to generate the best possible responses within the context it can access. Lets take a look at the Public Preview.","header":{"teaser":"/images/posts/BlobHeading_Microsft_Copilot_Azure_Preview.PNG"},"categories":["Azure"]},"unlisted":false,"prevItem":{"title":"Deploying and Testing Azure Email Communication Services","permalink":"/azure/azure-email-communication-services"},"nextItem":{"title":"Azure Dev/Test Subscription considerations","permalink":"/azure/Azure-DevTest-Subscription-Considerations"}},"content":"[Copilot for Azure](https://azure.microsoft.com/en-us/products/copilot?WT.mc_id=AZ-MVP-5004796) is an AI companion that simplifies how you design, operate, optimize, and troubleshoot apps and infrastructure from cloud to edge.\\n\\nWith Copilot, gain new insights, discover more benefits of the cloud, and orchestrate data across both the cloud and the edge. Copilot AI assistance **utilizes language models**, the **Azure control plane**, and **insights** about **your** **Azure** and **Arc\u2013enabled** assets.\\n\\nThis is carried out within Azure\'s steadfast commitment to safeguarding your data security and privacy.\\n\\n> This article contains my own personal thoughts and experiences; make sure you approach this capability with an open eye; my views may not necessarily be your own.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview\\n\\nMicrosoft Copilot for Azure is designed to generate the best possible responses within the context it can access.\\n\\n**However, like any AI system, Microsoft Copilot for Azure\'s responses will not always be perfect. All of Microsoft Copilot for Azure\'s responses should be carefully tested, reviewed, and vetted before making changes to your Azure environment.**\\n\\n> Microsoft Copilot for Azure *(preview)* requires registration and is only available to approved enterprise customers and partners. Customers who wish to use Microsoft Copilot for Azure *(preview)* must submit a\xa0registration form.\\n> Access to Microsoft Copilot for Azure *(preview)* is subject to Microsoft\'s sole discretion based on eligibility criteria and a vetting process, and customers must acknowledge that they have read and understand the Azure terms of service for Microsoft Copilot for Azure *(preview)*.\\n\\n![Microsoft Copilot for Azure - Mission Statement](/images/posts/MicrosoftCopilotForAzure_MissionStatement.PNG)\\n\\nCopilot for Azure can be accessed directly in the Azure Portal.\\n\\n![Microsoft Copilot for Azure](/images/posts/MicrosoftCopilotForAzure_PortalView.png)\\n\\n### Limitations\\n\\n> It is in preview! But there are some [limitations](https://learn.microsoft.com/azure/copilot/capabilities?WT.mc_id=AZ-MVP-5004796#current-limitations), to be aware of.\\n\\n| **Limitation**                                                | **Notes**                                                                                                                                             |\\n|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| 10 questions per conversation (5 convos per day)              | Essentially, 10 prompts (and competitions) per memory-aware conversation and 5 conversations. Reducing potential hallucinations and API exhaustion. |\\n| Responses that use lists are limited to the top 5 | When requesting a list of resources, the completions will be restricted to 5. The token length for hundreds of resources can be very slow           |\\n| Some tasks won\'t accept Resource Names; resource ID required | Resource ID of the resources may need to be specified when referencing a resource.                                                                           |\\n| Available in English only                                     | English only                                                                                                                                          |\\n| Cost management scope at Subscription (not Management Group)  | Able to reference Cost analysis against Subscription and Resource Groups but not across multiple subscriptions across Management Groups            |\\n| Limited number of skills                                      | Can only complete a certain amount of tasks; no doubt more will be added over time due to increasing capabilities and user feedback.                  |\\n\\n### Copilot Stack\\n\\nSo, how does Microsoft Copilot for Azure work? Let us take a closer look:\\n\\n![Microsoft Copilot for Azure](/images/posts/MicrosoftCopilotforAzure_CopilotStack.gif)\\n\\n| **Step** | **Stack** | **Action** |\\n|---|---|---|\\n| 1 | Apps | User enters prompt in portal web interface |\\n| 2 | Apps | Microsoft Bot Framework takes the prompt and sends it to AI orchestration |\\n| 3 | AI orchestration | Does prompt filtering (i.e. Content Safety) |\\n| 4 | AI orchestration | Brings in context (i.e. Azure resource/blade that is opened) |\\n| 5 | AI orchestration | RAG (Retrieval Augmented generation), brings in Microsoft Learn documentation, Azure Resource Graph, Cost Management, Skills, Guardrials (i.e. Azure policy, RBAC (Role Based Access Control), Budgets |\\n| 6 | AI orchestration | Added additional context to Prompt (i.e. metapromot) and ground the data |\\n| 7 | Foundational Model | Metaprompt, supplied to foundational model. |\\n| 8 | Foundational Model | Competition prompt response from foundational model (based on Copilot system prompt and context) |\\n| 9 | Apps | Competition prompt, supplied to user |\\n\\n## Functionality\\n\\nMicrosoft Copilot for Azure is designed to help with the following:\\n\\n* Design\\n* Operate\\n* Optimize\\n* Troubleshoot\\n\\n![Microsoft Copilot for Azure](/images/posts/MicrosoftCopilotforAzure_Circle.PNG)\\n\\nSo let us go through trialling this! All tests below will be done with an account with Owner permissions across Azure.\\n\\n### Design\\n\\n> **Design** - Configure and create the right services.\\n\\nTo test the design pillar of Microsoft Copilot for Azure, I will be using the following prompts to ease in and look at my design elements:\\n\\n* I want to set up a new web application for Internet shopping; what infrastructure should I use?\\n* When setting up a new web service that allows people to purchase my products using Azure services, what infrastructure should I use?\\n* List all suggested Azure services for my internet shopping service and how they could connect together, including considerations.\\n* Create these services\\n* Are there any services I am missing? Also, list all the suggested services in a table, including a column on what each service could be used for when looking at my internet shopping application\\n* What are the recommended requirements that I need to gather to create my scalable and secure Internet shopping application? List requirements, as a list\\n* Give me examples of security requirement questions.\\n\\nAlthough these prompts are very high-level and not as specific as they could be, I approached this as someone who was new to Azure design, with a specific outcome I wanted, with one prompt building on the next.\\n\\n![Microsoft Copilot for Azure - Design](/images/posts/MicrosoftCopilotforAzure_PillarTest_Design.gif)\\n\\n> Overall, Copilot for Azure responded to each prompt, and starting off with a generic prompt, I was able to use the completition prompts, supplied back to me. I did like how most prompts included links to additional Microsoft Learn documentation *(including Azure architecture centre references)* based on the subjects at the time. I did ask Microsoft Copilot for Azure to create a range of Cloud-native services, and it failed, so from my perspective, it is missing either additional error handling or the Skills/Tasks to complete the creation of these services *(which would have been my ideal state)*.\\n\\n### Operate\\n\\n> **Operate** - Answer questions, author complex commands, and manage resources.\\n\\nTo test the Operate pillar of Microsoft Copilot for Azure, I will be using the following prompts to ease in and look at the Operate elements:\\n\\n* Take me to view all resources\\n* Change portal theme to light\\n* Change back\\n* Create a table of all resources and their region?\\n* Give me a list of the top resource types?\\n* List all virtual machines and their power state?\\n* Shutdown all virtual machines currently running\\n* Start all virtual machines currently stopped\\n\\n![Microsoft Copilot for Azure - Operate](/images/posts/MicrosoftCopilotforAzure_PillarTest_Operate.gif)\\n\\n> Copilot for Azure responded to each prompt; it did a great job of taking us to the resources within the Azure Portal when prompted *(i.e. take me to view all resources)*; it even successfully changed our portal theme and reverted the change. However, I ran into a bug when attempting to shut down and start running Virtual Machines, where the selection prompt wasn\'t fully loaded, and I had to reset the selection to get it to display. Multiple times, I ran into issues with Azure Resource Graph, not being fully capable of creating and running queries, such as listing all resources and their regions, which I know is accessible by a Resource Graph query. However, Copilot, for Microsoft Azure, was able to successfully stop and start my virtual machines in bulk, making it a lot easier to make adjustments at scale; overall, I believe there are some improvements needed in this area to make it more functional.\\n\\n### Optimize\\n\\n> **Optimize** - Improve costs, scalability, and reliability through recommendations for your environment.\\n\\nTo test the Optimize pillar of Microsoft Copilot for Azure, I will be using the following prompts to ease in and look at the Optimize elements:\\n\\n* List all recommendations from Azure Advisor?\\n* Show me the highest resource costs for the past 12 months?\\n* How can I optimize my environment?\\n* Create a naming convention for SQL databases?\\n* How can I protect my storage accounts?\\n\\n> Copilot for Azure responded to each prompt, and it was able to achieve and display the highest resource costs for the past 12 months; once my scope was set *(as per current limitations)*, it gave me recommendations on how I could generally optimize my environment, Copilot was also able to give recommendations on *(non-Azure)* SQL Database naming standards and implementations. Being able to review and recommend security recommendations is an enhanced Azure Copilot skill; so glad to know this worked and came back with valid recommendations for my test storage account; the automated implementation of security recommendations, however, isn\'t currently implemented.\\n\\n![Microsoft Copilot for Azure - Optimize](/images/posts/MicrosoftCopilotforAzure_PillarTest_Optimize.gif)\\n\\n### Troubleshoot\\n\\n> **Troubleshoot** - Orchestrate across Azure services for insights to summarize issues, identify causes and suggest solutions.\\n\\nIn this section, one of my Virtual Machines *(VM-1)* is turned off, so I am unable to RDP to it; let us see if we can use Microsoft Copilot for Azure to troubleshoot this.\\n\\nTo test the Troubleshoot pillar of Microsoft Copilot for Azure, I will be using the following prompts to ease in and look at the Troubleshoot elements:\\n\\n* How can I troubleshoot not being able to RDP to VM-1?\\n* Check if port 3390 is open on VM-1?\\n* What are the security implications of opening port 3389?\\n* Create a PowerShell script to test 3389 is opened on VM-1\\n* Adjust the script to use its public IP instead\\n* Take me to VM-1\\n* What is the Public IP of VM-1?\\n* What is this resource?\\n* What are the open alerts?\\n\\n> Overall, using Microsoft Copilot for Azure to Troubleshoot RDP to connectivity to \'VM-1\' would have helped, though the scenario that Copilot had come back with was due to being unable to RDP due to a potential brute force attack, had I followed the recommendations, it would have directed me to the appropriate blades, where I saw that the Virtual Machine was turned off. However, Copilot itself didn\'t have a status check that the Virtual Machine was even started or triggered any alerts that the Virtual Machine was deallocated. There was a delay, when I first prompted for how I troubleshoot, enough of a delay that I cancelled it and then reprompted again, which returned a result resulting in 2 of my 10 requests being used up. It was able to supply information on what resource I had opened in the portal and any active alerts on that resource *(Had I been at the All Resources blade; it would have viewed all alerts)*.\\n\\n![Microsoft Copilot for Azure - Troubleshoot](/images/posts/MicrosoftCopilotforAzure_PillarTest_Troubleshoot.gif)\\n\\n## Conclusion\\n\\nOverall, I feel Copilot for Microsoft Azure is a suitable companion to working with Microsoft Azure as a *copilot* in the Azure Portal. However, it is clear that this is very much in Preview, and I would argue, unfortunately, not for Commercial or Generally available functionality yet, although I could have high standards on what this could be capable of, so make sure you draw your own conclusions and run your own tests.\\n\\nAs with any prompt, working with LLM *(Large Language Models)*, context is key - and having a curious mindset in your questioning can help draw out more information.\\n\\nIf you are a beginner user of Azure, I feel that Microsoft Copilot for Azure can help point you in the right direction and to the right information.\\n\\nContext is key \u2013 the service takes context on what resource you are at in the Azure Portal, so the more context it has on the resource, the better outputs you will get.\\n\\nAs with all generative AI services, clear prompts are key and don\u2019t trust the outputs; not all Graph queries are right.\\n\\nIn terms of more intermediate and advanced services, it won\u2019t replace full-on Azure management capabilities and troubleshooting yet, remember this is a copilot, an assistant.\\n\\n> The more feedback can be offered using the built-in feedback buttons as part of the user interface, as part of its use \u2013 the better it will be, and the more skills and tasks it will be able to complete.\\n\\nRemember that users can only use Microsoft Copilot for Azure to view resource data and make changes to areas they already have the privilege to do! So as you prepare to roll this out, use it as an opportunity to increase your security and apply just in time and with just enough practice. If you using Infrastructure as Code, then your users may only need Reader access to view data regardless, then Copilot cannot make any changes outside of the toolsets like Terraform you may be deploying.\\n\\nOverall, this is a product I will be following and cannot wait to see what it becomes; where this product shines at the moment is really in the Enhanced Skills and being able to bring the correct Microsoft Learn documentation straight to you while you are in the Azure Portal, making your learning and engagement a lot more streamlined!\\n\\n![Microsoft Copilot for Azure - Enchanced SKills](/images/posts/MicrosoftCopilotforAzure_CopilotEnchancedSkills.png)\\n\\n## Reference\\n\\nThe links below are some relevant reference material for further reading.\\n\\n* [What is Microsoft Copilot for Azure?](https://learn.microsoft.com/azure/copilot/overview?wt.mc_id=copilot_1a_webpage_gdc&WT.mc_id=AZ-MVP-5004796)\\n* [Join the Preview](https://learn.microsoft.com/azure/copilot/overview?wt.mc_id=copilot_1a_webpage_gdc&WT.mc_id=AZ-MVP-5004796#join-the-preview)\\n* [Enchanced Scenarios](https://learn.microsoft.com/azure/copilot/get-information-resource-graph?WT.mc_id=AZ-MVP-5004796)\\n* [Simplify IT management with Microsoft Copilot for Azure \u2013 save time and get answers fast](https://techcommunity.microsoft.com/t5/azure-infrastructure-blog/simplify-it-management-with-microsoft-copilot-for-azure-save/ba-p/3981106?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/Azure-DevTest-Subscription-Considerations","metadata":{"permalink":"/azure/Azure-DevTest-Subscription-Considerations","source":"@site/blog/2023-12-21-Azure-DevTest-Subscription-Considerations.md","title":"Azure Dev/Test Subscription considerations","description":"When looking at your Dev/Test workloads in Azure, you may be considering the Azure Plan for Dev/Test Enterprise subscription type, available on both the Microsoft Customer Agreement (MCA) and Enterprise Agreement (EA) billing account types.","date":"2023-12-21T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.865,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Dev/Test Subscription considerations","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlobHeading_Azure-DevTest-Considerations.gif"},"date":"2023-12-21 00:00:00 +1300","slug":"azure/Azure-DevTest-Subscription-Considerations"},"unlisted":false,"prevItem":{"title":"Microsoft Copilot for Azure - Preview","permalink":"/azure/microsoft-copilot-azure-preview"},"nextItem":{"title":"Azure Image Builder Image Build with Bicep and Azure DevOps","permalink":"/azure/Azure-Image-Builder-Build-Pipeline-with-Azure-DevOps"}},"content":"When looking at your Dev/Test workloads in Azure, you may be considering the [Azure Plan for Dev/Test Enterprise](https://azure.microsoft.com/en-us/pricing/offers/ms-azr-0148p?WT.mc_id=AZ-MVP-5004796) subscription type, available on both the [Microsoft Customer Agreement (MCA)](https://learn.microsoft.com/azure/cost-management-billing/understand/mca-overview?WT.mc_id=AZ-MVP-5004796) and [Enterprise Agreement (EA)](https://www.microsoft.com/en-us/licensing/licensing-programs/enterprise?WT.mc_id=AZ-MVP-5004796) billing account types.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview\\n\\nSo, let us look at what this entails...\\n\\n> You get **Azure Plan for DevTest** when you sign the **Microsoft Customer Agreement**. The **Enterprise DevTest** offer is available only to **Enterprise Agreement** customers, but they are *essentially* the **same plan**. The SKU value for Enterprise Dev/Test is: [ms-azr-0148p](https://azure.microsoft.com/en-in/pricing/offers/ms-azr-0148p?WT.mc_id=AZ-MVP-5004796) and Azure Plan for Dev/Test Enterprise is: [ms-azr-0148g](https://azure.microsoft.com/pricing/offers/ms-azr-0148g?WT.mc_id=AZ-MVP-5004796), but as usual **make sure you clarify and confirm with your direct Microsoft licensing/customer success manager, as these could change**.\\n\\nThere are a few gotchas to using the Dev/Test Enterprise Azure Plan that you need to be aware of, but before we go into that - let us take a look at some of the benefits:\\n\\nRun your development and testing workloads on Azure by using the Azure Plan for DevTest, which includes:\\n\\n* Access to exclusive DevTest images in the gallery, including Windows 8.1 and Windows 10/11 images.\\n* Lower DevTest rates running Windows on Azure Virtual Machines, Azure App Service, Azure Cloud Services, Azure HDInsight, Azure Logic Apps, and Azure SQL Database.\\n* Centralized management via the Azure portal.\\n* The ability to create multiple subscriptions makes the plan ideal for development teams.\\n\\nWhen looking at the DevTest plan, this particular one will catch your eye:\\n\\n> **Lower DevTest rates** running Windows on Azure Virtual Machines, Azure App Service, Azure Cloud Services, Azure HDInsight, Azure Logic Apps, and Azure SQL Database. You can save even more with reservations.\\n\\nWhat this means is that for items like Azure Virtual Machines running Windows as an example, you won\'t be charged the OS *(Operating System)* license costs, so essentially the same cost as if you were running a Linux workload, the same for the SQL services. However, this is a small number of resources *(in comparison to the entire set of services offered by Microsoft Azure)*, which covers standard services that over time can make a huge difference to the cost of running these services.\\n\\n> Provision fast, lean, and highly secure dev/test workloads on Azure\u2014and realize substantial cost savings compared to your production workloads. Your dev/test discounted rates continue as long as you maintain your Visual Studio subscription.\\n\\nIt is worth noting that although the intended readers of this are those with MCA or Enterprise Agreements with Microsoft, for those Visual Studio subscribers, you can go down the Dev/Test subscription route with [PayAsYouGo/DevTest](https://azure.microsoft.com/en-gb/pricing/offers/ms-azr-0023p/?WT.mc_id=AZ-MVP-5004796) with a credit card as well, keep in mind the considerations below.\\n\\nCSP *(Cloud Solution Provider)* doesn\'t currently have a DevTest offer, but you could fall back to the Pay As You Go route.\\n\\n## Considerations\\n\\nNow that we know some of the benefits of the DevTest plans, there are a few things to keep in mind that I have found a few people either didn\'t know *(and these subscriptions were deployed like candy at Halloween across organisations)*or were confused about, so let us cover it here.\\n\\n![VisualStudio Subscription Types](/images/posts/BlobHeading_Azure-DevTest-Considerations.gif)\\n\\nI want it to be perfectly clear: if you meet the criteria to use the DevTest subscriptions, you should! But you might find you will end up with a mix of Pay As You Go and DevTest subscriptions, depending on your requirements *(and that\'s ok!)*.\\n\\n> Remember: The Dev/Test plan is exclusively intended for developing and testing your applications.\\n\\nThe considerations that you need to consider when selecting a Dev/Test subscription type over a Pay As You Go are as follows:\\n\\n* The Dev/Test Azure Plan doesn\'t contain a financially backed service-level agreement *(SLA)*. The only exceptions are Azure DevOps, Azure Monitor and Visual Studio App Center. The entire purpose of this subscription is to give you the ability to trial and test services - and NOT run production workloads or services. This makes sense.\\n* Only active Visual Studio subscribers with standard subscriptions can use the Azure resources running within an Enterprise Dev/Test subscription (yes, that\'s right, everyone who has access to modify/create/delete resources needs to be a Visual Studio subscriber). This is the one that usually catches people out; although there are no technical guardrails preventing subscription use by unlicensed users, you still need to comply with the license agreement. Although it\'s worth pointing out, end users can also access the applications you build on top of the platform to provide feedback or perform acceptance tests without requiring a license.\\n* DevTest subscriptions are also compute quota limited, ie the compute quota restrictions are more restrictive (ie 10 cores on DevTest subscriptions vs 350 quota limit on non-devtest subscriptions).\\n\\n> Even though there is no SLA for any resources deployed in the subscription, Support is still available *(i.e. open support tickets for resources)* if a suitable support plan exists as part of your MCA or Enterprise Agreement or is purchased for that subscription.\\n\\nThose are the two primary considerations that you need to consider when deciding on what subscription type to use; you will find for some teams that have relevant Visual Studio subscriptions, DevTest will work fine, but for others, Pay As You Go will be the right choice - your environment, may end up with a mix of both subscription types - make sure its clear what subscription license type it is, an example is I usually encourage subscriptions to be standing by pre-created ready to go and named something like \'PLACEHOLDER-PAYASYOUGO\' or \'PLACEHOLDER-DEVTEST\', although you can use Tags for this as well - the Overview pane of the Subscription resource also indicates the Subscription type.\\n\\nThe last thing I will cover is what *Visual Studio subscribers* actually mean. To do that, we will take a look at the [Visual Studio subscriptions](https://visualstudio.microsoft.com/subscriptions?WT.mc_id=AZ-MVP-5004796).\\n\\nIf you navigate down the page, you will see a dropdown list with all the Visual Studio subscription types.\\n\\n![VisualStudio Subscription Types](/images/posts/VisualStudio_Subscription-Types.png)\\n\\nIf you click on the Azure tab, you will see the Azure credit *(for their own non-EA or MCA subscription)* and whether they are eligible as a valid Azure Dev/Test subscription user. To make it easier, here is a current table:\\n\\n| **Visual Studio Subscription Type**                             | **Azure Personal Credit** | **Valid Azure Dev/Test user** |\\n|-----------------------------------------------------------------|---------------------------|-------------------------------|\\n| Visual Studio Enterprise monthly subscriptions                  | $0                        | No                            |\\n| Visual Studio Enterprise subscription                           | $150                      | Yes                           |\\n| Visual Studio subscriptions with GitHub Enterprise              | $150                      | Yes                           |\\n| Visual Studio Professional monthly subscriptions                | $0                        | No                            |\\n|  Visual Studio Professional                                     | $50                       | Yes                           |\\n| Visual Studio Professional subscriptions with GitHub Enterprise | $50                       | Yes                           |\\n| Visual Studio Test Professional subscription                    | $50                       | Yes                           |\\n| MSDN Platforms                                                  | $100                      | Yes                           |\\n\\n*Azure personal credit is in USD.*\\n\\n## Reference\\n\\nThe links below are some relevant reference material for further reading.\\n\\n* [Enterprise Dev/Test](https://azure.microsoft.com/en-us/pricing/offers/ms-azr-0148p?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Plan for DevTest](https://azure.microsoft.com/pricing/offers/ms-azr-0148g?WT.mc_id=AZ-MVP-5004796)\\n* [Create an Enterprise Agreement subscription](https://learn.microsoft.com/azure/cost-management-billing/manage/create-enterprise-subscription?WT.mc_id=AZ-MVP-5004796)\\n* [Create a Microsoft Customer Agreement subscription](https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/create-subscription?WT.mc_id=AZ-MVP-5004796)\\n* [Visual Studio subscriptions](https://visualstudio.microsoft.com/subscriptions?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/Azure-Image-Builder-Build-Pipeline-with-Azure-DevOps","metadata":{"permalink":"/azure/Azure-Image-Builder-Build-Pipeline-with-Azure-DevOps","source":"@site/blog/2023-11-22-Azure-Image-Builder-Build-Pipeline-with-Azure-DevOps.md","title":"Azure Image Builder Image Build with Bicep and Azure DevOps","description":"Azure Image Builder is an Azure managed service (running Packer underneath) that allows you to create customised Virtual Machine images.","date":"2023-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":20.55,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Image Builder Image Build with Bicep and Azure DevOps","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlobHeading_Azure-Image-Builder-Image-Build-Bicep-Azure-DevOps.png"},"date":"2023-11-22 00:00:00 +1300","slug":"azure/Azure-Image-Builder-Build-Pipeline-with-Azure-DevOps"},"unlisted":false,"prevItem":{"title":"Azure Dev/Test Subscription considerations","permalink":"/azure/Azure-DevTest-Subscription-Considerations"},"nextItem":{"title":"Book Review Azure Architecture Explained","permalink":"/azure/misc/Book-Review-Azure-architecture-explained"}},"content":"[Azure Image Builder](https://learn.microsoft.com/azure/virtual-machines/image-builder-overview?tabs=azure-powershell&WT.mc_id=AZ-MVP-5004796) is an Azure managed service *(running [Packer](https://www.packer.io/) underneath)* that allows you to create customised Virtual Machine images.\\n\\n> By using standardized virtual machine (VM) images, your organization can more easily migrate to the cloud and help ensure consistency in your deployments. Images ordinarily include predefined security, configuration settings, and any necessary software. Setting up your own imaging pipeline requires time, infrastructure, and many other details. With Azure VM Image Builder, you only need to create a configuration that describes your image and submit it to the service where it is built and distributed.\\n> With VM Image Builder, you can migrate your image customization pipeline to Azure as you continue using existing scripts, commands, and processes. You can integrate your core applications into a VM image so that your VMs can take on workloads after the images are created. You can even add configurations to build images for Azure Virtual Desktop as virtual hard discs (VHDs) for use in Azure Stack or for ease of exporting.\\n\\n![Azure Image Builder Image Build with Bicep and Azure DevOps](/images/posts/BlobHeading_Azure-Image-Builder-Image-Build-Bicep-Azure-DevOps.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview\\n\\nWith the AIB (Azure Image Builder) service, you can quickly and easily create machine images for Azure environments using standardized virtual machine (VM) images, create a configuration that describes your image and submit it to the service for building and distribution.\\n\\n> When you submit the configuration to the service, Azure creates an image template resource. When the image template resource is made, a staging resource group is created in your Azure subscription in the following format: IT_DestinationResourceGroupTemplateName(GUID). The staging resource group contains files and scripts, which are referenced in the File, Shell, and PowerShell customization in the ScriptURI property.\\n\\nTo run the build, you invoke Run on the VM Image Builder template resource. The service then deploys additional resources for the build, such as a VM, network, disk, and network adapter. If you build an image without using an existing virtual network, VM Image Builder also deploys a public IP and network security group, and it connects to the build VM using Secure Shell (SSH) or Windows Remote Management (WinRM) protocol. If you select an existing virtual network, the service is deployed via Azure Private Link, and a public IP address isn\'t required.\\n\\nWhen the build finishes, all resources are deleted except for the staging resource group and the storage account. You can remove them by deleting the image template resource or leaving them in place to run the build again.\\n\\nTo use Azure Image Builder, you need to create a configuration that describes your image and submit it to the service where it is built and distributed. A high-level workflow is illustrated in the following diagram:\\n\\n![Azure Image Builder Flow](/images/posts/image-builder-flow.png)\\n\\nToday, we will be using Azure Bicep to create the following:\\n\\n* Azure Compute Gallery *(this will be used to hold our Image Template, so we can build our Virtual Machines from it)*\\n* Image definition *(for the purpose, we will using a Windows Server 2022 Marketplace image, with additional customisations)*\\n* Image Template *(this will include our customisations)*\\n* User Assigned Managed Identity *(the user assigned managed identity will be used to build the image and read the blobs from the Azure storage account)*\\n* Azure Storage account *(the Azure storage account, will hold our Application install files)*\\n\\n![Azure Image Builder - Bicep](/images/posts/AIB_Bicep.png)\\n\\nAnd then an Azure DevOps pipeline to deploy and build our template, including an application install of [Azure Storage Explorer](https://azure.microsoft.com/products/storage/storage-explorer?WT.mc_id=AZ-MVP-5004796) and [bginfo](https://learn.microsoft.com/sysinternals/downloads/bginfo?WT.mc_id=AZ-MVP-5004796).\\n\\nAll the code can be found on my public GitHub repository here: [lukemurraynz/AzureImageBuilder](https://github.com/lukemurraynz/AzureImageBuilder)\\n\\nThe Azure DevOps pipeline will complete the following steps:\\n\\n1. Create a new Azure Resource Group *(if required)*\\n1. Create an Azure storage account *(with public access enabled, and anonymous blob access)*\\n1. Copy the app install files in the /apps/ directory in git to the Azure storage account *(if they don\'t already exist)*\\n1. Deploy the Azure Image Builder infrastructure and Template\\n1. Trigger an Azure CLI command to run the template build\\n1. Adjust public access to the Azure storage account to disable.\\n\\nThe git repository is laid out like so:\\n\\n| Folder       | Description                                              |\\n|--------------|----------------------------------------------------------|\\n| .pipelines   | Contains CI/CD pipeline configurations and scripts.      |\\n| apps         | Houses application code and related resources.            |\\n| iac          | Stores Infrastructure as Code (IaC) configurations.      |\\n| scripts      | Contains utility scripts and other automation tools.     |\\n\\nWithin the iac folder is a customizations.bicep file. This file is intended for your image customisations; I intended to keep this separate from the infrastructure for easy modification without impacting any other file.\\n\\n## Azure DevOps Configuration\\n\\n### Create Project\\n\\nBefore we can build an image using Bicep and Azure DevOps, we need to create a project to hold our code.\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. Click **+ New Project**\\n1. Type in a **name for the project** *(ie AIB (short for Azure Image Builder))*\\n1. Click **Create**\\n\\n![Azure DevOps - Create Project](/images/posts/CreateADOAIBProject.gif)\\n\\n### Initialize Repo\\n\\nNow that we have our project, we can initialize the repository by cloning the [lukemurraynz/AzureImageBuilder](https://github.com/lukemurraynz/AzureImageBuilder) repository, containing all the code we need.\\n\\n*Note: Due to git limitations, the Azure Storage Explorer install file is unable to be stored in the GitHub repository, for my demo. I will upload it to Azure DevOps after the clone. It\'s not recommended to store the binary files for large installs in Git directly for production and to store them in an Azure storage account. This process can be modified to skip the copy application step and install existing files from the Azure storage account.*\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. **Navigate** to your **AIB** project\\n1. Click **Repos**\\n1. Click **Import**\\n1. Make sure the repository type is: **Git** and clone **URL** is: [https://github.com/lukemurraynz/AzureImageBuilder](https://github.com/lukemurraynz/AzureImageBuilder)\\n1. Click **Import**\\n\\n![Azure DevOps - Create Project](/images/posts/ImportADOAIBProject.gif)\\n\\n### Create Azure Service Connection\\n\\nNow that we have our repository and the base code - we need a service connection. This connection will allow our Azure DevOps pipeline authorization to our Microsoft Azure subscription. To do this, we will use, the new [Workload Identity Federation](https://learn.microsoft.com/entra/workload-id/workload-identity-federation?WT.mc_id=AZ-MVP-5004796), which allows us to authenticate to Azure, without worrying about secret management.\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. **Navigate** to your **AIB** project\\n1. Click on **Project Settings**\\n1. Click on **Service connections**\\n1. Click on **Create service connection**\\n1. Select **Azure Resource Manager**\\n1. Click **Next**\\n1. Click **Workload Identity federation (automatic)**\\n1. Click **Next**\\n1. Select your **Subscription** *(after authenticating to Azure)*, and give it a **name** *(you will need the name for the Pipeline)*.\\n1. Click **Save**\\n\\nNote: Consider scoping your Service Connection to the individual Image Builder Resource Group for the least permissions.\\n\\n![Azure DevOps - Create Service Connection](/images/posts/CreateAzureServiceConnectionAIBProject.gif)\\n\\n### Import Pipeline\\n\\nNow that we have our Service Connection let us import our pipeline, which will be used to run our image build.\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. **Navigate** to your **AIB** project\\n1. Click **Pipelines**\\n1. Click **Create pipeline**\\n1. Specify **Azure Repos Git**\\n1. Select your **repository** *(ie AIB)*\\n1. Select **Existing Azure Pipelines YAML file**\\n1. Under the **dropdown** list for the Path, select: **/.pipelines/azure-pipelines.yml**\\n1. Click **Continue**\\n1. Click on the ellipsis and select **Save**\\n\\n![Azure DevOps - Create Project](/images/posts/Import-AzureDevOpsPiipeline-AIBProject.gif)\\n\\n### Edit Pipeline to use service connection\\n\\nNow that the pipeline has been imported, we need to edit it to use the Azure resource manager service connection we created earlier. Make sure you know the name of your service connection *(ie ServiceConnection-AIB)*\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. **Navigate** to your **AIB** project\\n1. Click **Pipelines**\\n1. Click on your **Pipelines**, select **Edit**\\n1. The Service Connection has been created as a **Variable**, named **serviceconnection** in the pipeline, so we need to update the section:\\n\\n    serviceconnection: azserviceconnections\\n\\nTo match the name of our service connection:\\n\\n    serviceconnection: ServiceConnection-AIB\\n\\nThen click **Save**\\n\\n*Note: The Pipeline, by default, is set to run automatically; when it triggers a new commit to the main branch of the repo, you can adjust this to None else. It will try to run and fail *(as its missing variables, etc)*. If you do this, you can trigger the pipeline manually, but it depends on your requirements; as part of the testing - I liked that it automatically ran on every change to the code without having been manually triggered.*\\n\\n![Azure DevOps - Edit Pipeline](/images/posts/Edit-AzureDevOpsPiipelineServiceConnection-AIBProject.gif)\\n\\n### Edit Pipeline to add variables\\n\\nHere is where some of the customisation comes into play. The pipeline is looking for the following variables:\\n\\n* imagetemplatename\\n* location\\n* resourceGroupName\\n* storageaccountname\\n\\nThese variables will be consumed by the Azure CLI, to create the Resource Group and Azure Bicep, to create the Image template and Azure storage account, in your preferred region.\\n\\nBecause in my demo, I am creating a \'Windows Server 2022\' image in Australia East, my variables and values will be as follows:\\n\\n| Variables          | Values             |\\n| ------------------ | ------------------ |\\n| storageaccountname | saccimg4545        |\\n| resourceGroupName  | azimagebuild-rg    |\\n| location           | australiaeast      |\\n| imagetemplatename  | server2022template |\\n\\nSo let us add them.\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. **Navigate** to your **AIB** project\\n1. Click **Pipelines**\\n1. Click on your **Pipelines**, select **Edit**\\n1. Click **Variables**\\n1. Select **New Variable**\\n1. **Add** the required **variables**, make sure they are unique to your own environment *(ie the storage account name, needs to be globally unique)*\\n1. Click **Save**\\n\\n![Azure DevOps - Edit Pipeline](/images/posts/Save-AzureDevOpsPipelineVariables-AIBProject.gif)\\n\\n> There is an [Azure DevOps Task](https://learn.microsoft.com/en-us/azure/virtual-machines/linux/image-builder-devops-task?WT.mc_id=AZ-MVP-5004796), to do the build, however its in preview and doesn\'t support elements such as a Virtual Machine restart at this time, so in my Pipeline, I am triggering the build using the Azure CLI.\\n\\n## Bicep Configuration\\n\\nYou can modify the Bicep according to your own environment; items in the \'main.bicep\' file you may want to change *(that I didn\'t turn into a variable)* are:\\n\\n* Compute Gallery Name\\n* Compute Gallery image name\\n* Paired region that the image template will be replicated to. This is set to the same region as my source.\\n* Name of the User Assigned Managed Identity\\n\\n> To reduce build time, the build VM size I am using is custom: Standard_D4ds_v5. If you remove this, it will default to the Standard_D2ds_v4 size, as you only pay for the time that the VM is building and sys prepping; I found the extra cores useful.\\n\\nI separated the Image Template [customization](https://learn.microsoft.com/azure/virtual-machines/linux/image-builder-json?tabs=json%2Cazure-powershell&WT.mc_id=AZ-MVP-5004796#properties-customize) components, into its own Bicep module, that gets called into the main variable, I intend the customizations.bicep to be separate from the Azure Image Builder infrastructure components, to reduce impact or data loss, and make it easier for a first timer to customize the image separately, allowing you to revert any changes to the image build itself easily.\\n\\nYou may also want to test [Image Optimization](https://learn.microsoft.com/en-us/azure/virtual-machines/vm-boot-optimization?WT.mc_id=AZ-MVP-5004796); I have this enabled; however, it does add time to the image build process, but could improve your Virtual Machine creation time, could be useful to use in conjunction with a service, like Azure Virtual Desktop.\\n\\n> This is the main file you will want to start modifying for your own image customization; I recommend starting small *(i.e. create a folder)*, then adding the apps and customisations one after the other.\\n\\nAs part of the image build, I am:\\n\\n1. Creating a c:\\\\Apps\\\\ folder\\n2. Setting the timezone *(although this gets overwritten by the sysprep back to UTC)*\\n3. Install the latest Windows Updates\\n4. Restart\\n5. Copy bginfo from the storage account *(this has been copied to the Azure storage account as part of the pipeline step)* to the apps folder. As it is small in size, I am using the File Packer provider to do the copy.\\n6. Copy the Azure Storage Explorer install file from the storage account *(this has been copied to the Azure storage account as part of the pipeline step)* to the apps folder. As this is a larger file, the File packer provider locked up when attempting to copy, so I am leveraging Invoke-RestMethod to download it to the apps folder.\\n7. Extract the bginfo zip file\\n8. Install the Azure Storage Explorer\\n9. Do a final Windows Update install *(in a past life, I\'ve found new updates - ie Visual Studio can popup after application installs, if your image build time is quite long, feel free to remove this step, for me its a personal preference)*\\n10. Do a final restart\\n\\nAs you can see, everything is mostly PowerShell-driven. Be aware of the double backslashes needed as well; for directories, using a single backslash will cause issues, as it is classed as an escape character. When editing the Bicep file, make sure you make sure of the [Bicep Visual Studio code extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-bicep). I have also created a base [Codespace](https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/) in the GitHub repo that you could work from as well.\\n\\nThen, ideally, you can delete your previous image template every month and create a new one with the latest Windows Updates. Hopefully, this has given you enough of a sample to work from.\\n\\n## Run image build deployment\\n\\nSo, we have modified our Bicep and have our pipeline ready to go! It\'s time to run the build and create our Azure resources. We do this from Azure DevOps.\\n\\n1. Login to your **[Azure DevOps](https://dev.azure.com/)** instance\\n1. **Navigate** to your **AIB** project\\n1. Click **Pipelines**\\n1. Click on your **Pipeline**\\n1. Select **Run pipeline**\\n1. Before the pipeline runs, we need to **verify that the Pipeline has permission** to use the service connection\\n1. Select **Verify**\\n1. Select **Approve** to allow the pipeline stages, the ability to use the service principal\\n\\n> If your build Build fails halfway through with the following error when attempting to create the Azure Image Builder resources:\\n> ERROR: {\\"code\\": \\"InvalidTemplateDeployment\\", \\"message\\": \\"Deployment failed with multiple errors: \'Authorization failed for template resource \'e6eaf07c-bb64-57d4-a69a-6d1463a77bdb\' of type \'Microsoft.Authorization/roleAssignments\'. The client \'1d06bec0-6bb8-4ba2-b33b-98a4eafd5b84\' with object id \'1d06bec0-6bb8-4ba2-b33b-98a4eafd5b84\' does not have permission to perform action \'Microsoft.Authorization/roleAssignments/write\\n> This is due to the Service Connection not having Owner rights to the Azure subscription. Owner rights are created for the User Assigned Managed identity role assignments. You can fix this by copying the Object ID of the error, navigating to [Entra ID Enterprise Apps](https://portal.azure.com/#view/Microsoft_AAD_IAM/StartboardApplicationsMenuBlade/~/AppAppsPreview/menuId~/null), and search by your Object ID. This will find an Enterprise Application, that is used by Azure DevOps to talk to Azure. Copy the Name, and navigate to your Azure subscription, and Access Control (IAM) blade, and add in your Enterprise Application as Owner.\\n\\n> It can take 30+ minutes to build a Virtual Machine; the more customisations, the longer it will take, so please be patient, after the build has kicked off. As part of the build, you can also download and review the packer logs directly from the Storage account that was created in the IT resource group. I highly recommend testing one customization before proceeding to the next until you have the implementation downpack. The build step, if completed, will move to Optimizing, then Distributing to the Compute Gallery before it can be used.\\n\\n![Azure DevOps - Run Pipeline](/images/posts/Run-AzureDevOpsBuildPipeline-AIBProject.gif)\\n\\n## Build Virtual Machine\\n\\nNow that we have successfully built our Windows Server 2022 image, entirely automated using Bicep and Azure DevOps pipeline, let us create a Virtual Machine using the image that now resides in our Compute Gallery. To do this, we will simply use the Azure Portal to build and test from the Compute Gallery.\\n\\n1. Login to the **[Azure Portal](https://portal.azure.com/#home)**\\n1. Navigate to your **Azure Compute Gallery**\\n1. Click your **Image definition**, which should now have a version associated with it *(ie 1.0.0 (latest version))*\\n1. Click **+ Create VM**\\n1. Navigate through the normal steps to create your virtual machine using your custom template.\\n\\n![Azure Portal - Create Virtual Machine](/images/posts/Create-TestVM-AIBProject.gif)\\n\\nAs you can see, the Virtual Machine has the Apps folder, Azure Storage Explorer installed and the bginfo executable stored as part of the image template.\\n\\n## Code\\n\\nAll the code can be found on my public GitHub repository here: [lukemurraynz/AzureImageBuilder](https://github.com/lukemurraynz/AzureImageBuilder). This is the master *(and happy to take any contributions to it; just open a Pull Request)*,; for reference the code can be reviewed below:\\n\\n### main.bicep\\n\\n```bicep title=\\"main.bicep\\"\\n// Define parameters for the Bicep template\\nparam location string = resourceGroup().location\\nparam imagetemplatename string\\nparam azComputeGalleryName string = \'myGallery\'\\n@description(\'The name of the Storage account.\')\\nparam stgaccountname string\\nparam azUserAssignedManagedIdentity string = \'useri\'\\n\\n// Define the details for the VM offer\\nvar vmOfferDetails = {\\n  offer: \'WindowsServer\'\\n  publisher: \'MicrosoftWindowsServer\'\\n  sku: \'2022-datacenter-azure-edition\'\\n}\\n\\n// Include the customizations module\\nmodule customizationsModule \'customizations.bicep\' = {\\n  name: \'customizationsModule\'\\n  params: {\\n    stgaccountname: stgaccountname\\n  }\\n}\\n\\n// Create a user-assigned managed identity\\nresource uami \'Microsoft.ManagedIdentity/userAssignedIdentities@2023-01-31\' = {\\n  name: azUserAssignedManagedIdentity\\n  location: location\\n}\\n\\n// Create a Compute Gallery\\nresource azComputeGallery \'Microsoft.Compute/galleries@2022-03-03\' = {\\n  name: azComputeGalleryName\\n  location: location\\n  properties: {\\n    description: \'mygallery\'\\n  }\\n}\\n\\n// Assign the Contributor role to the managed identity at the resource group scope\\nresource uamicontribassignment \'Microsoft.Authorization/roleAssignments@2022-04-01\' = {\\n  name: guid(resourceGroup().id, \'contributor\')\\n  properties: {\\n    principalId: uami.properties.principalId\\n    principalType: \'ServicePrincipal\' \\n    roleDefinitionId: resourceId(\'Microsoft.Authorization/roleDefinitions\', \'b24988ac-6180-42a0-ab88-20f7382dd24c\') // Contributor\\n  }\\n  scope: resourceGroup()\\n}\\n\\n// Assign the Storage Blob Data Reader role to the managed identity at the resource group scope\\nresource uamiblobassignment \'Microsoft.Authorization/roleAssignments@2022-04-01\' = {\\n  name: guid(resourceGroup().id, \'blobreader\')\\n  properties: {\\n    principalId: uami.properties.principalId\\n    principalType: \'ServicePrincipal\' \\n    roleDefinitionId: resourceId(\'Microsoft.Authorization/roleDefinitions\', \'2a2b9908-6ea1-4ae2-8e65-a410df84e7d1\') // Storage Blob Data Reader\\n  }\\n  scope: resourceGroup()\\n}\\n\\n// Create an image in the Compute Gallery\\nresource azImage \'Microsoft.Compute/galleries/images@2022-03-03\' = {\\n  name: \'${azComputeGallery.name}/myImage\'\\n  location: location\\n  properties: {\\n    description: \'myImage\'\\n    osType: \'Windows\'\\n    osState: \'Generalized\'\\n    hyperVGeneration: \'V2\'\\n    identifier: {\\n      publisher: vmOfferDetails.publisher\\n      offer: vmOfferDetails.offer\\n      sku: vmOfferDetails.sku\\n    }\\n  }\\n  dependsOn: [\\n    uami\\n  ]\\n}\\n\\n// Create an image template\\nresource azImageTemplate \'Microsoft.VirtualMachineImages/imageTemplates@2022-07-01\' = {\\n  name: imagetemplatename\\n  location: location\\n  identity: {\\n    type: \'UserAssigned\'\\n    userAssignedIdentities: {\\n      \'${uami.id}\': {}\\n    }\\n  }\\n  properties: {\\n    buildTimeoutInMinutes: 360\\n    distribute: [\\n      {\\n        type: \'SharedImage\'\\n        galleryImageId: azImage.id\\n        runOutputName: \'myImageTemplateRunOutput\'\\n        replicationRegions: [\\n          \'Australia East\'\\n        ]\\n      }\\n    ]\\n    source: {\\n      type: \'PlatformImage\'\\n      publisher: vmOfferDetails.publisher\\n      offer: vmOfferDetails.offer\\n      sku: vmOfferDetails.sku\\n      version: \'latest\'\\n    }\\n    customize: customizationsModule.outputs.customizationsOutput\\n    vmProfile: {\\n      vmSize: \'Standard_D4ds_v5\'\\n      osDiskSizeGB: 0 // Leave size as source image size.\\n    \\n    }\\n    \\n    optimize: {\\n      vmBoot: {\\n        state: \'Enabled\'\\n      }\\n    }\\n  }\\n}\\n}\\n```\\n### storageaccount.bicep\\n\\n```bicep title=\\"storageaccount.bicep\\"\\n// Define the location parameter for all resources\\n@description(\'Location for all resources.\')\\nparam location string = resourceGroup().location\\n\\n// Define the parameter for the Storage account name\\n@description(\'The name of the Storage account.\')\\nparam stgaccountname string\\n\\n// Define the parameter for the public access setting of the Storage account\\n@description(\'Sets the public access of the storage account.\')\\nparam publicaccess bool\\n\\n// Create a Storage account\\nresource storgeaccount \'Microsoft.Storage/storageAccounts@2023-01-01\' = {\\n  name: toLower(stgaccountname) // Convert the Storage account name to lowercase\\n  location: location // Set the location of the Storage account\\n  kind: \'StorageV2\' // Use the \'StorageV2\' kind\\n  sku: {\\n    name: \'Standard_LRS\' // Use the \'Standard_LRS\' SKU\\n  }\\n  properties: {\\n    accessTier: \'Hot\' // Set the access tier to \'Hot\'\\n    allowBlobPublicAccess: publicaccess // Set the public access setting\\n  }\\n}\\n\\n// Create a Blob service for the Storage account\\nresource blobService \'Microsoft.Storage/storageAccounts/blobServices@2022-09-01\' = {\\n  parent: storgeaccount // Set the parent resource to the Storage account\\n  name: \'default\' // Use the \'default\' name\\n  properties: {\\n    isVersioningEnabled: true // Enable versioning\\n  \\n  }\\n}\\n\\n// Create a container in the Blob service\\nresource appcontainer \'Microsoft.Storage/storageAccounts/blobServices/containers@2022-09-01\' = {\\n  name: \'iac\' // Set the container name to \'iac\'\\n  parent: blobService // Set the parent resource to the Blob service\\n  properties: {\\n    publicAccess: publicaccess ? \'Blob\' : \'None\' // Set the public access setting\\n  }\\n}\\n\\n// Output the ID of the Storage account\\noutput storgeaccountid string = storgeaccount.id\\n\\n// Output the name of the Storage account\\noutput storgeaccountname string = storgeaccount.name\\n\\n// Output the name of the container\\noutput containername string = appcontainer.name\\n```\\n\\n### customizations.bicep\\n\\n```bicep title=\\"customizations.bicep\\"\\n// Define the parameter for the Storage account name\\n@description(\'The name of the Storage account.\')\\nparam stgaccountname string\\n\\n// Get the environment-specific metadata\\nvar environmentMetadata = environment()\\n\\n// Define the customizations for the image\\nvar customizations = [\\n  // Create a apps directory on the OS drive\\n  {\\n    type: \'PowerShell\'\\n    name: \'Create Apps Directory on OS drive\'\\n    runElevated: false\\n    runAsSystem: false\\n    inline: [\\n      // Use double backslashes to represent a single backslash in the file path\\n      \'New-Item -ItemType Directory -Force -Path $env:SystemDrive\\\\\\\\apps\\\\\\\\\'\\n    ]\\n  }\\n\\n  // Set the timezone to New Zealand Standard Time\\n  {\\n    type: \'PowerShell\'\\n    name: \'Set Timezone - New Zealand Standard Time\'\\n    runElevated: true\\n    runAsSystem: true\\n    inline: [\\n      \'Set-TimeZone -Id \\"New Zealand Standard Time\\"\'\\n    ]\\n  }\\n\\n  // Install Windows updates, excluding preview updates\\n  {\\n    type: \'WindowsUpdate\'\\n    searchCriteria: \'IsInstalled=0\'\\n    filters: [\\n      \'exclude:$_.Title -like \\\\\'*Preview*\\\\\'\' // Exclude preview updates\\n      \'include:$true\'\\n    ]\\n    updateLimit: 20\\n  }\\n\\n  // Restart the system after Windows updates have completed\\n  {\\n    type: \'WindowsRestart\'\\n    restartCheckCommand: \'write-host \\\\\'restarting post Windows Updates\\\\\'\'\\n    restartTimeout: \'10m\'\\n  }\\n\\n  // Copy BGInfo from the Storage account to the temporary directory\\n  {\\n    type: \'File\'\\n    name: \'Copy BGInfo from Storage account\'\\n    sourceUri: \'https://${stgaccountname}.blob.${environmentMetadata.suffixes.storage}/iac/bginfo/BGInfo.zip\'\\n    destination: \'$env:SystemDrive\\\\\\\\apps\\\\\\\\BGInfo.zip\'\\n  }\\n\\n  // Copy Storage Explorer from the Storage account to the temporary directory\\n  {\\n    type: \'PowerShell\'\\n    name: \'Copy Storage Explorer from Storage account\'\\n    runElevated: true\\n    runAsSystem: true\\n    inline: [\\n      // Use double backslashes to represent a single backslash in the file path\\n      \'Invoke-RestMethod  https://${stgaccountname}.blob.${environmentMetadata.suffixes.storage}/iac/storageexplorer/StorageExplorer-windows-x64.exe -OutFile  $env:SystemDrive\\\\\\\\apps\\\\\\\\StorageExplorer-windows-x64.exe\'\\n    ]\\n  }\\n\\n  // Extract BGInfo\\n  {\\n    type: \'PowerShell\'\\n    name: \'Extract BGInfo\'\\n    runElevated: true\\n    runAsSystem: true\\n    inline: [\\n      // Use double backslashes to represent a single backslash in the file path\\n      \'Expand-Archive -LiteralPath $env:SystemDrive\\\\\\\\apps\\\\\\\\BGInfo.zip -DestinationPath $env:SystemDrive\\\\\\\\apps\\\\\\\\\'\\n    ]\\n  }\\n\\n  // Install Storage Explorer\\n  {\\n    type: \'PowerShell\'\\n    name: \'Install Storage Explorer\'\\n    runElevated: true\\n    runAsSystem: true\\n    inline: [\\n      // Use double backslashes to represent a single backslash in the file path\\n      // Check if the executable file exists\\n    \'$exePath = \\"$env:SystemDrive\\\\\\\\apps\\\\\\\\StorageExplorer-windows-x64.exe\\"\'\\n    \'if (Test-Path $exePath) {\'\\n    \'  & $exePath /VERYSILENT /SUPPRESSMSGBOXES /NORESTART /ALLUSERS\'\\n    \'} else {\'\\n    \'  Write-Output \\"The file $exePath does not exist.\\"\'\\n    \'}\'\\n    ]\\n  }\\n\\n   // Install any Windows updates, that are left or needed after app installs\\n   {\\n    type: \'WindowsUpdate\'\\n    searchCriteria: \'IsInstalled=0\'\\n    filters: [\\n      \'exclude:$_.Title -like \\\\\'*Preview*\\\\\'\' // Exclude preview updates\\n      \'include:$true\'\\n    ]\\n    updateLimit: 20\\n  }\\n\\n  // Restart the system after Windows updates have completed and fresh restart before sysprep.\\n  {\\n    type: \'WindowsRestart\'\\n    restartCheckCommand: \'write-host \\\\\'restarting post image customisation\\\\\'\'\\n    restartTimeout: \'10m\'\\n  }\\n\\n]\\n\\n// Output the customizations array\\noutput customizationsOutput array = customizations\\n```\\n\\n### azure-pipelines.yml\\n\\n```yaml title=\\"azure-pipelines.yml\\"\\n# Define the pipeline name and trigger\\nname: Azure Image Builder - Build and Publish Image Template\\ntrigger:\\n- main\\n\\n# Define pipeline variables\\nvariables:\\n  serviceconnection: azserviceconnections\\n  overwrite: false\\n\\n# Define the VM image for the pipeline\\npool:\\n  vmImage: ubuntu-latest\\n\\n# Define the stages of the pipeline\\nstages:\\n\\n  # First stage: Deploy Azure Storage Account\\n- stage: ImageBuilderDeploy\\n  jobs:\\n  - deployment: Bicepstgaccount\\n    displayName: \'Deploy Azure Storage Account to Azure for Apps\'\\n    environment: \'AzureDeployment\'\\n    strategy:\\n      runOnce:\\n        deploy:\\n          steps:\\n          - checkout: self\\n           # Deploy the Bicep template for the Azure Storage account\\n          - task: AzureCLI@2\\n            displayName: \'Deploy Bicep - Azure Storage account and IaC App Container\'\\n            inputs:\\n              azureSubscription: $(serviceconnection) # replace with your service connection name\\n              scriptType: \'pscore\'\\n              scriptLocation: \'inlineScript\'\\n              inlineScript: |\\n                az group create --name $(ResourceGroupName) --location $(location) \\n                az deployment group create `\\n                        --template-file $(Build.SourcesDirectory)/iac/storageaccount.bicep `\\n                        --resource-group $(resourceGroupName) `\\n                        --parameters location=$(location) stgaccountname=$(storageaccountname) publicaccess=true\\n\\n          # Copy the app install files to the Azure Storage account\\n          - task: AzureCLI@2\\n            displayName: \'Copy App install files to Azure Storage Account\'\\n            inputs:\\n              azureSubscription: $(serviceconnection)\\n              scriptType: \'bash\'\\n              scriptLocation: \'inlineScript\'\\n              inlineScript: |\\n                az storage blob upload-batch -d \'iac\' --account-name $(storageaccountname) -s $(Build.SourcesDirectory)/apps --type block --overwrite $(overwrite) --verbose\\n                blobs=$(az storage blob list --account-name $(storageaccountname) --container-name \'iac\' --query \'[].{name:name, url:properties.url}\' -o tsv)\\n                echo $blobs\\n\\n  # Second job: Deploy Azure Image Builder Infrastructure\\n  - job: ImageBuilderDeployment\\n    dependsOn: Bicepstgaccount\\n    displayName: \'Deploy Azure Image Builder Infrastructure\'\\n    steps:\\n     # Build the Azure Image Builder template\\n      - task: AzureCLI@2\\n        displayName: \' Build Azure Image Builder Template\'\\n        inputs:\\n          azureSubscription: $(serviceconnection) # replace with your service connection name\\n          scriptType: \'pscore\'\\n          scriptLocation: \'inlineScript\'\\n          inlineScript: |\\n            az group create --name $(ResourceGroupName) --location $(location) \\n            az deployment group create `\\n                    --template-file $(Build.SourcesDirectory)/iac/main.bicep `\\n                    --resource-group $(resourceGroupName) `\\n                    --parameters location=$(location) imagetemplatename=$(imagetemplatename) stgaccountname=$(storageaccountname) # Add more parameters as needed\\n\\n# Second stage: Run Azure Image Builder Template Build\\n- stage: ImageBuilderRun\\n  jobs:\\n  - job: ImageBuilderRun\\n    displayName: \'Run Azure Image Builder Template Build\'\\n    steps:\\n      # Run the Azure Image Builder\\n      - task: AzureCLI@2\\n        displayName: \'Run Azure Image Builder\'\\n        inputs:\\n          azureSubscription: $(serviceconnection) # replace with your service connection name\\n          scriptType: \'pscore\'\\n          scriptLocation: \'inlineScript\'\\n          inlineScript: |\\n            az image builder run -n $(imagetemplatename) -g $(resourceGroupName) --no-wait --verbose\\n                        az image builder wait -n $(imagetemplatename) -g $(resourceGroupName) --custom \\"lastRunStatus.runState!=\'Running\'\\" --verbose\\n\\n      - task: AzureCLI@2\\n        displayName: \'Deploy Bicep - Set Azure Storage account public access to false\'\\n        inputs:\\n              azureSubscription: $(serviceconnection) # replace with your service connection name\\n              scriptType: \'pscore\'\\n              scriptLocation: \'inlineScript\'\\n              inlineScript: |\\n                az group create --name $(ResourceGroupName) --location $(location) \\n                az deployment group create `\\n                        --template-file $(Build.SourcesDirectory)/iac/storageaccount.bicep `\\n                        --resource-group $(resourceGroupName) `\\n                        --parameters location=$(location) stgaccountname=$(storageaccountname) publicaccess=false\\n```"},{"id":"azure/misc/Book-Review-Azure-architecture-explained","metadata":{"permalink":"/azure/misc/Book-Review-Azure-architecture-explained","source":"@site/blog/2023-11-18-Book-Review-Azure-architecture-explained.md","title":"Book Review Azure Architecture Explained","description":"Book review of Azure Architecture Explained: A comprehensive guide to building effective cloud solutions","date":"2023-11-18T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":12.295,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Book Review Azure Architecture Explained","authors":["Luke"],"tags":["Azure","Misc"],"toc":true,"header":{"teaser":"/images/posts/BlobHeading_Review_AzureArchitectureExplained.gif"},"date":"2023-11-18 00:00:00 +1300","slug":"azure/misc/Book-Review-Azure-architecture-explained"},"unlisted":false,"prevItem":{"title":"Azure Image Builder Image Build with Bicep and Azure DevOps","permalink":"/azure/Azure-Image-Builder-Build-Pipeline-with-Azure-DevOps"},"nextItem":{"title":"Mastering CIDRs With Azure Bicep","permalink":"/azure/Working-with-CIDR-Azure-Bicep"}},"content":"Book review of [Azure Architecture Explained: A comprehensive guide to building effective cloud solutions](https://www.packtpub.com/product/azure-architecture-explained/9781837634811)\\n\\n\x3c!-- truncate --\x3e\\n\\n> Disclaimer: This is the first book review I have ever done, and this review is entirely my opinion and may not fully reflect the value you may or may not get from this book. Everyone will have their unique viewpoint and priorities of what they want out of a book, so please take my review as an opinionated view from my own viewpoint; I am, of course, happy to discuss different points of view and discussions on this page comments. We are all here to learn something!\\n\\nBook synopsis:\\n\\n> \\"Azure is a sophisticated technology that requires a detailed understanding to reap its full potential and employ its advanced features. This book provides you with a clear path to designing optimal cloud-based solutions in Azure by delving into the platform\'s intricacies.\\n> You\u2019ll begin by understanding the effective and efficient security management and operation techniques in Azure to implement the appropriate configurations in Microsoft Entra ID. Next, you\u2019ll explore how to modernize your applications for the cloud, examining the different computation and storage options, as well as using Azure data solutions to help migrate and monitor workloads. You\u2019ll also learn how to build your solutions, including containers, networking components, security principles, governance, and advanced observability. With practical examples and step-by-step instructions, you can work on infrastructure-as-code to effectively deploy and manage resources in your environment.\\"\\n\\n![Azure Architecture Explained](/images/posts/Review_AzureArchitectureExplained_TitleBoat.jpg)\\n\\n## Introduction\\n\\n[Azure Architecture Explained](https://www.packtpub.com/product/azure-architecture-explained/9781837634811) is a Packtpub published (in September 2023) book written by the following authors:\\n\\n* [David Rend\xf3n](https://www.linkedin.com/in/daverndn/)\\n* [Brett Hargreaves](https://www.linkedin.com/in/bretthargreaves/)\\n\\nForward by [Sarah Kong](https://www.linkedin.com/in/konger/)\\n\\n> Although this book was given to me to review (for free, I have no formal relationship with either Packetpub, although I thank them for allowing me to review this book or the Authors, and I am reviewing this book from an entirely independent view), as someone who architects and builds Azure solutions, this is the type of book, I usually would read, based on the title and synopsis, so let us take under the cover...  \\n\\n## Overview of Azure Architecture\\n\\nThe book\'s slogan is \\"A **comprehensive** guide to **building** effective cloud solutions\\".\\n\\n> Architecting solutions in Azure is a **massive** subject, requiring knowledge about the details of specific services, knowledge and understanding of **Cloud adoption**, **well-architected frameworks**, **patterns and practices** and **interoperability** between different services and systems, if one were to use this slogan and book title, I would expect the book to cover across all these areas.\\n\\n![Azure Architecture Book - Courtesy of Bing Image Creator](/images/posts/AzureBook_AzureArchitectureExplained_AIGenerated.jpg)\\n\\nSo let us look to see if the book meets these details...\\n\\n### Cloud Adoption\\n\\nThe book includes a section on the Microsoft Cloud Adoption Framework, including links, diagrams and explanations of how the Cloud Adoption framework fits in - not only with Microsoft Azure (which is the focus of this book) but with other Clouds as well!\\n\\n**What I liked about the representation of the [Cloud Adoption](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) within this book was - it was enough for a beginner and intermediate in Cloud adoption, to get going - without the need for third-party resources (i.e. resources outside of this book), but for those after a bit more advanced information or detail - supplied the relevant internet links, to the relevant Microsoft Learn resources *(in short format, i.e. bit.ly - so easily can be copied from a hard covered paper book into your browser)*, so the authors didn\'t try to shove a lot of unnecessary detail into the book, but was to the point, as they started the journey into Cloud governance and covered the basics and knowledge at an intermediate level, without a lot of the detail, that you don\'t necessarily need as part of the book.**\\n\\n### Well-Architected Framework\\n\\nIn terms of the Well-Architected framework, however, there needed to be more direct representation, which surprised me, as although the Cloud Adoption Framework guides the Cloud adoption journey, the Well-Architected Framework focuses more on helping you make informed decisions for building systems in These frameworks can work side by side - HOWEVER, having said that...\\n\\nThe Microsoft Well-Architected Framework consists of 5 pillars, these are:\\n\\n* Performance Efficiency\\n* Reliability\\n* Security\\n* Cost Optimization\\n* Operational Excellence\\n\\nUnderneath these pillars are design principles, such as design for business requirements, design for resiliency, design for recovery, etc.\\n\\nThese design principles were woven throughout the implementation portions of the books and, although not directly called out in some areas, had high-level representation that you can subtly pick up.\\n\\n**I believe, reading the [Cloud Adoption](https://learn.microsoft.com/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796) and [Well-Architected Frameworks](https://learn.microsoft.com/azure/well-architected/?WT.mc_id=AZ-MVP-5004796), directly on the Microsoft Learn website, can help supplement your understanding of this book, and some of the decisions made by the authors.**\\n\\n### Patterns and practices\\n\\nLet us look at a particular part of the book; my example is the \'Making the Most of the Infrastructure-as-Code for Azure\' section.\\n\\nWhen I talk about patterns and practices, I am thinking of 2 things:\\n\\n1. Does a Pattern provide a prescribed solution to a problem?\\n2. Does a Practice provide an established method or way of working?\\n\\n> \\"Infrastructure-as-code enables organizations to define and manage their infrastructure and application resources in a programmatic and automated manner. Developers and operations teams can use configuration files or scripts to describe their desired Azure infrastructure and deploy it consistently and reliably. This part will discuss how infrastructure-as-code in Azure empowers organizations to optimize their cloud infrastructure, increase productivity, and drive successful digital transformations.\\"\\n\\nThis particular part included Chapters such as:\\n\\n* Governance in Azure\\n* Building Solutions using Azure Bicep\\n* Using Azure Pipelines to Build your Infrastructure in Azure\\n* Continous integration and deployment using Azure DevOps\\nand Tips from the Field!\\n\\nInstead of jumping straight into the build, the authors take the time to explain Azure governance by following the premise of a hypothetical company and that company challenges, and then working on the previous parts of the work, where the authors had done a very prescriptive *(great)* job of the management and creation of relevant Azure resources using the Azure Portal and explain the business challenges that Infrastructure as Code helps with, before jumping into the authoring, management and implementation of Azure Bicep for Infrastructure as Code.\\n\\n**Patterns and practices, in my opinion, are where this build shone through and where the true value is are the Patterns and practices. Throughout this book, the authors explain the WHY and WHAT before presenting the HOW. It was straightforward to follow and build upon what was learnt and discovered from previous chapters.**\\n\\n### Interoperability\\n\\nInteroperability is always very hard to review - the definition of interoperability I am using is *\\"the ability of computer systems or software to exchange and make use of information.\\"* Commonly, how *systems work together*. In the complex world we live in, systems being able to talk to other systems made by other third-party vendors and individuals can be key to successful business outcomes so that it can be a very technically driven area.\\n\\nThis is hard to review when discussing a book-related specifically to Azure architecture *(i.e., this is not a book about connecting Azure services to AWS, etc.)*.\\n\\n![Azure Architecture Book - Courtesy of Bing Image Creator](/images/posts/AzureBook_AzureArchitectureExplained_AIGenerated_Interopability.jpg)\\n\\nHowever, interoperability is a key area for a topic such as Entra ID, to which this book has entire chapters dedicated.\\n\\nThis book has all the problems that all books have! Information Technology changes so rapidly that there\'s a potential risk that the book you just purchased may be out of date.\\nFor context, this book was published in September of 2023 *(22 Sept 2023)*, which would mean a lot of writing and work, then peer-review, assuming both from a technical standpoint and writing standpoint, after the authors initial \'pre-prod\' draft, before it reaches the public. In mid-July of 2023 *(11 July 2023)* Microsoft announced a rename of Azure Active Directory to Entra ID. There was mention of the rename, but Azure AD was still used in certain areas of the book.\\n\\nIn the context of this book, Entra ID is referred to by its previous name, Azure Active Directory; as a reader who is aware of that rename, it was still incredibly easy to read and follow.\\n\\nThe book explains user identities, the differences between authentication and authorization *(Which I loved!)* and how Azure Active Directory *(i.e. Entrea ID)* can be used to integrate and secure applications and work alongside the rest of the Microsoft Entra ID suite of products, all within the context of a fictitious company, allowing you to follow on.\\n\\n**Overall, I believe the interoperability portions of Identity in this book are first class, from the creation of your first tenancy to Role Based Access to Azure resources; the authors do a great job in explaining how identity underpins the platforms and enables connectivity and access to multiple services, that even a beginner can get going quickly, with knowing WHY they are doing the stuff, that the book asks of them!**\\n\\n## Content and Coverage\\n\\nWhen considering Content and Coverage, my main concern here is:\\n\\n* Is the content in this book actually \'actionable\'?\\n* Can I use what I learn in this book in real-world scenarios, or is it entirely hypothetical?\\n\\nI am pleased to say **Yes** to this book; it is both actionable and useful in the real world! I was pleasantly surprised by this!\\n\\nArchitecture can be a very *dry* subject, and there are so many ways you can approach it, for example:\\n\\n| Architect Type          | Focus Area                                                |\\n|-------------------------|----------------------------------------------------------|\\n| **System Architect**    | Design and structure of individual systems or applications. |\\n| **Enterprise Architect**| Aligning IT strategy with overall business strategy.       |\\n| **Solution Architect**  | Designing solutions that meet specific project requirements.|\\n| **Data Architect**      | Designing and managing an organization\'s data architecture.|\\n| **Application Architect**| Designing the structure and interaction of applications.  |\\n| **Network Architect**   | Designing and implementing computer networks.              |\\n| **Security Architect**  | Designing and implementing security measures and controls. |\\n| **Cloud Architect**     | Designing and implementing cloud infrastructure solutions.|\\n| **Infrastructure Architect**| Designing and managing IT infrastructure components.   |\\n| **Integration Architect**| Ensuring seamless integration between different systems.   |\\n| **Business Architect**  | Aligning business processes and IT strategy.              |\\n\\n![Azure Architects - Courtesy of Bing Image Creator](/images/posts/AzureBook_AzureArchitectureExplained_AIGenerated_Architects.jpg)\\n\\n**When I look at this book, I am looking at it from a mix of Cloud, Solution and Infrastructure architecture, mainly due to my own personal journey, and this is where I feel the book fits.**\\n\\nWhen I look at the slogan of the book again, and the word **comprehensive** is used, I believe this is used accurately; the authors did a great book in discussing the WHY, WHAT and HOW of various solutions in the Networking section of this book, the authors covered design considerations for VNET *(Virtual Networks)*, before delving into more advanced topics, such as Network Security and Azure Virtual Virtual WAN, bringing you on the journey that you would take in real life.\\n\\n## Clarity and Accessibility\\n\\n**The authors did a very clear job of explaining, from start to finish, how to create the necessary Azure resources and why you should have created them in the first place. This was very well done. In just about all sections, the words were supplemented with images and diagrams, making it easier to further understand what was happening in the text. Any links to internet websites were done as short links, making it very easy to copy and navigate to, whether you have a physical copy of the book or a digital copy.**\\n\\n## Hands-On Exercises and Tutorials\\n\\n**This book contained valid hands-on exercises and tutorials, allowing you to follow along with the content.**\\n\\nSample code and exercise files were also provided publically on GitHub [PacktPublishing/Azure-Architecture-Explained](https://github.com/PacktPublishing/Azure-Architecture-Explained) and [daveRendon/azinsider](https://github.com/daveRendon/azinsider), allowing readers, to easily fork and get a copy of the code required for the relevant exercises, such as creating resources, such as an Azure Virtual Network with Bicep.\\n\\n## Updates and Relevance\\n\\nAt the time of this review (November 2023), this book (published in September 2023) is still entirely relevant; as mentioned earlier, there have been some renames of products from Microsoft that are a \'gotcha\' to know, as part of reading this book, but the theory and implementation steps are entirely relevant.\\n\\nI came across one link to a website [cafbaseline.com](https://cafbaseline.com/) that was not available at the time of reading, but all other links I used were relevant and working!\\n\\n**As with all books, if you come across this book 2-3 years down the track *(i.e. 2025/2026)* make sure you check the publishing date and version and make sure there\'s not a renewer version of this book! This is a book I personally would love to see various revisions and updates to as the world of Microsoft Azure changes!**\\n\\n## Author\'s Expertise\\n\\nI believe the authors have the correct expertise to make this book viable *(and based on my own knowledge, reading this book)* and accurate.\\n\\n* [David Rend\xf3n](https://www.linkedin.com/in/daverndn/) is a Microsoft MVP (Most Valuable Professional) and MCT (Microsoft Certified Trainer); these awards themselves require technical validation, and the display of knowledge to help solve organisation problems, and as an MCT, David, has the experience to explain the concepts, in a way that could be easily understood as a beginner.\\n* [Brett Hargreaves](https://www.linkedin.com/in/bretthargreaves/) as a current Cloud Practice Lead, with certifications, such as the Azure Solutions Architect Expert and a background in Architect, Brett has the practical know-how to help make this content relevant, to those looking to Architect solutions in Microsoft Azure.\\n\\n**It is my opinion that this duo is indeed suited to write this type of book, and this can be seen from the actionable and valid output of the book itself.**\\n\\n## Conclusion\\n\\n**Azure Architecture Explained is a book containing 409 pages of actionable content.**\\n\\nThe authors draw on their extensive experience and research to provide practical patterns and practices, to help build on the story of starting from scratch, understanding the *big picture* of how complex systems within the Azure ecosystem sit together, with the advice actually to get started and create and modify those resources.\\n\\nThe book is well-structured and full of relevant examples and exercises.\\n\\n> **I highly recommend this book to anyone interested in architecting solutions across Microsoft Azure, and it was very easy to read and consume the content! I feel smarter, having read it. Thank you to the authors for making the effort to write and release this book. It is a good addition to my library of books that I will be referencing in the future! I read this book on a Cruise while relaxing, and it didn\'t feel like I was doing work!**"},{"id":"azure/Working-with-CIDR-Azure-Bicep","metadata":{"permalink":"/azure/Working-with-CIDR-Azure-Bicep","source":"@site/blog/2023-11-18-Working-with-CIDR-Azure-Bicep.md","title":"Mastering CIDRs With Azure Bicep","description":"Classless Inter-Domain Routing (CIDR) is a method of allocating IP addresses and routing Internet Protocol (IP) packets.","date":"2023-11-18T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.635,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Mastering CIDRs With Azure Bicep","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlogHeading_Mastering-CIDRs-with-Azure-Bicep.gif"},"date":"2023-11-18 00:00:00 +1300","slug":"azure/Working-with-CIDR-Azure-Bicep"},"unlisted":false,"prevItem":{"title":"Book Review Azure Architecture Explained","permalink":"/azure/misc/Book-Review-Azure-architecture-explained"},"nextItem":{"title":"IPv6 in Microsoft Azure","permalink":"/azure/IPv6-on-Azure"}},"content":"[Classless Inter-Domain Routing](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) *(CIDR)* is a method of allocating IP addresses and routing Internet Protocol *(IP)* packets.\\n\\n![Mastering CIDRs With Azure Bicep](/images/posts/BlogHeading_Mastering-CIDRs-with-Azure-Bicep.gif)\\n\\nThis article includes sample Bicep functions for working with CIDR, for Azure Virtual Network and Subnet creation.\\n\\n\x3c!-- truncate --\x3e\\n\\n```bicep title=\\"CiDR.bicep\\"\\n    // This function parses the CIDR notation and returns an object with the network address, subnet mask, and other details.\\n    output v6addressspace object = parseCidr(\'2001:db8:1234::/48\')\\n\\n    // This function generates an array of subnets within the specified CIDR block. The subnet size is 64, and the index calculates the subnet.\\n    // The \'range(0, 5)\' function generates an array of numbers from 0 to 4. The \'for\' loop iterates over these numbers.\\n    // For each number \'i\', the \'cidrSubnet\' function calculates a subnet within the \'2001:db8:1234::/48\' CIDR block.\\n    // The subnet size is 64, and \'i\' is used as the index. The resulting array contains the calculated subnets.\\n    output v6subnets array = [for i in range(0, 5): cidrSubnet(\'2001:db8:1234::/48\', 64, i)]\\n\\n    // This function generates an array of host addresses within the specified CIDR block. The index is used to calculate the host address.\\n    // Similar to the \'v6subnets\' array, the \'range(0, 5)\' function generates an array of numbers from 0 to 4.\\n    // For each number \'i\', the \'cidrHost\' function calculates a host address within the \'2001:db8:1234::/48\' CIDR block.\\n    // The resulting array contains the calculated host addresses.\\n    output v6hosts array = [for i in range(0, 5): cidrHost(\'2001:db8:1234::/48\', i)]\\n\\n    // This function generates a string within the specified CIDR block. The host index is always 3 (Azure Reserved).\\n    // The \'cidrHost\' function calculates a host address within the \'2001:db8:1234::/48\' CIDR block.\\n    // The host index is \'0 + 3\', which is 3. This is because the first three addresses in a subnet are reserved in Azure.\\n    output v6hostsazure string = cidrHost(\'2001:db8:1234::/48\', 0 + 3)\\n```\\n\\nReferences:\\n\\n* [IPv6 in Microsoft Azure](https://luke.geek.nz/azure/IPv6-on-Azure/)\\n* [CIDR functions for Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-cidr?WT.mc_id=AZ-MVP-5004796)\\n* [CIDR math functions for Azure Networking calculations #3975](https://github.com/Azure/bicep/issues/3975)"},{"id":"azure/IPv6-on-Azure","metadata":{"permalink":"/azure/IPv6-on-Azure","source":"@site/blog/2023-11-09-IPv6-on-Azure.md","title":"IPv6 in Microsoft Azure","description":"Take a look at IPv6 support in Microsoft Azure.","date":"2023-11-09T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":12.42,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"IPv6 in Microsoft Azure","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlogHeading_ipv6inAzure.gif"},"date":"2023-11-09 00:00:00 +1300","keywords":["azure","network","security","ipv6"],"description":"Take a look at IPv6 support in Microsoft Azure.","slug":"azure/IPv6-on-Azure"},"unlisted":false,"prevItem":{"title":"Mastering CIDRs With Azure Bicep","permalink":"/azure/Working-with-CIDR-Azure-Bicep"},"nextItem":{"title":"Azure Bicep - Principal does not exist in directory","permalink":"/azure/PrincpalNotFound-bicep-roleassignment"}},"content":"With more support for IPv6 being added to native Azure products, its time to take a closer look into IPv6 and its use within Azure.\\n\\n> The [deployment of IPv6](https://en.wikipedia.org/wiki/IPv6_deployment), the latest version of the Internet Protocol (IP), has been in progress since the mid-2000s. IPv6 was designed as the successor protocol for IPv4 with an expanded addressing space. IPv4, which has been in use since 1982, is in the final stages of exhausting its unallocated address space but still carries most Internet traffic.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview\\n\\n> Most recently, [Gateway Load Balancer](https://azure.microsoft.com/en-us/updates/general-availability-gateway-load-balancer-ipv6-support/?WT.mc_id=AZ-MVP-5004796) and [Application Gateway](https://azure.microsoft.com/en-us/updates/public-preview-application-gateway-now-supports-ipv6-frontend/?WT.mc_id=AZ-MVP-5004796) has had IPv6 support announced, allowing these services to handle both IPv6 along with IPv4, from client and server.\\n> IPv6 or Internet Protocol Version 6 is an upgrade of IPv4. A network layer protocol allows data communications to pass packets over a network. IPv6 uses 128-bit addressing and supports over 340 trillion trillion devices, a significant improvement over IPv4\u2019s 32-bit addressing scheme that supports over 4.3 billion devices.\\n\\n|                           | IPv4              | IPv6            |\\n| ------------------------- | ----------------- | --------------- |\\n| No. of bits on IP Address | 32                | 128             |\\n| Format                    | decimal           | hexadecimal     |\\n| Capable of Addresses      | 4.3 billion       | infinite number |\\n| How to ping               | \xa0ping XXX.XXX.XXX | ping6           |\\n\\n> Azure Virtual Network enables you to host applications in Azure with IPv6 and IPv4 connectivity within a virtual network and to and from the Internet. Due to the exhaustion of public IPv4 addresses, new networks for mobility and the Internet of Things (IoT) are often built on IPv6.\\n> IPv6 provides a larger address space, improved security, and better performance than IPv4. [Azure Virtual Network](https://learn.microsoft.com/azure/virtual-network/ip-services/ipv6-overview?WT.mc_id=AZ-MVP-5004796) enables you to host applications in Azure with IPv6 and IPv4 connectivity both within a virtual network and to and from the Internet, which is beneficial for expanding the reach of your Azure-hosted applications into the growing mobile and Internet of Things markets\\n\\n> IPv6 for Azure virtual network is much more full-featured - enabling full IPv6 solution architectures to be deployed in Azure.\\n\\nThere are some current [limitations](https://learn.microsoft.com/en-us/azure/virtual-network/ip-services/ipv6-overview?WT.mc_id=AZ-MVP-5004796#limitations) to be aware of, such as:\\n\\n* VPN gateways currently support IPv4 traffic only, but they can still be deployed in a dual-stacked virtual network using Azure PowerShell and Azure CLI commands only.\\n* IPv6-only Virtual Machines or Virtual Machines Scale Sets aren\'t supported; each NIC must include at least one IPv4 IP configuration.\\n* ICMPv6 isn\'t currently supported in Network Security Groups.\\n* Azure Firewall doesn\'t currently support IPv6. It can operate in a dual-stack virtual network using only IPv4, but the firewall subnet must be IPv4-only.\\n\\nAlthough still in a transitory period with some services, Microsoft has come some way to enabling IPv6 dual stack (IPv4 and IPv6) services, with [Entra ID even supporting](https://learn.microsoft.com/troubleshoot/azure/active-directory/azure-ad-ipv6-support?WT.mc_id=AZ-MVP-5004796) running on IPv6, and with US Government mandates like below:\\n\\n> On November 19, 2020, the United States Office of Management and Budget (OMB) issued the latest U. S. federal government IPv6-only policy in its memorandum (M-21-07) directing all federal government agencies to complete at least 80% of the transition from IPv4 to the single stack of IPv6 by 2025.\\n\\nThis will be an ever-increasing functionality.\\n\\nAs the world becomes increasingly interconnected, the need for robust and scalable networking solutions has never been greater. IPv6 is the latest version of the Internet Protocol, offering a range of benefits over its predecessor, IPv4.\\n\\nThere are several benefits of using IPv6 over IPv4, including:\\n\\n* Larger Address Space: IPv6 uses 128-bit addresses, which provides a much larger address space than IPv4\'s 32-bit addresses. This means that there are more available addresses, an estimated 340 undecillion addresses, which is essential as the number of devices connected to the internet continues to grow. It was intended to replace\xa0IPv4.  _(The primary reason for IPv4 address exhaustion is insufficient capacity to design the original Internet infrastructure. The original designers of the network did not anticipate the rapid growth of the internet and the increasing demand for IP addresses.\xa0This depletion is one of the reasons for the development and deployment of its successor protocol, IPv6)_\\n* Improved Security: IPv6 includes built-in security features such as IPsec, which provides end-to-end encryption and authentication. This helps to ensure that data transmitted over the network is secure and protected from unauthorized access. Since IPv4 was already defined, IPsec is bolted on _(IPSec packets are payload)_, but IPv6 was still being determined, and IPsec is an integral part of IPv6. From the beginning, IPv6 has had the AH and ESP extension headers, something not possible with IPv4.\\n* Better Quality of Service: IPv6 includes features that allow for better quality of service _(QoS)_, such as flow labelling and traffic classification. This helps ensure that critical applications receive the bandwidth and priority they need to function properly. This is especially important for real-time applications like video conferencing and online gaming.\\n* Future-Proofing: IPv6 supports future technologies and applications, such as the Internet of Things _(IoT)_, smart homes/farms and cloud computing. By adopting IPv6, organizations can ensure their networks are ready to support these emerging technologies and applications.\\n\\nThere is [no charge to use Public IPv6 addresses or prefixes](https://azure.microsoft.com/updates/azure-public-ipv6-offerings-are-free-as-of-july-31/?WT.mc_id=AZ-MVP-5004796), in Microsoft Azure, resources and bandwidth on Azure are charged at the same rates as for IPv4.\\n\\nIn addition, Azure provides tools and services for monitoring and managing IPv6 traffic, such as Azure Network Watcher and Azure Traffic Manager. These tools allow you to monitor network traffic with IP flow verification, for example, that checks if packets from IPv6 are allowed or denied and also tells you what security rule allowed or denied the traffic, diagnose issues, and optimize performance, ensuring that your IPv6-enabled applications and services are running smoothly.\\n\\nIf we look at the Internet Society pulse _(current as of this article)_, 27% of the top 1000 websites globally support IPv6, growing every day.\\n\\n![Internet Society Pulse curates information about levels of IPv6 adoption in countries and networks around the world](/images/posts/IPv6Azure_InternetSecurityPulse_Nov2023.png)\\n\\nThe transition to IPv6 will take time. While many services and platforms already support IPv6, many still need to, and as more and more devices are connected to the internet, the need for IPv6 will only grow. So, we need to be prepared and start preparation and implementation of IPv6 solutions as soon as possible.\\n\\nSo, when you hear IPv6, always consider the following:\\n\\n1. **Availability**: There is no risk of IPv6 IP exhaustion.\\n2. **Cost**: IPv6 IPs cost Azure zero dollars. This could be a trigger for customers to choose Azure.\\n3. **Compliance**: IPv6 mandates are helping governments start their journeys to the cloud.\\n4. **Network Performance**: NATing (Network Address Translation) adds network latency. IPv6 does not require NATing for cross-region traffic.\\n\\n![IPv6 Dual Stack Azure Network](/images/posts/ipv6-sample-diagram.png)\\n\\n## Getting Started with IPv6 in Azure\\n\\n> Azure Virtual WAN currently supports IPv4 traffic only.\\n\\n### Add IPv6 address space to an existing Virtual Network\\n\\nWe can add IPV6 ranges to an existing Azure Virtual Network by adding an IPV6 address range and then dual stacking both IPV4 and IPV6 ranges.\\n\\n> The subnets for IPv6 must be exactly /64 in size. This ensures future compatibility should you decide to enable subnet routing to an on-premises network since some routers can only accept /64 IPv6 routes.\\n> To add IPv6 to existing IPv4 deployments, you can\'t add IPv6 ranges to a virtual network with existing resource navigation links.\\n\\nIn my demo, I have a Virtual Network with an IPV4 address space of 10.0.0.0/16 (65,536 addresses)\\n\\nFor my demo purposes, I will add in an IPV6 address space of: 2001:db8:1234::/48 _(1208925819614629174706176 addresses)_\\n\\nWith two subnets of:\\n\\n* 2001:db8:1234::/64 _(18446744073709551616 addresses)_\\n* 2001:db8:1234:1::/64 _(18446744073709551616)_\\n\\nReference: [IPV6 Calculator](https://www.cidr.eu/en/calculator)\\n\\n1. Navigate to the [Azure Portal](https://portal.azure.com/#home)\\n1. Navigate to [Virtual Networks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Network%2FvirtualNetworks)\\n1. Navigate to the Virtual Network you want to add an IPV6 address range into.\\n1. Add in the IPV6 address space, click Save\\n1. Once completed, you can add your subnets.\\n\\n![Add IPV6 Address range to existing Azure Virtual Network](/images/posts/Add_IPV6AddressSpace_AzVirtualNetworkExisting.gif)\\n\\n### Add IPv6 address space to a new existing Virtual Network\\n\\nAdding an IPV6 range to a New Virtual Network is reasonably easy using the Portal interface.\\n\\n> The subnets for IPv6 must be exactly /64 in size. This ensures future compatibility should you decide to enable subnet routing to an on-premises network since some routers can only accept /64 IPv6 routes.\\n\\n1. Navigate to the [Azure Portal](https://portal.azure.com/#home)\\n1. Navigate to [Virtual Networks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Network%2FvirtualNetworks)\\n1. Click on New Virtual Network\\n1. Select a name for your Virtual Network\\n1. Select the Region and Resource Group, click Next\\n1. Skip the Security pane _(unless needed)_ and click Next\\n1. Select the dropdown for  Add Ipv4 address space, and select Add IPv6 address space\\n\\n![Add IPV6 Address range to new Azure Virtual Network](/images/posts/Add_IPV6AddressSpace_AzVirtualNetworkNew.gif)\\n\\nAzure provides automatic IPv6 address allocation for resources in a Virtual Network. When you enable IPv6 for a Virtual Network, Azure assigns IPv6 addresses to resources like virtual machines and load balancers. Linux and Windows Virtual Machines can all use IPv6 for Azure Virtual Network.\\n\\nIf your Virtual Network is peered, sync the connections so that your peered networks can see the IPv6 resources.\\n\\n### Create a Virtual Machine in a IPV6 subnet\\n\\n1. Navigate to the [Azure Portal](https://portal.azure.com/#home)\\n1. Navigate to [Virtual Machines](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Compute%2FVirtualMachines)\\n1. Click Create Azure Virtual machine\\n1. Go through the Create Virtual Machine wizard until you hit Networking and select the Virtual Network and Subnet with the IPV6 addresses. Your Network Interface needs to sit in the subnet that has IPV6 address ranges.\\n\\nYou can also also create a separate NIC (Network Interface Card), assign it to the IPv6 subnet, and have this as a secondary NIC.\\n\\n![Dual IPV4 and IP6 NIC](/images/posts/DualIPV46Nic.png)\\n\\n### Creating dual-stack network with Azure Bicep\\n\\n``` bicep\\n\\n// This function parses the CIDR notation and returns an object with the network address, subnet mask, and other details.\\noutput v6addressspace object = parseCidr(\'2001:db8:1234::/48\')\\n\\n// This function generates an array of subnets within the specified CIDR block. The subnet size is 64, and the index calculates the subnet.\\n// The \'range(0, 5)\' function generates an array of numbers from 0 to 4. The \'for\' loop iterates over these numbers.\\n// For each number \'i\', the \'cidrSubnet\' function calculates a subnet within the \'2001:db8:1234::/48\' CIDR block.\\n// The subnet size is 64, and \'i\' is used as the index. The resulting array contains the calculated subnets.\\noutput v6subnets array = [for i in range(0, 5): cidrSubnet(\'2001:db8:1234::/48\', 64, i)]\\n\\n// This function generates an array of host addresses within the specified CIDR block. The index is used to calculate the host address.\\n// Similar to the \'v6subnets\' array, the \'range(0, 5)\' function generates an array of numbers from 0 to 4.\\n// For each number \'i\', the \'cidrHost\' function calculates a host address within the \'2001:db8:1234::/48\' CIDR block.\\n// The resulting array contains the calculated host addresses.\\noutput v6hosts array = [for i in range(0, 5): cidrHost(\'2001:db8:1234::/48\', i)]\\n\\n// This function generates a string within the specified CIDR block. The host index is always 3 (Azure Reserved).\\n// The \'cidrHost\' function calculates a host address within the \'2001:db8:1234::/48\' CIDR block.\\n// The host index is \'0 + 3\', which is 3. This is because the first three addresses in a subnet are reserved in Azure.\\noutput v6hostsazure string = cidrHost(\'2001:db8:1234::/48\', 0 + 3)\\n\\n// This Bicep file creates an Azure Virtual Network with IPv4 and IPv6 subnets.\\n\\n// Set the default location to the location of the resource group\\nparam location string = resourceGroup().location\\n\\n// Set the number of subnets to be created\\nparam subnetnumber int = 6\\n\\n// Define the network address for the virtual network\\nvar vnetAddress = \'2001:db8:1234::\'\\n\\n// Define the network size for the virtual network\\nvar vnetSize = 48\\n\\n// Combine the network address and size into a CIDR block for the IPv6 virtual network\\nvar vnetCidrIPv6 = \'${vnetAddress}/${vnetSize}\'\\n\\n// Define the network address for the IPv4 virtual network\\nvar vnetAddressIPv4 = \'192.168.0.0\'\\n\\n// Define the network size for the IPv4 virtual network\\nvar vnetSizeIPv4 = 16\\n\\n// Combine the network address and size into a CIDR block for the IPv4 virtual network\\nvar vnetCidrIPv4 = \'${vnetAddressIPv4}/${vnetSizeIPv4}\'\\n\\n// Define the subnet sizes for IPv4 and IPv6\\nvar subnetSizeIPv4 = 24\\nvar subnetSizeIPv6 = 64\\n\\n// Generate the IPv4 subnets\\n// This loop will create a number of subnets equal to \'subnetnumber\'\\n// Each subnet will have a unique name and address prefix\\nvar subnetsIPv4 = [for i in range(0, subnetnumber): {\\n  name: \'subnetIPv4-${subnetSizeIPv4}-${i}\'\\n  properties: {\\n    addressPrefix: cidrSubnet(vnetCidrIPv4, subnetSizeIPv4, i)\\n  }\\n}]\\n\\n// Create the virtual network with IPv4 subnets\\n// This resource includes both IPv4 and IPv6 address spaces\\n// But only the IPv4 subnets are created at this point\\nresource vnet \'Microsoft.Network/virtualNetworks@2023-04-01\' = {\\n  name: \'myVnet\'\\n  location: location\\n  properties: {\\n    addressSpace: {\\n      // Add the IPv4 and IPv6 CIDR blocks to the address prefixes\\n      addressPrefixes: [ vnetCidrIPv4, vnetCidrIPv6 ]\\n    }\\n    subnets: subnetsIPv4\\n  }\\n}\\n\\n// Update the IPv4 subnets to add the IPv6 address prefixes\\n// Bicep does not currently support dual stack, so the subnets must be updated after the virtual network is created and the IPv6 address space is added. Bicep will attempt to create both the IPv4 and IPv6 subnets at the same time, leading to Retry and failed errors. Redeploy if required.\\nresource subnetsIPv6 \'Microsoft.Network/virtualNetworks/subnets@2023-04-01\' = [for i in range(0, subnetnumber): {\\n  name: \'subnetIPv4-${subnetSizeIPv4}-${i}\'\\n  parent: vnet\\n  properties: {\\n    addressPrefixes: [ subnetsIPv4[i].properties.addressPrefix, cidrSubnet(vnetCidrIPv6, subnetSizeIPv6, i) ]\\n  }\\n\\n}]\\n\\n\\n```\\n\\nReference: [Add \\"wait\\" and \\"retry\\" deployment options #1013](https://github.com/Azure/bicep/issues/1013)\\n\\n![IPv6/IPv4 Dual Stack Azure Network](/images/posts/DualIPV46VNET.png)\\n\\n![IPv6/IPv4 Dual Stack Azure Network](/images/posts/DualIPV46VNETSubnet.png)\\n\\n## Configure Virtual Machine with a private IPv6 address\\n\\nNow that we have IPv6 addresses in our Virtual Network, I have a Windows Server 2022 Azure Virtual Machine created in a dual-stacked subnet; we can add an IPv6 IP configuration by navigating to the NIC of the VM and adding ipconfiguration.\\n\\n![Azure Virtual Machine IPv6 Config](/images/posts/DualIPV46VM_IPconfiguration.png)\\n![ipconfig](/images/posts/DualIPV46VM_IPConfig.png)\\n\\n## Configure Virtual Machine with a public IPv6 address\\n\\nYou can add a public IPv6 address to a virtual machine similar to the private IPv6 address, but first, we must create our public IP.\\n\\n1. Navigate to the [Azure Portal](https://portal.azure.com/#home)\\n1. Navigate to [Public IP addresses](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Network%2FPublicIpAddresses)\\n1. Click + Create\\n1. Select your Subscription/Resource Group and Region\\n1. Enter a name for your Public IP and select IP Version as: Ipv6\\n1. Click Review + Create\\n1. Click Create\\n1. Once created, shutdown the Virtual Machine you want to attach the public IP to\\n1. Navigate to Networking on the Virtual Machine blade\\n1. Click Attach network interface, and select your Ipv6 Public IP\\n1. Start your Virtual Machine and navigate to [https://whatismyipaddress.com/](https://whatismyipaddress.com/); your ipv6 public IP address should now match your newly added address.\\n\\n![What is my IP](/images/posts/IPv6WhatisMyIP.png)\\n\\nYou can test connectivity to your newly published IPv6 address now.\\n\\nAny issues, make sure you checkout [https://test-ipv6.com/](https://test-ipv6.com/) and [http://www.ipv6scanner.com](http://www.ipv6scanner.com/cgi-bin/main.py), from your client."},{"id":"azure/PrincpalNotFound-bicep-roleassignment","metadata":{"permalink":"/azure/PrincpalNotFound-bicep-roleassignment","source":"@site/blog/2023-10-31-Azure-Bicep-Principal-does-not-exist-in-directory.md","title":"Azure Bicep - Principal does not exist in directory","description":"PrincpalNotFound when doing Role Assignment of a User Assigned Managed identity","date":"2023-10-31T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.155,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Bicep - Principal does not exist in directory","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlogHeading-PrincipalNotFound.gif"},"date":"2023-10-31 00:00:00 +1300","keywords":["azure","bicep","identity","PrincpalNotFound"],"description":"PrincpalNotFound when doing Role Assignment of a User Assigned Managed identity","slug":"azure/PrincpalNotFound-bicep-roleassignment"},"unlisted":false,"prevItem":{"title":"IPv6 in Microsoft Azure","permalink":"/azure/IPv6-on-Azure"},"nextItem":{"title":"Microsoft Azure Management with PowerShell - Introduction","permalink":"/azure/powershell/azure-management-powershell"}},"content":"Recently, I was deploying an [User Assigned Managed identity](https://learn.microsoft.com/entra/identity/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azp&WT.mc_id=AZ-MVP-5004796), and assigning the managed identity a role assignment with Azure Bicep, and ran into an issue, where the assignment would fail, but then after a rerun would work.\\n\\n> Principal e892476361114c90be141d9bf20cc94b does not exist in the directory 73160ae1-aa4a-48b5-a424-d5e43d808f53. Check that you have the correct principal ID. If you are creating this principal and then immediately assigning a role, this error might be related to a replication delay. In this case, set the role assignment principalType property to a value, such as ServicePrincipal, User, or Group. See [https://aka.ms/docs-principaltype](https://learn.microsoft.com/azure/role-based-access-control/troubleshooting?tabs=bicep&WT.mc_id=AZ-MVP-5004796)\\n\\n![PrincpalNotFound](/images/posts/PrincipalNotExistinDirectory.png)\\n\\nWhat was happening, was that the User Assigned Identity was created, and immediately after the role assignment was attempted, leaving no time for the role assignment API to be aware that the User Assigned Managed identity existed, even with a hard coded dependson!\\n\\nThe fix was to Set the: [principalType](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/scenarios-rbac?WT.mc_id=AZ-MVP-5004796#principal) into the Bicep, as ServicePrincipal, making sure that the Azure platform can wait for the replication to complete, before trying the role assignment.\\n\\n> The principalType property specifies whether the principal is a user, a group, or a service principal. Managed identities are a form of service principal.\\n\\n\\n``` bicep\\n\\nresource uami \'Microsoft.ManagedIdentity/userAssignedIdentities@2023-01-31\' = {\\n  name: azUserAssignedManagedIdentity\\n  location: location\\n}\\n\\nresource uamiassignment \'Microsoft.Authorization/roleAssignments@2022-04-01\' = {\\n  name: guid(resourceGroup().id, \'contributor\')\\n  properties: {\\n    principalId: uami.properties.principalId\\n    principalType: \'ServicePrincipal\' \\n    roleDefinitionId: resourceId(\'Microsoft.Authorization/roleDefinitions\', \'b24988ac-6180-42a0-ab88-20f7382dd24c\') // Contributor\\n  }\\n  scope: resourceGroup()\\n}\\n\\n\\n```"},{"id":"azure/powershell/azure-management-powershell","metadata":{"permalink":"/azure/powershell/azure-management-powershell","source":"@site/blog/2023-10-30-Microsoft-Azure-Management-with-PowerShell-Introduction.md","title":"Microsoft Azure Management with PowerShell - Introduction","description":"Delve into the common ways to work with Microsoft Azure, using PowerShell.","date":"2023-10-30T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":7.095,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Azure Management with PowerShell - Introduction","authors":["Luke"],"tags":["Azure","PowerShell"],"toc":true,"header":{"teaser":"/images/posts/BlogHeading-BlogHeading-Microsoft-Azure-Management-with-PowerShell.gif"},"date":"2023-10-30 00:00:00 +1300","keywords":["azure","powershell","landing zones"],"description":"Delve into the common ways to work with Microsoft Azure, using PowerShell.","slug":"azure/powershell/azure-management-powershell"},"unlisted":false,"prevItem":{"title":"Azure Bicep - Principal does not exist in directory","permalink":"/azure/PrincpalNotFound-bicep-roleassignment"},"nextItem":{"title":"Monitor your Azure Landing Zone with Baseline Alerts","permalink":"/azure/Monitor-Azure-LandingZones-with-AMBA"}},"content":"> To see a video of these commands in action, take a look at the following YouTube-hosted video: [Microsoft Azure Management with PowerShell - Introduction](https://youtu.be/a4gehHwlwBQ)\\n\\n## PowerShell\\n\\nPowerShell is a powerful scripting and automation framework developed by Microsoft. It is designed for task automation and configuration management and is particularly useful for managing and automating Microsoft Windows environments. PowerShell uses a command-line interface with a scriptable approach, and it\'s built on the .NET Framework.\\n\\n* [What is PowerShell?](https://learn.microsoft.com/powershell/scripting/overview?WT.mc_id=AZ-MVP-5004796)\\n\\n## PowerShell and Microsoft Azure\\n\\nWhen it comes to Microsoft Azure, PowerShell provides a robust set of cmdlets (pronounced \\"command-lets\\") that enable you to interact with and manage Azure resources, making it a valuable tool for working with Azure services.\\n\\nWhen you run a PowerShell cmdlet to, for example, create a virtual machine or retrieve information about an Azure resource, the cmdlet translates your request into an HTTP request to the relevant Azure REST API endpoint.\\n\\n* [Azure PowerShell Documentation](https://learn.microsoft.com/powershell/azure/?view=azps-10.4.1&WT.mc_id=AZ-MVP-5004796)\\n\\n> There is an assumption, that you have access to an Azure environment, and the ability to create resources, within that environment.\\n\\n![Microsoft Azure Management with PowerShell](/images/posts/BlogHeading-BlogHeading-Microsoft-Azure-Management-with-PowerShell.gif)\\n\\nFirst up, let\'s verify the version of PowerShell we have, open a Powershell terminal and run:\\n\\n``` powershell\\n$PSVersionTable\\n```\\n\\nA supported version of PowerShell version 7 or higher is the recommended version of PowerShell for use with the Az PowerShell module on all platforms including Windows, Linux, and macOS.\\n\\nThe Az PowerShell module is preinstalled in [Azure Cloud Shell](https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796) and in [Docker](https://learn.microsoft.com/powershell/azure/azureps-in-docker?view=azps-10.4.1&tabs=amd64&WT.mc_id=AZ-MVP-5004796) images.\\n\\n### Setting up your Azure environment for PowerShell\\n\\nFirst thigs first, lets installed the Azure ([Azure Az PowerShell module](https://learn.microsoft.com/powershell/azure/new-azureps-module-az?view=azps-10.4.1&WT.mc_id=AZ-MVP-5004796))modules.\\n\\n``` powershell\\n# Install Azure Modules\\n# The Set-PSRepository cmdlet is used to set values for a registered repository. \\n# The Install-Module cmdlet is used to download one or more modules from an online gallery and installs them on the local computer. In this case, the command is installing the Az module, which provides cmdlets for managing Azure resources.\\nSet-PSRepository -Name \'PSGallery\' -InstallationPolicy Trusted\\nInstall-Module Az \\n```\\n\\nIf you don\'t have local administrator rights, you can install it into your user profile like below:\\n\\n``` powershell\\n# Install Azure Module as Current User (as opposed to System)\\n# \'-Scope CurrentUser` specifies that the module should be installed only for the current user. If you don\'t specify a scope, the default is `AllUsers`, which requires administrator permissions.\\n\\nInstall-Module -Name Az -Repository PSGallery -Scope CurrentUser\\n```\\n\\nYou can install a specific version of the Az module by specifying RequiredVersion, like below:\\n\\n``` powershell\\n# Install specific version of Azure Modules\\n\\nInstall-Module -Name Az -RequiredVersion 7.1.0\\n```\\n\\nYou will find that the Azure powershell cmdlets will constantly get updated, to resolve bugs, offer new functionality, to update the modules to the ltest you can use:\\n\\n``` powershell\\n# Update Azure Module\\n# The command Get-InstalledModule -Name Az* | Update-Module is used to update all installed PowerShell modules that start with \\"Az\\".\\nSet-PSRepository -Name \'PSGallery\' -InstallationPolicy Trusted\\nGet-InstalledModule -Name Az* | Update-Module\\n```\\n\\nAz is a collection of different cmdlets, across multiple Azure resource types, to view a list of avaliable commands you use \'Get-Command\', like below:\\n\\n``` powershell\\n# Gets Az Commands\\n# Note: Will take a while for all the cmdlets to list.\\nGet-Command -Noun Az*\\n```\\n\\n### Getting started with Azure PowerShell module\\n\\nLets connect to Microsoft Azure\\n\\n``` powershell\\n# Connect to Azure - Interactive will prompt for credentials\\nConnect-AzAccount\\n```\\n\\nIf your wanting to connect to Azure, from a device that doesn\'t supoport the credential prompt, like Azure Cloudshell you can connect using another device, using a device code:\\n\\n``` powershell\\n# Connect to Azure\\nConnect-AzAccount -UseDeviceAuthentication\\n```\\n\\nIf you have a Service principal, you can use this to authenticate, commonly used for automation scripts, Azure DevOps agents and GitHub runners.\\n\\n``` powershell\\n# Connect to Azure using Service Principal authentication\\n$SecurePassword = ConvertTo-SecureString -String \\"Password123!\\" -AsPlainText -Force\\n$TenantId = \'yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyy\'\\n$ApplicationId = \'zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzz\'\\n$Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $ApplicationId, $SecurePassword\\nConnect-AzAccount -ServicePrincipal -TenantId $TenantId -Credential $Credential\\n```\\n\\nOnce, connected - its time to check what Azure subscriptions you can use.\\n\\n``` powershell\\n# Get Azure Subscriptions\\nGet-AzSubscription\\n```\\n\\nOnce you have selected the right Azure subscription to connect to, you can set your Subscription context (modify/delete resources). Add your subscription ID, from the earlier step in between \'\'.\\n\\n``` powershell\\n# Select Azure Subscription\\n$subid = \'\'\\nSet-AzContext -SubscriptionId $subid\\n```\\n\\nNow you have connected to your, Azure subscription, it is time to get your Azure resources and Resource Groups\\n\\n``` powershell\\n# Get Azure resource groups and resources\\nGet-AzResourceGroup | Format-Table\\nGet-AzResource | Format-Table\\n```\\n\\n``` powershell\\n# Get Azure resource\\nGet-AzResource\\n```\\n\\n``` powershell\\n# Get Azure resource by ResourceType\\nGet-AzResource | Where-Object {$_.ResourceType -eq \'Microsoft.Network/virtualNetworks\'} \\n```\\n\\n``` powershell\\n# Sort Azure resource by Name and Resource Group\\nGet-AzResource | Where-Object {$_.ResourceType -eq \'Microsoft.Storage/storageAccounts\'} | Sort-Object Name\\nGet-AzResource | Sort-Object ResourceGroupName \\n```\\n\\n``` powershell\\n# Working with Variables\\n\\n# Working with variables and data types in PowerShell\\n$resourceType = \'Microsoft.Network/virtualNetworks\'\\nGet-AzResource | Where-Object {$_.ResourceType -eq $resourceType}\\n```\\n\\n``` powershell\\n# Using PowerShell operators for comparisons and calculations\\n\\n$resources = Get-AzResource\\n$count = $resources.Count\\nWrite-Host \\"You have $count resources in your Azure subscription.\\"\\n```\\n\\n``` powershell\\n# Scripting constructs: loops and conditional statements\\n$resources = Get-AzResource\\n\\nforeach ($resource in $resources) {\\n    if ($resource.ResourceType -eq \'Microsoft.Network/virtualNetworks\') {\\n        Write-Host \\"Found a virtual network: $($resource.Name)\\"\\n        Write-Host \\"This virtual network is in $($resource.ResourceGroupName)\\" -ForegroundColor Green\\n    }\\n}\\n```\\n\\n``` powershell\\n# Scripting constructs: loops and conditional statements\\n$subscriptions = Get-AzSubscription\\n\\nforeach ($subscription in $subscriptions) {\\n$resource = Get-AzResource | Where-Object {$_.ResourceType -eq \'Microsoft.Network/virtualNetworks\'} \\n\\n    if ($resource.ResourceType -eq \'Microsoft.Network/virtualNetworks\') {\\n        Write-Host \\"Found a virtual network: $($resource.Name)\\" -BackgroundColor Black -ForegroundColor White\\n        Write-Host \\"This virtual network is in $($resource.ResourceGroupName)\\" -ForegroundColor Green\\n        Write-Host \\"This Virtual Network is in $($subscription.Name)\\" -ForegroundColor Green\\n    }\\n}\\n```\\n\\n``` powershell\\n# Error handling in PowerShell\\ntry {\\n    Get-AzResource -ResourceGroupName \\"NonexistentResourceGroup\\" -ErrorAction Stop\\n} catch {\\n    Write-Host \\"An error occurred: $_. Make sure you have selected the right Resource Group\\" -ForegroundColor Red\\n}\\n```\\n\\n### Resource Creation\\n\\n``` powershell\\n# Import Module\\nImport-Module Az -Verbose\\n```\\n\\n``` powershell\\n#Create Azure Resource Group\\nNew-AzResourceGroup -Name \\"MyResourceGroup\\" -Location \\"West US\\"\\n```\\n\\n``` powershell\\n# Get Regions\\nGet-AzLocation | Select-Object -First 1\\nGet-AzLocation | Select-Object DisplayName, Location, PhysicalLocation, GeographyGroup | Format-Table\\n```\\n\\n``` powershell\\nate Azure Resource Group\\n$region = \'AustraliaEast\'\\nNew-AzResourceGroup -Name \\"MyResourceGroup$region\\" -Location $region\\n```\\n\\n``` powershell\\n# Create a storage account\\n$uniqueId = [guid]::NewGuid().ToString().Substring(0,4)\\n$region = \'AustraliaEast\'\\n$ResourceGroupName = \\"MyResourceGroup$region\\"\\nNew-AzStorageAccount -ResourceGroupName $ResourceGroupName -Name \\"mystgacc$uniqueId\\" -Location $region -SkuName Standard_LRS -AllowBlobPublicAccess $false -verbose\\n```\\n\\n``` powershell\\n#Remove Azure Storage account\\n$region = \'AustraliaEast\'\\n$ResourceGroupName = $ResourceGroupName\\nRemove-AzStorageAccount -ResourceGroupName $ResourceGroupName -Name \\"mystgacc$uniqueId\\" -Force -verbose\\nGet-AzStorageAccount -ResourceGroupName $ResourceGroupName -Name \\"mystgacc$uniqueId\\" -verbose\\n```\\n\\n``` powershell\\n# Create an Azure Virtual Network\\n$region = \'AustraliaEast\'\\n$ResourceGroupName = \'network-prod-rg\'\\n$VNetname = \'vnet-prod\'\\n$subnetname = \'infraservers\'\\n$subnetAddressPrefix = \'10.0.0.0/24\'\\n\\n# Create a resource group\\n$ResourceGroup = Get-AzResourceGroup -Name $ResourceGroupName -ErrorAction SilentlyContinue\\n\\nif ($null -eq $ResourceGroup)\\n{\\n    Write-Host \\"Creating Resource Group $ResourceGroupName in $region\\" -ForegroundColor Yellow\\n    $ResourceGroup = New-AzResourceGroup -Name $ResourceGroupName -Location $region -Force\\n}\\nelse\\n{\\n    Write-Host \\"Resource Group $ResourceGroupName already exists in $region\\" -ForegroundColor Green\\n}\\n\\n# Create a virtual network\\n$AzVNET = New-AzVirtualNetwork -ResourceGroupName $ResourceGroupName -Name $VNetname -AddressPrefix \'10.0.0.0/16\' -Location $region\\n\\n# Create a subnet\\n$subnetConfig = Add-AzVirtualNetworkSubnetConfig -Name $subnetname -AddressPrefix $subnetAddressPrefix -VirtualNetwork $AzVNET\\n```\\n\\n``` powershell\\n# Get full object output\\n# Alias (This is a pipeline to the Format-List cmdlet (fl is an alias for Format-List). It formats the output as a list of properties for each object. This can make it easier to read the details of the virtual network.)\\n\\nGet-AzVirtualNetwork -ResourceGroupName $ResourceGroupName -Name $VNetname # | fl\\n```\\n\\n``` powershell\\n# Alias\\n\\nGet-Alias  | Select-Object -First 2\\n```\\n\\n``` powershell\\n#splat\\n\\n$configData = @{\\n    ResourceGroupName = \\"MyResourceGroup\\"\\n    Location = \\"West US\\"\\n    StorageAccountName = \\"stgacctest100\\"\\n}\\n\\ntry {\\n    New-AzStorageAccount -ResourceGroupName $configData.ResourceGroupName -Name $configData.StorageAccountName -Location $configData.Location -SkuName Standard_LRS\\n} catch {\\n    Write-Error \\"Failed to create storage account: $_\\"\\n}\\n```\\n\\n``` powershell\\n#splat as parameters\\n\\n$configData = @{\\n    \\"ResourceGroupName\\" = \\"MyResourceGroup\\"\\n    \\"Location\\" = \\"West US\\"\\n    \\"StorageAccountName\\" = \\"stgacctest100\\"\\n    \\"SkuName\\" = \\"Standard_LRS\\"\\n\\n}\\n\\ntry {\\n    New-AzStorageAccount @configData\\n} catch {\\n    Write-Error \\"Failed to create storage account: $_\\"\\n}\\n```\\n\\n``` powershell\\n# Tags\\n$ResourceGroupName = \'TagTestRG\'\\nNew-AzResourceGroup -Name $ResourceGroupName -Location \'AustraliaEast\'\\nSet-AzResourceGroup -Name $ResourceGroupName -Tag @{ Department = \\"Finance\\"; Project = \\"Q1\\" }\\n```\\n\\n``` powershell\\n# Get Tag Resource Group\\n$ResourceGroupName = \'TagTestRG\'\\n(Get-AzResourceGroup -Name $ResourceGroupName).Tags\\n```\\n\\n``` powershell\\n$ResourceGroupName = \'TagTestRG\'\\n$tags = (Get-AzResourceGroup -Name $ResourceGroupName).Tags\\n$tags.Remove(\\"Project\\")\\nSet-AzResourceGroup -Name $ResourceGroupName -Tag $tags\\n```"},{"id":"azure/Monitor-Azure-LandingZones-with-AMBA","metadata":{"permalink":"/azure/Monitor-Azure-LandingZones-with-AMBA","source":"@site/blog/2023-10-21-Monitor-Azure-LandingZones-with-AMBA.md","title":"Monitor your Azure Landing Zone with Baseline Alerts","description":"Monitor your Azure Landing Zone with Azure Monitor Baseline Alerts (AMBA).","date":"2023-10-21T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.92,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Monitor your Azure Landing Zone with Baseline Alerts","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlogHeading_AMBA.gif"},"date":"2023-10-21 00:00:00 +1300","keywords":["azure","monitor","landing zones"],"description":"Monitor your Azure Landing Zone with Azure Monitor Baseline Alerts (AMBA).","slug":"azure/Monitor-Azure-LandingZones-with-AMBA"},"unlisted":false,"prevItem":{"title":"Microsoft Azure Management with PowerShell - Introduction","permalink":"/azure/powershell/azure-management-powershell"},"nextItem":{"title":"Private Endpoint traffic not appearing in Azure Firewall","permalink":"/azure/private-endpoint-traffic-not-appearing-azure-firewall"}},"content":"> When deploying Azure resources, it is crucial to configure alerts to ensure your resources\' health, performance, and security.\\n\\nBy setting up alerts, you can proactively monitor your resources and take timely actions to address any issues that may arise.\\n\x3c!-- truncate --\x3e\\nHere are the key reasons why configuring alerts is essential:\\n\\n* Early detection of issues: Alerts enable you to identify potential problems or anomalies in your Azure resources at an early stage. By monitoring key metrics and logs, you can detect high CPU usage, low memory, network connectivity problems, or security breaches. This allows you to take immediate action and prevent any negative impact on your applications or services.\\n* Reduced downtime: By configuring alerts, you can minimise downtime by being notified of critical events or failures in real time. This allows you to quickly investigate and resolve issues before they escalate, ensuring the availability and reliability of your applications.\\n* Optimized resource utilisation: Alerts help you optimise resource utilization by providing insights into resource usage patterns and trends. By monitoring metrics such as CPU utilisation, memory consumption, or storage capacity, you can identify opportunities for optimisation and cost savings.\\n* Compliance and security: Configuring alerts is essential for maintaining compliance with regulatory requirements and ensuring the security of your Azure resources. By monitoring security logs and detecting suspicious activities or unauthorised access attempts, you can immediately mitigate potential risks and protect your data.\\n* Proactive capacity planning: Alerts provide valuable information for capacity planning and scaling your resources. By monitoring resource utilisation trends over time, you can identify patterns and forecast future resource requirements. This helps you avoid performance bottlenecks and ensure a smooth user experience.\\"\\n\\n\\n## Overview\\n\\n[Azure Monitor Baseline Alerts (AMBA)](https://azure.github.io/azure-monitor-baseline-alerts/welcome/) exists to help you get started with proactive and reactive monitoring straight out of the bat, focused on standard scenarios for your platform and application Landing Zone that are not industry or application-specific.\\n\\nPreviously under the azure/alz-monitor repository, it is now generally available under the [Azure/azure-monitor-baseline-alerts](https://github.com/Azure/azure-monitor-baseline-alerts) GitHub repository.\\n\\nThe objectives of AMBA are to:\\n\\n* Help simplify onboarding to Azure Monitor through a scalable and consistent approach.\\n* Reduce time to identify failure within Azure tenants and platforms (i.e., outages).\\n\\nMore documentation can be found on the official Microsoft Learn documentation page: [Monitor Azure platform landing zone components](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/design-area/management-monitor?WT.mc_id=AZ-MVP-5004796).\\n\\n![Monitor your Azure Landing Zone with Azure Monitor Baseline Alerts](/images/posts/BlogHeading_AMBA.gif)\\n\\n> \\"AMBA Provides best practice guidance around creating and configuring Azure Monitor Alerts for Azure services and workload pattern/scenarios).\\"\\n\\nOne of the ways that makes Azure Monitor Baseline Alerts (AMBA) so useful is the details around the Alerts.\\n\\nReference: [Alerts Details](https://azure.github.io/azure-monitor-baseline-alerts/patterns/alz/Alerts-Details/)\\n\\nOut-of-the-box alerts that are part of the base AMBA solution include (but are not limited to) alerts such as:\\n\\n| **Resource Type**       | **Name**                           | **Description**                                                                                                                                                                                                                                                                                                                                                                 | **metricName**                     |\\n| ----------------------- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- |\\n| automationAccounts      | TotalJob                           | The total number of jobs                                                                                                                                                                                                                                                                                                                                                        | TotalJob                           |\\n| storageAccounts         | Availability                       | The percentage of availability for the storage service or the specified API operation. Availability is calculated by taking the TotalBillableRequests value and dividing it by the number of applicable requests, including those that produced unexpected errors. All unexpected errors result in reduced availability for the storage service or the specified API operation. | Availability                       |\\n| virtualMachines         | Available Memory Bytes (MBytes)    | Amount of physical memory, in bytes, immediately available for allocation to a process or for system use in the Virtual Machine                                                                                                                                                                                                                                                 | Available Memory Bytes             |\\n| virtualMachineScaleSets | Percentage CPU                     | The percentage of allocated compute units that are currently in use by the Virtual Machine(s)                                                                                                                                                                                                                                                                                   | Percentage CPU                     |\\n| virtualMachineScaleSets | OS Disk IOPS Consumed Percentage   | Percentage of operating system disk I/Os consumed per minute                                                                                                                                                                                                                                                                                                                    | OS Disk IOPS Consumed Percentage   |\\n| virtualMachineScaleSets | Data Disk IOPS Consumed Percentage | Percentage of data disk I/Os consumed per minute                                                                                                                                                                                                                                                                                                                                | Data Disk IOPS Consumed Percentage |\\n| virtualMachineScaleSets | Outbound Flows                     | Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM)                                                                                                                                                                                                                                                                              | Outbound Flows                     |\\n| virtualMachineScaleSets | Inbound Flows                      | Inbound Flows are number of current flows in the inbound direction (traffic going into the VM)                                                                                                                                                                                                                                                                                  | Inbound Flows                      |\\n| virtualMachineScaleSets | Available Memory Bytes             | Amount of physical memory, in bytes, immediately available for allocation to a process or for system use in the Virtual Machine                                                                                                                                                                                                                                                 | Available Memory Bytes             |\\n| virtualMachineScaleSets | Network In Total                   | The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic)                                                                                                                                                                                                                                                                             | Network In Total                   |\\n| virtualMachineScaleSets | Network Out Total                  | The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)                                                                                                                                                                                                                                                                                  | Network Out Total                  |\\n| virtualMachineScaleSets | VmAvailabilityMetric               | Measure of Availability of Virtual machines over time.                                                                                                                                                                                                                                                                                                                          | VmAvailabilityMetric               |\\n| virtualMachineScaleSets | Disk Read Operations/Sec           | Disk Read IOPS                                                                                                                                                                                                                                                                                                                                                                  | Disk Read Operations/Sec           |\\n| virtualMachineScaleSets | Disk Write Operations/Sec          | Disk Write IOPS                                                                                                                                                                                                                                                                                                                                                                 | Disk Write Operations/Sec          |\\n| virtualNetworks         | If Under DDoS Attack               | Metric Alert for VNet DDOS Attack                                                                                                                                                                                                                                                                                                                                               | ifunderddosattack                  |\\n| publicIPAddresses       | Bytes In DDoS                      | Metric Alert for Public IP Address Bytes IN DDOS                                                                                                                                                                                                                                                                                                                                | bytesinddos                        |\\n| publicIPAddresses       | If Under DDoS Attack               | Metric Alert for Public IP Address Under Attack                                                                                                                                                                                                                                                                                                                                 | ifunderddosattack                  |\\n| publicIPAddresses       | Packets In DDoS                    | Inbound packets DDoS                                                                                                                                                                                                                                                                                                                                                            | PacketsInDDoS                      |\\n| publicIPAddresses       | VIP Availability                   | Average IP Address availability per time duration                                                                                                                                                                                                                                                                                                                               | VipAvailability                    |\\n| expressRouteCircuits    | ARP Availability                   | ARP Availability from MSEE towards all peers.                                                                                                                                                                                                                                                                                                                                   | ArpAvailability                    |\\n| expressRouteCircuits    | BGP Availability                   | BGP Availability from MSEE towards all peers.                                                                                                                                                                                                                                                                                                                                   | BgpAvailability                    |\\n\\nFor:\\n\\n* Metric Alerts\\n* Log Alerts\\n* Activity Logs\\n* Service health alerts\\n\\n![Graph 1](/images/posts/AMBA_Graph1.png)\\n\\n![Graph 2](/images/posts/AMBA_Graph1.png)\\n\\n![Monitoring Management Resource Group](/images/posts/AMBA_MangementRG.png)\\n\\n## Deployment\\n\\nThe Azure Monitor Baseline Alerts are deployed using Azure Policy *(DINE \u2013 Deploy If Not Exist)* and are included in the [Azure Enterprise-Scale landing zone accelerator](https://github.com/Azure/Enterprise-Scale).\\n\\nDeployment is currently done via an Azure Resource Manager (ARM) template and can be deployed with the following:\\n\\n* PowerShell\\n* Azure CLI\\n* GitHub Actions\\nDevOps\\n\\nBecause the Azure Monitor Baseline alerts are intended to follow the Azure Landing Zone framework, in accordance with the Ready phase of the Cloud Adoption Framework, the deployment can be scaled across multiple Management groups, such as:\\n\\n* Connectivity\\n* Identity\\n* Management\\n* Service Health\\n* Landing Zone (i.e., Application)\\n\\nOr a single management group/subscription consisting of all features, with Alert processing rules sending relevant subscription alerts to an Action group that will notify by email if an alert is raised.\\n\\nFor my own Azure environment, I only have 2 Subscriptions that sit under the following Management Group hierarchy:\\n\\n![Management Group structure](/images/posts/AMBA_LukeGeekNZMG.png)\\n\\nWe will deploy these initiatives to the MG Management Group, which allows us to then Assign those initiatives to anything beneath it, in our case, the mg-landingzones and trey-platform management groups.\\n\\nAs part of the deployment, we will adjust the following parameters to match our environment:\\n\\n* Management groups\\n* Assignment of PolicySetDefinitions\\n* Resource Group Name and Tags\\n* Location\\n* Action group emails\\n\\nWe can adjust the alert rule parameters, such as the severity, and change their effect, whether they automitigate or adjust the MonitorDisable tag, which can be used to exclude from alert monitoring; we will leave these to the default.\\n\\nMake sure you review and test these policies first! Before proceeding to Production.\\n\\nWe will deploy the ARM template using the [Azure Cloud Shell](https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n1. Login to the Azure Portal\\n2. Click on Cloud Shell *(top right)*\\n3. Select your subscription and create a Storage account to host your Cloud Shell drives *(if it hasn\'t already been done)*\\n4. I\'m going to adjust the parameters to match my environment by changing the following (these will need to be tuned to your specific environment):.\\n5. Type in:\\n\\n    git clone [https://github.com/Azure/azure-monitor-baseline-alerts](https://github.com/Azure/azure-monitor-baseline-alerts)\\n\\nType in:\\n\\n    cd azure-monitor-baseline-alerts\\\\patterns\\\\alz\\\\\\n\\nType in:\\n\\n    nano alzArm.param.json\\n\\nI\'m going to adjust the parameters to match my environment by changing the following *(these will need to be tuned to your specific environment)*:\\n\\n| Parameters                      | Values                                          |\\n| ------------------------------- | ----------------------------------------------- |\\n| enterpriseScaleCompanyPrefix    | lukegeeknz                                      |\\n| platformManagementGroup         | trey-platform                                   |\\n| IdentityManagementGroup         | mg-landingzones                                 |\\n| managementManagementGroup       | trey-platform                                   |\\n| connectivityManagementGroup     | trey-platform                                   |\\n| LandingZoneManagementGroup      | mg-landingzones                                 |\\n| ALZMonitorResourceGroupName     | rg-management-monitoring-001                    |\\n| ALZMonitorResourceGroupLocation | australiaeast                                   |\\n| ALZMonitorActionGroupEmail      | <email@luke.geek.nz> |\\n\\n> If you want the alerts to go to more than one email address, add them as comma-separated (i.e. \\"<email1@contos.com>, <email2@contoso.com>, <email3@contoso.com>\\")\\n\\n8. Press Ctrl+X\\n9. Press Y to save buffer\\n10. Press Enter to overwrite the parameter json file.\\n11. Before proceeding to the next step, you can cat the file *(cat ./alzArm.param.json)* to view it and make sure the parameters are correct and nothing else needs changing.\\n12. Now that the parameters are sorted, it is time to deploy:\\n\\n    $location = \'AustraliaEast\'\\n    $psedudoRootManagementGroup = \\"mg\\"\\n    New-AzManagementGroupDeployment -ManagementGroupId $psedudoRootManagementGroup -Location $location -TemplateUri  \'[https://raw.githubusercontent.com/Azure/azure-monitor-baseline/alerts/main/patterns/alz/alzArm.json](https://raw.githubusercontent.com/Azure/azure-monitor-baseline/alerts/main/patterns/alz/alzArm.json)\' -TemplateParameterFile  \'alzArm.param.json\'\\n\\n> The ARM template location needs to be internet accessible due to links in the ARM template to dependent resources, although the parameter file can be sourced locally.\\n> Error: Code=InvalidTemplate; Message=Deployment template validation failed: \'The template variable \'deploymentUris\' is not valid: The language expression property \'templateLink\' doesn\'t exist, available properties are \'template, templateHash, parameters, mode, provisioningState\'.. Please see [https://aka.ms/arm-functions](https://learn.microsoft.com/azure/azure-resource-manager/templates/template-functions?WT.mc_id=AZ-MVP-5004796) for usage details.\'\\n\\nAll going well after a few minutes \u2013 your initiatives and policies have been deployed.\\n\\n![Deploy - AMBA](/images/posts/AzureCloudShell_DeployAMBA.gif)\\n\\n![AMBA - Policy Definition](/images/posts/AMBA_PolicyDefinitions.png)\\n\\n![AMBA - Policy Assignments](/images/posts/AMBA_PolicyAssignments.png)\\n\\nIf you have already existing Azure resources, you can remediate the policies, forcing them to create the Resource Group, Alerts and action groups by running the following PowerShell commands:\\n\\n    cd scripts\\n\\n    $pseudoRootManagementGroup = \\"mg\\"\\n    $identityManagementGroup = \\"mg-landingzones\\"\\n    $managementManagementGroup = \\"trey-platform\\"\\n    $connectivityManagementGroup = \\"trey-platform\\"\\n    $LZManagementGroup= \\"mg-landingzones\\"\\n\\n    #Run the following commands to initiate remediation\\n    .\\\\\\\\Start-AMBARemediation.ps1 -managementGroupName $managementManagementGroup -policyName Alerting-Management\\n    .\\\\Start-AMBARemediation.ps1 -managementGroupName $connectivityManagementGroup -policyName Alerting-Connectivity\\n    .\\\\Start-AMBARemediation.ps1 -managementGroupName $identityManagementGroup -policyName Alerting-Identity\\n    .\\\\Start-AMBARemediation.ps1 -managementGroupName $LZManagementGroup -policyName Alerting-LandingZone\\n    .\\\\Start-AMBARemediation.ps1 -managementGroupName $pseudoRootManagementGroup -policyName Alerting-ServiceHealth\\n\\nYou can check the Alert rules in Azure Monitor:\\n\\n![AMBA - Alert Rules](/images/posts/AMBA_AlertRules.png)\\n\\n> It won\'t deploy all alert rules unless you have the resources; an example is the Deploy VNetG Tunnel Bandwidth alert; the policy rule will only deploy if it matches:\\n> The existence of a Virtual Network Gateway, with a VPN Gateway type, doesn\'t include the MonitorDisable tag; if I were to deploy a VPN Gateway, it would create the alert rule for me.\\n\\nFinally, if you want to clean up *(delete the resources)*, you can leverage the Start-AMBACleanup.ps1 script; it will leave the Resource Group, Alert processing rules and action group."},{"id":"azure/private-endpoint-traffic-not-appearing-azure-firewall","metadata":{"permalink":"/azure/private-endpoint-traffic-not-appearing-azure-firewall","source":"@site/blog/2023-10-03-Private-Endpoint-Traffic-Not-Appearing-Firewall.md","title":"Private Endpoint traffic not appearing in Azure Firewall","description":"Private Endpoint traffic can take a different route than your standard traffic and cause some confusion and dropped packets.","date":"2023-10-03T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.03,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Private Endpoint traffic not appearing in Azure Firewall","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/BlobHeading_PrivateEndpointtrafficnotappearing.gif"},"date":"2023-10-03 00:00:00 +1300","keywords":["azure","firewall","private endpoint"],"slug":"azure/private-endpoint-traffic-not-appearing-azure-firewall","description":"Private Endpoint traffic can take a different route than your standard traffic and cause some confusion and dropped packets."},"unlisted":false,"prevItem":{"title":"Monitor your Azure Landing Zone with Baseline Alerts","permalink":"/azure/Monitor-Azure-LandingZones-with-AMBA"},"nextItem":{"title":"Empowering Resilience with Azure backup services","permalink":"/azure/Empowering-Resilience-with-Azure-backup-services"}},"content":"You may have a situation where you have implemented [Private endpoints](https://learn.microsoft.com/azure/private-link/private-endpoint-overview?WT.mc_id=AZ-MVP-5004796) and the traffic from on-premises to those Private Endpoints, either doesn\'t work, even though on-premises firewalls say otherwise, or is working, but doesn\'t appear in the Azure Firewall.\\n\\nI had this recently with [Azure Arc](https://learn.microsoft.com/azure/azure-arc/overview?WT.mc_id=AZ-MVP-5004796), where the endpoints failed to connect once a site-to-site VPN connection _(which was working)_ was replaced with an expressroute connection, but going through the Azure Firewall logs, was unable to see any 443 traffic for Arc, hitting the Firewall even when the connection was working.\\n\\n![Private Endpoint traffic not appearing in Azure Firewall](/images/posts/BlobHeading_PrivateEndpointtrafficnotappearing.gif)\\n\\n> Traffic flow: Onpremises -- (ER Circuit) -- ER gateway -- Secured hub Azure Firewall -- (Vnet Connection) -- PE (Private Endpoint)\\n\\nThe issue was related to how private endpoint traffic is routed differently.\\n\\nIf the traffic has reached the Expressroute gateway from on-premises, with routing intent, normal traffic will be forced to the AzFW first before reaching its destination, as you would think and expect.\\n\\n> However, for the private endpoint scenario, once a Private Endpoint is deployed to any VNET, there will be an automatic system route with the PE IP and prefix /32 installed on all of the linked NICs.\\n> The next hop for this route will be InterfaceEndpoint.\\n> This route will allow the traffic to go directly to the PE, bypassing the routing intent and other user-defined routes that are larger than /32. The /32 route propagates to these areas: Any VPN or ExpressRoute connection to an on-premises system.\\n\\nSee: [Considerations for Hub and Spoke topology](https://learn.microsoft.com/en-us/azure/architecture/guide/networking/private-link-hub-spoke-network?WT.mc_id=AZ-MVP-5004796#considerations).\\n\\nIn an Azure Virtual Wide Area Network (VWAN), you could see this route in the virtual hub [effective routes](https://learn.microsoft.com/azure/virtual-wan/effective-routes-virtual-hub?WT.mc_id=AZ-MVP-5004796) and which gets propagated to the expressroute gateway.\\n\\nMy traffic from on-premises to the Azure Arc went directly to the private endpoints (bypassing the Azure Firewall). Still, the route back via the Azure Firewall was completely different, leading to asymmetric routing _(a packet traverses from a source to a destination in one path and takes a different path when it returns to the source)_.\\n\\n**To resolve this**, we need to enable network security policies for User-Defined Routes on the subnet of the private endpoint(s):\\n\\n![Azure Portal - Private Endpoint - Routes](/images/posts/AzurePortal_RouteTables_PrivateEndpoint.png)\\n\\nOnce enabled, you should see the traffic connect and flow through your Azure Firewall almost immediately.\\n\\nReference:\\n\\n* [Manage network policies for private endpoints](https://learn.microsoft.com/azure/private-link/disable-private-endpoint-network-policy?tabs=network-policy-portal&WT.mc_id=AZ-MVP-5004796)\\n* [Secure traffic destined to private endpoints in Azure Virtual WAN](https://learn.microsoft.com/azure/firewall-manager/private-link-inspection-secure-virtual-hub?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/Empowering-Resilience-with-Azure-backup-services","metadata":{"permalink":"/azure/Empowering-Resilience-with-Azure-backup-services","source":"@site/blog/2023-09-06-Empowering-Resilience-with-Azure-backup-services.md","title":"Empowering Resilience with Azure backup services","description":"This article is part of Azure Back to School - 2023 event! Make sure to check out the fantastic content created by the community!","date":"2023-09-23T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":22.395,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Empowering Resilience with Azure backup services","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/Header-Blog-AzureBackup_Services_Innovations.gif"},"date":"2023-09-23 00:00:00 +1300","slug":"azure/Empowering-Resilience-with-Azure-backup-services"},"unlisted":false,"prevItem":{"title":"Private Endpoint traffic not appearing in Azure Firewall","permalink":"/azure/private-endpoint-traffic-not-appearing-azure-firewall"},"nextItem":{"title":"Get Ahead with Self-Hosted Agents and Container Apps Jobs","permalink":"/azure/hosted-agents-container-apps-job"}},"content":"This article is part of [Azure Back to School](https://azurebacktoschool.github.io/) - 2023 event! Make sure to check out the fantastic content created by the community!\\n\\n![Empowering Resilience with Azure backup services](/images/posts/Header-Blog-AzureBackup_Services_Innovations.gif)\\n\\nAlong with the basics of the Azure Backup solutions, particularly on Virtual Machines running on Microsoft Azure, there have been a lot of changes in the last year, including Immutable vaults, enhanced policies, intelligence tiering, and cross-region restore.\\n\\n## Introduction\\n\\nLet us start with the basics with a user story; what do we need to achieve:\\n\\n> \\"As a Cloud Infrastructure Administrator at Contoso, I want to implement an automated backup solution for virtual machines (Windows and Linux) hosted in Microsoft Azure,\\nSo that I can ensure data reliability, disaster recovery, and compliance with minimal manual intervention.\\"\\n\\nWith some assumptions around further requirements, we can jump into solutions using native Microsoft Azure services to fulfil the Cloud Administrator\'s need.\\nIt is worth mentioning, especially around disaster recovery, that there is much more you can (and should do) do around [mission-critical](https://learn.microsoft.com/azure/well-architected/mission-critical/mission-critical-overview?WT.mc_id=AZ-MVP-5004796) Azure architecture. This article will focus primarily on the data loss portion of disaster recovery with Azure Backup services.\\n\\n| **Requirement**                                                     | **Azure Service(s) Used**                       |\\n| ------------------------------------------------------------------- | ----------------------------------------------- |\\n| Specific (S): Backup virtual machines in Azure                      | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796), [Azure Site Recovery (ASR)](https://learn.microsoft.com/azure/site-recovery/site-recovery-overview?WT.mc_id=AZ-MVP-5004796)\\n| Measurable (M):                                                     |                                                 |\\n| \\\\- Achieve 99% backup success rate                                  | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796), [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796)                     |\\n| \\\\- Define and meet RTO _(recovery time objective)_ for critical VMs | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796), [Azure Site Recovery (ASR)](https://learn.microsoft.com/azure/site-recovery/site-recovery-overview?WT.mc_id=AZ-MVP-5004796)         |\\n| \\\\- Monitor and optimise storage consumption                         | [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796), [Microsoft Cost Management](https://learn.microsoft.com/azure/cost-management-billing/costs/reporting-get-started?WT.mc_id=AZ-MVP-5004796)            |\\n| Achievable (A):                                                     |                                                 |\\n| \\\\- Select and configure Azure-native backup solution                | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796)                                    |\\n| \\\\- Configure Azure permissions and access controls                  | [Azure Role-Based Access Control (RBAC)](https://learn.microsoft.com/azure/role-based-access-control/rbac-and-directory-admin-roles?WT.mc_id=AZ-MVP-5004796)          |\\n| \\\\- Define backup schedules and retention policies                   | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796), [Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796)                      |\\n| Relevant (R):                                                       |                                                 |\\n| \\\\- Align with Azure best practices                                  | [Azure Well-Architected Framework](https://learn.microsoft.com/en-us/azure/well-architected/?WT.mc_id=AZ-MVP-5004796), [Azure Advisor](https://learn.microsoft.com/azure/advisor/advisor-overview?WT.mc_id=AZ-MVP-5004796) |\\n| \\\\- Comply with data protection regulations                          | [Azure Compliance Center](https://learn.microsoft.com/azure/compliance/?WT.mc_id=AZ-MVP-5004796), [Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796)           |\\n| \\\\- Support disaster recovery and business continuity                | [Azure Site Recovery (ASR)](https://learn.microsoft.com/azure/site-recovery/site-recovery-overview?WT.mc_id=AZ-MVP-5004796)                       |\\n| Time-bound (T):                                                     |                                                 |\\n| \\\\- Implement within the next two Azure sprint cycles                | [Azure DevOps - Boards](https://learn.microsoft.com/azure/devops/boards/get-started/what-is-azure-boards?view=azure-devops&WT.mc_id=AZ-MVP-5004796)                             |\\n| \\\\- Regular progress reviews during sprint planning                  | [Azure DevOps - Boards](https://learn.microsoft.com/azure/devops/boards/get-started/what-is-azure-boards?view=azure-devops&WT.mc_id=AZ-MVP-5004796)                             |\\n\\n| **Definition of Done (DoD):**                              |                                                          |\\n| ---------------------------------------------------------- | -------------------------------------------------------- |\\n| 1\\\\. Select a cost-effective Azure-native backup solution   | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796)                                           |\\n| 2\\\\. Configure Azure permissions and access controls        | [Azure Role-Based Access Control (RBAC)](https://learn.microsoft.com/azure/role-based-access-control/rbac-and-directory-admin-roles?WT.mc_id=AZ-MVP-5004796)                   |\\n| 3\\\\. Define backup policies and RTOs                        | [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796), [Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796)                               |\\n| 4\\\\. Monitor and meet 99% backup success rate               | [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796)                                            |\\n| 5\\\\. Optimize backup storage utilisation                    | [Microsoft Cost Management](https://learn.microsoft.com/azure/cost-management-billing/costs/reporting-get-started?WT.mc_id=AZ-MVP-5004796)                                    |\\n| 6\\\\. Create backup and recovery documentation               | [Microsoft Learn documentation](https://learn.microsoft.com/azure/backup/backup-azure-restore-files-from-vm?WT.mc_id=AZ-MVP-5004796)                             |\\n| 7\\\\. Train the team to manage and monitor the backup system | [Azure Training](https://learn.microsoft.com/training/modules/intro-to-azure-backup/?WT.mc_id=AZ-MVP-5004796)                        |\\n| 8\\\\. Integrate with Azure monitoring and alerting           | [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796)                                            |\\n| 9\\\\. Conduct disaster recovery tests                        | [Azure Site Recovery (ASR)](https://learn.microsoft.com/azure/site-recovery/site-recovery-overview?WT.mc_id=AZ-MVP-5004796)                                |\\n\\n*Note: [Azure DevOps - Boards](https://learn.microsoft.com/azure/devops/boards/get-started/what-is-azure-boards?view=azure-devops&WT.mc_id=AZ-MVP-5004796) are outside of the scope of this article; the main reflection here is to make sure that your decisions and designs are documented in line with business requirements.*\\n*There are also some further assumptions we will make, particularly around security and RTO requirements for the organisation of Contoso.*\\n\\nWe know to fulfil the requirements, we need to implement the following:\\n\\n* [Azure Backup](https://learn.microsoft.com/azure/backup/?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Site Recovery (ASR)](https://learn.microsoft.com/azure/site-recovery/site-recovery-overview?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796)\\n* [Microsoft Cost Management](https://learn.microsoft.com/azure/cost-management-billing/costs/reporting-get-started?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Role-Based Access Control (RBAC)](https://learn.microsoft.com/azure/role-based-access-control/rbac-and-directory-admin-roles?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796)\\n\\nSo, let us take our notebooks and look at the Backup sections.\\n\\n## Backup Center\\n\\nWhen needing a single control plane for your Backups across multiple tenancies _(using [Azure Lighthouse](https://learn.microsoft.com/azure/lighthouse/overview?WT.mc_id=AZ-MVP-5004796)_), Subscriptions and regions, then Backup Center is the place to start with.\\n\\n> \\"Backup center provides a single unified management experience in Azure for enterprises to govern, monitor, operate, and analyze backups at scale. It also provides at-scale monitoring and management capabilities for Azure Site Recovery. So, it\'s consistent with Azure\'s native management experiences. Backup center is designed to function well across a large and distributed Azure environment. You can use Backup center to efficiently manage backups spanning multiple workload types, vaults, subscriptions, regions, and Azure Lighthouse tenants.\\"\\n\\n![Backup center](/images/posts/AzureBackupCenter_Portal_Overview.png)\\n\\nAs you can see, Backup center can be used to see manage:\\n\\n* Backup instances\\n* Backup policies\\n* Vaults\\n* Monitor and report on backup jobs\\n* Compliance *(ie Azure Virtual Machines that are not configured for backup)*\\n\\nYou can find the [Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted) directly in the Azure Portal.\\n\\n![Backup center](/images/posts/AzurePortal-SearchBackupCenter.png)\\n\\nWe can create and manage these resources by ourselves, but throughout this article, we will refer back to the Backup Center, to take advantage of the single pane of glass and integrate these resources.\\n\\n### Create Vault\\n\\nIn Microsoft Azure, there are two types of Vaults that the Backup center works with. These vaults are:\\n\\n* [Recovery Services vault](https://learn.microsoft.com/azure/backup/backup-azure-recovery-services-vault-overview?WT.mc_id=AZ-MVP-5004796)\\n* [Backup vault](https://learn.microsoft.com/azure/backup/backup-vault-overview?WT.mc_id=AZ-MVP-5004796)\\n\\n![Backup center](/images/posts/AzurePortal_CreateVault_VaultTypes.png)\\n\\nDepending on your requirements depends on which Vault you will need to create _(for our purposes, we will need the Recovery Services vault)_; Backup Center makes it remarkably easy to configure a new vault and select the right vault type by using the wizard.\\n\\n*Please refer to: [Support matrix for Azure Backup](https://learn.microsoft.com/en-us/azure/backup/backup-support-matrix?WT.mc_id=AZ-MVP-5004796) for further information.*\\n\\n1. Navigate to **[Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted)**\\n1. Click on **Vaults**\\n1. Click **+ Vault**\\n1. Select **Recovery Services vault**\\n1. Select **Continue**\\n1. Specify a **location** and **Resource Group** to house your Recovery Services vault\\n1. Specify your **vault name** *[(abbreviation examples for Azure resources)](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-abbreviations?WT.mc_id=AZ-MVP-5004796)*\\n1. Click **Next: Vault properties**\\n\\n> Immutability:\\nI talked a bit about immutability in another blog article: [You Can\'t Touch This: How to Make Your Azure Backup Immutable and Secure](https://luke.geek.nz/azure/you-can-t-touch-this-how-to-make-your-azure-backup-immutable-and-secure/). Essentially an immutable vault, prevents unauthorised changes and restore point deletions, for this article, we will enable it to prevent unintended or malicious data loss (keep in mind with immutable vaults, reducing retention of recovery points is not allowed).\\n\\n1. Check **enabled immutability**, and click **Next: Networking**.\\n1. We can join our Recovery Services vault to our [private network using private endpoints](https://learn.microsoft.com/azure/backup/private-endpoints?WT.mc_id=AZ-MVP-5004796), forcing Azure Backup and Site Recovery to traverse a private network, for the purposes of this article, we will skip it. **Click Next: Tags**\\n1. **Enter in Tags** *(tags useful for a Recovery Service vault, could be: Application, Support Team, Environment, Cost Center, Criticality)*\\n1. Click **Review + Create**\\n\\n![Create Azure Recovery Services Vault](/images/posts/Create_RSV_BackupCenter_AzurePortal.gif)\\n\\nIf we navigate back to the [Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted) and then Vaults (under Manage), we will be able to see the newly created vault.\\n\\nWe now have our Backup solution provisioned for the Cloud Administrator to use, but we next need to define the policies for the backup.\\n\\n### Create Backup Policies\\n\\nNow that we have our Recovery Services vault, we need to create backup policies; these backup policies will help define the frequency of backups, the retention (Daily, weekly, monthly, yearly) and [vault tiering](https://learn.microsoft.com/azure/backup/use-archive-tier-support?pivots=client-portaltier&WT.mc_id=AZ-MVP-5004796#enable-smart-tiering-to-vault-archive-using-a-backup-policy-preview), which enables the Recovery Services Vault to move recovery vaults to an [archive tier](https://learn.microsoft.com/en-us/azure/backup/archive-tier-support?WT.mc_id=AZ-MVP-5004796#archive-recommendations-only-for-azure-virtual-machines) _(slower to restore, but can be cheaper overall, for those long retention policies)_.\\n\\n> Backup policies are very organisation-specific and can depend a lot on operational and industry requirements; some industries have a legal obligation to store their backups for a certain number of years, the [Azure compliance center documentation](https://learn.microsoft.com/en-us/azure/compliance/?WT.mc_id=AZ-MVP-5004796) may help, around security and data requirements, make sure your backup policies are understood by the business you are working with.\\n\\nFor Contoso, we have the following requirements:\\n\\n| Resource                   | Daily     | Weekly    | Monthly   | Yearly    | Snapshot Retention (Hot) |\\n|----------------------------|-----------|-----------|-----------|-----------|--------------------------|\\n| **Critical Application DB - Prod**| 7 days - Every 4 Hours   | 4 weeks   | 6 months  | 7 years   | 5 days                   |\\n| **File Server- Prod**            | 7 days   - Every 4 Hours | 6 weeks   | 6 months  | 7 years   | 5 days                   |\\n| **Web Application VM - Dev**     | 20 days   | 8 weeks   | 12 months | 2 years   |  2 days             |\\n| **Database Server - Dev**        | 30 days   | 8 weeks   | 12 months | 2 years   |  2 days            |\\n\\nThere are a few things to call out here:\\n\\n* We can see that for Development, items need to be retained for 2 years\\n* For Production, its 7 years\\n* Snapshots need to be stored for 5 days and 2 days to allow fast restore\\n* Production requires a backup to be taken every 4 hours to reduce RTO *(Recovery point objective)*\\n\\n![Create Azure Recovery Services Vault](/images/posts/rpo-rto-infographic.jpg)\\n\\nIf we take a look at the Snapshot retention, we can leverage [Instant restore](https://learn.microsoft.com/en-us/azure/backup/backup-instant-restore-capability?WT.mc_id=AZ-MVP-5004796) snapshots, to restore the workloads, quickly from the previous 5 days, reducing our time RTO (recovery time objective), and overall impact of an outage or restore, by storing the snapshots locally (as close to the original disk) without putting it (waiting for it) into archive (slower disk), this will incurr more cost, but dramatically reduces restores time. I recommend always keeping a few Instant restore snapshots available for all production systems.\\n\\n![Snapshot](/images/posts/instant-rp-flow.png)\\n\\nLet us create the policies _(we will only create one policy, but the same process can be used to create the others)_.\\n\\n1. Navigate to **[Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted)**\\n1. Click on **Backup policies**\\n1. Click **+ Add**\\n1 .Select **Azure Virtual Machines**\\n1. **Select** the **Vault** created earlier\\n1. Click **Continue**\\n1. As this will be the policy for the Critical Application DB, we will specify: **Enhanced** *(due to the multiple backups, [Zone-redundant storage (ZRS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy?WT.mc_id=AZ-MVP-5004796) snapshots)*\\n1. Specify a **Policy name**, ie Tier-1-Prod-AppDB\\n1. Specify **Frequency** to: Hourly, Schedule to Every 4 Hours, and Duration: 24 Hours\\n1. Specify **Retain instance recovery snapshots** for \'5\' days\\n1. Update **Daily Backup point** to: 7 days\\n1. **Configure the Weekly backup point** to occur every Sunday and retain for 4 weeks\\n1. **Configure the Monthly backup point** to occur on the first Sunday of the month and retain for 6 months\\n1. **Configure the yearly backup point** to occur on the first Sunday of the year and retain for 7 years\\n1. Select **enable Tiering**, and specify Recommended recovery points\\n1. You can also update the Resource Group name used to store the Snapshots.\\n1. Click **Create**\\n\\n![Snapshot](/images/posts/Create_RSV_AzurePolicyEnhanced.gif)\\n\\nNote: If you want, you can repeat the same process to create any others you need. Remember, with immutable vaults, you cannot reduce the retention (but you can add), so if starting for the first time, keep the retention low until you have a clear direction of what is required. A workload can use the same policy. A Standard _(not Enhanced)_ policy may be all you need for Development workloads.\\n\\n### Add Virtual Machines\\n\\nNow that we have our Recovery Services Vault and custom backup policies, it\'s time to add our Virtual Machines to the backup! To do this, we can use the Backup center to view Virtual Machines that are not getting backed up, and then configure the backup.\\n\\n1. Navigate to **[Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted)**\\n1. Click on **Protectable data sources**\\n1. Click on the **ellipsis** of a  Virtual Machine you want to backup\\n1. Click on **Backup**\\n1. **Select** the appropriate **Backup vault** and **policy**\\n1. Click **Enable backup**\\n\\n> Although cross-region restore is now supported on a Recovery Services vault, the second region is read-only (RA-GRS), so make sure you have a backup recovery vault created in the region (and subscription) of the virtual machines you are trying to protect. Backup center, can see all Recovery services vaults across multiple regions and subscriptions that you have access to.\\n\\n![Add Virtual Machines](/images/posts/BackupItem_BackupCenter_RSV_AzurePolicyEnhanced.gif)\\n\\nOnce added, the Virtual Machine will now get backed up according to the specified policy.\\n\\n> Its worth noting that you can backup a Virtual Machine if it is deallocated, but it will Crash-consistent _(Only the data that already exists on the disk at the time of backup is captured and backed up, and it triggers a disk check during recovery)_ compared to Application consistent, which is more application and OS aware, so can prepare to the OS and applications for the backups to make sure that everything is written successfully to the disk ahead of the backup. You can read more about [Snapshot consistency](https://learn.microsoft.com/en-us/azure/backup/backup-azure-vms-introduction?WT.mc_id=AZ-MVP-5004796#snapshot-consistency).\\n\\n## Monitor Backups\\n\\nNow that we have our Recovery Services Vault, policies and protected items _(backed up Virtual Machines)_, we need to monitor to make sure that the backups are working. Backup center gives us a complete view of Failed, In Progress, and Completed jobs in the overview pane, which is excellent for a quick view of the status across subscriptions and regions.\\n\\n![Azure BackupCenter](/images/posts/Azure_BackupCenter_Overview.png)\\n\\nBut you may want something a bit more detailed; let us look into some of the options for monitoring your backups.\\n\\n### Alerts\\n\\nAs part of operational checks, you may want assurance or a ticket raised if there\'s an issue with a backup; one of the ways to achieve this is to set up an email alert that will send an email if a backup fails.\\n\\nBy default, these types of alerts are enabled out-of-the-box on a recovery services vault; examples of alerts can be found here: [Azure Monitor alerts for Azure Backup](https://learn.microsoft.com/azure/backup/backup-azure-monitoring-built-in-monitor?tabs=recovery-services-vaults&WT.mc_id=AZ-MVP-5004796#azure-monitor-alerts-for-azure-backup), these can be displayed in the Recovery Services Vault or Backup Center blade, immediately.\\n\\n> If a destructive operation, such as stop protection with deleted data is performed, an alert is raised, and an email is sent to subscription owners, admins, and co-admins even if notifications aren\'t configured for the Recovery Services vault.\\n\\n| Type                                       | Description                                                                                                                                                                | Example alert scenarios                                                                                          | Benefits                                                                                                                                                                                                     |\\n| ------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| Built-in Azure Monitor alerts (AMP alerts) | These are alerts that will be available out-of-the-box for customers without needing additional configuration by the customer.                                            | Security scenarios like deleting backup data, soft-delete disabled, vault deleted, etc.                            | Useful for critical scenarios where the customer needs to receive alerts without the possibility of alerts being subverted by a malicious admin. Alerts for destructive operations fall under this category |\\n| Metric alerts                              | Here, Azure Backup will surface backup health-related metrics for customers\' Recovery Services vaults and Backup vaults. Customers can write alert rules on these metrics. | Backup health-related scenarios such as backup success alerts, restore success, schedule missed, RPO missed, etc. | Useful for scenarios where customers would like some control over the creation of alert rules but without the overhead of setting up LA or any other custom data store.                                     |\\n| Custom Log Alerts                          | Customers configure their vaults to send data to the Log Analytics workspace and write alert rules on logs.                                                                    | \'N\' consecutive failed backup jobs, Spike in storage consumed, etc.                                               | Useful for scenarios where there is a relatively complex, customer-specific logic needed to generate an alert.                                                                                               |\\n\\nBackup alerts are supported by Azure Monitor, so under [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796), and Alerts pane you can see all your other alerts, including Azure Backup alerts from a single pane.\\n\\n![Azure BackupCenter](/images/posts/AzureMonitor_BackupAlert_AzurePortal.png)\\n\\nIf you want to configure notifications via emails for other types of alerts, such as Backup failures, we can use [Azure Monitor](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796) Action Groups and Alert processing rules, to let us know, without having to login to the Azure Portal directly, so let us create an email alert.\\n\\nTo do this, we will create an Action Group and Alert Processing rule.\\n\\n| Component              | Description                                                                                          |\\n|------------------------|------------------------------------------------------------------------------------------------------|\\n| Action Group           | An Action Group is a collection of actions or tasks that are executed automatically when an alert that matches specific criteria is triggered. Actions can include sending notifications, running scripts, triggering automation, or escalating the alert. Action Groups help streamline incident response and automate actions based on the nature and severity of an alert. |\\n| Alert Processing Rule  | An Alert Processing Rule is a set of conditions and criteria used to filter, categorize, or route incoming alerts within a monitoring or alerting system. These rules enable organizations to define how alerts are processed, prioritize them, and determine the appropriate actions to take when specific conditions are met. Alert Processing Rules are crucial for managing and efficiently responding to alerts. |\\n\\n1. Navigate to **[Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted)**\\n1. Click on **Alerts**\\n1. Click on **Alert Processing rule**\\n1. Click **+ Select Scope**\\n1. Click **All Resource Types**, and Filter by: **Recovery Services Vault**\\n1. **Select** your **Recovery Services vault**, you would like to alert on\\n1. Click **Apply**\\n1. Click on **Filter, and change: Alert condition = Fired**.\\n1. Click Next: **Rule Settings**\\n1. Click **Apply action group**\\n1. Click **+ Create action group**\\n1. **Select** the Subscription, **Resource Group** to store your action group (i.e. monitor resource group)\\n1. Give the **Action Group** a **name**, and give it a Display name\\n1. Specify **Notification** type _(ie Email/SMS message/push/voice)_\\n1. For this article, we will add an Email _(but you can have it ring a number, push a notification to the [Azure Mobile App](https://azure.microsoft.com/en-us/get-started/azure-portal/mobile-app?WT.mc_id=AZ-MVP-5004796))_\\n1. Enter in **your details**, then click **Next: Actions**\\n1. In the Actions pane, is where you can trigger automation, such as Azure Logic Apps, Runbooks, ITSM connections, Webhooks etc., to help self-remediate the issues, or better notifications, such as a Logic App that posts in a Teams channel when an alert is fired, or a wehbook that triggers a webpage to update. In this example, we will leave it empty and rely on email notifications and click Next: Tags\\n1. Enter any Tags and click **Review + create**\\n1. Make note of Suppress Notifications; this could be handy during scheduled maintenance windows where backups may fail due to approved work.\\n1. Once the Action Group has been created, click **Next: Scheduling**\\n1. Select **Always**\\n1. Click **Next: Details**\\n1. Enter in a **Resource Group**, for the Alert processing rule to be placed\\n1. Enter in **Rule name**, **description** and click **Review + Create**\\n\\n![Azure BackupCenter](/images/posts/BackupItem_BackupCenter_RSV_BackupAlertRuleCreation.gif)\\n\\nAs you can see Azure Monitor integration into backups, gives you some great options to keep on top of your backups, and integrate with other systems, like your IT Service Management toolsets.\\n\\n## Azure Site Recovery\\n\\n[Azure Site Recovery (ASR)](https://learn.microsoft.com/azure/site-recovery/site-recovery-overview?WT.mc_id=AZ-MVP-5004796) can be used to migrate workloads, across [Availability Zones and regions](https://learn.microsoft.com/azure/reliability/availability-zones-overview?WT.mc_id=AZ-MVP-5004796), by replicating the disks of a Virtual Machine to another region (GRS) or zone (ZRS), in fact [Azure Resource Mover](https://learn.microsoft.com/azure/resource-mover/overview?WT.mc_id=AZ-MVP-5004796) uses Azure Site Recovery when moving virtual machines between regions. Azure Site Recovery can also help with migrating workloads outside of Azure, to Azure, for disaster recovery.\\n\\nWhen looking at migrating workloads, to Azure from the VMWare stack, consider the [Azure Site Recovery Deployment Planner for VMware](https://learn.microsoft.com/azure/site-recovery/site-recovery-deployment-planner?WT.mc_id=AZ-MVP-5004796) to Azure to assist.\\n\\nFor the purposes of this guide, we will achieve disaster recovery of our virtual machine, by replicating it to another region _(i.e. from Australia East to Central India)_.\\n\\n> Azure Recovery Services contributes to your BCDR strategy:\\nSite Recovery service: Site Recovery helps ensure business continuity by keeping business apps and workloads running during outages. Site Recovery replicates workloads running on physical and virtual machines (VMs) from a primary site to a secondary location. When an outage occurs at your primary site, you fail over to a secondary location, and access apps from there. After the primary location is running again, you can fail back to it.\\nBackup service: The Azure Backup service keeps your data safe and recoverable.\\n\\n![Azure BackupCenter](/images/posts/CausesITDisasters.png)\\n\\n> Just as important (if not more) than the technology to enable this, clear business requirements and preparation is paramount for a successful disaster recovery solution, I highly recommend the [Azure Business Continuity Guide](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/introducing-the-azure-business-continuity-guide/ba-p/3905424?WT.mc_id=AZ-MVP-5004796). Supplied by the Microsoft Fastrack team, this guide includes resources to prepare for thorough disaster recovery plan.\\n\\nThe key to successful disaster recovery is not only the workloads themselves but supporting services, such as DNS, Firewall rules, connectivity etc., that need to be considered, These are out of the scope of this article but the following Microsoft Azure architecture references are worth a read:\\n\\n* [Enterprise-scale disaster recovery](https://learn.microsoft.com/azure/architecture/solution-ideas/articles/disaster-recovery-enterprise-scale-dr?WT.mc_id=AZ-MVP-5004796)\\n* [SMB disaster recovery with Azure Site Recovery](https://learn.microsoft.com/azure/architecture/solution-ideas/articles/disaster-recovery-smb-azure-site-recovery?WT.mc_id=AZ-MVP-5004796)\\n* [Azure to Azure disaster recovery architecture](https://learn.microsoft.com/azure/site-recovery/azure-to-azure-architecture?WT.mc_id=AZ-MVP-5004796)\\n* [Multi-region N-tier application](https://learn.microsoft.com/azure/architecture/reference-architectures/n-tier/multi-region-sql-server?WT.mc_id=AZ-MVP-5004796)\\n* [Build high availability into your BCDR strategy](https://learn.microsoft.com/azure/architecture/solution-ideas/articles/build-high-availability-into-your-bcdr-strategy?WT.mc_id=AZ-MVP-5004796)\\n* [Retain IP addresses during failover](https://learn.microsoft.com/azure/site-recovery/site-recovery-retain-ip-azure-vm-failover?WT.mc_id=AZ-MVP-5004796)\\n* [Empowering Disaster Recovery for Azure VMs with Azure Site Recovery and Terraform](https://techcommunity.microsoft.com/t5/azure-architecture-blog/empowering-disaster-recovery-for-azure-vms-with-azure-site/ba-p/3885378?WT.mc_id=AZ-MVP-5004796)\\n\\nFor Azure Site Recovery to work, it relies on a mobility service running within the Virtual Machine to replicate changes, the source virtual machine needs to be on to replicate the changes.\\n\\n> When you enable replication for a VM to set up disaster recovery, the Site Recovery Mobility service extension installs on the VM and registers it with Azure Site Recovery. During replication, VM disk writes are sent to a cache storage account in the source region. Data is sent from there to the target region, and recovery points are generated from the data.\\n\\nAzure Site Recovery, does not currently support virtual machines protected with [Trusted Launch](https://learn.microsoft.com/azure/virtual-machines/trusted-launch?WT.mc_id=AZ-MVP-5004796).\\n\\n### Enable Azure Site Recovery\\n\\nFor now, we have \'VM1\' a Ubuntu workload, running in Australia East, with a Public IP, that we will failover to Central India. The source Virtual Machine can be backed up normally by a vault in the source region, and replicated to another vault in the destination region.\\n\\n> Azure Site Recovery has a specific Operating System and Linux kernel [support](https://learn.microsoft.com/azure/site-recovery/azure-to-azure-support-matrix?WT.mc_id=AZ-MVP-5004796#replicated-machine-operating-systems). Make sure you confirm that your workloads are supported.\\n\\n1. Navigate to **[Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted)**\\n1. Click on **Vaults**\\n1. **Create** a new Recovery Services **vault** in your **DR** _(Disaster Recovery **region** - ie Central India)_\\n1. Click on **Site Recovery**\\n1. Under Azure Virtual Machines, click on: **Enable replication**\\n1. Specify the **source Virtual Machine**, you wish to migrate\\n1. Click **Next**\\n1. Select your **source Virtual Machine**\\n1. Click **Next**\\n1. Select the **target location (i.e. Central India)**\\n1. Select the **target Resource Group**\\n1. Select the **Target Virtual Network** _(create one if it doesn\'t exist)_\\n1. Select the **target subnet**\\n1. Under the Storage, you can consider changing the replica disk to Standard, to reduce cost (this can be [changed](https://learn.microsoft.com/azure/virtual-machines/disks-convert-types?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796#change-the-type-of-an-individual-managed-disk) later).\\n1. Select a cache storage account _(The cache storage account is a storage account used for transferring the replication data before its written to the destination disk)_\\n1. You can then adjust the availability zone of the destination virtual machine\\n1. Click **Next**\\n1. Here we can **define a Replication Policy** _(a replication policy in Azure Site Recovery is a set of rules and configurations that determine how data is replicated from the source environment to the target environment (Azure) in case of a disaster or planned failover, such as retention, ie you can restore a point within the retention period)_ we will leave the default 24-hour retention policy.\\n1. We can specify a Replication Group, An example of a replication group is application servers that need to be consistent with each other, in terms of data _( replication policy in Azure Site Recovery is a set of rules and configurations that determine how data is replicated from the source environment to the target environment (Azure) in case of a disaster or planned failover.)_.\\n1. Specify an automation account to manage the mobility service, and we will leave the update extension to be ASR (Azure Site Recovery) managed.\\n1. Click **Next**\\n1. Click **Enable replication**\\n1. At the Recovery Services Vault, under Site Recovery Jobs you can monitor the registration, registration and initial replication can take 30-60 minutes to install the agent and start the replication.\\n\\n![Azure BackupCenter](/images/posts/BackupCenter_RSV_EnableAzureSiteRecovery.gif)\\n\\n### Failover to the secondary region using Azure Site Recovery\\n\\nOnce your virtual machine has been replicated in the secondary region. you can do a Failover, or Test failover. A Test failover is recommended, in your DR testing, and application testing.\\n\\n![Azure BackupCenter](/images/posts/ASR_AUE_to_CentralIndia.png)\\n\\n| Aspect               | Failover                                      | Test Failover                               |\\n|----------------------|----------------------------------------------|---------------------------------------------|\\n| Purpose              | To switch to a secondary site during a disaster or planned maintenance event. | To validate your disaster recovery plan without impacting production. |\\n| Impact on Production | Disrupts production services as the primary site becomes unavailable during the failover process. | No impact on production services; the primary site remains operational. |\\n| Data Replication     | Replicates data from primary to secondary site, making it the active site during the failover. | Uses the same replicated data but doesn\'t make the secondary site the active site; it\'s for testing purposes only. |\\n| Recovery Time        | Longer recovery time, as it involves setting up and activating the secondary site. | Faster recovery time, as it doesn\'t require making the secondary site the active site. |\\n| Data Consistency     | Ensures data consistency and integrity during the failover process. | Ensures data consistency for testing but doesn\'t make the secondary site the primary site. |\\n| Cost                 | May incur additional costs due to the resources activated at the secondary site. | Typically incurs minimal additional costs as it\'s for testing purposes. |\\n| Use Cases            | Actual disaster recovery scenarios or planned maintenance events. | Testing and validating disaster recovery procedures, training, and compliance. |\\n| Post-Operation       | The secondary site becomes the new primary site until failback is initiated. | No change to the primary site; the secondary site remains inactive. |\\n| Rollback Option      | Failback operation is required to return to the primary site once it\'s available. | No need for a rollback; the primary site remains unaffected. |\\n\\n1. Navigate to your destination Recovery Services Vault\\n1. Click on REplicated Items\\n1. Select the Virtual Machine you wish to recover in your second region\\n1. Select Test Failover (or Failover, depending on your requirements)\\n1. Select your Recovery point and destination Virtual network\\n1. Select Failover\\n1. If it is a test failover, you can then Clean up your Test failover _(deleted replicated item)_ after you have tested\\n\\n![Azure BackupCenter](/images/posts/BackupCenter_RSV_TestFailoverAzureSiteRecovery.gif)\\n\\n## Azure Policies\\n\\nAutomatically, mapping of Virtual Machines, to backup policies can be done using [Azure Policy](https://learn.microsoft.com/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796).\\n\\nAzure policies such as:\\n\\n* Azure Backup should be enabled for Virtual Machines\\n* Configure backup on virtual machines without a given tag to an existing recovery services vault in the same location\\n* Disable Cross Subscription Restore for Backup Vaults\\n* Soft delete should be enabled for Backup Vaults\\n\\nMore, are built-in to the Azure policy engine and can be easily assigned, across subscriptions and management groups, found in the Backup Center.\\n\\n1. Navigate to **[Backup Center](https://portal.azure.com/#view/Microsoft_Azure_DataProtection/BackupCenterMenuBlade/~/gettingstarted)**\\n1. Click on **Azure policies for backup**\\n1. Click on a policy and click **Assign**\\n\\n> You can find a list of custom and built-in policies at the [AzPolicyAdvertizerPro](https://www.azadvertizer.net/azpolicyadvertizer_all.html) website.\\n\\n![Azure AutoManage](/images/posts/azure-automanage-intelligently-onboard-services.png)\\n\\n[Azure Automanage](https://learn.microsoft.com/azure/automanage/overview-about?WT.mc_id=AZ-MVP-5004796) can be used alongside Azure policy, to onboard Virtual Machines, into backup, patching etc automatically, with reduced manual intervention, and although not directly part of this article, what you have learned can be used to develop your automanage profiles."},{"id":"azure/hosted-agents-container-apps-job","metadata":{"permalink":"/azure/hosted-agents-container-apps-job","source":"@site/blog/2023-09-13-Get-Ahead-with-Self-Hosted-Agents-and-Container-Apps-Jobs.md","title":"Get Ahead with Self-Hosted Agents and Container Apps Jobs","description":"When considering build agents to use in Azure DevOps (or GitHub), there are 2 main options to consider:","date":"2023-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":22.255,"hasTruncateMarker":true,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Get Ahead with Self-Hosted Agents and Container Apps Jobs","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/BlogHeader-GetAheadwithSelf-HostedAgentsandContainerAppsJobs.gif"},"date":"2023-09-13 00:00:00 +1300","slug":"azure/hosted-agents-container-apps-job"},"unlisted":false,"prevItem":{"title":"Empowering Resilience with Azure backup services","permalink":"/azure/Empowering-Resilience-with-Azure-backup-services"},"nextItem":{"title":"Azure Bicep Deployment with Deployment Stacks","permalink":"/azure/Azure-Bicep-Deployment-with-Deployment-Stacks"}},"content":"When considering [build agents](https://learn.microsoft.com/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser&WT.mc_id=AZ-MVP-5004796) to use in [Azure DevOps](https://azure.microsoft.com/products/devops?WT.mc_id=AZ-MVP-5004796) *(or [GitHub](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners))*, there are 2 main options to consider:\\n\\n| Agent type              | Description                                              |\\n| ----------------------- | -------------------------------------------------------- |\\n| [Microsoft-hosted agents](https://learn.microsoft.com/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser&WT.mc_id=AZ-MVP-5004796#microsoft-hosted-agents) | Agents hosted and managed by Microsoft                   |\\n| [Self-hosted agents](https://learn.microsoft.com/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser&WT.mc_id=AZ-MVP-5004796#install)      | Agents that you configure and manage, hosted on your VMs |\\n\\n[Microsoft-hosted agents](https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796), can be used for most things, but there are times where you may need to talk to internal company resources, or security is a concern, which is when you would consider self-hosting the agent yourself.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\nHere is a table that summarizes the pros and cons of self-hosted Azure DevOps agents and Microsoft-hosted agents:\\n\\n| **Agent Type** | **Pros** | **Cons** |\\n|----------------|----------|----------|\\n| Self-hosted    | More control over the environment, ability to install dependent software needed for builds and deployments, machine-level caches and configuration persist from run to run, which can boost speed. | Maintenance and upgrades are not taken care of for you; you must manage the agent yourself. |\\n| Microsoft-hosted | Maintenance and upgrades are taken care of for you; each time you run a pipeline, you get a fresh virtual machine discarded after one use. Microsoft-hosted agents can run jobs directly on the VM or in a container. The pre-defined Azure Pipelines agent pool offers several virtual machine images, each including various tools and software. You can see the installed software for each hosted agent by choosing the Included Software link in the table. Microsoft-hosted agents run on a secure Azure platform. | You have less control over the environment, you cannot install dependent software needed for builds and deployments, and machine-level caches and configurations do not persist from run to run. |\\n\\n## Overview\\n\\nSelf-hosted agents give you more control over your environment, allowing you to install dependent software needed for your builds and deployments.\\n\\nAs Azure DevOps pipeline jobs come and go as they complete each task required, you want to be able to scale the agents out as required and pay for only what you use, you could consider [Azure Virtual Machine Scale Set agents](https://learn.microsoft.com/azure/devops/pipelines/agents/scale-set-agents?view=azure-devops&WT.mc_id=AZ-MVP-5004796). Still, you have to have to maintain virtual machine images and storage, they can be slow to provision and start, and they could become inconsistent as manual changes can be easier to do.\\n\\nHere is a table that summarizes the comparison between Container Apps Jobs for an Azure DevOps Agent and using an Azure Virtual Machine scale set:\\n\\n| **Agent Type** | **Pros** | **Cons** |\\n|----------------|----------|----------|\\n| Container Apps Jobs | Can run containerized tasks that execute for a finite duration and exit, allowing you to perform tasks such as data processing, machine learning, or any scenario requiring on-demand processing. Container apps and jobs run in the same environment, allowing them to share capabilities such as networking and logging. | You have less control over the environment, you cannot install dependent software needed for builds and deployments, and machine-level caches and configurations do not persist from run to run. |\\n| Azure Virtual Machine scale set | You have more control over the environment, allowing you to install dependent software needed for your builds and deployments. Machine-level caches and configuration persist from run to run, which can boost speed. | Maintenance and upgrades are not taken care of for you; you need to manage the agent yourself. |\\n\\nContainer Apps Jobs allows you to run containerized tasks that execute for a finite duration and exit, performing tasks such as data processing, machine learning, or any scenario requiring on-demand processing. Container apps and jobs run in the same environment, allowing them to share capabilities such as networking and logging. However, you have less control over the environment, you cannot install dependent software needed for builds and deployments, and machine-level caches and configurations do not persist from run to run.\\n\\n*The choice between Container Apps and VM scale sets for Azure DevOps agents should consider your specific project requirements and constraints. Each option has its own set of advantages and trade-offs.*\\n\\nFor our discussion today, we will provision Azure DevOps Agents using [Azure Container Apps Jobs](https://learn.microsoft.com/azure/container-apps/jobs?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796).\\n\\n![Get Ahead with Self-Hosted Agents and Container Apps Jobs](/images/posts/BlogHeader-GetAheadwithSelf-HostedAgentsandContainerAppsJobs.gif)\\n\\nAs we want a self-hosted agent to have access to our internal resources, we will deploy a [Consumption based Internal Container Apps Environment](https://learn.microsoft.com/en-us/azure/container-apps/networking?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796), to host our jobs.\\n\\n[Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview?WT.mc_id=AZ-MVP-5004796) is a service that allows you to run containerized applications in the cloud. It provides a platform for running and scaling containerized applications, and it can be used to deploy and manage containerized applications in a variety of environments.\\n\\nThere are two types of compute resources in Azure Container Apps: **apps** and **jobs**.\\n\\nApps are services that run continuously. If a container in an app fails, it\'s restarted automatically. Examples of apps include HTTP APIs, web apps, and background services that continuously process input.\\n\\nWithout [scaled job](https://github.com/microsoft/azure-container-apps/issues/24) support by Azure Container App Jobs, a job could fail during execution; this has now been resolved with Container App Jobs.\\n\\n> Azure Container Apps jobs enable you to run containerized tasks that execute for a finite duration and exit. You can use jobs to perform tasks such as data processing, machine learning, or any scenario where on-demand processing is required.\\n> Jobs are tasks that start, run for a finite duration, and exit when finished. Each execution of a job typically performs a single unit of work. Job executions start manually, on a schedule, or in response to events. [Examples of jobs include batch processes that run on demand and scheduled tasks](https://learn.microsoft.com/azure/container-apps/jobs?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796).\\n\\nRunning self-hosted agents as event-driven jobs allows you to take advantage of the serverless nature of Azure Container Apps. Jobs execute automatically when a workflow is triggered and exit when the job completes.\\n\\n> Container apps and jobs don\'t support running Docker in containers. Any steps in your workflows that use Docker commands will fail when run on a self-hosted runner or agent in a Container Apps job; other [restrictions also exist](https://learn.microsoft.com/en-us/azure/container-apps/jobs?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796#jobs-restrictions).\\n\\nFor an Azure DevOps Agent, we want to execute tasks or remove them. This is where Container Apps Jobs and [KEDA](https://keda.sh/) come in handy.\\n\\n> KEDA (Kubernetes-based Event Driven Autoscaling) is an open-source project that provides event-driven autoscaling for Kubernetes workloads. KEDA can scale any container in response to events from various sources such as Azure Service Bus, Azure Event Hubs, Azure Storage Queues, Azure Storage Blobs, RabbitMQ, Kafka, and more.\\n\\nOne of the supported scalers is [Azure Pipelines](https://keda.sh/docs/2.11/scalers/azure-pipelines/).\\n\\n*This specification describes the azure-pipelines trigger for Azure Pipelines. It scales based on the number of pipeline runs pending in a given agent pool.*\\n\\nJobs can be triggered in three ways:\\n\\n* Manual jobs are triggered on demand.\\n* Scheduled jobs are triggered at specific times and can run repeatedly.\\n* Event-driven jobs are triggered by a message arriving in a queue.\\n\\nWe will use both **Manual** and **Event-driven**.\\n\\nThe **Manual** job will be run once to create a [placeholder](https://keda.sh/blog/2021-05-27-azure-pipelines-scaler/#placeholder-agent), Azure DevOps agent in the pool.\\n\\n> \\"You cannot queue an Azure Pipelines job on an empty agent pool because Azure Pipelines cannot validate if the pool matches the requirements for the job.\\"\\n\\nAs our Container Jobs are temporary, a placeholder agent **needs to remain** in the Agent pool *(i.e. don\'t delete it)* to keep it active. This agent will be offline and can be Disabled if required in Azure DevOps. The Azure resource, however, can then be deleted.\\n\\nFor the actual agents themselves that will run our code, they will be **event-driven**.\\n\\nTo provision our Azure Container App Job build agents, we will use [Azure Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796) to create our resources.\\n\\nOur resources will consist of:\\n\\n* [Internal Container Apps Environment](https://learn.microsoft.com/azure/container-apps/networking?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796#accessibility-levels) (Internal environments have no public endpoints and are deployed with a virtual IP (VIP) mapped to an internal IP address)\\n* [Virtual Network](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview?WT.mc_id=AZ-MVP-5004796) with 2 subnets (One subnet for resources, such as Azure Key vault, Container Registry, the other subnet dedicated to the Container App environment)\\n* [Azure Container Registry](https://learn.microsoft.com/azure/container-registry/container-registry-intro?WT.mc_id=AZ-MVP-5004796) (this registry will be used to build and contain our container for the DevOps agent. The container registry will have a private endpoint to the internal network)\\n* [Log Analytics workspace](https://learn.microsoft.com/azure/azure-monitor/logs/log-analytics-workspace-overview?WT.mc_id=AZ-MVP-5004796) (to hold the Logs from the Container App Environment)\\n* [Azure Key Vault](https://learn.microsoft.com/azure/key-vault/general/overview?WT.mc_id=AZ-MVP-5004796) (the key vault will hold our PAT (Personal Access Token), which will be used to join our COntainer App Job agents to the agent pool. The key vault will also be on the internal network, accessed via a private endpoint)\\n* [Azure Private DNS zones](https://learn.microsoft.com/azure/dns/private-dns-privatednszone?WT.mc_id=AZ-MVP-5004796) (the DNS zones, will allow the Container App Environment, to reach the Key vault and Container Registry over the internal network)\\n* [Deployment scripts](https://learn.microsoft.com/azure/azure-resource-manager/templates/deployment-script-template?WT.mc_id=AZ-MVP-5004796) (these can be deleted afterwards, but they will run the scripts to build our container image, and placeholder agent within the confines of Bicep)*\\n\\nWe will also need a [User Assigned Managed Identity](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796) for this article*(and the scope only being to the Resource Group)*I have a pre-created User Assigned Managed identity named:*usrmi*. This Managed identity has the following role assignments to the Resource Group to which the resources will be deployed.\\n\\n| **Role**                   | **Assigned To** | **Notes**                                                                                                                          |\\n|------------------------|-------------|--------------------------------------------------------------------------------------------------------------------------------|\\n| Contributor            | usrmi       | Contributor role on the container registry resource to push the container image and create the Container App Jobs and resources. |\\n| Key Vault Secrets User | usrmi       | Secret Reader to access the Key Vault secrets.                                                                                 |\\n\\nThe cost of the overall solution \'depends\' on how active it is and how it is used.\\n\\n* Resources such as [Azure Container Apps](https://azure.microsoft.com/pricing/details/container-apps/?WT.mc_id=AZ-MVP-5004796), under Consumption, are pay-per-use and dependent on the number of requests and the length of those requests. The idea here is that they only cost something if in use.\\n* [Container Registry](https://azure.microsoft.com/en-us/pricing/details/container-registry/?WT.mc_id=AZ-MVP-5004796) requires the Premium SKU for Private Endpoint support, but for demo environments, you could get away with a Basic.\\n* [Key Vault](https://azure.microsoft.com/pricing/details/key-vault/?WT.mc_id=AZ-MVP-5004796) also depends on the number of transactions and functionality.\\n\\nIt is recommended to do an estimate using the [Azure Pricing Calculator](https://azure.microsoft.com/pricing/calculator/?WT.mc_id=AZ-MVP-5004796) in your currency and region to work out the costs, but the true reflection will be once your Azure DevOps pipelines start consuming the infrastructure.\\n\\n## Let us get building\\n\\nTo deploy our environment:\\n\\n![Container App Jobs - High-level architecture](/images/posts/privatecontainerappsjob_architecture.png)\\n\\nWe will Azure Bicep, a User Managed Identity and Resource Group.\\n\\nThe [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796) file I have written is scoped to a single Resource Group, but to do this in production and work with your existing resources, it may be better to move it to [modules](https://learn.microsoft.com/azure/azure-resource-manager/bicep/modules?WT.mc_id=AZ-MVP-5004796).\\n\\n> All the code required to get this to work can be found in the following GitHub repository: [lukemurraynz/containerapps-selfhosted-agent](https://github.com/lukemurraynz/containerapps-selfhosted-agent), including the [GitHub Codespace](https://luke.geek.nz/azure/Getting-Started-with-GitHub-Codespaces/), configuration I am using to deploy.\\n\\nWe will start with a Resource Group consisting of our Managed Identity.\\n\\n![Azure Container Apps - Resource Group](/images/posts/AzureContainerApps_AzurePortal_ado-containerapp-rg.png)\\n\\n### Prepare - Azure DevOps Agent Pool\\n\\nBefore deploying anything into Azure, we must prepare our Azure DevOps environment.\\n\\n1. Login to your **[Azure DevOps](https://aex.dev.azure.com/)** organisation\\n2. Click on **Organization Settings**\\n3. Click on **Agent Pools**\\n4. Click **Add Pool**\\n5. Select **Self-hosted**\\n6. Give the Agent pool a **name** *(ie containerapp-adoagent - we will need the name later in our Bicep code)*\\n7. Enter a description and click **Create**\\n\\n![Create Azure DevOps - Agent Pool](/images/posts/AzureDevOps_CreateAgentPool_CAPPS.gif)\\n\\nOnce the agent pool has been created, we need our token to allow the Agents to register to the Agent Pool we have just created.\\n\\n*This token is a secret and will be stored in an Azure Key Vault as part of our deployment, allowing the secret to be protected from unauthorised people and allowing you to regenerate the secret when required by updating the key vault secret without having to redeploy any of the infrastructure.*\\n\\n1. Login to your **[Azure DevOps](https://aex.dev.azure.com/)** organisation.\\n2. Click on the little User icon at the top right *(next to your initials)*\\n3. Click on **Personal Access Tokens**\\n4. Click **+ New Token**\\n5. Type in a **name** *(ie JoinADOPool)*\\n6. Specify a valid **expiration date** *(for our demo purposes, we will go with 30 days)*.\\n7. Click **Show all scopes**\\n8. Find **Agent Pools**\\n9. Click **Read & manage**\\n10. Click **Create**\\n11. Copy the Token for later; if you lose this token before it can be uploaded to Key Vault, you will have to generate a new Token.\\n\\n![Create Azure DevOps - Agent Pool](/images/posts/AzureDevOps_CreatePATToken_CAPPS.gif)\\n\\n### Deploy - Azure Container Apps Environment\\n\\nNow that we have our Azure DevOps Agent Pool and PAT token - it is time to deploy our Container Apps infrastructure.\\n\\n> The [deployment scripts](https://learn.microsoft.com/azure/azure-resource-manager/templates/deployment-script-template?WT.mc_id=AZ-MVP-5004796) used by this solution do not currently support Private Endpoints*(this is coming)*, so during the build process the Container Registry has Public endpoint enabled. This can be disabled after your initial build has been completed if required. If needed, you could add another deployment script to the Container Registry back to private at the end.\\n> If this is the first time you have deployed Container Apps or Container Registry, you may need to register the [providers](https://learn.microsoft.com/azure/azure-resource-manager/management/resource-providers-and-types?WT.mc_id=AZ-MVP-5004796). This can be done with the following PowerShell commands, against your target subscription, else the deployment will fail:\\n\\n    Register-AzResourceProvider -ProviderNamespace Microsoft.ContainerRegistry\\n    Register-AzResourceProvider -ProviderNamespace Microsoft.KeyVault\\n\\nTo proceed, I will use my GitHub Codespace to deploy the Bicep; you could either run your own Codespace if you need it or fork the code [lukemurraynz/containerapps-selfhosted](https://github.com/lukemurraynz/containerapps-selfhosted-agent) and run it locally or from the [Azure CloudShell](https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796). The repository will have any updated code.\\n\\nThe Bicep code will be deployed as follows:\\n\\n```bicep title=\\"main.bicep\\"\\n// Define parameters\\n// https://github.com/lukemurraynz/containerapps-selfhosted-agent\\nparam location string = resourceGroup().location\\n@description(\'The location where the resources will be deployed. Based on the Resource Group location.\')\\nparam poolName string = \'containerapp-adoagent\'\\n@description(\'The name of the Azure DevOps agent pool.\')\\n@maxLength(50)\\nparam containerregistryName string = \'adoregistry\'\\n@description(\'The name of the Azure Container Registry.\')\\nparam adourl string = \'https://dev.azure.com/contoso\'\\n@description(\'The URL of the Azure DevOps organization.\')\\n// Don\'t include an end \'/\' in the URL. Else the ADO agent will fail to register.\\n@secure()\\nparam token string = \'tokenstringhere\'\\n@description(\'The personal access token (PAT) used to authenticate with Azure DevOps.\')\\nparam imagename string = \'adoagent:1.0\'\\n@description(\'The name of the container image.\')\\nparam managedenvname string = \'cnapps\'\\n@description(\'The name of the managed environment.\')\\nparam containerappsspokevnetName string = \'containerappsspokevnet\'\\n@description(\'The name of the virtual network.\')\\nparam userassignedminame string = \'usrmi\'\\n@description(\'The name of the managed environment.\')\\nparam isProduction bool = true\\n@description(\'Determines whether the environment is production or development and updates Tag accordingly.\')\\n\\n// Define tags\\nvar tags = {\\n  environment: isProduction ? \'Production\' : \'Development\'\\n  createdBy: \'Luke Murray\'\\n}\\n@description(\'Tags to apply to the resources.\')\\n\\n// Define virtual network resource\\nvar sharedServicesSubnet = {\\n  name: \'sharedservices\'\\n  properties: {\\n    addressPrefix: \'10.0.0.0/24\'\\n  }\\n}\\n\\nvar containerAppsSubnet = {\\n  name: \'containerappssnet\'\\n  properties: {\\n    addressPrefix: \'10.0.2.0/23\'\\n  }\\n}\\n\\nresource containerappsspokevnet \'Microsoft.Network/virtualNetworks@2023-04-01\' = {\\n  name: containerappsspokevnetName\\n  location: location\\n  tags: tags\\n  properties: {\\n    addressSpace: {\\n      addressPrefixes: [ \'10.0.0.0/16\' ]\\n    }\\n    subnets: [ sharedServicesSubnet, containerAppsSubnet ]\\n  }\\n}\\n\\n// Define Key Vault resource\\nvar keyvaultName = \'keyvault-ado\'\\n\\nresource keyvault \'Microsoft.KeyVault/vaults@2023-02-01\' = {\\n  name: keyvaultName\\n  location: location\\n  tags: tags\\n\\n  properties: {\\n    sku: {\\n      family: \'A\'\\n      name: \'standard\'\\n    }\\n    tenantId: tenant().tenantId\\n    networkAcls: {\\n      bypass: \'AzureServices\'\\n      defaultAction: \'Deny\'\\n      ipRules: []\\n      virtualNetworkRules: []\\n    }\\n    enableRbacAuthorization: true\\n    accessPolicies: []\\n    publicNetworkAccess: \'Disabled\'\\n    enableSoftDelete: false\\n    // Change SoftDelete to True for Production\\n    enabledForTemplateDeployment: true\\n  }\\n\\n}\\n\\n// Define Key Vault secrets\\nvar kvtokensecretName = \'personal-access-token\'\\n\\nresource kvtokensecret \'Microsoft.KeyVault/vaults/secrets@2023-02-01\' = {\\n  name: kvtokensecretName\\n  parent: keyvault\\n  properties: {\\n    value: token\\n  }\\n}\\n\\n// Define Private Endpoint resource\\nvar kvprivatelinkName = \'pe-${keyvaultName}\'\\n\\nresource kvprivatelink \'Microsoft.Network/privateEndpoints@2023-04-01\' = {\\n  name: kvprivatelinkName\\n  location: location\\n  tags: tags\\n  properties: {\\n    privateLinkServiceConnections: [\\n      {\\n        name: \'keyvault\'\\n        properties: {\\n          privateLinkServiceId: keyvault.id\\n          groupIds: [ \'vault\' ]\\n        }\\n      }\\n    ]\\n    subnet: {\\n      id: containerappsspokevnet.properties.subnets[0].id\\n    }\\n  }\\n}\\n\\n// Define Private DNS Zone resource\\nvar keyvaultdnszoneName = \'privatelink.vaultcore.azure.net\'\\n\\nresource keyvaultdnszone \'Microsoft.Network/privateDnsZones@2020-06-01\' = {\\n  name: keyvaultdnszoneName\\n  location: \'global\'\\n}\\n\\n// Define Private DNS Zone Group resource\\nresource keyvaultprivatednszonegrp \'Microsoft.Network/privateEndpoints/privateDnsZoneGroups@2023-04-01\' = {\\n  name: keyvaultdnszoneName\\n  parent: kvprivatelink\\n  properties: {\\n    privateDnsZoneConfigs: [\\n      {\\n        name: \'keyvault\'\\n        properties: {\\n          privateDnsZoneId: keyvaultdnszone.id\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\n// Define Private DNS Zone VNet Link resource\\nresource keyVaultPrivateDnsZoneVnetLink \'Microsoft.Network/privateDnsZones/virtualNetworkLinks@2020-06-01\' = {\\n  name: uniqueString(keyvault.id)\\n  parent: keyvaultdnszone\\n  location: \'global\'\\n  properties: {\\n    registrationEnabled: false\\n    virtualNetwork: {\\n      id: containerappsspokevnet.id\\n    }\\n  }\\n}\\n\\n// Define A record resource\\nresource aarecord \'Microsoft.Network/privateDnsZones/A@2020-06-01\' = {\\n  name: keyvaultName\\n  parent: keyvaultdnszone\\n  properties: {\\n    aRecords: [\\n      {\\n        ipv4Address: kvprivatelink.properties.customDnsConfigs[0].ipAddresses[0]\\n      }\\n    ]\\n    ttl: 300\\n  }\\n}\\n\\n// Define Managed Environment resource\\nresource cnapps \'Microsoft.App/managedEnvironments@2023-05-01\' = {\\n  name: managedenvname\\n  location: location\\n  tags: tags\\n\\n  properties: {\\n    appLogsConfiguration: {\\n      destination: \'azure-monitor\'\\n    }\\n    vnetConfiguration: {\\n      infrastructureSubnetId: containerappsspokevnet.properties.subnets[1].id\\n      internal: true\\n    }\\n    zoneRedundant: true\\n  }\\n}\\n\\n// Define Diagnostic Settings resource\\nresource cnappsdiag \'Microsoft.Insights/diagnosticSettings@2021-05-01-preview\' = {\\n  name: \'cnappsdiag\'\\n  scope: cnapps\\n\\n  properties: {\\n    logs: [\\n      {\\n        categoryGroup: \'allLogs\'\\n        enabled: true\\n      }\\n    ]\\n    metrics: [\\n      {\\n        category: \'AllMetrics\'\\n        enabled: true\\n      }\\n    ]\\n    workspaceId: law.id\\n  }\\n}\\n\\n// Define Container Registry resource\\n\\nresource containerregistry \'Microsoft.ContainerRegistry/registries@2023-06-01-preview\' = {\\n  name: containerregistryName\\n  location: location\\n  tags: tags\\n  identity: {\\n    type: \'UserAssigned\'\\n    userAssignedIdentities: {\\n      \'${usrmi.id}\': {}\\n    }\\n  }\\n  // Identity required to allow Container Apps job to talk to the Registry.\\n  sku: {\\n    name: \'Premium\'\\n    //Premium is required for private endpoint support.\\n  }\\n  properties: {\\n    publicNetworkAccess: \'Enabled\'\\n    //Required for the Deployment Scripts to build the images. Public Network access, can be disabled after.\\n    networkRuleBypassOptions: \'AzureServices\'\\n    adminUserEnabled: false\\n  }\\n\\n}\\n\\n// Define Private DNS Zone resource for Container registry\\n\\n// Define Private DNS Zone resource\\nvar containerregistrydnszoneName = \'privatelink.azurecr.io\'\\n\\nresource containerregistrydnszone \'Microsoft.Network/privateDnsZones@2020-06-01\' = {\\n  name: containerregistrydnszoneName\\n  location: \'global\'\\n}\\n\\n// Define Private DNS Zone Group resource\\nresource containerregistrydnszonegrp \'Microsoft.Network/privateEndpoints/privateDnsZoneGroups@2023-04-01\' = {\\n  name: containerregistrydnszoneName\\n  parent: conregprivatelink\\n  properties: {\\n    privateDnsZoneConfigs: [\\n      {\\n        name: \'containerregistry\'\\n        properties: {\\n          privateDnsZoneId: containerregistrydnszone.id\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\n// Define Private Endpoint resource\\nvar containerregistryprivatelinkName = \'pe-${containerregistry.name}\'\\n\\nresource conregprivatelink \'Microsoft.Network/privateEndpoints@2023-04-01\' = {\\n  name: containerregistryprivatelinkName\\n  location: location\\n  tags: tags\\n  properties: {\\n    privateLinkServiceConnections: [\\n      {\\n        name: \'registry\'\\n        properties: {\\n          privateLinkServiceId: containerregistry.id\\n          groupIds: [ \'registry\' ]\\n        }\\n      }\\n    ]\\n    subnet: {\\n      id: containerappsspokevnet.properties.subnets[0].id\\n    }\\n  }\\n}\\n\\n// Define Private DNS Zone VNet Link resource\\nresource containerregistryPrivateDnsZoneVnetLink \'Microsoft.Network/privateDnsZones/virtualNetworkLinks@2020-06-01\' = {\\n  name: uniqueString(containerregistry.id)\\n  parent: containerregistrydnszone\\n  location: \'global\'\\n  properties: {\\n    registrationEnabled: false\\n    virtualNetwork: {\\n      id: containerappsspokevnet.id\\n    }\\n  }\\n}\\n\\n// Define A record resource\\nresource containerregistryarc \'Microsoft.Network/privateDnsZones/A@2020-06-01\' = {\\n  name: containerregistry.name\\n  parent: containerregistrydnszone\\n  properties: {\\n    aRecords: [\\n      {\\n        ipv4Address: kvprivatelink.properties.customDnsConfigs[0].ipAddresses[0]\\n      }\\n    ]\\n    ttl: 300\\n  }\\n}\\n\\n// Define User Assigned Managed Identity resource\\n// The user managed identity associated with the container app job needs to have the following permissions to run this script:\\n// 1. Secret Reader to access the Key Vault secrets.\\n// 2. Contributor role on the container registry resource to push the container image.\\n// 3. Contributor role on the managed environment resource to create the job.\\n// 4. Contributor role on the log analytics workspace resource to enable diagnostic settings.\\n// 5. Contributor role on the virtual network resource to create the private endpoint.\\n// 6. Contributor role on the private DNS zone resource to create the A record.\\n// You can grant these permissions by adding the managed identity to the appropriate role assignments in Azure.\\n\\nresource usrmi \'Microsoft.ManagedIdentity/userAssignedIdentities@2023-01-31\' existing = {\\n  name: userassignedminame\\n}\\n\\n// Define Log Analytics Workspace resource\\nvar lawName = \'law-${uniqueString(resourceGroup().id)}\'\\n\\nresource law \'Microsoft.OperationalInsights/workspaces@2022-10-01\' = {\\n  name: lawName\\n  location: location\\n  tags: tags\\n  properties: {\\n    sku: {\\n      name: \'PerGB2018\'\\n    }\\n    retentionInDays: 30\\n    publicNetworkAccessForIngestion: \'Enabled\'\\n    publicNetworkAccessForQuery: \'Enabled\'\\n  }\\n}\\n\\n// Define Deployment Script resource for ACR build\\nvar arcbuildName = \'acrbuild\'\\n\\nresource arcbuild \'Microsoft.Resources/deploymentScripts@2020-10-01\' = {\\n  name: arcbuildName\\n  location: location\\n  tags: tags\\n  kind: \'AzureCLI\'\\n  identity: {\\n    type: \'UserAssigned\'\\n    userAssignedIdentities: {\\n      \'${usrmi.id}\': {}\\n    }\\n  }\\n\\n  properties: {\\n    azCliVersion: \'2.50.0\'\\n    retentionInterval: \'P1D\'\\n    timeout: \'PT30M\'\\n    arguments: \'${containerregistryName} ${imagename}\'\\n    scriptContent: \'\'\'\\n    az login --identity\\n    az acr build --registry $1 --image $2  --file Dockerfile.azure-pipelines https://github.com/lukemurraynz/containerapps-selfhosted-agent.git\\n    \'\'\'\\n    cleanupPreference: \'OnSuccess\'\\n  }\\n}\\n\\n// Define Deployment Script resource for ACR placeholder\\nvar arcplaceholderName = \'arcplaceholder\'\\n\\nresource arcplaceholder \'Microsoft.Resources/deploymentScripts@2020-10-01\' = {\\n  name: arcplaceholderName\\n  location: location\\n  tags: union(tags, { Note: \'Can be deleted after original ADO registration (along with the Placeholder Job). Although the Azure resource can be deleted, Agent placeholder in ADO cannot be.\' })\\n  kind: \'AzureCLI\'\\n  identity: {\\n    type: \'UserAssigned\'\\n    userAssignedIdentities: {\\n      \'${usrmi.id}\': {}\\n    }\\n  }\\n\\n  properties: {\\n    azCliVersion: \'2.50.0\'\\n    retentionInterval: \'P1D\'\\n    timeout: \'PT30M\'\\n    arguments: \'${containerregistryName} ${imagename} ${poolName} ${resourceGroup().name} ${adourl} ${token} ${managedenvname} ${usrmi.id}\'\\n    scriptContent: \'\'\'\\n    az login --identity\\n    az extension add --name containerapp --upgrade --only-show-errors\\n    az containerapp job create -n \'placeholder\' -g $4 --environment $7 --trigger-type Manual --replica-timeout 300 --replica-retry-limit 1 --replica-completion-count 1 --parallelism 1 --image \\"$1.azurecr.io/$2\\" --cpu \\"2.0\\" --memory \\"4Gi\\" --secrets \\"personal-access-token=$6\\" \\"organization-url=$5\\" --env-vars \\"AZP_TOKEN=$6\\" \\"AZP_URL=$5\\" \\"AZP_POOL=$3\\" \\"AZP_PLACEHOLDER=1\\" \\"AZP_AGENT_NAME=dontdelete-placeholder-agent\\" --registry-server \\"$1.azurecr.io\\" --registry-identity \\"$8\\"  \\n    az containerapp job start -n \\"placeholder\\" -g $4\\n    \'\'\'\\n    cleanupPreference: \'OnSuccess\'\\n  }\\n  dependsOn: [\\n    arcbuild\\n    cnapps\\n    cnappsdiag\\n  ]\\n}\\n\\n// Define App Service Job resource for ADO agent\\nvar adoagentjobName = \'adoagentjob\'\\n\\nresource adoagentjob \'Microsoft.App/jobs@2023-05-01\' = {\\n  name: adoagentjobName\\n  location: location\\n  tags: tags\\n  identity: {\\n    type: \'UserAssigned\'\\n    userAssignedIdentities: {\\n      \'${usrmi.id}\': {}\\n    }\\n  }\\n  properties: {\\n    environmentId: cnapps.id\\n\\n    configuration: {\\n      triggerType: \'Event\'\\n\\n      secrets: [\\n        {\\n          name: \'personal-access-token\'\\n          keyVaultUrl: kvtokensecret.properties.secretUri\\n          identity: usrmi.id\\n        }\\n        {\\n          name: \'organization-url\'\\n          value: adourl\\n        }\\n        {\\n          name: \'azp-pool\'\\n          value: poolName\\n        }\\n      ]\\n      replicaTimeout: 1800\\n      replicaRetryLimit: 1\\n      eventTriggerConfig: {\\n        replicaCompletionCount: 1\\n        parallelism: 1\\n        scale: {\\n          minExecutions: 0\\n          maxExecutions: 10\\n          pollingInterval: 30\\n          rules: [\\n            {\\n              name: \'azure-pipelines\'\\n              type: \'azure-pipelines\'\\n\\n              // https://keda.sh/docs/2.11/scalers/azure-pipelines/\\n              metadata: {\\n                poolName: poolName\\n                targetPipelinesQueueLength: \'1\'\\n              }\\n              auth: [\\n                {\\n                  secretRef: \'personal-access-token\'\\n                  triggerParameter: \'personalAccessToken\'\\n                }\\n                {\\n                  secretRef: \'organization-url\'\\n                  triggerParameter: \'organizationURL\'\\n                }\\n              ]\\n            }\\n\\n          ]\\n        }\\n      }\\n      registries: [\\n        {\\n          server: containerregistry.properties.loginServer\\n          identity: usrmi.id\\n        }\\n      ]\\n    }\\n    template: {\\n      containers: [\\n        {\\n          image: \'${containerregistry.properties.loginServer}/adoagent:1.0\'\\n          name: \'adoagent\'\\n          env: [\\n            {\\n              name: \'AZP_TOKEN\'\\n              secretRef: \'personal-access-token\'\\n            }\\n            {\\n              name: \'AZP_URL\'\\n              secretRef: \'organization-url\'\\n            }\\n\\n            {\\n              name: \'AZP_POOL\'\\n              secretRef: \'azp-pool\'\\n            }\\n          ]\\n          resources: {\\n            cpu: 2\\n            memory: \'4Gi\'\\n          }\\n        }\\n      ]\\n    }\\n  }\\n  dependsOn: [\\n    arcplaceholder\\n    cnappsdiag\\n  ]\\n}\\n```\\n\\nWe will need our Azure DevOps token created earlier, the name of the Agent Pool and the Azure DevOps URL.\\n\\nWe can adjust the parameters to suit our environment and deploy.\\n\\n*The token is classified as a secure value, so although it is visible in plain text here, it will not be parsed through in the deployment logs.*\\n\\nVerify that the Resource Group and the User-Assigned managed identity exist with appropriate permissions.\\n\\n1. Open the **Codespace**\\n2. Navigate to the **IaC** folder and select main.bicep\\n3. Select the **[Deployment Pane](https://luke.geek.nz/azure/Azure-Bicep-Deploy-Pane/)** *(top right)*\\n4. Select your **Scope** - i.e. the Resource Group you will want to deploy to; you will need to log in with your Azure credentials\\n5. **Update the parameters**, such as your ADO URL *(make sure it doesn\'t include an end \'/\')*, token and Agent Pool, and add the name of your user assigned managed identity.\\n6. Click **Validate** to validate that Bicep code syntax is correct, then click **Deploy**\\n\\nThe bicep code will now do the following:\\n\\n* Create Azure Virtual Network\\n* Create Azure Key Vault and private link to the virtual network\\n* Create DNS zone for Key Vault\\n* Create a Container Registry and a private link to the virtual network\\n* Create DNS zone for container registry\\n* Places the token into a Key Vault secret\\n* Run a deployment script, which will build the Azure DevOps Agent Container image from the following docker file: [containerapps-selfhosted-agent/Dockerfile.azure-pipelines](https://github.com/lukemurraynz/containerapps-selfhosted-agent/blob/main/Dockerfile.azure-pipelines)\\n* Create the Consumption Container Apps environment\\n* Create a Log Analytics workspace and attach it to the Container Apps environment as a diagnostic setting\\n* Run a deployment script that runs the acrbuild command to deploy the placeholder Azure DevOps agent\\n* Creates the Azure Container Apps job, used for DevOps agents, with the azure-pipelines KEDA scaler configuration.\\n\\n![Create Azure DevOps - Agent Pool](/images/posts/Create_AzureContainerApps_BicepCodeSpace.gif)\\n\\n*In my testing, end-to-end deployment seemed to range between 12 to 15 minutes.*\\n\\nTo validate it worked, you can go into the Azure DevOps and Agent Pools, and you should now have a placeholder agent. This agent needs to stay to keep the Agent pool active but can be toggled to Disabled.\\n\\n![Create Azure DevOps - Agent Pool](/images/posts/AzureDevOps_ContainerApp_AgentPlaceholder.png)\\n\\nOnce deployed, you can go into the Container Registry, Networking blade and change Public network access to Disabled. You can also delete the DeploymentScript resources, as these are no longer required.\\n\\n![Public Access disabled - Container Registry](/images/posts/AzureContainerRegistry_PublicAccess_Disabled.png)\\n\\n## Testing\\n\\nTo test that it is working - I have deployed a Virtual Machine into the sharedservices subnet with RDP open internally.\\n\\nLet\'s import a test Azure pipeline to test that the event scaling is working and that the Container App job can communicate with internal resources.\\n\\n```yml title=\\"azure-pipelines.yml\\"\\ntrigger:\\n- main\\n\\npool:\\n  name: containerapp-adoagent\\n\\njobs:\\n- job: Setup\\n  displayName: Get Environment - Linux\\n\\n- job: GetIPAddress\\n  displayName: Get IP Address\\n  dependsOn: Setup\\n  steps:\\n  - script: echo Hello, world!\\n    displayName: \'Run a one-line script\'\\n\\n  - bash: |\\n      apt install net-tools -y\\n      Current_IP=$(curl ipinfo.io/ip)\\n      echo \\"Current IP address is: $Current_IP\\"\\n      echo \\"##vso[task.setvariable variable=IP_ADDR;isOutput=true]$Current_IP\\"\\n      ifconfig\\n      hostname\\n    displayName: Get IP on Linux\\n\\n- job: CheckRDP\\n  displayName: Check RDP Port  - PowerShell\\n  dependsOn: GetIPAddress\\n  steps:\\n  - script: |\\n      # Check if PowerShell is installed, and if not, install it\\n      if ! command -v pwsh &> /dev/null; then\\n          echo \\"PowerShell is not installed. Installing PowerShell...\\"\\n          apt-get install wget\\n          # Install PowerShell Core\\n          wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\\n          dpkg -i packages-microsoft-prod.deb\\n          apt-get update\\n          apt-get install -y powershell\\n      else\\n          echo \\"PowerShell is already installed.\\"\\n      fi\\n    displayName: Install PowerShell if not installed\\n\\n  - powershell: |\\n      $IP_ADDR = $env:IP_ADDR\\n      $Target_IP = \\"10.0.0.5\\"\\n      $Port = \'3389\'\\n      $connection = New-Object System.Net.Sockets.TcpClient($Target_IP, $Port)\\n      if ($connection.Connected) { Write-Host \\"Success\\"   } else {   Write-Host \\"Failed\\"  }\\n    displayName: Check RDP Port\\n```\\n\\n1. Login to your **[Azure DevOps](https://aex.dev.azure.com/)** organisation.\\n2. Navigate to a **repository** that you can test *(create one if it doesn\'t exist and initialize it)*.\\n3. Create a new file called **azure-pipelines.yml**, and copy the pipeline into it.\\n4. Note: I am not talking from experience or anything, but make sure you **update the RDP IP** to make sure it\'s a valid destination IP in the YML.\\n5. You can then navigate to **Pipelines**\\n6. **Import Pipeline**\\n7. **Run**\\n\\n![Run Azure DevOps - Agent Pool](/images/posts/Run_AzureContainerApps_Agent.gif)\\n\\nAs the Container App agent runs, the Container App Job will execute (in some cases, multiple job instances will be spawned to fulfil your pipeline needs across multiple parallel jobs and tasks), and the resources get spawned in Azure.\\n\\n## Logging\\n\\nThe Container App Environment logs are stored in the Log Analytics workspace.\\n\\n1. Login to the **Azure Portal**\\n2. Navigate to the **Log Analytics workspace** created *(i.e. law-jcdbxrheta7ug)*\\n3. Navigate to **Logs**\\n4. The default logs rules look for an incorrect table *(ContainerAppConsoleLogs_CL)*.\\n5. To retrieve the Logs, click on **Category** and uncollapse **Azure Resources**\\n\\nTable names are called:\\n\\n* ContainerAppSystemLogs\\n* ContainerAppConsoleLogs\\n\\n![Run Azure DevOps - Agent Pool](/images/posts/Run_AzureContainerApps_LogAnalytics.gif)\\n\\n## Additional Reading\\n\\n* A great tutorial exists to use Azure CLI to build the self-hosted Azure DevOps and GitHub Runners: [Tutorial: Deploy self-hosted CI/CD runners and agents with Azure Container Apps jobs](https://learn.microsoft.com/azure/container-apps/tutorial-ci-cd-runners-jobs?tabs=bash&pivots=container-apps-jobs-self-hosted-ci-cd-azure-pipelines&WT.mc_id=AZ-MVP-5004796)\\n* [Jobs in Azure Container Apps](https://learn.microsoft.com/azure/container-apps/jobs?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796#event-driven-jobs)\\n* [KEDA - Kubernetes Event-driven Autoscaling](https://keda.sh/)"},{"id":"azure/Azure-Bicep-Deployment-with-Deployment-Stacks","metadata":{"permalink":"/azure/Azure-Bicep-Deployment-with-Deployment-Stacks","source":"@site/blog/2023-08-28-Azure-Bicep-Deployment-with-Deployment-Stacks.md","title":"Azure Bicep Deployment with Deployment Stacks","description":"Deployment Stacks! What is it? insert confused look*","date":"2023-08-28T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":13.055,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Bicep Deployment with Deployment Stacks","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/Blog-Header-AzureDeploymentStacks.gif"},"date":"2023-08-28 00:00:00 +1300","slug":"azure/Azure-Bicep-Deployment-with-Deployment-Stacks"},"unlisted":false,"prevItem":{"title":"Get Ahead with Self-Hosted Agents and Container Apps Jobs","permalink":"/azure/hosted-agents-container-apps-job"},"nextItem":{"title":"Azure Bicep - Deploy Pane","permalink":"/azure/Azure-Bicep-Deploy-Pane"}},"content":"[Deployment Stacks](https://learn.microsoft.com/azure/azure-resource-manager/bicep/deployment-stacks?tabs=azure-powershell&WT.mc*id=AZ-MVP-5004796)! What is it? *insert confused look*\\n\\nMaybe you have been browsing the Microsoft Azure Portal and noticed a new section in the management blade called: Deployment stacks and wondered what it was, and how you can use it.\\n\\nLet us take a look!\\n\\n> Before we get started its worth noting that as of the time of this article, this feature is under Public Preview. Features or ways of working with Deployment Stacks may change, when it becomes generally available. If you run into issues, make sure you have a look at the current [known issues](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/deployment-stacks?tabs=azure-powershell&WT.mc*id=AZ-MVP-5004796#known-issues).\\n\\n![Automate your Azure Bicep deployment with ease using Deployment Stacks](/images/posts/Blog-Header-AzureDeploymentStacks.gif)\\n\\n##### Overview\\n\\n[Azure Deployment Stacks](https://learn.microsoft.com/azure/azure-resource-manager/bicep/deployment-stacks?tabs=azure-powershell&WT.mc*id=AZ-MVP-5004796) are a type of Azure resource that allows you to manage a group of Azure resources as an atomic unit.\\n\\nWhen you submit a Bicep file or an ARM template to a deployment stack, it defines the resources that are managed by the stack.\\n\\nYou can create and update deployment stacks using Azure CLI, Azure PowerShell, or the Azure portal along with Bicep files. These Bicep files are transpiled into ARM JSON templates, which are then deployed as a deployment object by the stack.\\n\\nDeployment stacks offer additional capabilities beyond regular deployment resources, such as simplified provisioning and management of resources, preventing undesired modifications to managed resources, efficient environment cleanup, and the ability to utilize standard templates like Bicep, ARM templates, or Template specs.\\n\\n> When planning your deployment and determining which resource groups should be part of the same stack, it\'s important to consider the management lifecycle of those resources, which includes creation, updating, and deletion. For instance, suppose you need to provision some test VMs for various application teams across different resource group scopes.\\n\\n##### Comparisons\\n\\nBefore we dig into it further, it may help to give you a comparison between the different products and where Deployment Stacks, could be used, lets us take a look at a comparison, between similar products, that may come to mind, such as:\\n\\n* Azure Blueprints\\n* Bicep *(on its own)*\\n* Template Specs\\n* Terraform\\n\\n| Feature                 | Deployment Stacks | Azure Blueprints | Using Bicep | Template Specs | Terraform |  \\n|-------------------------|------------------------|------------------|-------------|----------------|-----------|  \\n| Management of Resources | Manages a group of Azure resources as an atomic unit. | Defines and deploys a repeatable set of Azure resources that adhere to organizational standards. | Defines and deploys Azure resources using a declarative language. | Defines and deploys reusable infrastructure code using template specs. | Defines and provisions infrastructure resources across various cloud providers using a declarative language. |  \\n| Resource Definition     | Bicep files or ARM JSON templates are used to define the resources managed by the stack. | Blueprint artifacts, including ARM templates, policy assignments, role assignments, and resource groups, are used to define the blueprint. | Bicep files are used to define the Azure resources. | Template specs are used to define reusable infrastructure code. | Terraform configuration files are used to define the infrastructure resources. |  \\n| Access Control          | Access to the deployment stack can be restricted using Azure role-based access control (Azure RBAC). | Access to blueprints is managed through Azure role-based access control (Azure RBAC). | Access to Azure resources is managed through Azure role-based access control (Azure RBAC). | Access to template specs is managed through Azure role-based access control (Azure RBAC). | Access to cloud resources is managed through provider-specific authentication mechanisms. |  \\n| Benefits                | *Simplified provisioning and management of resources as a cohesive entity.*  Preventing undesired modifications to managed resources.*Efficient environment cleanup.*Utilizing standard templates such as Bicep, ARM templates, or Template specs for your deployment stacks. | *Rapidly build and start up new environments with organizational compliance.*  Built-in components for speeding up development and delivery. | *Easier management and deployment of Azure resources.*Improved readability and understanding of resource configurations. | *  Publish libraries of reusable infrastructure code. | *  Infrastructure-as-Code approach for provisioning resources across multiple cloud providers. |  \\n| Deprecation              | N/A | Azure Blueprints (Preview) will be deprecated. | N/A | N/A | N/A |  \\n\\n*It is always recommended to refer to the [official documentation](https://learn.microsoft.com/?WT.mc*id=AZ-MVP-5004796) for the most up-to-date and comprehensive information. The comparison table above, was created with the help of AI.*\\n\\nIt is hard to do a complete comparison, as always \'it depends\' on your use cases and requirements, but hopefully this makes it clear where Deployment Stacks come into play (and it does not replace Bicep but works with it for better governance), with out-of-the-box benefits such as:\\n\\n* Simplified provisioning and management of resources across different scopes as a cohesive entity.\\n* Preventing undesired modifications to managed resources through deny settings.\\n* Efficient environment cleanup by employing delete flags during deployment stack updates.\\n* Utilizing standard templates such as Bicep, ARM templates, or Template specs for your deployment stacks.\\n\\n> The key here is that Azure Deployment Stacks, is a native way to treat your infrastructure components as an atmonic unit or stack, so you manage the lifecycle of the resources as a whole vs every resource separately.\\n\\n##### Using Deployment Stacks\\n\\nDeployment stacks requires [Azure PowerShell](https://learn.microsoft.com/powershell/azure/install-azure-powershell?WT.mc*id=AZ-MVP-5004796) *(version 10.1.0 or later)* or [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli?WT.mc*id=AZ-MVP-5004796) *(version 2.50.0 or later)*.\\n\\nFor the purposes of this article, I will be using PowerShell.\\n\\n###### PowerShell\\n\\nOnce you have the latest Azure PowerShell modules, its time to take a look at the cmdlets, that are offered to us for Deployment Stacks.\\n\\nOpen your PowerShell terminal and type in:\\n\\n```\\nGet-Command -Name *DeploymentStack*\\n```\\n\\n![Get-Command -Name *DeploymentStack*](/images/posts/DeploymentStacks-PowerShellCmdlets.gif)\\nAs you can see, there are a range of cmdlets we have to work with.\\n\\nFor the purpose of this article, I will be using a Bicep file, I already have on hand (unmodified for Deployment Stacks). This bicep file will create:\\n\\n* 2 Virtual Networks\\n* 4 Subnets *(2 subnets in each Virtual Network)*\\n* 4 NSGs *(and assign to each subnet, with Deny All rules)*\\n* Then finally, peer the virtual networks.\\n\\nThis is the Bicep file:\\n\\n```bicep title=\\"main.bicep\\"\\n@description(\'Name of the virtual network.\')\\nparam vnetName string = \'myVnet\'\\n\\n@description(\'Name of the first subnet.\')\\nparam subnet1Name string = \'subnet1\'\\n\\n@description(\'Name of the second subnet.\')\\nparam subnet2Name string = \'subnet2\'\\n\\n@description(\'Name of the first network security group.\')\\nparam nsg1Name string = \'nsg1\'\\n\\n@description(\'Name of the second network security group.\')\\nparam nsg2Name string = \'nsg2\'\\n\\n@description(\'Name of the second virtual network.\')\\nparam vnet2Name string = \'myVnet2\'\\n\\n@description(\'Name of the third subnet.\')\\nparam subnet3Name string = \'subnet3\'\\n\\n@description(\'Name of the fourth subnet.\')\\nparam subnet4Name string = \'subnet4\'\\n\\n@description(\'Name of the third network security group.\')\\nparam nsg3Name string = \'nsg3\'\\n\\n@description(\'Name of the fourth network security group.\')\\nparam nsg4Name string = \'nsg4\'\\n\\n@description(\'Location for all resources.\')\\nparam location string = resourceGroup().location\\n\\nresource vnet \'Microsoft.Network/virtualNetworks@2023-04-01\' = {\\n  name: vnetName\\n  location: location\\n  properties: {\\n    addressSpace: {\\n      addressPrefixes: [\\n        \'10.0.0.0/16\'\\n      ]\\n    }\\n  }\\n}\\n\\nresource subnet1 \'Microsoft.Network/virtualNetworks/subnets@2023-04-01\' = {\\n  parent: vnet\\n  name: subnet1Name\\n  properties: {\\n    addressPrefix: \'10.0.1.0/24\'\\n    networkSecurityGroup: {\\n      id: resourceId(\'Microsoft.Network/networkSecurityGroups\', nsg1Name)\\n    }\\n  }\\n}\\n\\nresource subnet2 \'Microsoft.Network/virtualNetworks/subnets@2023-04-01\' = {\\n  parent: vnet\\n  name: subnet2Name\\n  properties: {\\n    addressPrefix: \'10.0.2.0/24\'\\n    networkSecurityGroup: {\\n      id: resourceId(\'Microsoft.Network/networkSecurityGroups\', nsg2Name)\\n    }\\n  }\\n}\\n\\nresource nsg1 \'Microsoft.Network/networkSecurityGroups@2023-04-01\' = {\\n  name: nsg1Name\\n  location: location\\n  properties: {\\n    flushConnection: false\\n    securityRules: [\\n\\n      {\\n        name: \'Deny-All-Inbound\'\\n        properties: {\\n          priority: 4096\\n          access: \'Deny\'\\n          direction: \'Inbound\'\\n          destinationPortRange: \'*\'\\n          protocol: \'Tcp\'\\n          sourcePortRange: \'*\'\\n          destinationAddressPrefix: \'*\'\\n          sourceAddressPrefix: \'*\'\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nresource nsg2 \'Microsoft.Network/networkSecurityGroups@2023-04-01\' = {\\n  name: nsg2Name\\n  location: location\\n  properties: {\\n    flushConnection: false\\n    securityRules: [\\n\\n      {\\n        name: \'Deny-All-Inbound\'\\n        properties: {\\n          priority: 4096\\n          access: \'Deny\'\\n          direction: \'Inbound\'\\n          destinationPortRange: \'*\'\\n          protocol: \'Tcp\'\\n          sourcePortRange: \'*\'\\n          destinationAddressPrefix: \'*\'\\n          sourceAddressPrefix: \'*\'\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nresource vnet2 \'Microsoft.Network/virtualNetworks@2023-04-01\' = {\\n  name: vnet2Name\\n  location: location\\n  properties: {\\n    addressSpace: {\\n      addressPrefixes: [\\n        \'10.1.0.0/16\'\\n      ]\\n    }\\n  }\\n}\\n\\nresource subnet3 \'Microsoft.Network/virtualNetworks/subnets@2023-04-01\' = {\\n  parent: vnet2\\n  name: subnet3Name\\n  properties: {\\n    addressPrefix: \'10.1.1.0/24\'\\n    networkSecurityGroup: {\\n      id: resourceId(\'Microsoft.Network/networkSecurityGroups\', nsg3Name)\\n    }\\n  }\\n}\\n\\nresource subnet4 \'Microsoft.Network/virtualNetworks/subnets@2023-04-01\' = {\\n  parent: vnet2\\n  name: subnet4Name\\n  properties: {\\n    addressPrefix: \'10.1.2.0/24\'\\n    networkSecurityGroup: {\\n      id: resourceId(\'Microsoft.Network/networkSecurityGroups\', nsg4Name)\\n    }\\n  }\\n}\\n\\nresource nsg3 \'Microsoft.Network/networkSecurityGroups@2023-04-01\' = {\\n  name: nsg3Name\\n  location: location\\n  properties: {\\n    flushConnection: false\\n    securityRules: [\\n\\n      {\\n        name: \'Deny-All-Inbound\'\\n        properties: {\\n          priority: 4096\\n          access: \'Deny\'\\n          direction: \'Inbound\'\\n          destinationPortRange: \'*\'\\n          protocol: \'Tcp\'\\n          sourcePortRange: \'*\'\\n          destinationAddressPrefix: \'*\'\\n          sourceAddressPrefix: \'*\'\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nresource nsg4 \'Microsoft.Network/networkSecurityGroups@2023-04-01\' = {\\n  name: nsg4Name\\n  location: location\\n  properties: {\\n    flushConnection: false\\n    securityRules: [\\n\\n      {\\n        name: \'Deny-All-Inbound\'\\n        properties: {\\n          priority: 4096\\n          access: \'Deny\'\\n          direction: \'Inbound\'\\n          destinationPortRange: \'*\'\\n          protocol: \'Tcp\'\\n          sourcePortRange: \'*\'\\n          destinationAddressPrefix: \'*\'\\n          sourceAddressPrefix: \'*\'\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nresource vnetPeering \'Microsoft.Network/virtualNetworks/virtualNetworkPeerings@2023-04-01\' = {\\n  parent: vnet\\n  name: vnet2Name\\n  properties: {\\n    remoteVirtualNetwork: {\\n      id: vnet2.id\\n    }\\n    allowVirtualNetworkAccess: true\\n    allowForwardedTraffic: false\\n    allowGatewayTransit: false\\n    useRemoteGateways: false\\n  }\\n  dependsOn: [\\n    subnet1\\n    subnet3\\n  ]\\n}\\n\\nresource vnetPeering2 \'Microsoft.Network/virtualNetworks/virtualNetworkPeerings@2023-04-01\' = {\\n  parent: vnet2\\n  name: vnetName\\n  properties: {\\n    remoteVirtualNetwork: {\\n      id: vnet.id\\n    }\\n    allowVirtualNetworkAccess: true\\n    allowForwardedTraffic: false\\n    allowGatewayTransit: false\\n    useRemoteGateways: false\\n  }\\n  dependsOn: [\\n    subnet2\\n    subnet4\\n    vnetPeering\\n  ]\\n}\\n```\\n\\nI have already deployed a new Resource Group to deploy our virtual network into:\\n\\n```\\nNew-AzResourceGroup -Name \'rg-network\' -Location \'Australia East\'\\n```\\n\\nSo let us create our first Deployment Stack!\\n\\n###### New-AzResourceGroupDeploymentStack\\n\\nThe \'New-AzResourceGroupDeploymentStack\' cmdlet is the first one we will look into.\\n\\nLet us look at the most common syntax that you may use:\\n\\n```\\nNew-AzResourceGroupDeploymentStack -Name \\"<deployment-stack-name>\\" -TemplateFile \\"<bicep-file-name>\\" -DeploymentResourceGroupName \\"<resource-group-name>\\" -DenySettingsMode \\"none\\"\\n```\\n\\n| Parameter                  | Description                                                                                                                                                                                                                   |  \\n|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  \\n| `-Name`                    | Specifies the name of the deployment stack.                                                                                                                                                                                   |  \\n| `-Location`                | Specifies the Azure region where the deployment stack will be created. This is valid for Subscription based DeploymentStacks.                                                                                                                                                        |  \\n| `-TemplateFile`            | Specifies the Bicep file that defines the resources to be managed by the deployment stack.                                                                                                                                    |  \\n| `-DeploymentResourceGroupName` | Specifies the name of the resource group where the managed resources will be stored.                                                                                                                                         |  \\n| `-DenySettingsMode`        | Specifies the operations that are prohibited on the managed resources to safeguard against unauthorized deletion or updates. Possible values include \\"none\\", \\"DenyDelete\\", \\"DenyWriteAndDelete\\".                           |  \\n| `-DeleteResources`         | Deletes the managed resources associated with the deployment stack.                                                                                                                                                            |  \\n| `-DeleteAll`               | Deletes all deployment stacks and their associated resources.                                                                                                                                                                  |  \\n| `-DeleteResourceGroups`    | Deletes the resource groups associated with the deployment stacks.                                                                                                                                                            |  \\n\\nThese parameters allow you to customize the creation and management of deployment stacks.\\n\\nThe DenySettingsMode parameter is used in Azure Deployment Stacks to assign specific permissions to managed resources, preventing their deletion by unauthorized security principals, this is a key differentiator to some of the other solutions mentioned earlier, but it does mean you need to think about how your resources will be managed, let us take a look at the DenySettingsMode a bit deeper.\\n\\nThe DenySettingsMode parameter accepts different values to define the level of deny settings. Some of the possible values include:\\n\\n* \\"none\\": No deny settings are applied, allowing all operations on the managed resources.\\n* \\"DenyDelete\\": Denies the delete operation on the managed resources, preventing their deletion.\\n* \\"DenyWriteAndDelete\\": Denies all operations on the managed resources, preventing any modifications or deletions.\\n\\nBy specifying the appropriate DenySettingsMode value, you can control the level of permissions and restrictions on the managed resources within the deployment stack.\\n\\nFor our testing, we will deploy our Azure Virtual Networks, NSGs to a new Deployment Stack, using the DenyDelete DenySettingMode.\\n\\n```\\n$RGName = \'rg-network\'\\n$DenySettings = \'DenyDelete\'\\n$BicepFileName = \'main.bicep\'\\n$DeploymentStackName = \'NetworkProd\'\\n\\nNew-AzResourceGroupDeploymentStack -Name $DeploymentStackName -TemplateFile $BicepFileName -ResourceGroupName $RGName -DenySettingsMode $DenySettings -DenySettingsApplyToChildScopes\\n```\\n\\n![New-AzResourceGroupDeploymentStack](/images/posts/DeploymentStacks-NewAzResourceGroupDeployment.gif)\\n\\nAs you can see, creating a new Azure Deployment Stack is easy, with no adjustments to the underlying Bicep configuration needed.\\n\\n*Note: If you get an error, that the cmdlet is missing -Name parameter, make sure that the -ResourceGroupName parameter has been added.*\\n\\nIf we navigate to the Azure Portal, we can see the Deployment Stack natively, including the Stack properties, such as what are the actions if resources are removed, what the denyDelete mode is.\\n\\n![New-AzResourceGroupDeploymentStack](/images/posts/DeploymentStacks-AzurePortalOverview.gif)\\n\\n###### Testing Deny-Assignment\\n\\nAs we deployed our virtual networks, using the denyDelete assignment, lets take a look and attempt to delete a Network Security Group, before we do that we need to dissociate it from the subnet.\\n\\n*Note: Its worth noting my permissions are: Owner.*\\n\\nWhen I attempted to delete a Network Security Group I get the error below:\\n\\n> Failed to delete network security group \'nsg1\'. Error: The client \'************\' with object id \'cb059544-e63c-4543-930f-4b6e6b7aece1\' has permission to perform action \'Microsoft.Network/networkSecurityGroups/delete\' on scope \'rg-network/providers/Microsoft.Network/networkSecurityGroups/nsg1\'>nsg1\'; however, the access is denied because of the deny assignment with name \'Deny assignment \'55ebfe82-255d-584a-8579-0e0c9f0219ff\' created by Deployment Stack \'/subscriptions/f0ee3c31-ff51-4d47-beb2-b1204a511f63\'.\\n\\n![Azure Deployment Stack - Delete Resource Test](/images/posts/DeploymentStacks-AzurePortal-DenyAssignmentTest.gif)\\n\\nTo delete the resource, I would need to, do one of the following:\\n\\n* Delete the Deployment Stack (and detach the resources and delete it manually)\\n* Delete the Deployment Stack (and delete all the resources)\\n* Remove from the bicep code and update deployment stack.\\n\\nNote: In our testing, we were able to disassociate the Network Security Group, from the Subnet, because when the deployment stack was deployed - it was with the: denyDelete assignment, not the:\'DenyWriteAndDelete\'.\\n\\n###### Redeploy - Deployment Stack (Portal)\\n\\nUsing the Azure Portal, we can Edit and re-deploy our existing Deployment stack, if you have changes or resources that you may want to roll back:\\n\\n![Azure Deployment Stack - Delete Resource Test](/images/posts/DeploymentStacks-AzurePortal-RedeployDeploymentStack.gif)\\n\\n###### Redeploy - Deployment Stack (Bicep)\\n\\nWhat if we want to make further changes, such as removing resources from our Deployment Stack?\\n\\nIn this example, we will modify our bicep code to remove the second Virtual network, subnets and associated NSGs (Network Security Groups), and remove the resources from Azure completely *(we can unattach them, which will remove them from being managed by the deployment stack)*, but I want my Virtual Network resources to be managed completely by Bicep.\\n\\nWe could use the: Save-AzResourceGroupDeploymentStackTemplate, to save the Deployment Stack to an ARM template, if we wanted to deploy it later.\\n\\n*Note: In the bicep code example supplied earlier I removed everything after NSG2.*\\n\\nWe will run the Set-AzResourceGroupDeploymentStack, pointing to the modified bicep code:\\n\\n```\\n$RGName = \'rg-network\'\\n$DenySettings = \'DenyWriteAndDelete\'\\n$BicepFileName = \'main.bicep\'\\n$DeploymentStackName = \'NetworkProd\'\\n\\nSet-AzResourceGroupDeploymentStack -Name $DeploymentStackName -ResourceGroupName $RGName -TemplateFile $BicepFileName -DenySettingsMode $DenySettings -DeleteResources -Verbose -DenySettingsApplyToChildScopes\\n```\\n\\nIn this example, we tell Deployment Stacks to Delete Resources that are no longer part of the stack, and this time we will add the Verbose flag, so we can see what it is doing.\\n\\n![Azure Deployment Stack - Delete Resource Test](/images/posts/DeploymentStacks-PowerShell-RedeployDeploymentStack.gif)\\n\\n*Note: I cut the gif, thats why the timestamps don\'t match, or you could be spending 10 minutes staring at the verbose output.*\\n\\nIf we navigate to the Azure Portal, we can see the deleted resources listed in the Deployment stack history *(only displays the last Deployment stack changes vs keeping a history of everything)*, and the Resource un-managed state has changed to: delete.\\n\\n![Azure Deployment Stack - Delete Resource Test](/images/posts/DeploymentStacks-PowerShell-RedeployDeploymentStackOverview.gif)\\n\\nNote: A manually created Virtual Network in the same Resource Group (but not part of the deployment stack) remained untouched.\\n\\n*I forgot to update, the DenySettings variable, so once I re-deployed with the \'DenyWriteAndDelete\' instead of: \'DenyDelete\'. I was unable to disassociate my Network Security Group.*\\n\\n![Azure Deployment Stack - Delete Resource Test](/images/posts/DeploymentStacks-Portal-NSG_Modification.gif)\\n\\n###### Permissions\\n\\nI have \'Owner\' rights over my own demo subscriptions, so a bit more flexibility than I would have in a Production environment.\\n\\nYou can add [exclusions](https://learn.microsoft.com/azure/azure-resource-manager/bicep/deployment-stacks?tabs=azure-powershell&WT.mc_id=AZ-MVP-5004796#protect-managed-resources-against-deletion) to your Deployment Stack, allowing certain principals or actions to be completed.\\n\\nYou could also create custom role (Microsoft.Resources/deploymentStacks) to be able to Read, Update or delete deployment stacks, giving you the flexibility to allow people to modify their own stacks and redeploy, without accessing to other tooling required and self-service functionality, such as being able to give someone a deployment stack, that the users can then delete the resources and redeploy later straight from the Azure Portal when required for testing."},{"id":"azure/Azure-Bicep-Deploy-Pane","metadata":{"permalink":"/azure/Azure-Bicep-Deploy-Pane","source":"@site/blog/2023-08-18-Azure Bicep - Deploy Pane.md","title":"Azure Bicep - Deploy Pane","description":"Working with Azure Bicep using Visual Studio Code, is as native as an experience as you can get, made even better by the Bicep Visual Studio Code extension.","date":"2023-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.325,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Bicep - Deploy Pane","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/Header-AzureBicepDeployPane.gif"},"date":"2023-08-18 00:00:00 +1300","slug":"azure/Azure-Bicep-Deploy-Pane"},"unlisted":false,"prevItem":{"title":"Azure Bicep Deployment with Deployment Stacks","permalink":"/azure/Azure-Bicep-Deployment-with-Deployment-Stacks"},"nextItem":{"title":"Coding on the Cloud - Getting Started with GitHub Codespaces","permalink":"/azure/Getting-Started-with-GitHub-Codespaces"}},"content":"Working with [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/overview?tabs=bicep&WT.mc_id=AZ-MVP-5004796) using Visual Studio Code, is as native as an experience as you can get, made even better by the [Bicep Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-bicep).\\n\\nThe Bicep Visual Studio Code extension keeps evolving, with a recent (Experimental) feature being added called the **Deploy Pane**.\\n\\n> The Deployment Pane is a UI panel in VSCode that allows you to connect to your Azure subscription and execute validate, deploy & whatif operations and get instant feedback without leaving the editor.\\n\\n![Azure Bicep - Deploy Pane](/images/posts/VisualStudioCode_DeployPane.png)\\n\\nThe Deploy Pane, brings together some key actions:\\n\\n* Deploy\\n* Validate\\n* What-If\\n\\nThe Deploy step, will deploy the Bicep file using the Subscription Scope and ID specified in the pane.\\nThe validate step, will validate that the Bicep syntax is correct for the Azure Resource Manager to process the template.\\nThe What-If step, will let you know what it will deploy and what changes will be made, without having to deploy or touch any resources.\\n\\nTo enable the new Experimental Feature, make sure you are running the latest version of both Bicep, and the Bicep Visual Studio Code extension.\\n\\n1. Click on **Settings**\\n2. Expand **Extensions**\\n3. Navigate to: **Bicep**\\n4. Check the box labelled: **Experimental: Deploy Pane**\\n\\n![Azure Bicep - Deploy Pane](/images/posts/Bicep-DeploymentPreviewPane.gif)\\n\\nOnce enabled, you will see the new Deploy Pane, appear in the top right of your Visual Studio code interface, next to Bicep Visualizer, once you have a Bicep file loaded.\\n\\n![Azure Bicep - Deploy Pane](/images/posts/VisualStudioCode_DeployPaneIcon.png)\\n\\nIf you have any feedback regarding this extension, make sure to add your feedback to the [azure/bicep issues](https://github.com/Azure/bicep/issues)"},{"id":"azure/Getting-Started-with-GitHub-Codespaces","metadata":{"permalink":"/azure/Getting-Started-with-GitHub-Codespaces","source":"@site/blog/2023-08-15-Getting-Started-with-GitHub-Codespaces.md","title":"Coding on the Cloud - Getting Started with GitHub Codespaces","description":"Github Codespaces gives you the full capability of your favourite IDE (Integrated Development Environment) like Visual Studio Code, Jupyter, or JetBrains and an extension, to your web browser. With it, you can develop your applications without needing any dependant service or tool installed or configured locally, giving developers a standard way of working on applications and scripts.","date":"2023-08-15T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":16.72,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Coding on the Cloud - Getting Started with GitHub Codespaces","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/Header_Getting_Started_with_GitHub_Codespace.gif"},"date":"2023-08-15 00:00:00 +1300","slug":"azure/Getting-Started-with-GitHub-Codespaces"},"unlisted":false,"prevItem":{"title":"Azure Bicep - Deploy Pane","permalink":"/azure/Azure-Bicep-Deploy-Pane"},"nextItem":{"title":"Move an Azure public IP to another subscription","permalink":"/azure/Move-Azure-Public-IP-to-another-subscription"}},"content":"[Github Codespaces](https://github.com/features/codespaces) gives you the full capability of your favourite IDE (Integrated Development Environment) like Visual Studio Code, Jupyter, or JetBrains and an extension, to your web browser. With it, you can develop your applications without needing any dependant service or tool installed or configured locally, giving developers a standard way of working on applications and scripts.\\n\\n![Github Codespaces - Getting Started](/images/posts/Header_Getting_Started_with_GitHub_Codespace.gif)\\n\\nGithub Codespaces does this by leveraging the power of the Cloud and GitHub to run containers that you can personalize to run your IDE, extensions, and any dependencies that you may need, whether you are a developer needing Python, apache, react, or a devops engineer requiring Bicep and Terraform support, Codespaces is an ideal enabler for our toolkit, in fact, this article was written in a Github Codespace, using Visual Studio Code and Markdown extensions.\\n\\n> A codespace is a development environment that\'s hosted in the cloud. You can customize your project for GitHub Codespaces by configuring dev container files to your repository (often known as Configuration-as-Code), creating a repeatable codespace configuration for all project users.\\n> GitHub Codespaces run on various VM-based compute options hosted by GitHub.com, which you can configure from 2-core machines up to 32-core machines. You can connect to your codespaces from the browser or locally using an IDE like Visual Studio Code or IntelliJ.\\n\\nLet\'s delve into [Github Codespaces](https://github.com/features/codespaces) a bit more!\\n\\n#### Introduction\\n\\nGitHub Codespaces is a cloud-based development environment provided by GitHub, designed to enhance the coding experience and streamline collaboration for developers. It allows you to create, manage, and access your development environments directly from your web browser. With GitHub Codespaces, you can code, build, test, and debug your projects without setting up complex local development environments on your machine.\\n\\n![Github Codespaces](/images/posts/github_codespaces.png)\\n\\nKey features and benefits of GitHub Codespaces include:\\n\\n* Access Anywhere: You can access your coding environment from anywhere with an internet connection. This is particularly useful for remote work, collaboration, and coding on the go.\\n* Consistency: Codespaces ensure consistency across development environments, which can help avoid the \\"it works on my machine\\" issue often encountered in software development.\\n* Collaboration: Multiple developers can collaborate on the same Codespace simultaneously, making it easier to pair programs, review code, and troubleshoot together in real-time.\\n* Isolation: Each project or repository can have its own isolated Codespace, preventing conflicts between dependencies and configurations.\\n* Quick Setup: Setting up a development environment is quick and easy. You don\'t need to spend time installing and configuring software locally.\\n* Configurability: Codespaces can be customized with extensions, tools, and settings to match your preferred development environment.\\n* Scalability: GitHub Codespaces can scale according to your needs, making it suitable for individual developers and larger teams.\\n* Version Control Integration: Codespaces is tightly integrated with GitHub repositories, making it seamless to switch between your code and the development environment.\\n* Security: Codespaces offer a secure environment for development, as it doesn\'t store sensitive data and is protected by GitHub\'s security measures.\\n* Project Setup: Codespaces can be configured to automatically set up a project with required dependencies and tools, reducing the time needed to get started.\\n\\nGithub Codespaces went into [general availability](https://azure.microsoft.com/updates/general-availability-github-codespaces/?WT.mc_id=AZ-MVP-5004796) on August 2021 and is built on top of the [devcontainers](https://containers.dev/) schema.\\n\\n#### Prerequisites\\n\\nYou need a [Github](https://github.com/) account to use GitHub Codespaces.\\n\\nGitHub Codespaces are available for developers in every organization. However, organizations can choose whether codespaces created from their repositories will be user-owned or organization-owned.. All personal GitHub.com accounts include a monthly quota of free usage each month.\\n\\nGitHub will provide **users in the Free plan 120 core hours, or 60 hours of run time on a two core codespace, plus 15 GB of storage each month**.\\n\\nFor further pricing information, make sure you review:\\n\\n* [About billing for GitHub Codespaces](https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces)\\n* [Pricing calculator](https://github.com/pricing/calculator)\\n\\n> Pricing, features and offerings could change at any time. For the most up-to-date information, make sure you review the documentation on github.com.\\n\\nTo use GitHub Codespaces, you need an active repository; by default, Github Codespaces is configured for the repository you set.\\n\\nYou will also need a [supported browser](https://docs.github.com/en/get-started/using-github/supported-browsers) (the latest versions of Chrome, Edge, Firefox, or Safari are recommended) to view your IDE; in this article, we will be using Visual Studio Code.\\n\\n#### Setting Up GitHub Codespaces\\n\\nGithub Codespaces can be accessed directly from the GitHub interface.\\n\\n1. **Navigate** to a new **repository**\\n2. Click **Code**\\n3. Click **+** in the Codespaces tab to open a new **Codespace** on your repo, by default a Visual Studio Code instance will open in your browser; note the \'funky\' name and URL that create to give you a unique container.\\n\\n_Note: Don\'t worry; nothing is saved to your repository unless you want to commit any changes._\\n\\nYour Codespace is now started and running in a default GitHub-supplied development container.\\n\\n> A development container is a running Docker container with a well-defined tool/runtime stack and its prerequisites.\\n\\n![Github Codespaces - Run](/images/posts/OpenCodespace.gif)\\n\\n#### Exploring the Interface\\n\\nOnce you have your Codespace running, you have access to most native [Visual Studio Code](https://code.visualstudio.com/) capability\'s and all the files in your repository.\\n\\n![Github Codespaces - Overview](/images/posts/Codespaces_VSCode_Overview.png)\\n\\nWe now have our workspace, consisting of Visual Studio code, running in your own docker container! The Host field _(lower left)_ indicates that you are running in a Codespace.\\n\\nOut of the box, Visual Studio Code has git integration, so you can easily commit any changes to the repository as you would if you were working from your local development workstation - this is critical to remember when making a change to your devcontainer configuration - you have to commit it before you can rebuild or you will lose your configuration (we will get to this further in the article).\\n\\nAs its runs in a hosted container, you can switch easily between computers and browsers by opening the Codespace (the same way you created your Codespace, but instead selecting an already running instance) or copying the URL of your Codespace and log back into Github on another computer to go directly to the container instance:\\n\\n![Github Codespaces - Run](/images/posts/Github_Codespaces_OpenRunning.png)\\n\\nIf you leave your Codespace running without interaction, or if you exit your codespace without explicitly stopping it, the codespace will timeout after a period of inactivity and stop running. You can [adjust the timeout](https://docs.github.com/en/codespaces/customizing-your-codespace/setting-your-timeout-period-for-github-codespaces) of your codespace to a maximum of 240 minutes (4 hours) for new Codespaces, but keep in mind you will be charged unless the Codespace is stopped. If the Codespace remains inactive for some time, it could be deleted. You should get an email notification before this happens, but I suggest keeping an eye on your Codespace and ensuring it\'s only running when needed.\\n\\n> Warning: Codespaces compute usage is billed for the duration a codespace is active. If you\'re not using a codespace that remains running and hasn\'t yet timed out, you are billed for the total time that the codespace was active, irrespective of whether you were using it. For more information, see \\"[About billing for GitHub Codespaces](https://docs.github.com/en/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces#codespaces-pricing).\\"\\n\\nAs with any Visual Studio Code instance, you can also log in to your GitHub account to pull your settings and extensions, but to keep things clean and distraction-free, you can customize your Codespace instead for only what you or others working in the same repository need.\\n\\n#### Customizing Your Codespace\\n\\nYou can customize your Codespace to suit the project you are working on; some examples I use personally are:\\n\\n* Markdown editing _(for example, my website is hosted on Github Pages, and the formatting is done using Markdown, so I have a devcontainer preconfigured to run Markdown Visual Studio Code extensions and linting, so as soon as I open it - its good to go!)_\\n* Infrastructure as Code development _(I have preconfigured devcontainer, running on a container, that has the latest version of PowerShell, Terraform, Bicep installed and relevant Visual Studio extensions)_\\n\\nI used to install everything locally, to the point when I would be reinstalling Windows every few months. To keep my device clean, I moved to an Azure Virtual Desktop host as my development environment, but Codespaces now give me the flexibility to install what I need (when I need it) within a Linux environment, and I know when I rebuild the Codespace, I will have the latest libraries.\\n\\nThere are a lot of customisation you can do, we won\'t be able to cover all possible customisations in this article, but I aim to cover the basics to get you up and running!\\n\\n##### Setting Sync\\n\\nBefore delving into some of the customisation of the devcontainer configuration itself, let us remember the Visual Studio Code settings sync.\\n\\nIf you are someone who works on the same products and services and has invested time in configuring Visual Studio profiles, there\'s nothing indicating that you can\'t use this in a Github Codespace, especially if it is a trusted repository.\\n\\nYou will already be logged into Visual Studio Code with your GitHub account; you can turn on Setting Sync to have your Visual Studio code settings and profiles sync straight into your devcontainer.\\n\\n![Github Codespaces - Setting Sync](/images/posts/VisualStudioCode_Codespace_TurnonSettingSync.gif)\\n\\nOne of the downsides of this method is the container can get bloated with extensions and configurations you don\'t need, and you will have to turn on Setting Sync each time a Codespace is launched.\\n\\n[Setting Sync](https://docs.github.com/en/codespaces/customizing-your-codespace/personalizing-github-codespaces-for-your-account#turning-on-settings-sync-in-a-codespace) is an easy way to import your configuration from your Desktop into the Cloudspace.\\n\\n##### Codespace templates\\n\\nInstead of spending time, developing your template, you may find a devcontainer template already exists for your use case; some examples consist of:\\n\\n* Ruby on Rails\\n* React\\n* Juypter Notebooks and more.\\n\\nThese [Codespace Templates](https://github.com/codespaces/templates) can easily be launched from the web browser and are a great resource to test the power of Codespace and refer to when customising your own devcontainer.\\n\\n See [devcontainers/template-starter](https://github.com/devcontainers/template-starter) and [devcontainers/feature-starter](https://github.com/devcontainers/feature-starter) for information on creating your own!\\n\\n##### Devcontainers\\n\\nWithin each customised Codespace is a \\"[devcontainer.json](https://containers.dev/implementors/json_reference/)\\" json file, and some containers will have a [dockerfile](https://code.visualstudio.com/docs/devcontainers/create-dev-container#_dockerfile).\\n\\nThese files will sit inside a /.devcontainer/ folder at the root of your git repository. It is worth noting that you can have multiple devcontainer files within a single repository; you will be prompted which one to be used when you start the Codespace up.\\n\\nThese files are crucial to customising your devcontainer.\\n\\nAlthough they serve different purposes they can work standalone or together to create a consistent and reproducible development environment for your project.\\n\\n|File | Purpose  |\\n|---|---|\\n|devcontainer.json|  The devcontainer.json file configures how your development environment is set up within the Docker container when using the Remote - Containers extension. |\\n|dockerfile| The dockerfile defines the environment you need for your project. When you create a Codespace, GitHub will use the specified Dockerfile to build a container image that includes all the tools, libraries, and configurations required to work on your project.   |\\n\\nWhen you open your project in a GitHub Codespace that uses a devcontainer.json file, Visual Studio Code will automatically detect the configuration and set up your development environment according to the specified settings.\\n\\nYou can use a dockerfile to define the environment you need for your project. When you create a Codespace, GitHub will use the specified Dockerfile to build a container image that includes all the tools, libraries, and configurations required to work on your project.\\n\\nEven without using a dockerfile, you can install any dependant libraries onto your codespace, but they are lost when the container gets rebuilt; there are certain approved features you can add to your devcontainer file that will be installed when a container is launched, which is great when making sure you are working on with the latest component.\\n\\nThe idea with both these files is to _keep them lean_ and make sure that you are running the components you only need. To keep launch time and performance as quick as possible, it is possible to \'[prebuild](https://docs.github.com/en/codespaces/prebuilding-your-codespaces/about-github-codespaces-prebuilds)\' your image if it is largely complex, but we won\'t be covering that here.\\n\\n###### devcontainer.json\\n\\nLet us take a look at the \'devcontainer.json\' file. As Codespaces uses the [devcontainer](https://containers.dev/implementors/spec/) schema, all the customisations offered such as:\\n\\n* entrypoint\\n* onCreateCommand\\n* customizations\\n* features\\n  \\nCan be used, offering a vast range of customisation opportunities to suit your needs.\\n\\nFor most purposes, you may be able to find you can get away with a devcontainer.json file without having to delve into building your own dockerfile.\\n\\nLet us look into the devcontainer.json file I am using for this blog article:\\n\\n```\\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the\\n// README at: https://github.com/devcontainers/templates/tree/main/src/markdown\\n{\\n    \\"name\\": \\"Markdown Editing\\",\\n    // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\\n    \\"image\\": \\"mcr.microsoft.com/devcontainers/base:bullseye\\",\\n\\n    // Features to add to the dev container. More info: https://containers.dev/features.\\n    // \\"features\\": {},\\n\\n    // Configure tool-specific properties.\\n    \\"customizations\\": {\\n        // Configure properties specific to VS Code.\\n        \\"vscode\\": {            \\n            // Add the IDs of extensions you want to be installed when the container is created.\\n            \\"extensions\\": [\\n                \\"yzhang.markdown-all-in-one\\",\\n                \\"streetsidesoftware.code-spell-checker\\",\\n                \\"DavidAnson.vscode-markdownlint\\",\\n                \\"shd101wyy.markdown-preview-enhanced\\",\\n                \\"bierner.github-markdown-preview\\"\\n            ]\\n        }\\n    }\\n\\n    // Use \'forwardPorts\' to make a list of ports inside the container available locally.\\n    // \\"forwardPorts\\": [],\\n\\n    // Use \'postCreateCommand\' to run commands after the container is created.\\n    // \\"postCreateCommand\\": \\"uname -a\\",\\n\\n    // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.\\n    // \\"remoteUser\\": \\"root\\"\\n}\\n\\n```\\n\\nThis was from an already existing template, that had everything I needed from: [devcontainers/templates](https://github.com/devcontainers/templates/tree/main/src).\\n\\nA few things stand out:\\n\\n* image\\n* customizations\\n* features\\n\\n1. The image is the docker image, that this Codespace is running.\\n1. Customizations are application-specific customisations, in my example vscode, and the extensions that are automatically provisioned for me when I started this codespace.\\n1. Features, which are currently blank, but will allow us to run scripts to install any relevant dependencies when the container is started.\\n\\nThis json file is a great reference point, as we go into creating our own. You can create your own devcontainer.json file - or we can do it within the devcontainer itself using the Codespaces extension, preinstalled into your new Codespace.\\n\\nIn our newly created Codespace from the Setting up Codespace step earlier, it\'s time to create our own devcontainer. The project we will be working on will be Terraform development, so we want to customise our own codespace for Infrastructure as Code development.\\n\\n1. Press **Ctrl+Shift+P** (or click View, Command Palette)\\n2. Type in: **Codespaces** _(with the codespaces commands you can rebuild your container, resize and modify your codespace)_\\n3. Select **Add Dev Container Configuration Files**\\n4. Select **Create a new Configuration**\\n5. Type in **Ubuntu**\\n6. Select the latest version (or default)\\n7. In the **[features](https://github.com/devcontainers/features)** list we have the option to install third-party tools and dependencies that will be installed when we launch our Codespace, search for **Terraform**\\n8. Select **Terraform, tflint, and TFGrunt**\\n9. Click **Ok**\\n10. Select **Configure Options**\\n11. Check the **installTFsec** and **instalLTerraformDocs**\\n12. Click **Ok**\\n13. Select the **latest** Terraform version\\n14. Select the latest **Tflint** version\\n\\nThis will now create a devcontainer json file, using a base Ubuntu image, with the latest version of Terraform, tflint and Terragrunt installed!\\n\\n![Github Codespaces - Create](/images/posts/VisualStudioCode_Codespace_CreateTerraform.gif)\\n\\n**Make sure you save and commit the devcontainer.json file to the repository! You have now created your first custom codespace**.\\n\\nYou can now rebuild your container, to run inside your Terraform container:\\n\\n1. Press **Ctrl+Shift+P** (or click View, Command Palette)\\n2. Type in **Codespaces**\\n3. Select **Full Rebuild Container**\\n4. Accept the prompt. that it will be rebuild with the devcontainer configuration.\\n5. GitHub Codespaces will then grab the Ubuntu image, and the Terraform feature and run.\\n\\n> Note: If the build fails, at the time of writing, there looked to be an issue with the latest version of [terragrunt](https://terragrunt.gruntwork.io/docs/getting-started/supported-terraform-versions/), I pined it to this specific version: 0.48.0, and it fixed it. So edit the JSON file and update latest to the version. Feel free to review my example codespace here: [lukemurraynz/codespaces](https://github.com/lukemurraynz/codespaces/tree/main).\\n\\n1. Once loaded, I can immediately run \'terraform init\'\\n\\n![Github Codespaces - Terraform init](/images/posts/VisualStudioCode_Codespace_Terraforminit.gif)\\n\\nNow that we have Terraform installed, the Azure Terraform and HashiCorp extension - we may want the [GitLens](https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens) extension, to help with working with other developers, so let us add this!\\n\\n1. Navigate to the **Extensions**\\n2. Search for **GitLens**\\n3. **Right** click the **extension** button and select \'**Add to devcontainer.json**\'\\n4. Then commit your save, you have now added the GitLens extension into your devcontainer, this will automatically be installed on your next rebuild.\\n\\n![Github Codespaces - Install Extension](/images/posts/VisualStudioCode_Codespace_AddGitLens.gif)\\n\\nNow we have created our own Github Codespaces using devcontainers, using features and adding in extensions, last thing to add is a few settings to Visual Studio Code, such as Format on Save Mode.\\n\\n1. Click on the **Settings** gear\\n2. Click **Settings**\\n3. Search for: **Format**\\n4. Click on the **gear** next to: **Editor: Format On Save**\\n5. Click **Copy** **Setting** **ID**\\n6. **Navigate** to your **devcontainer**.json **file**\\n7. **Under** vscode **customization**, **add** a new **item** called **Settings** and add:\\n  \\n        \\"settings\\": {\\n                \\"editor.formatOnSave\\": true \\n            },\\n\\n8. Intellisense should help you, add it in and any other settings you may want configured. you may want to consider configuring a default formatter or linter for your project.\\n\\n![Github Codespaces - Set Setting](/images/posts/VisualStudioCode_Codespace_AddVSCodeSetting.gif)\\n\\nAs usual, make sure you **Commit the change to the repository**, before you rebuild to confirm the settings have worked.\\n\\n###### dockerfile\\n\\nFor those more complex scenarios, where there may not be a feature or shellscript that you can run as part of the launch, you may want to consider your own dockerfile.\\n\\nIn this example, I am going to use the same scenario, but use a non-devcontainer image for Apache.\\n\\nYou can create a dockerfile in the same repository.\\n\\nTo make this work, you need an adjustment to your devcontainer.json file.\\n\\n1. **Create** a new **file** called: **dockerfile** - in the same location as the \'devcontainer.json\' file\\n2. In the dockerfile **add** the following line:\\n3.\\n        FROM httpd:latest\\n\\n4. **Save**\\n5. In the devcontainer.json file **replace** the image section with:\\n\\n        \\"build\\": {\\n        // Path is relataive to the devcontainer.json file.\\n        \\"dockerfile\\": \\"Dockerfile\\"\\n        },\\n\\n6. Now **start** your **Codespace**\\n7. Github will now grab the image directly from dockerhub and overly your devcontainers configuration on top of it.\\n\\n#### Port Forwarding\\n\\nGithub Codespaces can do port forwarding, which is either Private (ie visible only to your GitHub user), or Public (open to the internet). This is useful for local development and testing.\\n\\nLet us take our Apache, httpd image supplied earlier.\\n\\nIn the same directory, we will **create an index.html** page:\\n\\n```\\n<!DOCTYPE html>\\n<html lang=\\"en\\">\\n<head>\\n    <meta charset=\\"UTF-8\\">\\n    <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\">\\n    <title>GitHub Codespace Port Forwarding Test</title>\\n</head>\\n<body>\\n    <h1>Hello, GitHub Codespaces!</h1>\\n    <p>This is a test webpage for port forwarding.</p>\\n</body>\\n</html>\\n```\\n\\nAnd **adjust the dockerfile** like so:\\n\\n```\\nFROM httpd:latest\\nCOPY index.html /usr/local/apache2/htdocs\\nEXPOSE 80\\n```\\n\\nThis will take our index.html page and feed it to the apache htdocs folder.\\n\\nThen we go to our **devcontainer.json** file and add these:\\n\\n```\\n  \\"forwardPorts\\": [\\"80\\"],\\n\\n  \\"postStartCommand\\": \\"httpd\\"\\n```\\n\\nNow **save the changes** and **launch** your Codespace.\\n\\nFeel free to review my example codespace here: [lukemurraynz/codespaces](https://github.com/lukemurraynz/codespaces/tree/main).\\n\\n![Github Codespaces - Port Forwarding](/images/posts/VisualStudioCode_Codespace_PortFowarding.gif)\\n\\n#### Working from your own device\\n\\nThis is all great, but sometimes it feels more natural to work from a locally installed Visual Studio Code instance.\\n\\nUsing the: [GitHub Codespaces](https://marketplace.visualstudio.com/items?itemName=GitHub.codespaces) Visual Studio Code extension, you can connect to a Codespace (or start one), directly from your own Visual Studio Code installation.\\n\\n1. Install **[GitHub Codespaces](https://marketplace.visualstudio.com/items?itemName=GitHub.codespaces)** extension\\n2. Press **Ctrl+Shift+P** (or click View, Command Palette)\\n3. Type in **Codespaces**\\n4. Click **Connect to a Codespace**\\n5. Select your codespace\\n\\n![Github Codespaces - Connect to Codespace](/images/posts/VisualStudioCode_Codespace_CodespaceLocally.gif)\\n\\nAs you can see you can now connect to one or multiple GitHub Codespaces, from your own locally installed Visual Studio instance!\\n\\n#### Additional Settings\\n\\nAdditional settings, that can be configured are:\\n\\n* Setup [Prebuild](https://docs.github.com/en/codespaces/prebuilding-your-codespaces/configuring-prebuilds)\\n* Codespace [Secrets](https://docs.github.com/en/codespaces/managing-your-codespaces/managing-encrypted-secrets-for-your-codespaces) (Repo & Org Level)\\n* [Dotfiles](https://docs.github.com/en/codespaces/customizing-your-codespace/personalizing-github-codespaces-for-your-account#dotfiles) (Org Level)\\nOrganisation\\n* [Configuring automatic deletion of your codespaces](https://docs.github.com/en/codespaces/customizing-your-codespace/configuring-automatic-deletion-of-your-codespaces)\\n\\nHopefully this article has given you a taste of what GitHub Codespaces can do."},{"id":"azure/Move-Azure-Public-IP-to-another-subscription","metadata":{"permalink":"/azure/Move-Azure-Public-IP-to-another-subscription","source":"@site/blog/2023-08-11-Move-Azure-Public-IP-to-another-subscription.md","title":"Move an Azure public IP to another subscription","description":"You can move a Public IP address in Microsoft Azure, to a new Subscription (BUT NOT A DIFFERENT REGION).","date":"2023-08-11T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.5,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Move an Azure public IP to another subscription","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/PublicIP_MoveAnotherSub.png"},"date":"2023-08-11 00:00:00 +1300","slug":"azure/Move-Azure-Public-IP-to-another-subscription"},"unlisted":false,"prevItem":{"title":"Coding on the Cloud - Getting Started with GitHub Codespaces","permalink":"/azure/Getting-Started-with-GitHub-Codespaces"},"nextItem":{"title":"Entra ID Domain Services - Referenced account locked out","permalink":"/azure/AADDS-Referenced-account-locked-out"}},"content":"You can move a [Public IP](https://learn.microsoft.com/azure/virtual-network/ip-services/public-ip-addresses?WT.mc_id=AZ-MVP-5004796) address in Microsoft Azure, to a new Subscription (BUT NOT A [DIFFERENT REGION](https://learn.microsoft.com/azure/virtual-network/move-across-regions-publicip-portal?WT.mc_id=AZ-MVP-5004796)).\\n\\nThere are some limitations as below:\\n\\n> You can\'t move a VPN Gateway that is associated with a Standard SKU public IP address to a new subscription.\\n> Azure Public IPs are region specific and can\'t be moved from one region to another.\\n\\nSome resources aren\'t [supported](https://learn.microsoft.com/azure/azure-resource-manager/management/move-support-resources?WT.mc_id=AZ-MVP-5004796#microsoftnetwork) to be moved, but a Public IP is.\\n\\nBefore you move the Public IP, make sure its dissociated (unattached) from a resource.\\n\\n![Dissociate IP](/images/posts/PublicIP_dissociate.png)\\n\\nThen once dissociated, you can trigger the move.\\n\\n![Dissociate IP](/images/posts/PublicIP_MoveAnotherSub.png)"},{"id":"azure/AADDS-Referenced-account-locked-out","metadata":{"permalink":"/azure/AADDS-Referenced-account-locked-out","source":"@site/blog/2023-07-31-AADDS-Referenced-account-locked-out.md","title":"Entra ID Domain Services - Referenced account locked out","description":"So you have just stood up Entra ID Domain Services (formally - or still known as of July 2023 as","date":"2023-07-31T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.245,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Entra ID Domain Services - Referenced account locked out","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/AADDS_ReferencedAccount.png"},"date":"2023-07-31 00:00:00 +1300","slug":"azure/AADDS-Referenced-account-locked-out"},"unlisted":false,"prevItem":{"title":"Move an Azure public IP to another subscription","permalink":"/azure/Move-Azure-Public-IP-to-another-subscription"},"nextItem":{"title":"Entra ID login to a CIS Hardened Linux Azure Virtual Machine","permalink":"/azure/Entra-ID-login-to-a-CIS-Hardened-Linux-Azure-Virtual-Machine"}},"content":"So you have just stood up [Entra ID Domain Services](https://learn.microsoft.com/azure/active-directory-domain-services/?WT.mc_id=AZ-MVP-5004796) _(formally - or still known as of July 2023 as: Active Directory Domain Services (AAD DS)_) and trying to login and join a computer to the AAD DS domain and you get the following error:\\n\\n> The referenced account is currently locked out and may not be logged on to.\\n\\n![The referenced account is currently locked out and may not be logged on to.](/images/posts/AADDS_ReferencedAccount.png)\\n\\nThis is commonly due to the following:\\n\\n> Entra ID doesn\'t generate or store password hashes in the format that\'s required for NTLM or Kerberos authentication until you enable Azure AD DS for your tenant. For security reasons, Entra ID also doesn\'t store any password credentials in clear-text form. Therefore, Azure AD can\'t automatically generate these NTLM or Kerberos password hashes based on users\' existing credentials.\\n\\nEssentially this means, that the password hashes are not comaptible with Entra ID Domain Services, and the fix is to **reset the password of an Entra ID account** used to access Entra ID services, to allow Entra ID Domain Services, to hash and validate the Entra ID credentials. The reset only needs to happen, if the password for the accounts haven\'t been reset since Entra ID Domain Services has been deployed.\\n\\nAfter resetting the password to my Entra ID account, and waiting an hour for replication I was successfully able to login and join a Windows Server to Entra ID Domain Services.\\n\\n![Entra ID Domain Services - Join account](/images/posts/AADDS_JoinAccount.png)"},{"id":"azure/Entra-ID-login-to-a-CIS-Hardened-Linux-Azure-Virtual-Machine","metadata":{"permalink":"/azure/Entra-ID-login-to-a-CIS-Hardened-Linux-Azure-Virtual-Machine","source":"@site/blog/2023-07-28-Entra-ID-login-to-a-CIS-Hardened-Linux-Azure-Virtual-Machine.md","title":"Entra ID login to a CIS Hardened Linux Azure Virtual Machine","description":"Currently, CIS (Center for Internet Security) Azure Marketplace images, do not support being Entra ID (Azure Active Directory) Joined.","date":"2023-07-28T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.245,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Entra ID login to a CIS Hardened Linux Azure Virtual Machine","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/Blog_Headler-EntraID_login_CIS_Linux_Azure_Virtual_Machine.gif"},"date":"2023-07-28 00:00:00 +1300","slug":"azure/Entra-ID-login-to-a-CIS-Hardened-Linux-Azure-Virtual-Machine"},"unlisted":false,"prevItem":{"title":"Entra ID Domain Services - Referenced account locked out","permalink":"/azure/AADDS-Referenced-account-locked-out"},"nextItem":{"title":"Changing the default Management Group in Azure","permalink":"/azure/Changing-default-management-group-for-azure"}},"content":"Currently, [CIS](https://www.cisecurity.org/) (Center for Internet Security) [Azure Marketplace images](https://azuremarketplace.microsoft.com/en-us/marketplace/apps?search=CIS&page=1?WT.mc_id=AZ-MVP-5004796), do not support being Entra ID (Azure Active Directory) Joined.\\n\\nAlthough this article is about allowing Entra ID login to a Ubuntu machine, its worth noting the current decisions around Windows as well currently:\\n\\n>\'The Windows CIS Benchmarks are written for Active Directory domain-joined systems using Group Policy, not standalone/workgroup systems. Adjustments/tailoring to some recommendations will be needed to maintain functionality if attempting to implement CIS hardening on standalone systems or a system running in the cloud.\\n>\'Currently, we do not support Azure Active Directory and it is not compatible with our EXISTING Hardened Images.\\n\\nIn fact when you, attempt to create a CIS Level 1 Ubuntu image in the Azure Portal, you get:\\n\\n\\"This image does not support Login with Azure AD.\\"\\n\\n![CIS Image does not support Login with Azure AD](/images/posts/AzurePortal_CIS_Level1_Ubuntu_NoEntraIDLogin.png)\\n\\nHowever, as I go into below, we can indeed login to the CIS hardened image, using the [Microsoft Azure AD based SSH Login](https://learn.microsoft.com/en-us/azure/active-directory/devices/howto-vm-sign-in-azure-ad-linux?WT.mc_id=AZ-MVP-5004796) extension.\\n\\n> Be wary, that although this works, you may run into issues with operational support of this from CIS, due to the hardening. This is also Entra ID LOGIN (not JOINED!). You won\'t see the device under Entra ID Devices.\\n\\n![Entra ID login to a CIS Hardened Linux Azure Virtual Machine](/images/posts/Blog_Headler-EntraID_login_CIS_Linux_Azure_Virtual_Machine.gif)\\n\\nThere are many security benefits of using Azure AD with SSH log in to Linux VMs in Azure, including:\\n\\n1. Use of your Entra ID (Azure AD) credentials to log in to Azure Linux VMs.\\n1. Get SSH certificate-based authentication without needing to distribute SSH keys to users or provision SSH public keys on any Azure Linux VMs you deploy.\\n1. Reduce reliance on local administrator accounts, credential theft, and weak credentials.\\n1. Password complexity and password lifetime policies configured for Azure AD help secure Linux VMs as well.\\n1. With Azure role-based access control, specify who can login to a VM as a regular user or with administrator privileges. When users join or leave your team, you can update the Azure RBAC policy for the VM to grant access as appropriate. When employees leave your organization and their user account is disabled or removed from Azure AD, they no longer have access to your resources.\\n1. With Conditional Access, configure policies to require multi-factor authentication and/or require client device you are using to SSH be a managed device (for example: compliant device or hybrid Azure AD joined) before you can SSH to Linux VMs.\\n1. Use Azure deploy and audit policies to require Azure AD login for Linux VMs and to flag use of non-approved local accounts on the VMs.\\n\\nSo after your CIS Hardened Image, in my case I am using Ubuntu 20.04 has been deployed in Azure. Lets set this up.\\n\\nYou will need to make sure you have a few prerequsites.\\n\\n### Prerequisites\\n\\n#### Network\\n\\nVM network configuration must permit outbound access to the following endpoints over TCP port 443.\\n\\n[https://packages.microsoft.com](https://packages.microsoft.com): For package installation and upgrades.\\n[http://169.254.169.254](http://169.254.169.254): Azure Instance Metadata Service endpoint.\\n[https://login.microsoftonline.com](https://login.microsoftonline.com): For PAM-based (pluggable authentication modules) authentication flows.\\n[https://pas.windows.net](https://pas.windows.net): For Azure RBAC (Role Based Access Control) flows.\\n\\nAlso make sure you have enabled TCP Port 80 for: ubuntu.com, specifically [http://archive.ubuntu.com](http://archive.ubuntu.com) as the Microsoft Azure AD based SSH Login, will need to download and install the following packages: aadsshlogin and aadsshlogin-selinux as needed.\\n\\n#### Virtual Machine\\n\\nThe CIS hardened image, will need to have a [System Managed Identity](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796#managed-identity-types) setup. This can be easily enabled in the Identity blade of the Virtual Machine.\\n\\nThe Entra ID (Azure Active Directory) users that need to login with to the Linux Virtual Machine are a member of one of the following Azure RBAC (Role Based Access Control) groups, as per their requirements:\\n\\n| RBAC Role                           | Notes                                                            |\\n| ----------------------------------- | ---------------------------------------------------------------- |\\n| Virtual Machine Administrator Login | View Virtual Machines in the portal and login as administrator   |\\n| Virtual Machine User Login          | View Virtual Machines in the portal and login as a regular user. |\\n\\nOnly one role is required. These roles are supported for both Windows and Linux.\\n\\n#### Client\\n\\nOn the jumphost, client PC you will be connecting to the Linux virtual machine from, you need the latest [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli?WT.mc_id=AZ-MVP-5004796) with the Azure CLI extension \'ssh\' installed.\\n\\n    az extension add --name ssh\\n\\nThe minimum version required for the extension is 0.1.4.\\n\\n    az extension show --name ssh\\n\\n### Install Extension\\n\\nMake sure the Virtual Machine is on, for the extension to install.\\n\\n##### Install using the Azure Portal\\n\\nOnce the pre-requsites have been met, it is time to install the extension.\\n\\n1. Login to the [Azure Portal](https://portal.azure.com/)\\n1. Navigate to your CIS Hardened Virtual Machine\\n1. Click on Extensions + Applications\\n1. Click + Add\\n1. ![Azure Portal - Extensions](/images/posts/AzurePortal_CISHardenedVM_Extensions.png)\\n1. Search for: Azure AD based SSH Login\\n1. ![Azure Portal - SSH Extension](/images/posts/AzurePortal_CISHardenedVM_SSH_Extension.png)\\n1. Select the Azure AD base SSH Login extension\\n1. Click Next\\n1. Click Review and Create\\n1. Click Create\\n\\nAfter a few moments, the extension and supporting components, will be installed. You can check the status under the Extensions + Application blade make sure that Provisioning has been succeded, before provisioning to the next step to login.\\n\\n![Azure Virtual Machine Extension](/images/posts/AzurePortal_CIS_Level1_Ubuntu_ADSSHExtensionStatus.png)\\n\\n##### Install using Terraform\\n\\nYou can use the following Terraform code snippet, to install the extension to a Linux Virtual Machine:\\n\\nMake sure you assign the extension, to the virtual machine, using the ID.\\n\\n    resource \\"azurerm_virtual_machine_extension\\" \\"aad_login\\" {\\n    name                 = \\"AADLogin\\"\\n    virtual_machine_id   = \\n    publisher            = \\"Microsoft.Azure.ActiveDirectory\\"\\n    type                 = \\"AADSSHLoginForLinux\\" # For Windows VMs: AADLoginForWindows\\n    type_handler_version = \\"1.0\\"                 # There may be a more recent version\\n    }\\n\\n##### Install using PowerShell\\n\\nTo install the AADLogin extension to a Linux Virtual Machine in Microsoft Azure using PowerShell, you can follow these steps:\\n\\n1. Open PowerShell on your local machine, or [Azure Cloud Shell](https://learn.microsoft.com/azure/cloud-shell/overview?WT.mc_id=AZ-MVP-5004796).\\n\\n    Connect-AzAccount\\n    Select-AzSubscription -SubscriptionName \\"Your Subscription Name\\"\\n    $vm = Get-AzVM -ResourceGroupName \\"Your Resource Group Name\\" -Name \\"Your VM Name\\"\\n    Set-AzVMExtension -ResourceGroupName $vm.ResourceGroupName -VMName $vm.Name -Name \\"AADLoginForLinux\\" -Publisher \\"Microsoft.Azure.ActiveDirectory.LinuxSSH\\" -Type \\"AADLoginForLinux\\" -TypeHandlerVersion \\"1.0\\"\\n\\n### Login to Virtual Machine\\n\\nNow that the extension is stood, up its time to connect.\\n\\nREMEMBER to make sure that the Entra ID (AAD) accounts you want to login to the Linux image is a member of either of the following roles, directly assigned to the Virtual Machine/Resource Group, or in an Entra ID (AAD) group that has been delegated the permissions.\\n\\n| RBAC Role                           | Notes                                                            |\\n| ----------------------------------- | ---------------------------------------------------------------- |\\n| Virtual Machine Administrator Login | View Virtual Machines in the portal and login as administrator   |\\n| Virtual Machine User Login          | View Virtual Machines in the portal and login as a regular user. |\\n\\n1. Open a Command Prompt\\n1. Log in to the Azure using:\\n    az login\\n1. A web browser will prompt and ask you to authenticate, where you will go through the MFA (Multifactor Authentication) and complete login to your Entra ID account.\\n1. Once authenicated you can run:\\n    az ssh vm -n cistest -g cistest\\nNote: -n is the VM name and -g is the Resource Group, that the VM is located inside.\\n\\n![Entra ID Login - Linux VM](/images/posts/CISHardenedImageEntraIDLogin.gif)\\n\\nYou should now have successfully authenticated to your Linux virtual machine using Entra ID credendials.\\n\\n### Troubleshooting\\n\\nIf you are unable to connect, it may be due to an issue with the AADSSHLogin extension. You can troubleshoot by reviewing the extension log.\\n\\n    cat /var/log/azure/Microsoft.Azure.ActiveDirectory.AADSSHLoginForLinux/CommandExecution.log\\n\\nThe Azure directory is protected, so you will need Administrator rights to delve into the logs."},{"id":"azure/Changing-default-management-group-for-azure","metadata":{"permalink":"/azure/Changing-default-management-group-for-azure","source":"@site/blog/2023-07-17-Changing-default-management-group-for-azure.md","title":"Changing the default Management Group in Azure","description":"By default, when a Management Group gets created, it goes under the Root Management Group, the same is true for Subscriptions.","date":"2023-07-17T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.965,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Changing the default Management Group in Azure","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/ChangeDefaultManagementGroup.png"},"date":"2023-07-17 00:00:00 +1300","slug":"azure/Changing-default-management-group-for-azure"},"unlisted":false,"prevItem":{"title":"Entra ID login to a CIS Hardened Linux Azure Virtual Machine","permalink":"/azure/Entra-ID-login-to-a-CIS-Hardened-Linux-Azure-Virtual-Machine"},"nextItem":{"title":"Access denied on Azure VM when using aztfexport","permalink":"/azure/Access-denied-aztfexport"}},"content":"By default, when a [Management Group](https://learn.microsoft.com/azure/governance/management-groups/overview?WT.mc_id=AZ-MVP-5004796) gets created, it goes under the Root Management Group, the same is true for [Subscriptions](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/azure-best-practices/organize-subscriptions?WT.mc_id=AZ-MVP-5004796).\\n\\nThis works fine, when you have a simple Microsoft Azure environment, but as soon as you start expanding into areas such as [Subscription vending](https://learn.microsoft.com/azure/architecture/landing-zones/subscription-vending?WT.mc_id=AZ-MVP-5004796) or limited access to who can see the Root Management Group and start to look into Visual Studio Enterprise subscriptions, you may want to consider moving new subscriptions, under its own Management Group, away from any policies or RBAC controls, essentially into a Management Group that acts as a shopping cart, to then be picked up and moved later.\\n\\nIf we refer to a [recommendation](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/landing-zone/design-area/resource-org-management-groups?WT.mc_id=AZ-MVP-5004796#management-group-recommendations) on the Microsoft Cloud Adoption Framework:\\n\\n> Configure a default, dedicated management group for new subscriptions. This group ensures that no subscriptions are placed under the root management group. This group is especially important if there are users eligible for Microsoft Developer Network (MSDN) or Visual Studio benefits and subscriptions. A good candidate for this type of management group is a [sandbox](https://learn.microsoft.com/azure/cloud-adoption-framework/ready/considerations/sandbox-environments?WT.mc_id=AZ-MVP-5004796) management group.\\n\\nSo, how can we change the default Management Group, that new Subscriptions go into?\\n\\nLets take a look at the different ways we could use to update the default management group, that new subscriptions go into.\\n\\n### Configure using Azure Portal\\n\\n1. Use the search bar to search for and select \'Management groups\'.\\n1. On the root management group, select details next to the name of the management group.\\n1. Under Settings, select Hierarchy settings.\\n1. Select the Change default management group button.\\n\\n### Configuring using Bicep\\n\\n    resource symbolicname \'Microsoft.Management/managementGroups/settings@2021-04-01\' = {\\n    name: \'default\'\\n    parent: resourceSymbolicName\\n    properties: {\\n    defaultManagementGroup: \'string\'\\n    requireAuthorizationForGroupCreation: bool\\n    }\\n    }\\n\\nReference: [Microsoft.Management managementGroups/settings](https://learn.microsoft.com/en-us/azure/templates/microsoft.management/managementgroups/settings?pivots=deployment-language-bicep&WT.mc_id=AZ-MVP-5004796)\\n\\n### Configure using REST API using PowerShell\\n\\n    $root_management_group_id = \\"Enter the ID of root management group\\"\\n    $default_management_group_id = \\"Enter the ID of default management group (or use the same ID of the root management group)\\"\\n    $body = \'{\\n     \\"properties\\": {\\n          \\"defaultManagementGroup\\": \\"/providers/Microsoft.Management/managementGroups/\' + $default_management_group_id + \'\\",\\n          \\"requireAuthorizationForGroupCreation\\": true\\n     }\\n    }\'\\n    $token = (Get-AzAccessToken).Token\\n    $headers = @{\\"Authorization\\"= \\"Bearer $token\\"; \\"Content-Type\\"= \\"application/json\\"}\\n    $uri = \\"https://management.azure.com/providers/Microsoft.Management/managementGroups/$root_management_group_id/settings/default?api-version=2021-04-01\\"\\n    Invoke-RestMethod -Method PUT -Uri $uri -Headers $headers -Body $body\\n\\n### Configure using Terraform\\n\\n    resource \\"azurerm_management_group_subscription_association\\" \\"example\\" {\\n    management_group_id = data.azurerm_management_group.example.id\\n    subscription_id     = data.azurerm_subscription.example.id\\n    }\\n\\nNote: Not quite the same, as configuring the default setting as above - but you can specify the Managament Group association for subscriptions using Terraform."},{"id":"azure/Access-denied-aztfexport","metadata":{"permalink":"/azure/Access-denied-aztfexport","source":"@site/blog/2023-07-10-Access-denied-aztfexport.md","title":"Access denied on Azure VM when using aztfexport","description":"When attempting to use aztfexport, a tool designed to export currently deployed Azure resources into HashiCorp Configuration Language (HCL) for use with Terraform, you may get: Access denied.","date":"2023-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.54,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Access denied on Azure VM when using aztfexport","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/aztfexport_access_denied.png"},"date":"2023-07-10 00:00:00 +1300","slug":"azure/Access-denied-aztfexport"},"unlisted":false,"prevItem":{"title":"Changing the default Management Group in Azure","permalink":"/azure/Changing-default-management-group-for-azure"},"nextItem":{"title":"Azure OpenAI error log summarization with completion","permalink":"/azure/Azure-OpenAI-error-log-summarization"}},"content":"When attempting to use [aztfexport](https://github.com/Azure/aztfexport), a tool designed to export currently deployed Azure resources into HashiCorp Configuration Language (HCL) for use with Terraform, you may get: Access denied.\\n\\nWhen using aztfexport, the first thing you need to do is make sure you have the [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli?WT.mc_id=AZ-MVP-5004796) installed, and run an:\\n\\n    az login\\n\\nAfter logging in, you need to verify you are on the right subscription, by running:\\n\\n    az account show\\n\\nIf you are on the right subscription, you don\'t need to do anything. If you are in the wrong subscription then run:\\n\\n    az account list\\n\\nFind the susbcription ID then use:\\n\\n    az account set --subscription <name or id>\\n\\n**You only need Reader rights to be able to export the Terraform configuration.**\\n\\nIf you find you are still running into access denied issues, such as below:\\n\\n![aztfexport - Access denied](/images/posts/aztfexport_access_denied.png \\"aztfexport - Access denied\\")\\n\\nAnd you are running the aztfexport program on an Azure Virtual Machine, such as Azure Virtual Desktop or Devbox, what is happening is the [Managed Identity](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/overview?WT.mc_id=AZ-MVP-5004796) permissions of your Azure Virtual Machine is overriding your own permissions you used to login to Azure using the CLI.\\n\\nTo get around this, you either have to run aztfexport locally, on a device thats not an Azure Virtual Machine, or supply the Managed Identity of the Virtual Machine, Reader rights to the subscription you wish to do the export from.\\n\\nYou can do this, by navigating to your Azure Virtual Machine, in the Azure Portal, click on the Virtual Machine, select Identity, select Azure role assignments and grant it Reader rights to the Resource Group or Subscription you are targeting.\\n\\nYou could try Disabling the System Assigned Managed Identity as well, which appeared to work for me.\\n\\nFor more information about this error, please refer to the following Github issue: [Access Denied during xport on Azure Virtual Desktop](https://github.com/Azure/aztfexport/issues/380)."},{"id":"azure/Azure-OpenAI-error-log-summarization","metadata":{"permalink":"/azure/Azure-OpenAI-error-log-summarization","source":"@site/blog/2023-07-10-Azure-OpenAI-error-log-summarization.md","title":"Azure OpenAI error log summarization with completion","description":"I was assisting a user on Microsoft Q&A with an issue, that involved looking over some event logs.","date":"2023-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.26,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure OpenAI error log summarization with completion","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/AzureOpenAI_Completion_ErrorLog.png"},"date":"2023-07-10 00:00:00 +1300","slug":"azure/Azure-OpenAI-error-log-summarization"},"unlisted":false,"prevItem":{"title":"Access denied on Azure VM when using aztfexport","permalink":"/azure/Access-denied-aztfexport"},"nextItem":{"title":"Bring Your Data to Life with Azure OpenAI","permalink":"/azure/Bring-Your-Data-to-Life-with-Azure-OpenAI"}},"content":"I was assisting a user on [Microsoft Q&A](https://learn.microsoft.com/en-us/answers/questions/?WT.mc_id=AZ-MVP-5004796) with an issue, that involved looking over some event logs.\\n\\nThe issue itself was related to the Nested Virtualization, with the user unable to install Hyper-V or WSL (Windows Subsystem for Linux), it turned out to be [incompatilibies with the SKU size and Secure boot](https://learn.microsoft.com/azure/virtual-machines/trusted-launch?WT.c_id=AZ-MVP-5004796#unsupported-features).\\n\\nBut as part of troubleshooting this issue, I recreated the Azure compute environment, this user had and started to delve into the Windows logs.\\n\\nHowever, in this case I did something a bit different, I exported the logs as text file and opened up [Azure OpenAI](https://learn.microsoft.com/azure/cognitive-services/openai/overview?WT.mc_id=AZ-MVP-5004796), then navigated to Azure OpenAI Studio, clicked on Completion and used the summarization powers of the GPT 3.5 Large language model, to delve into the logs for me:\\n\\n![Azure OpenAI - Summarize Error Log](/images/posts/AzureOpenAI_Completion_ErrorLog.png \\"Azure OpenAI - Summarize Error Log\\")\\n\\nCopying, the Log into the Completion pane of Azure OpenAI\\n\\nAnd using  the Prompt of:\\n\\n    ----\\n    Summarize all the errors and warnings from above and sort by potential cause of the issues, with the most likely cause first. Format as a table.\\n\\nAzure OpenAI was able to use the reasoning ability of the GPT 3.5 LLM (Large language Model) to sort through 115 lines of Logs, and work out the probability of what could be causing the root cause of the issue.\\n\\nAs you can see, Azure OpenAI and the LLMs can not just be used as an assistant in writing, studying it can be learned to assist in Incident and root-cause resolution."},{"id":"azure/Bring-Your-Data-to-Life-with-Azure-OpenAI","metadata":{"permalink":"/azure/Bring-Your-Data-to-Life-with-Azure-OpenAI","source":"@site/blog/2023-07-03-Bring-Your-Data-to-Life-with-Azure-OpenAI.md","title":"Bring Your Data to Life with Azure OpenAI","description":"Today, we will look at using Azure OpenAI and \'Bring Your Data\' to allow recent documentation to be referenced and bring life (and relevance) to your data.","date":"2023-07-04T15:15:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":12.935,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Bring Your Data to Life with Azure OpenAI","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/Header-BringYourDatatoLifewithAzureOpenAI.gif"},"date":"2023-07-04 15:15:00 +1300","slug":"azure/Bring-Your-Data-to-Life-with-Azure-OpenAI"},"unlisted":false,"prevItem":{"title":"Azure OpenAI error log summarization with completion","permalink":"/azure/Azure-OpenAI-error-log-summarization"},"nextItem":{"title":"Azure OpenAI - Call to LLM failed","permalink":"/azure/Call-to-LLM-failed"}},"content":"Today, we will look at using Azure OpenAI and \'Bring Your Data\' to allow recent documentation to be referenced and bring life (and relevance) to your data.\\n\\n![Bring Your Data to Life with Azure OpenAI](/images/posts/Header-BringYourDatatoLifewithAzureOpenAI.gif \\"Bring Your Data to Life with Azure OpenAI\\")\\n\\nThe example we are going to use today is the Microsoft Learn documentation for [Microsoft Azure](https://learn.microsoft.com/azure/?product=popular&WT.mc_id=AZ-MVP-5004796).\\n\\nOur scenario is this:\\n\\n* We would like to use ChatGPT functionality to query up-to-date information on Microsoft Azure; in this example, we will look for information on Azure Elastic SAN (which was added in late 2022).\\n\\nWhen querying ChatGPT for Azure Elastic SAN, we get the following:\\n\\n![ChatGPT - Azure Elastic SAN Query](/images/posts/ChatGPT_Query_AzureElasticSAN.png \\"ChatGPT - Azure Elastic SAN Query\\")\\n\\nJust like the prompt states, ChatGPT only has data up to September 2021 and isn\'t aware of Elastic SAN *(or any other changes/updates or new (or retired) services after this date)*.\\n\\nSo let us use the Azure OpenAI and bring in outside data [(ground data)](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions&WT.mc_id=AZ-MVP-5004796#using-data-for-grounding), in this case, the Azure document library, to overlay on top of the GPT models, giving the illusion the model is aware of the data.\\n\\nTo do this, we will leverage native \'[Bring Your Own Data](https://learn.microsoft.com/azure/cognitive-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-studio&WT.mc_id=AZ-MVP-5004796)\' functionality, now in Azure OpenAI - **this is in Public Preview as of 04/07/2023**.\\n\\n**To be clear, I don\'t expect you to start downloading from GitHub; this is just an example I have used to add your data. The ability to bring in updated data on Azure, specifically, will be solved by [Plugins](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/generative-ai-for-developers-exploring-new-tools-and-apis-in/ba-p/3817003?WT.mc_id=AZ-MVP-5004796), such as Bing Search.**\\n\\nTo do this, we will need to provision a few Microsoft Azure services, such as:\\n\\n1. [Azure Storage Account](https://learn.microsoft.com/azure/storage/common/storage-account-overview?WT.mc_id=AZ-MVP-5004796) *(this will hold the Azure document library (markdown files) in a blob container)*\\n1. [Cognitive Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search?WT.mc_id=AZ-MVP-5004796) *(this search functionality, is the glue that will hold this solution together, by breaking down and indexing the documents in the Azure blob store)*\\n1. [Azure OpenAI](https://learn.microsoft.com/azure/cognitive-services/openai/overview?WT.mc_id=AZ-MVP-5004796) *(to do this, we will need GPT3.5 turbo or GPT4 models deployed)*\\n1. Optional - [Azure Web App](https://learn.microsoft.com/azure/app-service/overview?WT.mc_id=AZ-MVP-5004796) *(this can be created by the Azure OpenAI service, to give users access to your custom data)*\\n\\n*[Make sure you have Azure OpenAI approved for your subscription](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-studio&WT.mc_id=AZ-MVP-5004796#prerequisites)*\\n\\nWe will use the following tools to provision and configure our services:\\n\\n1. Azure Portal\\n1. PowerShell [(Az Modules)](https://learn.microsoft.com/powershell/azure/install-azure-powershell?WT.mc_id=AZ-MVP-5004796)\\n1. [AzCopy](https://learn.microsoft.com/azure/storage/common/storage-use-azcopy-v10?WT.mc_id=AZ-MVP-5004796)\\n\\n### Download Azure Documents\\n\\nFirst, we will need the Azure documents to add to our blob storage.\\n\\nThe Microsoft Learn documentation is open-sourced and constantly updated using a git repository hosted on GitHub. We will download and extract the document repository locally *(roughly 6 GB)*. To do this, we will use a PowerShell script:\\n\\n         $gitRepoUrl = \\"https://github.com/MicrosoftDocs/azure-docs\\"\\n         $localPath = \\"C:\\\\temp\\\\azuredocs\\"\\n         $zipPath = \\"C:\\\\temp\\\\azurdocs.zip\\"\\n         #Download the Git repository and extract\\n         Invoke-WebRequest -Uri \\"$gitRepoUrl/archive/master.zip\\" -OutFile $zipPath\\n         Expand-Archive -Path $zipPath -DestinationPath $localPath\\n\\n### Create Azure Storage Account\\n\\nNow that we have a copy of the Azure document repository, it\'s time to create an Azure Storage account to copy the data into. To create this storage account, we will use PowerShell.\\n\\n         # Login to Azure\\n         Connect-AzAccount\\n         # Set variables\\n         $resourceGroupName = \\"azuredocs-ai-rg\\"\\n         $location = \\"eastus\\"\\n         $uniqueId = [guid]::NewGuid().ToString().Substring(0,4)\\n         $storageAccountName = \\"myaistgacc$uniqueId\\"\\n         $containerName = \\"azuredocs\\"\\n         # Create a new resource group\\n         New-AzResourceGroup -Name $resourceGroupName -Location $location\\n         # Create a new storage account\\n         New-AzStorageAccount -ResourceGroupName $resourceGroupName -Name $storageAccountName -Location $location -SkuName Standard_LRS -AllowBlobPublicAccess $false\\n         # Create a new blob container\\n         New-AzStorageContainer -Name $containerName -Context (New-AzStorageContext -StorageAccountName $storageAccountName -StorageAccountKey (Get-AzStorageAccountKey -ResourceGroupName $resourceGroupName -Name $storageAccountName).Value[0])\\n\\nWe have created our Resource Group and Storage account to hold our Azure documentation.\\n\\n### Upload Microsoft Learn documentation to an Azure blob container\\n\\nNow that we have the Azure docs repo downloaded and extracted and an Azure Storage account to hold the documents, it\'s time to use AzCopy to copy the documentation to the Azure storage account.\\nWe will use PowerShell to create a SAS token (open for a day) and use that with AzCopy to copy the Azure repo into our newly created container.\\n\\n         # Set variables\\n         $resourceGroupName = \\"azuredocs-ai-rg\\"\\n         $storageAccountName = \\"myaistgacc958b\\"\\n         $containerName = \\"azuredocs\\"\\n         $storageAccountKey = (Get-AzStorageAccountKey -ResourceGroupName $resourceGroupName -Name $storageAccountName).Value[0]\\n         $localPath = \\"C:\\\\Temp\\\\azuredocs\\\\azure-docs-main\\"\\n         $azCopyPath = \\"C:\\\\tools\\\\azcopy_windows_amd64_10.19.0\\\\AzCopy.exe\\"\\n         # Construct SAS URL for destination container\\n         $sasToken = (New-AzStorageContainerSASToken -Name $containerName -Context (New-AzStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey) -Permission rwdl -ExpiryTime (Get-Date).AddDays(1)).TrimStart(\\"?\\")\\n         $destinationUrl = \\"https://$storageAccountName.blob.core.windows.net/$containerName/?$sasToken\\"\\n         # Run AzCopy command as command line\\n         $command = \\"& `\\"$azCopyPath`\\" copy `\\"$localPath`\\" `\\"$destinationUrl`\\" --recursive=true\\"\\n         Invoke-Expression $command\\n\\n*Note: I took roughly 6 minutes to copy the Azure docs repo from my local computer (in New Zealand) into a blob storage account in East US, so roughly a gigabyte a minute.*\\n\\n![Azure Storage Account - Microsoft Learn Docs](/images/posts/AzureStorage_AzureDocs_UploadedAzCopy.png \\"Azure Storage Account - Microsoft Learn Docs\\")\\n\\n### Create Cognitive Search\\n\\nNow that we have our Azure Blob storage accounts, it\'s time to create our Cognitive Search.\\nWe will need to create a Cognitive Search, with an SKU of Standard, to support the 6GBs of Azure documents that must be indexed. Please check your costs; this is roughly NZ$377.96 a month to run; you can reduce this cost by limiting the amount of data you need to index (i.e., only certain documents, not an entire large repository of markdown files). Make sure you refer to the [Pricing Calculator](https://azure.microsoft.com/pricing/details/search/?WT.mc_id=AZ-MVP-5004796).\\n\\n         # Set variables\\n         $resourceGroupName = \\"azuredocs-ai-rg\\"\\n         $searchServiceName = \\"azuredocssearchservice\\" #Cognitive Service name needs to be lowercase.\\n         # Create a search service\\n         Install-Module Az.Search\\n         $searchService = New-AzSearchService -ResourceGroupName $resourceGroupName -Name $searchServiceName -Location \\"eastus\\" -Sku Standard\\n\\nNow that the cognitive search has been created, we need to create the index, and indexers, which will index our Azure documents to be used by Azure OpenAI by creating the index and linking it to the Azuredocs blob container, we created earlier.\\n\\n*Note: There is no PowerShell cmdlet support for Azure Cognitive Search indexes; you can create using the [RestAPI](https://learn.microsoft.com/azure/search/search-get-started-powershell?WT.mc_id=AZ-MVP-5004796) - but we will do this in the Azure Portal as part of the next step.*\\n\\n### Create Azure Cognitive Search Index\\n\\nIt\'s time to time to create the Cognitive Search Index, an indexer that will index the content.\\n\\nWe will move away from PowerShell and into the Microsoft Azure Portal to do this.\\n\\n1. Navigate to the [Microsoft Azure Portal](https://portal.azure.com/#home)\\n1. In the top center search bar, type in Cognitive Search\\n1. Click on [Cognitive Search](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/AppliedAIHub/~/CognitiveSearch)\\n1. Click on your newly created Cognitive Search\\n![Azure Portal - Cognitive Search](/images/posts/AzurePortal_CognitiveSearch_Resource.png \\"Azure Portal - Cognitive Search\\")\\n1. Select Import Data\\n1. Select Azure Blob Storage\\n![Azure Portal - Cognitive Search - Add Azure Blob Storage](/images/posts/AzurePortal_CognitiveSearch_AddDataStorage.png \\"Azure Portal - Cognitive Search\\")\\n1. Type in your data source name *(i.e., azuredocs)*\\n1. For the Connection string, select Choose an existing connection\\n1. Select your Azure Storage account and a container containing the Azure document repository uploaded earlier.\\n1. Click Select\\n![Azure Portal - Cognitive Search - Add Azure Blob Storage](/images/posts/AzurePortal_CognitiveSearch_ConnectBlobStorage.png \\"Azure Portal - Cognitive Search - Add Azure Blob Storage\\")\\n1. Click Next: Add cognitive skills (Optional)\\n1. Here, you can Enrich your data, such as enabling OCR (extracting text from images automatically) or extracting people\'s names, and translating text from one language to another; these enrichments are billed separately, and we won\'t be using any enrichments so we will select Skip to: Customize target index.\\n1. Here is the index mapping that was done by Cognitive Search automatically by scanning the schema of the documents. You can bring in additional data about your documents if you want, but I am happy with the defaults, so I click: Next: Create an indexer\\n![Azure Portal - Cognitive Search - Search Index](/images/posts/AzurePortal_CognitiveSearch_SearchIndex.png \\"Azure Portal - Cognitive Search - Search Index\\")\\n1. The indexer is what is going to create your index, which will be referenced by Azure OpenAI later; you can schedule an indexer to run hourly, if new data is being added to the Azure blob container where your source files are sitting, for my purposes I am going leave the Schedule as: Once\\n1. Uncollapse Advanced Options and scroll down a bit\\n1. Here, we can select to only index certain files; for our purposes, we are going to exclude png files, the Azure document repository contains png images files that aren\'t able to be indexed (we aren\'t using OCR), so I am going to optimize the indexing time slightly by excluding them. You can also exclude gif/jpg image files.\\n![Azure Portal - Cognitive Search - Create Search Indexer](/images/posts/AzurePortal_CognitiveSearch_CreateIndexer.png \\"Azure Portal - Cognitive Search - Create Search Indexer\\")\\n1. Finally, hit Submit to start the indexing process. *This could take a while, depending on the amount of data*\\n1. Leave this running in the background and navigate to the Cognitive Search resource, Overview pane to see the status.\\n![Azure Portal - Cognitive Search - Indexer](/images/posts/AzurePortal_CognitiveSearch_OverviewIndexer.png.png \\"Azure Portal - Cognitive Search - Indexer\\")\\n\\n*Note: You can also run the Import Data in Azure OpenAI Studio, which will trigger an index - but you need to keep your browser open and responsive. Depending on how much data you are indexing, doing it manually through this process could be preferred to avoid browser timeout. You also get more options around the index.*\\n\\n### Create Azure OpenAI\\n\\nNow that we have our base Azure resources, it\'s time to create Azure OpenAI.\\nMake sure your region and subscription have been approved for Azure OpenAI.\\n\\nRun the following PowerShell cmdlets to create the Azure OpenAI service:\\n\\nTo create the Azure OpenAI service, we will be using the Azure Portal.\\n\\n1. Navigate to the [Microsoft Azure Portal](https://portal.azure.com/#home)\\n1. In the top center search bar, type in Azure OpenAI\\n1. In the Cognitive Services, [Azure OpenAI](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/OpenAI) section, click + Create\\n1. Select your subscription, region, name, and pricing tier of your Azure OpenAI service (remember certain [models](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models?WT.mc_id=AZ-MVP-5004796#model-summary-table-and-region-availability) are only available in specific regions - we need GPT 3.5+), then select Next\\n![Azure OpenAI - Create Resource](/images/posts/AzurePortal_AzureOpenAI_Create.png \\"Azure OpenAI - Create Resource\\")\\n1. Update your Network Configuration; for this demo, we will select \'All Networks\' - but the best practice is to restrict it. Click Next\\n![Azure OpenAI - Create Resource](/images/posts/AzurePortal_AzureOpenAI_CreateSelectNetwork.png \\"Azure OpenAI - Create Resource\\")\\n1. If you have any Tags, enter them, then click Next\\n1. The Azure platform will now validate your deployment (an example is ensuring that the Azure OpenAI has a unique name). Review the configuration, then click Create to create your resource.\\n![Azure OpenAI - Create Resource](/images/posts/AzurePortal_AzureOpenAI_CreateReview.png \\"Azure OpenAI - Create Resource\\")\\n\\nNow that the Azure OpenAI service has been created, you should now have the following:\\n\\n* An Azure OpenAI service\\n* A Storage account\\n* A Cognitive Search service\\n\\n![Azure OpenAI - RAG Deployed Resources](/images/posts/AzurePortal_AzureOpenAI_BYOD_Resources.png \\"Azure OpenAI - RAG Deployed Resources\\")\\n\\n### Deploy Model\\n\\nNow that we have our Azure OpenAI instance, it\'s time to deploy our Chat model.\\n\\n1. Navigate to the [Microsoft Azure Portal](https://portal.azure.com/#home)\\n1. In the top center search bar, type in Azure OpenAI\\n1. Open your Azure OpenAI instance to the Overview page, and click: Go to Azure OpenAI Studio\\n1. Click on Models, and verify that you have gpt models *(ie, gpt-36-turbo, or gpt-4)*. If you don\'t, then make sure you have been [onboarded](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service?WT.mc_id=AZ-MVP-5004796).\\n1. Once verified, click on Deployments\\n1. Click on + Create new deployment\\n1. Select your model (I am going to go with gpt-35-turbo), type in a deployment name, and then click Create\\n1. Once deployment has been completed, you may have to wait up to 5 minutes for the Chat API to be aware of your new deployment.\\n\\n![Azure OpenAI - Deploy Model](/images/posts/Create_AzureOpenAI_GPT3.5_Deployment.gif \\"Azure OpenAI - Deploy Model\\")\\n\\n### Run Chat against your data\\n\\nFinally, it\'s time to query and work with our Azure documents.\\n\\nWe can do this using the Chat Playground, a feature of Azure OpenAI that allows us to work with our Chat models and adjust [prompts](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions&WT.mc_id=AZ-MVP-5004796).\\n\\nWe will not change the System Prompt (although to get the most out of your own data, I recommend giving it ago); the System Prompt will remain as: You are an AI assistant that helps people find information.\\n\\n1. Navigate to the [Microsoft Azure Portal](https://portal.azure.com/#home)\\n1. In the top center search bar, type in Azure OpenAI\\n1. Open your Azure OpenAI instance to the Overview page, and click: Go to Azure OpenAI Studio\\n1. Click Chat\\n1. Click Add your data (preview) - if this doesn\'t show, ensure you have deployed a GPT model as part of the previous steps.\\n1. Click on + Add a data source\\n1. Select the dropdown list in the Select data source pane and select Azure Cognitive Search\\n1. Select your Cognitive Search service, created earlier\\n1. Select your Index\\n1. Check, I acknowledge that connecting to an Azure Cognitive Search account will incur usage to my account. [View Pricing](https://azure.microsoft.com/en-us/pricing/details/search/?WT.mc_id=AZ-MVP-5004796)\\n1. Click Next\\n1. It should bring in the index metadata; for example - our content data is mapped to content - so I will leave this as is; click Next\\n1. I am not utilizing Semantic search, so I click Next\\n1. Finally, review my settings and click Save and Close\\n1. Now we can verify our own data is getting checked by leaving the: Limit responses to your own data content checked\\n1. Then, in the Chat session, in the User message, I type: Tell me about Azure Elastic SAN?\\n1. It will now reference the Cognitive Search and bring in the data from the index, including references to the location where it found the data!\\n\\n![Azure OpenAI - Chat Playground](/images/posts/Query_AzureOpenAI_ChatPlayground_AzureDocs.gif \\"Azure OpenAI -Chat Playground\\")\\n\\n### Optional - Deploy to an Azure WebApp\\n\\nInteracting with our data in the Chat playground can be an enlightening experience, but we can go a step further and leverage the native tools to a chat interface - straight to an Azure web app.\\n\\nTo do this, we need to navigate back to the Chat playground, ensure you have added your own cognitive search, and can successfully retrieve data from your index.\\n\\n1. Click on Deploy to\\n1. Select A new web app...\\n1. If you have an existing WebApp, you can update it with an updated System Message or Search Index from the Chat playground settings, but we will: Create a new web app\\n1. Enter a suitable name (i.e., AI-WebApp-Tst - this needs to be unique)\\n1. Select the Subscription and Resource Group and location to deploy to. I had issues accessing my custom data, when deployed to Australia East (as AI services are in East US), so I will deploy in the same region as my OpenAI and Cognitive Search service - i.e., East US.\\n1. Specify a plan (i.e., Standard (S1))\\n1. Click Deploy\\n\\n![Azure OpenAI - Deploy](/images/posts/Deploy_AzureOpenAI_ChatPlayground_WebApp.gif \\"Azure OpenAI -Chat Playground\\")\\n\\nNote: Deployment may take up to 10 minutes to deploy; you can navigate to the Resource Group you are deploying to, select Deployments, and monitor the deployment. Once deployed, it can take another 10 minutes for authentication to be configured.\\n\\nNote: By default, the Azure WebApp is restricted to only be accessible by yourself; you can expand this out by adjusting the [authentication](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-studio&WT.mc_id=AZ-MVP-5004796#important-considerations).\\n\\nOnce it is deployed, you can access the Chat interface, directly from an Azure WebApp!\\n\\n![Azure OpenAI - Run Azure WebApp](/images/posts/Run_AzureOpenAI_ChatPlayground_WebApp.gif \\"Azure OpenAI - Run Azure WebApp\\")\\n\\nIf you navigate to the WebApp resource in the Azure Portal, and look at the Configuration and Application Settings of your WebApp, you can see variables used as part of the deployment. You can adjust these, but be wary as it could break the WebApp, I would recommend redeploying/updating the WebApp for major changes, from Azure OpenAI studio.\\n![Azure OpenAI - App Service - App Settings](/images/posts/AzureOpenAI_AzureAppService_Deployment_AppSettings.png \\"Azure OpenAI - App Service - App Settings\\")"},{"id":"azure/Call-to-LLM-failed","metadata":{"permalink":"/azure/Call-to-LLM-failed","source":"@site/blog/2023-07-04-Call-to-LLM-failed.md","title":"Azure OpenAI - Call to LLM failed","description":"I recently, ran into an error  when attempting to use Azure OpenAI and custom data, in the Chat Playground.","date":"2023-07-04T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.86,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure OpenAI - Call to LLM failed","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/posts/CalltoLLMFailed_AzureOpenAIError.png"},"date":"2023-07-04 00:00:00 +1300","slug":"azure/Call-to-LLM-failed"},"unlisted":false,"prevItem":{"title":"Bring Your Data to Life with Azure OpenAI","permalink":"/azure/Bring-Your-Data-to-Life-with-Azure-OpenAI"},"nextItem":{"title":"Azure Management Groups not displaying in the Azure Portal","permalink":"/azure/Management-Groups-not-displaying-AzurePortal"}},"content":"I recently, ran into an error  when attempting to use Azure OpenAI and custom data, in the Chat Playground.\\nThe error I was getting was:\\n\\n> Error\\n> Cannot connect to host aidocsopenaiaccount.openai.azure.com:443 ssl:default [Name of Service not known]\\n> Call to LLM failed.\\n\\n![Call to LLM failed.](/images/posts/CalltoLLMFailed_AzureOpenAIError.png \\"Azure OpenAI - Call to LLM failed.\\")\\n\\nAs I was calling data, that was indexed by Azure Cognitive Search, I thought the index was corrupted or invalid, but after mulitple index attempts, continuted to have the same error, all other prompts (not using my custom data) succeeded.\\n\\nI recreated the Azure OpenAI service, and was able to succesffuly call the custom data, so started to look at the comparisons between the Azure OpenAI instance, that was working and the Azure OpenAI instance that wasn\'t.\\n\\nI did this by taking a look at the JSON of each Azure OpenAI instance, then did a comparison, and discovered a major difference.\\n\\n![Azure OpenAI - Diff](/images/posts/AzureOpenAI_CalltoLLMFailed_ErrorJSON.png \\"Azure OpenAI - Diff\\")\\n\\n*Note: The one on the left is the working one, the one of the right is the failed one.*\\n\\nThe version that was working, had an endpoint of https://*.openai.azure.com , and the version that didn\'t work, had a different endpoint: eastus.api.cognitive.microsoft.com.\\n\\nYou can check the JSON output of a resource, by navigating to it on the Azure Portal, in the Overview Pane, clicking JSON view.\\n\\n![Azure OpenAI - JSON View](/images/posts/CalltoLLMFailed_AzureOpenAI_JSON_View.png \\"Azure OpenAI - JSON View\\")\\n\\nSo why would this have been the case?\\n\\nWhen I originally created the Azure OpenAI instance that had this issue, I used PowerShell to create the instance:\\n\\n    $skuName = \\"S0\\"\\n    $kind = \\"OpenAI\\"\\n    # Create Cognitive Services resource\\n    New-AzCognitiveServicesAccount -ResourceGroupName $resourceGroupName -Name $accountName -Location $location -SkuName $skuName -Kind $kind\\n\\nThis seemed to call an older API or different schema.\\n\\nThe version that worked, I created using the Azure Portal - which was correct.\\n\\n**If you run into the same problem, then recreate your Azure OpenAI instance using the Azure Portal, or one of the currently supported methods: [Create a resource and deploy a model using Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal&WT.mc_id=AZ-MVP-5004796), currently (as of the 4th of July 2023) PowerShell is not a supported method of creating an Azure OpenAI instance, until the cmdlets have been updated.**"},{"id":"azure/Management-Groups-not-displaying-AzurePortal","metadata":{"permalink":"/azure/Management-Groups-not-displaying-AzurePortal","source":"@site/blog/2023-06-24-Management-Groups-not-displaying-AzurePortal.md","title":"Azure Management Groups not displaying in the Azure Portal","description":"When logging into the Microsoft Azure Portal, to view your Management Group You might have found it blank or constantly attempting to load.","date":"2023-06-24T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.55,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Management Groups not displaying in the Azure Portal","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/azure-management-group-notloading.png"},"date":"2023-06-24 00:00:00 +1300","slug":"azure/Management-Groups-not-displaying-AzurePortal"},"unlisted":false,"prevItem":{"title":"Azure OpenAI - Call to LLM failed","permalink":"/azure/Call-to-LLM-failed"},"nextItem":{"title":"Cleanup your unwanted Azure resources on a schedule","permalink":"/azure/Cleanup-your-unwanted-Azure-resources-on-a-schedule"}},"content":"When logging into the Microsoft Azure Portal, to view your [Management Group](https://learn.microsoft.com/azure/governance/management-groups/overview?WT.mc_id=AZ-MVP-5004796 \\"What are Azure management groups?\\") You might have found it blank or constantly attempting to load.\\n\\n![Azure Management Group - Not loading](/images/posts/azure-management-group-notloading.png \\"Azure Management Group - Not loading\\")\\n\\nIt looks like a potential bug in the Portal interface, especially  if you have the correct permissions to see those Management Groups. Here is a few things to look for:\\n\\n### Elevated rights - Global Administrator\\n\\nBy default, if you are a Global Administrator, it does not allow you to see Azure Resources and manage Azure management groups.\\n\\nIf you have a Global Administrator role, and this is the first time you are setting up Management Groups, then you can [elevate](https://learn.microsoft.com/en-us/azure/role-based-access-control/elevate-access-global-admin?WT.mc_id=AZ-MVP-5004796#elevate-access-for-a-global-administrator) your rights, to manage the Azure Management Groups.\\n\\nNote: Because of the elevated nature of this role, it is recommended to enable it only for the period you need to do your work, and make sure you have [specific roles assigned to](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal?WT.mc_id=AZ-MVP-5004796) manage your Azure infrastructure, as necessary.\\n\\nIf this is not the first time you set up Management Groups, ensure you have the right to see the Management Groups.\\n\\n### Create an Azure Management Group using PowerShell\\n\\nOne of the fixes (workaround) discovered is that creating a Management Group triggers the Azure Portal to update, allowing management.\\n\\nYou can do this using the [Azure PowerShell](https://learn.microsoft.com/powershell/azure/install-azure-powershell?view=azps-10.0.0&WT.mc_id=AZ-MVP-5004796) cmdlets, by running:\\n\\n         New-AzManagementGroup -GroupName \'Contoso\'\\n\\nOnce the Management Group is created, you should be able to refresh the [Azure Management Group](https://portal.azure.com/#view/Microsoft_Azure_ManagementGroups/ManagementGroupBrowseBlade/%7E/MGBrowse_overview) page in the Portal and view your Management Groups, if that doesn\'t work then log out and back into the Portal.\\n\\nYou can then use the Remove cmdlet to delete the new Management Group you created.\\n\\n         Remove-AzManagementGroup -GroupName \'Contoso\'\\n\\nNote: This article was based on findings from the Microsoft Q&A article: [Management Groups Unavailable in Tenant: Limited Account Control and Organization](https://learn.microsoft.com/en-us/answers/questions/1315218/management-groups-unavailable-in-tenant-limited-ac?WT.mc_id=AZ-MVP-5004796)"},{"id":"azure/Cleanup-your-unwanted-Azure-resources-on-a-schedule","metadata":{"permalink":"/azure/Cleanup-your-unwanted-Azure-resources-on-a-schedule","source":"@site/blog/2023-06-06-Cleanup-your-unwanted-Azure-resources-on-a-schedule.md","title":"Cleanup your unwanted Azure resources on a schedule","description":"Cleanup your unwanted Azure resources on a schedule","date":"2023-06-06T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":11.445,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Cleanup your unwanted Azure resources on a schedule","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/images/posts/CleanupyourUnwantedAzureResourcesonaSchedule.png"},"date":"2023-06-06 00:00:00 +1300","slug":"azure/Cleanup-your-unwanted-Azure-resources-on-a-schedule"},"unlisted":false,"prevItem":{"title":"Azure Management Groups not displaying in the Azure Portal","permalink":"/azure/Management-Groups-not-displaying-AzurePortal"},"nextItem":{"title":"Getting Started with Azure Elastic SAN","permalink":"/azure/Bytes-Blocks-and-Elasticity-Getting-Started-with-Azure-Elastic-SAN"}},"content":"Cleanup your unwanted Azure resources on a schedule\\n\\nEvery few months, I get that dreaded email \\"Your Microsoft Azure subscription has been suspended\\" - this is due to creating resources, and leaving them provisioned, so I needed a method of deleting the resources I didn\'t need, or wanted to spin up for a few days. I also needed away to creating resources that can stay, either for learning or a demo, independent of how the resources were deployed into the environment *(via the Azure Portal, Terraform, Bicep)*.\\n\\nNaturally I went straight to [Azure Automation](https://learn.microsoft.com/azure/automation/?WT.mc_id=AZ-MVP-5004796 \\"Azure Automation documentation\\") and using PowerShell.\\n\\nWhat I ended up with was a Runbook capable of **EXTREME AZURE DESTRUCTION** which was exactly what I wanted.\\n\\n> This script is provided as-is with no warranties or guarantees. Use at your own risk. This is not intended to be a script to use in Production, mainly test environments, as this WILL CAUSE massive destruction and irretrievable data loss... You have been warned.\\n\\nI am not going to go into setting up Azure Automation, if interested you can refer to a few of my blog posts I have done previously that goes through the process:\\n\\n* [Deallocate \u2018Stopped\u2019 Virtual Machines using Azure Automation](https://luke.geek.nz/azure/deallocate-stopped-virtual-machines-using-azure-automation/ \\"Deallocate \u2018Stopped\u2019 Virtual Machines using Azure Automation\\")\\n* [Turn on a Azure Virtual Machine using Azure Automation](https://luke.geek.nz/azure/turn-on-a-azure-virtual-machine-using-azure-automation/ \\"Turn on a Azure Virtual Machine using Azure Automation\\")\\n* [Disable SFTP support on an Azure Storage account on a Schedule](https://luke.geek.nz/azure/disable-sftp-support-on-an-azure-storage-account-on-a-schedule/ \\"Disable SFTP support on an Azure Storage account on a Schedule\\")\\n\\nThe script named: Invoke-DakaraSuperWeapon, aptly named as a reference to the Dakara weapon from the TV series Stargate SG1 - a weapon if great power.\\n\\n> The\xa0[Dakara superweapon](https://stargate.fandom.com/wiki/Dakara_superweapon)\xa0was a\xa0Ancient\xa0device capable of reducing all matter to its basic elemental components, and/or restructuring it. Possessing the ability to pass through the\xa0shields\xa0of known ships it also functions (and has been used) as a devastating weapon to kill the entire crew of orbiting ships or wipe out all life on the surface of hundreds of planets at a time. \\"It is not only capable of destroying the Replicators but all life in the galaxy.\\"\\n\\n![Azure Dakara superweapon](/images/posts/CleanupyourUnwantedAzureResourcesonaSchedule.png \\"Azure Dakara superweapon\\")\\n\\nUsing the latest Windows PowerShell release - 7.2 (Preview), this script is built around the following capabilities:\\n\\n* **Delete ALL resource groups** (without a specific Tag) **under all subscriptions**, under a specific **Management Group**\\n* **Delete all resources** within those resource groups\\n* **Delete** Azure **Recovery Vaults** and their backed up items\\n* **Delete** any **Azure policy assignments**, assigned directly to any subscription under the Management Group\\n* **Delete** any Azure **RBAC role assignments**, assigned directly to any subscription under the Management Group.\\n\\nIn my demo environment, I have a range of Management Groups, and 2 Azure subscriptions.\\n\\n![Luke\'s Azure Management Group structure](/images/posts/VisualStudio_Luke_MG_Structure.png \\"Luke\'s Azure Management Group structure\\")\\\\`\\n\\nFor my purposes, I created a [System Managed Identity](https://learn.microsoft.com/azure/automation/enable-managed-identity-for-automation?WT.mc_id=AZ-MVP-5004796 \\"Using a system-assigned managed identity for an Azure Automation account\\") from the Azure Automation account, and applied it to the: \'mg\' Management Group as \'Owner\' (Contributor will work, as long as you don\'t plan on removing the rights from the Azure subscriptions - theoretically, so could Contributor + User Access Administrator roles).\\n\\nAgain - this was created for my own environment - if you decide to run this, TEST IT! And Make sure it has as limited permissions as possible, potentially the Managed Identity will only have access to a specific test Subscription that you may not care about. I take no responsibily.\\n\\nThe System Identity will be used to execute the runbook.\\n\\nI also needed a Tag *(ie a Safe word)* to save the Resource Groups that I need to remain, an example is a project I am working on, demo etc. This Tag is in name only - as Tags are Key/Value pairs in Azure - in this case I only cared about the Key (ie NotDelete) - what was in the value, didn\'t matter.\\n\\n![NotDelete - Azure Tag](/images/posts/Initiate-DakaraSuperWeapon_SafeWord.png \\"NotDelete - Azure Tag\\")\\n\\nImportant: When importing the Runbook it is imperative that you Tag the Resource Group it is in, with your safe word! Or else could will be deleted!\\n\\nThe script has a couple of parameters:\\n\\n| Parameter                | Type    | Notes                                                                                                                                                                                                                                                                                                                                           |\\n| ------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| ManagementGroupId        | String  | The ID of the management group to delete resource groups under. WARNING: This script will delete all resource groups under the specified management group except for the ones with the specified tag. Make sure you have specified the correct management group ID, or you may accidentally delete resources that you did not intend to delete. |\\n| TagName                  | String  | The name of the tag to check for. WARNING: This script will delete all resource groups that do not have this tag. Make sure you have specified the correct tag name, or you may accidentally delete resources that you did not intend to delete.                                                                                                |\\n| RemoveResourceGroups     | Boolean | True or False, do you want to Remove the Resource Groups? True means it will, and False means it will skip the Resource Group deletion.                                                                                                                                                                                                         |\\n| DeletePolicyAssignments  | Boolean | True or False, do you want to Remove the Azure Policy assignments on the subscriptions? True means it will, and False means it will skip the Azure Policy assignment deletion.                                                                                                                                                                  |\\n| DeleteSubRoleAssignments | Boolean | This will need Owner rights (or User Administrator role) in order to remove roles from a Subscription. Make sure your rights are set to be inherited from a Management Group, before running this. True or False, True means it will delete the Subscription direct assignments, False means it will skip it.                                   |\\n\\nAs you can tell, you can enable or disable specific parts of the script, for example - if you just want to use it to clean up direct role assignments on your subscriptions, while not deleting Azure resources you can by entering True or False.\\n\\n![Initiate-DakaraSuperWeapon - Azure Runbook Parameters](/images/posts/Initiate-DakaraSuperWeapon_Parameters.png \\"Initiate-DakaraSuperWeapon - Azure Runbook Parameters\\")\\n\\nWhen ran it will stream the Logs to the Azure Automation Log Stream, there is no waiting time or approval - it will just run.\\n\\n![Initiate-DakaraSuperWeapon - Azure Automation Log Stream](/images/posts/Initiate-DakaraSuperWeapon_Delete.png \\"Initiate-DakaraSuperWeapon - Azure Automation Log Stream\\")\\n\\nAs below, you can see the Resource Groups get removed (at the time of this recording, I had a limit on the amount of [parallel](https://devblogs.microsoft.com/powershell/powershell-foreach-object-parallel-feature/?WT.mc_id=AZ-MVP-5004796 \\"PowerShell ForEach-Object Parallel Feature\\") delete tasks:\\n\\n![Remove Azure Resoure Groups](/images/posts/Remove_AzResource.gif \\"Remove Azure Resoure Groups\\")\\n\\n```powershell title=\\"Initiate-DakaraSuperWeapon.ps1\\"\\n# This runbook deletes all resource groups under a management group except for the ones with a specific tag.\\n<#\\n.SYNOPSIS\\nDeletes all resource groups under a management group except for the ones with a specific tag.\\n\\n.DESCRIPTION\\nThis script deletes all resource groups under a specified management group except for the ones with a specific tag. It can also delete policy assignments and subscription role assignments if specified.\\n\\n.PARAMETER ManagementGroupId\\nThe ID of the management group to delete resource groups under. WARNING: This script will delete all resource groups under the specified management group except for the ones with the specified tag. Make sure you have specified the correct management group ID, or you may accidentally delete resources that you did not intend to delete.\\n\\n.PARAMETER TagName\\nThe name of the tag to check for. WARNING: This script will delete all resource groups that do not have this tag. Make sure you have specified the correct tag name, or you may accidentally delete resources that you did not intend to delete.\\n\\n.PARAMETER RemoveResourceGroups\\nIf specified, deletes the resource groups that do not have the specified tag.\\n\\n.PARAMETER DeletePolicyAssignments\\nIf specified, deletes the policy assignments for the management group and all child subscriptions.\\n\\n.PARAMETER DeleteSubRoleAssignments\\nIf specified, deletes the subscription role assignments for all child subscriptions.\\n\\n.EXAMPLE\\n.\\\\Initiate-DakaraSuperWeapon.ps1 -ManagementGroupId \\"my-management-group\\" -TagName \\"my-tag\\" -RemoveResourceGroups -DeletePolicyAssignments -DeleteSubRoleAssignments\\nDeletes all resource groups under the \\"my-management-group\\" management group that do not have the \\"my-tag\\" tag, and deletes the policy assignments and subscription role assignments for all child subscriptions.\\n\\n.NOTES\\nThis script requires the Azure PowerShell module to be installed. It also requires Owner rights (or User Administrator role) in order to remove roles from a subscription. Make sure your rights are set to be inherited from a management group before running this script.\\n#>\\n\\nparam (\\n    [Parameter(Mandatory = $true, HelpMessage = \\"The ID of the management group to delete resource groups under. WARNING: This script will delete all resource groups under the specified management group except for the ones with the specified tag. Make sure you have specified the correct management group ID, or you may accidentally delete resources that you did not intend to delete.\\")]\\n    [string]$ManagementGroupId,\\n    \\n    [Parameter(Mandatory = $true, HelpMessage = \\"The name of the tag to check for. WARNING: This script will delete all resource groups that do not have this tag. Make sure you have specified the correct tag name, or you may accidentally delete resources that you did not intend to delete.\\")]\\n    [string]$TagName,\\n       \\n    [Parameter(Mandatory = $false)]\\n    [switch][bool]$RemoveResourceGroups = $false,\\n    \\n    [Parameter(Mandatory = $false)]\\n    [switch][bool]$DeletePolicyAssignments = $false,\\n\\n    [Parameter(Mandatory = $false, HelpMessage = \\"This will need Owner rights (or User Administrator role) in order to remove roles from a Subscription. Make sure your rights are set to be inherited from an Management Group, before running this.\\")]\\n    [switch][bool]$DeleteSubRoleAssignments = $false\\n)\\n\\n# Convert string values to boolean values\\n$RemoveResourceGroups = [System.Boolean]::Parse($RemoveResourceGroups)\\n$DeletePolicyAssignments = [System.Boolean]::Parse($DeletePolicyAssignments)\\n$DeleteSubRoleAssignments = [System.Boolean]::Parse($DeleteSubRoleAssignments)\\n\\n# Ensures you do not inherit an AzContext in your runbook\\nDisable-AzContextAutosave -Scope Process\\n\\n#Toggle to stop warnings with regards to Breaking Changes in Azure PowerShell\\nSet-Item -Path Env:\\\\SuppressAzurePowerShellBreakingChangeWarnings -Value $true\\n\\n# Connect to Azure with system-assigned managed identity\\n(Connect-AzAccount -Identity).context\\n\\n# Write an initial log message\\nWrite-Output \\"Initilizing superweapon....\\"\\n\\n# Get the subscription IDs under the specified management group AND child management groups\\nfunction Get-AzSubscriptionsFromManagementGroup {\\n    param($ManagementGroupName)\\n    $mg = Get-AzManagementGroup -GroupId $ManagementGroupName -Expand\\n    foreach ($child in $mg.Children) {\\n        if ($child.Type -match \'/managementGroups$\') {\\n            Get-AzSubscriptionsFromManagementGroup -ManagementGroupName $child.Name\\n        }\\n        else {\\n            $child | Select-Object @{N = \'Name\'; E = { $_.DisplayName } }, @{N = \'Id\'; E = { $_.Name } }\\n        }\\n    }\\n}\\n$mgid = Get-AzManagementGroup -GroupId $ManagementGroupID -Expand\\n\\n$subIds = (Get-AzSubscriptionsFromManagementGroup -ManagementGroupName $mgid.DisplayName).id\\n\\n\\n# Delete the policy assignments\\n\\nif ($DeletePolicyAssignments -eq $true) {\\n    Write-Output \\"Deleting management group policy assignments...\\"\\n    Get-AzPolicyAssignment -Scope $mgid.Id | Remove-AzPolicyAssignment -Verbose\\n    Write-Output \\"Deleting subscription group policy assignments...\\"\\n\\n    foreach ($subId in $subIds) {\\n        Write-Output \\"Setting subscription context...\\"\\n        Set-AzContext -Subscription $subId\\n        Write-Output \\"Deleting subscription group policy assignments...\\"\\n        Get-AzPolicyAssignment -Scope \\"/subscriptions/$($subId)\\" | Remove-AzPolicyAssignment -Verbose\\n\\n    }\\n}\\nelse {\\n    Write-Output \\"Skipping policy assignment deletion...\\"\\n}\\n\\n# Delete the resource groups\\nif ($RemoveResourceGroups -eq $true) {\\n    Write-Output \\"Deleting resource groups...\\"\\n\\n    if ($null -ne $subIds -and $subIds.Count -gt 0) {\\n\\n        foreach ($subId in $subIds) {\\n            Write-Output \\"Setting subscription context...\\"\\n            Set-AzContext -Subscription $subId\\n\\n            $ResourceGroupsfordeletion = Get-AzResourceGroup | Where-Object { $_.Tags -eq $null -or $_.Tags.ContainsKey($tagName) -eq $false }\\n            Write-Output \\"The following Resource Groups will be deleted...\\"\\n            Write-Output -InputObject $ResourceGroupsfordeletion\\n\\n            ## Checks to see if a Recovery Services Vaults exists, the Recovery Services Vault and backups need to be deleted first.\\n            $RSV = Get-AzRecoveryServicesVault | Where-Object { $_.ResourceGroupName -in $ResourceGroupsfordeletion.ResourceGroupName }\\n            if ($null -ne $RSV) {\\n\\n                ForEach ($RV in $RSV) {\\n                    Write-Output  \\"Backup Vault deletion supports deletion of Azure VM backup vaults ONLY currently.\\"\\n                    #Credit to Wim Matthyssen for reference in the backup section of the script - https://wmatthyssen.com/2020/11/17/azure-backup-remove-a-recovery-services-vault-and-all-cloud-backup-items-with-azure-powershell/\\n                    Set-AzRecoveryServicesVaultProperty -Vault $RV.ID -SoftDeleteFeatureState Disable\\n                    Set-AzRecoveryServicesVaultContext -Vault $RV\\n                    $containerSoftDelete = Get-AzRecoveryServicesBackupItem -BackupManagementType AzureVM -WorkloadType AzureVM | Where-Object { $_.DeleteState -eq \\"ToBeDeleted\\" }\\n \\n                    foreach ($item in $containerSoftDelete) {\\n                        Undo-AzRecoveryServicesBackupItemDeletion -Item $item  -Force -Verbose\\n                    }\\n\\n                    $containerBackup = Get-AzRecoveryServicesBackupItem -BackupManagementType AzureVM -WorkloadType AzureVM  | Where-Object { $_.DeleteState -eq \\"NotDeleted\\" }\\n                    foreach ($item in $containerBackup) {\\n                        Disable-AzRecoveryServicesBackupProtection -Item $item -RemoveRecoveryPoints -Force -Verbose\\n                    }\\n                    Remove-AzRecoveryServicesVault -Vault $RV -Verbose\\n\\n                }\\n\\n            }\\n            ## Checks to see if a Azure Resource Mover resource exists, as this need to be deleted first.\\n\\n            $ARM = Get-AzResource | Where-Object { $_.ResourceGroupName -in $ResourceGroupsfordeletion.ResourceGroupName -and $_.ResourceType -eq \'Microsoft.Migrate/moveCollections\' }\\n\\n            Write-Output -InputObject $ARM\\n\\n            if ($null -ne $ARM) {\\n\\n                ForEach ($RM in $ARM) {\\n                    Write-Output  \\"Azure Resource Mover collections exists.\\"\\n                    Write-Output -InputObject $RM\\n                    $a = Get-AzResourceMoverMoveResource -ResourceGroupName $RM.ResourceGroupName -MoveCollectionName $RM.Name\\n                    Foreach ($b in $a) {\\n                        Write-Output -InputObject $b\\n                        # Remove a resource using the resource ID\\n                        Invoke-AzResourceMoverDiscard -ResourceGroupName $RM.ResourceGroupName -MoveResourceInputType $b.Id -MoveResource $b.Name\\n                        Remove-AzResourceMoverMoveResource -ResourceGroupName $RM.ResourceGroupName -MoveCollectionName $RM.Name -Name $b.Name -Verbose\\n                    }\\n                \\n                    Remove-AzResourceMoverMoveCollection -ResourceGroupName $RM.ResourceGroupName -MoveCollectionName $RM.Name\\n                }\\n\\n            }\\n\\n            Write-Output \\"Deleting resource groups...\\"\\n            $ResourceGroupsfordeletion | ForEach-Object -Parallel {\\n                Remove-AzResourceGroup -Name $_.ResourceGroupName -Force\\n            } -ThrottleLimit 20 -Verbose\\n\\n    \\n            # Remove the Network Watcher resource group - if remaining - in some scenarios the script left this RG behind.\\n            # Get the resource group with the specified tag\\n            $networkWatcherRG = Get-AzResourceGroup | Where-Object { $_.ResourceGroupName -eq \'NetworkWatcherRG\' }\\n            if ($null -ne $networkWatcherRG -and $null -ne $networkWatcherRG.Tags -and $networkWatcherRG.Tags.ContainsKey($tagName) -eq $false) {\\n                Remove-AzResourceGroup -Name $networkWatcherRG.ResourceGroupName -Force -ErrorAction Continue -Verbose\\n            }     \\n        }\\n\\n        # Write a final log message\\n        Write-Output \\"Resource group deletion process completed.\\"\\n    }\\n    else {\\n        Write-Output \\"No child subscriptions found under the specified management group.\\"\\n    }\\n\\n}\\nelse {\\n    Write-Output \\"Skipping resource group deletion...\\"\\n}\\n\\nif ($DeleteSubRoleAssignments -eq $true) {\\n    if ($null -ne $subIds -and $subIds.Count -gt 0) {\\n\\n        foreach ($subId in $subIds) {\\n            Write-Output \\"Setting subscription context...\\"\\n            Set-AzContext -Subscription $subId\\n            $roleAssignments = Get-AzRoleAssignment -Scope \\"/subscriptions/$($subId)\\" -IncludeClassicAdministrators\\n            Write-Output -InputObject $roleAssignments\\n            # Loop through each role assignment and delete it if it is not inherited a management group\\n            foreach ($roleAssignment in $roleAssignments) {\\n                if ($roleAssignment.Scope -like \\"/subscriptions/*\\" -and $null -ne $roleAssignment.ObjectId -and $roleAssignment.ObjectId -ne \\"\\") {\\n                    Write-Output \\"Deleting role assignment...\\"\\n                    Remove-AzRoleAssignment -Scope $roleAssignment.Scope -ObjectId $roleAssignment.ObjectId -RoleDefinitionName $roleAssignment.RoleDefinitionName -Verbose -ErrorAction Continue \\n                }\\n            }\\n            Write-Output \\"Deleting subscription role assignments...\\"\\n        }\\n\\n    }\\n\\n}\\nelse {\\n    Write-Output \\"Skipping policy subscription role assignments deletion...\\"\\n}\\n```\\n\\nUsing the Azure Automation schedule, I can then set this Runbook to run every Day, Week etc - knowing my environment will be fresh for my next project, learning exercise."},{"id":"azure/Bytes-Blocks-and-Elasticity-Getting-Started-with-Azure-Elastic-SAN","metadata":{"permalink":"/azure/Bytes-Blocks-and-Elasticity-Getting-Started-with-Azure-Elastic-SAN","source":"@site/blog/2023-05-28-Bytes-Blocks-and-Elasticity-Getting-Started-with-Azure-Elastic-SAN.md","title":"Getting Started with Azure Elastic SAN","description":"Deployment and look into Azure Elastic SAN","date":"2023-05-27T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":14.655,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Getting Started with Azure Elastic SAN","authors":["Luke"],"tags":["Azure"],"description":"Deployment and look into Azure Elastic SAN","toc":"True","header":{"teaser":"/images/posts/BytesBlocksAzureElasticSAN_Blog_Heading.gif"},"date":"2023-05-27T12:00:00.000Z","slug":"azure/Bytes-Blocks-and-Elasticity-Getting-Started-with-Azure-Elastic-SAN"},"unlisted":false,"prevItem":{"title":"Cleanup your unwanted Azure resources on a schedule","permalink":"/azure/Cleanup-your-unwanted-Azure-resources-on-a-schedule"},"nextItem":{"title":"Azure Policy Inheritance","permalink":"/azure/Demystifying-Azure-Policy-Inheritance-Understanding-the-Hierarchy-of-Control"}},"content":"Today we are going to take a look at the [Microsoft Azure Elastic SAN](https://azure.microsoft.com/products/storage/elastic-san/?WT.mc_id=AZ-MVP-5004796).\\n\\n#### Introduction\\n\\n> \u201cAzure Elastic SAN is a unique cloud-native and fully managed storage area network (SAN) service. Combining SAN-like capabilities with the benefits of being a cloud-native service, Azure Elastic SAN offers a massively scalable, cost-effective, high-performance, and resilient storage solution. It can connect to a variety of Azure compute services, enabling you to seamlessly transition your SAN data estate to the cloud without having to refactor your application architectures.\u201d\\n\\nAzure Elastic SAN provides a storage solution that is highly scalable, cost-effective, high-performing, and resilient. It caters to various storage needs, whether you\'re migrating your on-premises SAN to the cloud or creating your application directly in the cloud.\\n\\n> As Azure Elastic SAN is still in the preview stage, as of 28/05/2023, it is important to note that its [features\\n> and functionality](https://learn.microsoft.com/azure/storage/elastic-san/elastic-san-introduction?WT.mc_id=AZ-MVP-5004796#support-for-azure-storage-features) may change before it reaches production. Microsoft continues to actively gather feedback from users and refine the offering to ensure a seamless experience when it finally becomes\\n> generally available. Request access to the Preview by filling out this [form](https://aka.ms/AzureElasticSANPreviewAccess). This feature should not be used for production workloads until General Availability (GA).\\n\\n![Bytes, Blocks and Elasticity - Getting Started with Azure Elastic SAN](/images/posts/BytesBlocksAzureElasticSAN_Blog_Heading.gif \\"Bytes, Blocks and Elasticity - Getting Started with Azure Elastic SAN\\")\\n\\nA Storage Area Network (SAN) typically comprises one or more physical appliances equipped with multiple drive bays, which are used to create volumes \u2013 it is considered a high-performance and low-latency connectivity\\nstorage solution.\\n\\nThe benefits of a SAN are:\\n\\n* Grow storage footprint independent of Compute\\n* Low latency and high storage throughput\\n* Cost efficient with massive scale.\\n* Built for databases and IOPS-intensive applications.\\n* Supports large virtualization deployments.\\n\\nIntroducing [Azure Elastic SAN](https://azure.microsoft.com/products/storage/elastic-san/?WT.mc_id=AZ-MVP-5004796#overview).\\n\\n![Azure Elastic SAN - Overview](/images/posts/AzureElasticSAN.PNG \\"Azure Elastic SAN - Overview\\")\\n\\nWith the Azure Elastic SAN, we can the elasticity of the Microsoft Azure block storage systems, to supply expandable block storage capabilities to workloads via [iSCSI](https://learn.microsoft.com/en-gb/azure/storage/elastic-san/elastic-san-planning?WT.mc_id=AZ-MVP-5004796#iscsi-support) (Internet Small Computer Systems Interface), or services such as Azure Kubernetes Services through [Azure Container Storage](https://techcommunity.microsoft.com/t5/azure-storage-blog/azure-container-storage-in-public-preview/ba-p/3819246?WT.mc_id=AZ-MVP-5004796).\\n\\nWhen looking at some of the [benefits](https://learn.microsoft.com/azure/storage/elastic-san/elastic-san-introduction?WT.mc_id=AZ-MVP-5004796#benefits-of-elastic-san) of an Azure Elastic SAN, over a traditional SAN, we will delve into several\\ncommon user stories around SAN provisioning and capacity management, with key differences around time to deployment and skills required.\\n\\n![Azure Elastic SAN - User journey](/images/posts/AzureElasticSAN_UserJourney.PNG \\"Azure Elastic SAN - User journey\\")\\n\\n#### Architecture and Components\\n\\nThe Azure Elastic SAN consists of 3 layers:\\n\\n![Azure Elastic SAN - Overview](/images/posts/AzureElasticSAN_3Layers.PNG \\"Azure Elastic SAN - Overview\\")\\n\\n##### The Elastic SAN\\n\\nThe Elastic SAN itself, the Elastic SAN consists of the control plane, where you create and manage your Volume Groups from. The Elastic SAN is where the resources are provisioned, and the Cost Management takes place (i.e., Tags on the Elastic SAN resource).\\n\\n##### Volume Group\\n\\nAn Azure Elastic SAN can have up to 20 volume groups, the volume group is where your security, encryption, and data protection configurations get applied.\\n\\nThe volume group is where your Network Security rule and service endpoints are applied. Any settings or configurations applied to a volume group, such as virtual network rules, are inherited by any volumes associated with that volume group.\\n\\n![Azure Elastic SAN - Volume Group Network Security Rules](/images/posts/AzureElasticSAN_VolumeGroup_NetworkSecurityRule.PNG \\"Azure Elastic SAN - Volume Group Network Security Rules\\")\\n\\n##### Volume\\n\\nThe volume in an Azure Elastic SAN is the actual storage, that gets delivered and mapped to your workload or service.\\n\\nMultiple volumes can be a part of a single-volume group, or separate groups \u2013 depending on requirements, such as accessibility across different virtual networks.\\n\\n![Azure Elastic SAN - Volume](/images/posts/AzureElasticSAN_Volume.PNG \\"Azure Elastic SAN - Volume\\")\\n\\nYou partition the SAN\'s storage capacity into individual volumes. These individual volumes can be mounted to your clients with iSCSI. The name of your volume is part of their iSCSI IQD\\n\\n![Azure Elastic SAN - Volume](/images/posts/AzureElasticSAN_Volume2.PNG \\"Azure Elastic SAN - Volume\\")\\n\\n#### Deployment and Configuration\\n\\nNow that we know what Azure Elastic SAN is, let\'s deploy it using the Azure Portal.\\n\\nAt the time of this article, the Azure Elastic SAN is only available in [specific regions](https://azure.microsoft.com/en-us/updates/regional-expansion-azure-elastic-san-public-preview-is-now-available-in-more-regions/?WT.mc_id=AZ-MVP-5004796) \u2013 and whether the SAN is capable of ZRS or LRS [storage redundancy](https://learn.microsoft.com/azure/storage/common/storage-redundancy?WT.mc_id=AZ-MVP-5004796). As I am based in New Zealand, the closest region at this time to me is Australia East, this region only supports LRS so this is what I will be configuring.\\n\\nIf you haven\u2019t already \u2013 as part of Public Preview, your Azure subscription needs to be [enabled](https://aka.ms/AzureElasticSANPreviewAccess) to provision Azure Elastic SAN.\\n\\n##### Deployment\\n\\n1. Login to the Microsoft Azure [portal](https://portal.azure.com/)\\n2. In the search box at the top of the portal, type in Elastic SAN, and navigate to the [Elastic SAN](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.ElasticSan%2Felasticsans) resource page.\\n3. Click Create Elastic SAN\\n4. ![Create Azure Elastic SAN using the Azure Portal](/images/posts/Create-Portal-AzElasticSAN.png \\"Create Azure Elastic SAN using the Azure Portal\\")\\n5. I will create a new Resource Group named: AzureElasticSAN-dev-rg\\n6. I will name my Azure Elastic SAN: azelasticsan_aue\\n   (Name has to be between 3 to 24 characters in length, and may only contain\\n   lowercase letters, numbers, hyphens, and underscores (hyphens and underscores\\n   must be surrounded by letters or numbers).\\n7. ![Create an Azure Elastic SAN](/images/posts/Create-Portal-AzElasticSAN-Redunancy.png \\"Create an Azure Elastic SAN\\")\\n8. Now we need to specify the base and capacity size, the base size will determine what your iOPS and throughput your SAN will support. It\u2019s cheaper to go with, a lower Base size, and higher additional storage \u2013 but it will affect your IOPS and bandwidth. These values can be changed later (start with a minimum and increase as needed, as you can\u2019t downsize) \u2013 I will set my Base as the minimum of 1 TB and add size of 1TB.\\n9. ![Create Azure Elastic SAN - Resource Provisioning](/images/posts/Create-Portal-AzElasticSAN-ResourceProvisioning.png \\"Create Azure Elastic SAN - Resource Provisioning\\")\\n10. Click Next\\n11. This is where we can create a volume group, click + Create volume group\\n12. The volume group will be used to contain our volumes, I will name a volume group as demo\\n13. I will then allow the volume group, to connect to my DevBox virtual network and set up a service endpoint, on my devbox subnet\\n14. ![Create an Azure Elastic SAN - Volume Group](/images/posts/Create-Portal-AzElasticSAN-AddVolumeGrpVNET.png \\"Create an Azure Elastic SAN - Volume Group\\")\xa0\\n15. Click Create, and finally Review + Create to create your Azure Elastic SAN.\\n16. Configuration\\n\\nNow that we have an Azure Elastic SAN, it\u2019s now time to add some volumes. We can partition the SAN\'s total storage into individual volumes, used for block storage.\\n\\nA volume can only be part of one volume group, but you can have multiple volumes, across multiple volume groups \u2013 that equals the total size of the SAN (in my example 2 TB, the 1 TB Base size, and 1 TB additional capacity), however unlike the SAN \u2013 a volume can be Gigabytes in size.\\n\\nA volume cannot be higher than the total allocated capacity assigned to the SAN.\\n\\nRemember when you create a new Volume, you can increase the size later (but you can\u2019t downsize the volume).\\n\\nThe volume name is part of your volume\'s iSCSI Qualified Name and can\'t be changed once deployed.\\n\\n![Azure Elastic SAN - Overview](/images/posts/Create-Portal-AzElasticSAN-Overview.png \\"Azure Elastic SAN - Overview\\")\\n\\n1. To create a new Volume, let\'s navigate to our [Azure Elastic SAN](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.ElasticSan%2Felasticsans).\\n2. Click on Volumes (under SAN Management)\\n3. Click Create a volume.\\n4. We are going to give the volume a name, in this example I will go with: vol1\\n5. For the size, I will select 500GB.\\n6. Click Save\\n7. Once the volume has been created, we can see the volume and the assigned volume group, including the size of the volume and the remaining capacity of the SAN.\\n\\n##### Connect\\n\\nNow that the SAN, Volume Group, and Volume have been setup \u2013 it\u2019s time to connect to the storage.\\n\\nIn my demo environment, I have a Volume Group, assigned to a storage service endpoint on my devbox virtual network and subnet, so I will connect to the SAN through a Windows Server 2022 server, that is attached to the same virtual network.\\n\\n> Following this guide will set up a single path, for more production scenarios to achieve higher IOPS and throughput, and configure mulipathing using the Microsoft document \u2018[here](https://learn.microsoft.com/en-us/azure/storage/elastic-san/elastic-san-connect-windows?tabs=azure-portal\\\\&WT.mc_id=AZ-MVP-5004796#connect-to-a-volume)\u2019.\\n\\n1. To connect, we need to navigate to our volume, lets navigate to our [Azure Elastic SAN](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.ElasticSan%2Felasticsans).\\n2. Click Volumes\\n3. Click your volume, you want to connect to and select Connect.\\n4. Microsoft Azure, gives us the PowerShell/Bash cmdlets to run on Windows and Linux. Copy the scripts, we will need to run both.\\n5. Login to your Windows Server that you want to\\n   connect to the volume. If this is a new Windows Server, that hasn\u2019t been configured to iSCSI \u2013 you will need to [start the iSCSI Initiator service and set it to Automatic start.](https://github.com/lukemurraynz/PowerOfTheShell/blob/master/Other/Start-iSCSI.ps1)\\n6. Open a PowerShell prompt as Administrator and run the Add Target command, you should have \u2018Operation completed successfully\u2019.\\n7. Then run the login to target script.\\n8. ![Connect Windows Server to iSCSI ](/images/posts/Run_iSCSI_AzElasticSANConnect.gif \\"Connect Windows Server to iSCSI \\")\\n9. Once completed, you can confirm your connectivity, by opening up Server Manager and launching iSCSI Initiator, you should see \'Connected\'.\\n10. ![Azure iSCSI initiator](/images/posts/AzureElasticSAN_SingleThreaded_iSCSI_Initiator.png \\"Azure iSCSI initiator\\")\\n11. Now that your iSCSI target has been mapped \u2013 lets create a volume we can actually use!\\n12. Right Click your Start Menu and click Disk Management\\n13. You should see an Unallocated Disk\\n14. ![Azure Elastic SAN - Disk Management](/images/posts/AzureElasticSAN_DiskManagement_Unallocated.png \\"Azure Elastic SAN - Disk Management\\")\\n15. The unallocated disk here, is our 500GB Azure Elastic SAN volume, it\u2019s time to initialize it, right click and select Create New Simple Volume, I will go through the defaults and now I have a new drive I can use within Windows\\n16. ![Azure Elastic SAN - Windows mounted volume](/images/posts/AzureElasticSAN_WindowsExplorer_Allocated.png \\"Azure Elastic SAN - Windows mounted volume\\")\\n\\n#### Scalability and Performance\\n\\nNow that we have provisioned the Azure Elastic SAN, and connected to it, let\u2019s take a high-level look at the scalability and performance.\\n\\nFor performance, I ran a single threaded test on my 500GB iSCSI target using [Diskspd](https://learn.microsoft.com/azure-stack/hci/manage/diskspd-overview?WT.mc_id=AZ-MVP-5004796), using the following parameters:\\n\\n```bat\\n.\\\\diskspd.exe - d60 - W15 - C15 - c128M - t4 - o4 - b8k - L - r - Sh - w50 f: \\\\disk - speed - test.dat\\n```\\n\\nThe output was:\\n\\nTotal IO:\\n\\n* I/Os: 109,541\\n* I/O per second: 1,825.30\\n\\nRead IO:\\n\\n* I/Os: 54,547\\n* I/O per second: 908.93\\n\\nWrite IO:\\n\\n* I/Os: 54,994\\n* I/O per second: 916.38\\n\\nThe output does not directly provide the exact IOPS (Input/Output Operations Per Second) value, but we can calculate it based on the total number of I/Os and the test duration.\\n\\nTo calculate the overall IOPS, divide the total number of I/Os by the test duration:\\n\\n* Total IOPS: 109,541 I/Os / 60 seconds = 1,825.68 IOPS\\n\\nSimilarly, you can calculate the read and write IOPS:\\n\\n* Read\\n  IOPS: 54,547 I/Os / 60 seconds = 909.12 IOPS\\n* Write\\n  IOPS: 54,994 I/Os / 60 seconds = 916.57 IOPS\\n\\nTherefore, based on the given Diskspd output, the SAN is performing approximately 1,825 IOPS in total, with 909 IOPS for reads and 917 IOPS for writes.\\n\\nNote: This was a very basic test, running on a Windows Server 2022 Azure Virtual Machine (Standard D4s v3 (4 vcpus, 16 GiB memory)), single threaded.\\n\\nAt the time I ran this test \u2013 the IOPS limit of the SAN was 5000, and bandwidth was 80. I highly encourage you to run your own tests. This was more intended as a baseline for reference.\\n\\n![Azure Elastic SAN - storage throughput](/images/posts/AzureElasticSAN_StorageThroughput.PNG \\"Azure Elastic SAN - storage throughput\\")\\n\\nPathping, reported 0 hops for DNS resolution, this Virtual Machine is in another Availability Zone from the SAN.\\n\\n![Azure Elastic SAN - Pathping](/images/posts/AzureElasticSAN_Pathping.png \\"Azure Elastic SAN - Pathping\\")\\n\\nIn terms of Scalability, a few things to note:\\n\\n* You cannot increase the volume size, while an active session (i.e., in use by a workload) is in use.\\n* You can increase the Base and Additional size of the SAN, while sessions are in use.\\n* The Additional size of the Azure Elastic SAN has to be less than the Base size (for example, you cannot have an Azure Elastic SAN with a Base size of 3, and an Additional size of 4).\\n\\n#### Data Protection and Redundancy\\n\\nAt the moment, Azure Elastic SAN, only supports specific regions and [configurations](https://learn.microsoft.com/azure/storage/common/storage-redundancy?WT.mc_id=AZ-MVP-5004796), for example in Australia East, the Azure Elastic SAN only supports LRS, so the volumes sit within a single datacenter, replicated across 3 storage stacks.\\n\\n![LRS (Locally Redundant Storage)](/images/posts/locally-redundant-storage.png \\"LRS (Locally Redundant Storage)\\")\\n\\nLRS is still decent in terms of resiliency but for those who are use to cross SAN replication - I expect, we will see ZRS start to become common place, as this service is rolled out.\\n\\n![ZRS (Zone Redundant Storage)](/images/posts/zone-redundant-storage.png \\"ZRS (Zone Redundant Storage)\\")\\n\\nAt the time of this writing, there is no Azure Backup  Support, I expect this capacity to be released closer to GA (Generally Available).\\nFor day to day operations, I was able to enable the [Volume Shadow Copies](https://learn.microsoft.com/windows-server/storage/file-server/volume-shadow-copy-service?WT.mc_id=AZ-MVP-5004796), which allowed me to go back to  previous versions of files and folders.\\n\\n#### Security and Compliance\\n\\nOther than the usual filesystem permissions, you can use with your filesystems across Windows and Linux on the Azure Elastic SAN, there are built-in roles that can be leveraged for Azure Elastic SAN management.\\n\\nAssigning least privilege operations or creating your own custom role is possible with Azure Elastic SAN, and roles such as Volume Group Owner will be key to how the SAN is architectured for an organisation.\\n\\n| **Name**                           | **Description**                                                                                                                                             |\\n| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Elastic SAN Owner              | Allows for full access to all resources under Azure Elastic SAN including changing network security policies to unblock data path access.               |\\n| Elastic SAN Reader             | Allows for control path read access to Azure Elastic SAN                                                                                                |\\n| Elastic SAN Volume Group Owner | \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 Allows for full access to a volume group in Azure Elastic SAN including changing network security policies to unblock data path access. |\\n\\nAll data stored in an Elastic SAN is encrypted at rest using Azure storage service encryption (SSE). Storage service encryption works similarly to BitLocker on Windows: data is encrypted beneath the file system level. SSE protects your data and to helps you meet your organizational security and compliance commitments. Data stored in Elastic SAN is encrypted with Microsoft-managed keys, that are rotated frequently.\\n\\n#### Integration with Azure Services\\n\\nAt the moment, Azure Elastic SAN supports, Service Endpoints only, [Private endpoints](https://learn.microsoft.com/azure/private-link/private-endpoint-overview?WT.mc_id=AZ-MVP-5004796) are not yet available, but enabling service endpoints on subnet/s was easily done. \\n\\nConnectivity to Windows and Linux machines, will be done through native iSCSI protocols, as the Azure Portal displayed when attempting to connect.\\n\\nAt the time of this writing, [Azure Container Storage](https://techcommunity.microsoft.com/t5/azure-storage-blog/azure-container-storage-in-public-preview/ba-p/3819246?WT.mc_id=AZ-MVP-5004796) \u2013 connecting to Azure Kubernetes Service, can be leveraged by Azure Elastic SAN (Azure Container Storage is also in Public Preview).\\n\\n![Azure Elastic SAN - Azure services integration](/images/posts/AzureElasticSAN_ComputeIntegration.PNG \\"Azure Elastic SAN - Azure services integration\\")\\n\\n#### Cost and Pricing\\n\\nThe Azure Elastic SAN is [charged by the amount you provision based on units](https://azure.microsoft.com/en-us/pricing/details/elastic-san/?WT.mc_id=AZ-MVP-5004796):  \\n\\n* Base unit\\n* Capacity-only unit\\n\\nThey are designed to offer a simple bulk provisioning experience while also providing flexibility to expand your data footprint.\\n\\nThe Base unit has a capacity of 5000 IOPs and a throughput of 80 MBps per TiB.\\n\\nThe Capacity only unit, on the other hand, allows you to provision capacity only without provisioning performance at a lower cost.\\n\\nAs a result, you can cost-effectively scale performance while migrating workloads from on-premises to Azure.\\n\\nThe total price of Azure Elastic SAN depends on the base and capacity scale unit (LRS and ZRS dependent).\\n\\nElastic SAN will need to be provisioned with at least one base unit of 1TiB. It is also important to note that the SAN-provisioned resources are shared by all volume groups and volumes.\\n\\n![Azure Elastic SAN - Provisioning Model](/images/posts/AzureElasticSAN_ProvisioningModel.PNG \\"Azure Elastic SAN - Provisioning Model\\")\\n\\n![Azure Elastic SAN - Cost Management](/images/posts/AzureElasticSAN_CostManagement.PNG)\\n\\n#### Tips\\n\\nI will cover a few titbits, I found in my discovery.  \\n\\n* Authentication Failure \u2013 if you get \u2018Authentication\\n   Failure\u2019, when attempting to connect to your iSCSI target (Azure Elastic SAN Volume), from your Windows or Linux workload, make sure that the Volume Group has had the Service Endpoint enabled for the subnet and virtual network you are connecting from, by default all traffic is denied to the Azure Elastic SAN \u2013 this includes traffic from other subnets, the storage endpoint needs to be enabled on the subnet that hosts your virtual machines.\\n* Persistent Login - When connecting to the iSCSI target (the Volume) this is a once off session, if your system is rebooted, the volume won\u2019t automatically remap. You can set a Persistent Login, by replacing \u2018LoginTarget\u2019\\n   in your iscsicli command to: PersistentLoginTarget (for example: iscsicli PersistentLoginTarget iqn.2023-05.net.windows.core.blob.ElasticSan.es-rsip05eo4sx0:vol1 t es-rsip05eo4sx0.z40.blob.storage.azure.net 3260 Root\\\\ISCSIPRT\\\\0000\\\\_0 -1 \\\\* \\\\* \\\\* \\\\* \\\\* \\\\* \\\\* \\\\* \\\\* \\\\* \\\\* 0), then the next time you reboot your Windows server, the volume will be automatically mounted.\\n\\n#### Updates and New Features\\n\\nAs this is a Private Preview service, there will be updates on functionality and features that I have gone through today and may have missed.\\n\\n* [aka.ms/ElasticSANOverview](https://learn.microsoft.com/en-gb/azure/storage/elastic-san/elastic-san-introduction?WT.mc_id=AZ-MVP-5004796)\\n* [aka.ms/ElasticSANPlanning](https://learn.microsoft.com/azure/storage/elastic-san/elastic-san-planning?WT.mc_id=AZ-MVP-5004796)\\n* [aka.ms/ElasticSANPricing](https://azure.microsoft.com/pricing/details/elastic-san/?WT.mc_id=AZ-MVP-5004796)\\n* Keep an eye out on the [Azure Updates](https://azure.microsoft.com/updates/?query=SAN\\\\&WT.mc_id=AZ-MVP-5004796) page for updates.\\n* Or if you are Linkedin member, follow [Azure Feeds](https://www.linkedin.com/in/azure-feeds-709457212/recent-activity/all/ \\"Azure Feeds\\") on Linkedin."},{"id":"azure/Demystifying-Azure-Policy-Inheritance-Understanding-the-Hierarchy-of-Control","metadata":{"permalink":"/azure/Demystifying-Azure-Policy-Inheritance-Understanding-the-Hierarchy-of-Control","source":"@site/blog/2023-05-17-Demystifying-Azure-Policy-Inheritance-Understanding-the-Hierarchy-of-Control.md","title":"Azure Policy Inheritance","description":"Look into Azure Policy inheritance and how it works.","date":"2023-05-16T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.975,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Policy Inheritance","authors":["Luke"],"tags":["Azure"],"description":"Look into Azure Policy inheritance and how it works.","header":{"teaser":"/images/posts/AzurePolicy_Effects.png"},"date":"2023-05-16T12:00:00.000Z","slug":"azure/Demystifying-Azure-Policy-Inheritance-Understanding-the-Hierarchy-of-Control"},"unlisted":false,"prevItem":{"title":"Getting Started with Azure Elastic SAN","permalink":"/azure/Bytes-Blocks-and-Elasticity-Getting-Started-with-Azure-Elastic-SAN"},"nextItem":{"title":"Web Content Not Found on Azure Storage Account","permalink":"/azure/Web-Content-Not-Found-error-on-Azure-Storage-Account-static-website"}},"content":"Today, we are going to look into demystifying Azure Policy inheritance and how it works, so let\u2019s do some testing.\\n\\nIn my tests \u2013 I have a single resource group named: AzPolicy-Test. This resource group has been placed in the Australia East region.\\n\\nNow that we have a Resource Group to use in our testing, I need a policy \u2013 to keep things simple, I am going use the built-in policy of: [AllowedLocations](https://www.azadvertizer.net/azpolicyadvertizer/e56962a6-4747-49cd-b67b-bf8b01975c4c.html). This policy will allow us to control which region we can deploy our Azure resources into.\\n\\n##### **Scenario #1 \u2013 Policy assigned to the subscription with Allow Australia East ONLY**\\n\\nI have assigned my Azure Policy to a Subscription, that contains my AzPolicy-Test Resource Group. I have set the Allowed Locations to: Australia East.\\n\\n![Azure Policy - Allowed locations](/images/posts/AzurePolicy-AllowedLocations-DenyAllAustraliaE.png \\"Azure Policy - Allowed locations\\")\\n\\nLet us do some testing, and try to deploy an Azure Resource _(in my example, an Azure Storage account)_ into my Resource Group:\\n\\n| Can I deploy to Australia East? | Yes |\\n| ------------------------------- | --- |\\n| Can I deploy to UK South?       | No  |\\n\\nAs you can see my storage account deployed into Australia East successfully\u2026 as expected!\\n\\n![Azure Portal - filtered by location](/images/posts/AzurePolicy-FilterLocations_AustraliaEast.png \\"Azure Portal - filtered by location\\")\\n\\nNow, lets try and deploy a Storage account into the same Resource Group, but in the UK South region.\\n\\nAs Azure Resource Manager is analysing and verifying the inputs give it \u2013 it now knows there is an Azure Policy \u2013 enforcing specific locations and preventing my deployment into the UK South.\\n\\n![Azure Storage Account - Deny UK South deployment](/images/posts/AzurePolicy_DenyUKSouth.png \\"Azure Storage Account - Deny UK South deployment\\")\\n\\nThis is as expected, and even if I were to use another resource group \u2013 I still won\u2019t be able to create resources in other regions.\\n\\n##### **Scenario #2 \u2013 Policy assigned to the subscription with Allow Australia East but another policy with Allow UK South only on the Resource Group**\\n\\nNow that we know, we can create resources in Australia East \u2013 lets assign the same \u2018Allowed Locations\u2019 policy to the Resource Group, but Denying Australia East, and Allowing UK South. The policy allowing Australia East will still remain assigned to the subscription.\\n\\n![Azure Policy - Allowed locations - UK South](/images/posts/AzurePolicy-AllowedLocation_UkSouth.png \\"Azure Policy - Allowed locations - UK South\\")\\n\\nLet us do some testing, and try to deploy an Azure Resource (in my example, an Azure Storage account) into my Resource Group:\\n\\n| Can I deploy to Australia East? | No |\\n| ------------------------------- | -- |\\n| Can I deploy to UK South?       | No |\\n\\n![Azure Storage account deployment - Policy validation error](/images/posts/AzurePolicy-DisableUKSouthDeployment.png \\"Azure Storage account deployment - Policy validation error\\")\\n\\n![Azure Storage account deployment - Policy validation error](/images/posts/AzurePolicy-DisableAustraliaEastDeployment.png \\"Azure Storage account deployment - Policy validation error\\")\\n\\nI can\u2019t deploy to either UK South or Australia East, even though I have 2 separate policies, one policy allowing Australia East (deployed at the subscription) and one policy allowing UK South _(deployed at the Resource Group)_.\\n\\nIn this case, the policies have worked together, with the most restrictive of them both in effect \u2013 which is Deny.\\n\\n##### **Review**\\n\\nSo lets review, originally you might think that like Group Policy \u2013 the last policy wins \u2013 this is not always the case, when conflicting policies are assigned at different levels, the policy at the highest level in the hierarchy takes precedence over policies at lower levels. When preforming a Modify or Create on a resource \u2013 the Azure resource provider checks with the Azure Policy engine.. When deciding what policy to take the Azure Policy engine, will analyze all policies together (like the above scenarius, where both deployment to UK South and Australia East were in effect - however the most restrictive won).\\n\\nThere are, however, various effects which are analysed first.\\n\\n![Azure Policy Effects](/images/posts/AzurePolicy_Effects.png \\"Azure Policy Effects\\")\\n\\n1. **Disabled** is checked first to determine whether the policy rule should be evaluated.\\n2. **Append** and **Modify** are then evaluated. Since either could alter the request, a change made may prevent an audit or deny effect from triggering. These effects are only available with a Resource Manager mode.\\n3. **Deny** is then evaluated. By evaluating deny before audit, double logging of an undesired resource is prevented.\\n4. **Audit** is evaluated.\\n5. **Manual** is evaluated.\\n\\nAfter the Resource Provider returns a success code on a Resource Manager mode request, AuditIfNotExists and DeployIfNotExists evaluate to determine whether additional compliance logging or action is required.\\n\\nRemember that policy enforcement occurs during resource deployment or updates. Existing resources are not retroactively affected unless a manual remediation is performed.\\n\\nThe following Microsoft Learn documents are worth a read, if interested further.\\n\\n* [Understand scope in Azure Policy](https://learn.microsoft.com/azure/governance/policy/concepts/scope?WT.mc_id=AZ-MVP-5004796)\\n* [Order of evaluation](https://learn.microsoft.com/azure/governance/policy/concepts/effects?WT.mc_id=AZ-MVP-5004796#order-of-evaluation)"},{"id":"azure/Web-Content-Not-Found-error-on-Azure-Storage-Account-static-website","metadata":{"permalink":"/azure/Web-Content-Not-Found-error-on-Azure-Storage-Account-static-website","source":"@site/blog/2023-05-04-Web-Content-Not-Found-error-on-Azure-Storage-Account-static-website.md","title":"Web Content Not Found on Azure Storage Account","description":"Web Content Not Found error when hosting a static webpage using an Azure Storage account.","date":"2023-05-03T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.92,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Web Content Not Found on Azure Storage Account","authors":["Luke"],"tags":["Azure"],"description":"Web Content Not Found error when hosting a static webpage using an Azure Storage account.","date":"2023-05-03T12:00:00.000Z","toc":false,"header":{"teaser":"/images/posts/AzureStorageAccount_Static_Site_WebContent_Error.png"},"slug":"azure/Web-Content-Not-Found-error-on-Azure-Storage-Account-static-website"},"unlisted":false,"prevItem":{"title":"Azure Policy Inheritance","permalink":"/azure/Demystifying-Azure-Policy-Inheritance-Understanding-the-Hierarchy-of-Control"},"nextItem":{"title":"Open multiple Microsoft Teams instances using PowerShell","permalink":"/2023/05/03/Open-multiple-Microsoft-Teams-instances-using-PowerShell"}},"content":"Azure Storage accounts [can host static websites](https://learn.microsoft.com/azure/storage/blobs/storage-blob-static-website?WT.mc_id=AZ-MVP-5004796) by opening up a public endpoint to an Azure storage\\ncontainer ($web), so anything inside of $web will be accessible publicly.\\n\\nThis can be enabled easily by toggling the Static website to Enabled.\\n\\n![Azure Storage Account - Static website](/images/posts/AzureStorageAccount_Static_Site_WebContent_Config.png \\"Azure Storage Account - Static website\\")\\n\\nOnce enabled, the Azure storage account will add a NEW endpoint - \\\\<storageaccname>.z\\\\*.web.core.windows.net.\\n\\nOnce you have enabled the static website functionality, a new container named: $web will be created; this is the root of your static website \u2013 and where your HTML or static website will go.\\n\\nAfter you upload your website files to the $web folder.\\n\\n![Azure Storage Account - $web container](/images/posts/AzureStorageAccount_Static_Site_WebContent_WebContainer.png \\"Azure Storage Account - $web container\\")\\n\\nAdd the index document name (i.e., index.html) and click Save.\\n\\n![Azure Storage Account - Static Website primary endpoint](/images/posts/AzureStorageAccount_Static_Site_WebContent_PrimaryEndpoint.png \\"Azure Storage Account - Static Website primary endpoint\\")\\n\\nIf done correctly, your website should now show your website.![Azure Storage account static websitev](/images/posts/AzureStorageAccount_Static_Site_WebContent_Website.png \\"Azure Storage account static website\\")\\n\\nIf done incorrectly, you may get: The requested content does not exist.\\n\\n![The requested content does not exist](/images/posts/AzureStorageAccount_Static_Site_WebContent_Error.png \\"The requested content does not exist\\")\\n\\nIf this occurs, make sure:\\n\\n* There is no whitespace in the index document name.\\n\\n![Azure storage account - index.html](/images/posts/AzureStorageAccount_Static_Site_WebContent_Filenamespace.png \\"Azure storage account - index.html\\")\\n\\n* The Case matters, make sure if the filename is all lowercase in the container, then it\u2019s all lowercase in the Azure storage account static website configuration.\\n* Define a 404 page (the page that gets loaded) when attempting to browse paths that don\u2019t match the index - make sure the site exists in a container and is added to the site storage account configuration,\\n  like the index document name.\\n\\n![404.html](/images/posts/AzureStorageAccount_Static_Site_WebContent_404filename.png \\"404.html\\")\\n\\n* If you don\u2019t have a 404 page, you can have   index.html as both.\\n\\n![Azure static web site - filenames](/images/posts/AzureStorageAccount_Static_Site_WebContent_bothfiles.png \\"Azure static web site - filenames\\")\\n\\n* If you have a CDN _(Content Delivery Network)_\\n  in front of your Azure Storage account _(Azure CDN, Cloudflare)_, you may need to adjust the access level of your Container from Private to: Blob (Anonymous).\\n  You [shouldn\u2019t have to adjust this usually](https://learn.microsoft.com/azure/storage/blobs/storage-blob-static-website?WT.mc_id=AZ-MVP-5004796#impact-of-setting-the-access-level-on-the-web-container), as the Access level controls the container endpoint access \u2013 not the static website endpoint.\\n\\n![Azure storage account - blob access level](/images/posts/AzureStorageAccount_Static_Site_WebContent_containeraccesslevel.png \\"Azure storage account - blob access level\\")"},{"id":"/2023/05/03/Open-multiple-Microsoft-Teams-instances-using-PowerShell","metadata":{"permalink":"/2023/05/03/Open-multiple-Microsoft-Teams-instances-using-PowerShell","source":"@site/blog/2023-05-03-Open-multiple-Microsoft-Teams-instances-using-PowerShell.md","title":"Open multiple Microsoft Teams instances using PowerShell","description":"Open multiple Microsoft Team instances, for access to multiple accounts with PowerShell","date":"2023-05-03T00:00:00.000Z","tags":[{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":1.155,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Open multiple Microsoft Teams instances using PowerShell","authors":["Luke"],"toc":false,"tags":["PowerShell"],"description":"Open multiple Microsoft Team instances, for access to multiple accounts with PowerShell","header":{"teaser":"/images/posts/MS_Teams_logo_ws.png"},"date":"2023-05-03 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Web Content Not Found on Azure Storage Account","permalink":"/azure/Web-Content-Not-Found-error-on-Azure-Storage-Account-static-website"},"nextItem":{"title":"Failed to persist Terraform state using an Azure Blob Storage account","permalink":"/azure/failed-to-persist-terraform-state-using-an-azure-blob-storage-account"}},"content":"There may be circumstances, you need to open up multiple Microsoft Team instances, a reason for this - maybe to chat and join meetings across multiple accounts.\\n\\nMicrosoft are working on a version of [Microsoft Teams](https://techcommunity.microsoft.com/t5/microsoft-teams-public-preview/bd-p/MicrosoftTeamsPublicPreview?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Teams Public Preview\\") that supports multiple-accounts, but until thats released - you can use a PowerShell script to open up another version of Microsoft Teams in another profile *(or multiple, if you update the profilename)*.  \\n\\nThis script also works within your LocalAppData, so you don\'t need local administrator rights to run.\\n\\n```powershell\\n# Uses the file name as the profile name\\n$MSTEAMS_PROFILE = \'CustomProfile\'\\n\\nWrite-Host \\"- Using profile \'$MSTEAMS_PROFILE\'\\"\\n\\n# Set the custom profile path\\n$USERPROFILE = Join-Path $env:LOCALAPPDATA \\"Microsoft\\\\Teams\\\\CustomProfiles\\\\$MSTEAMS_PROFILE\\"\\n\\n# Set the old user profile\\n$OLD_USERPROFILE = $env:USERPROFILE\\n\\n# Launch MS Teams with the custom profile\\nWrite-Host \\"- Launching MS Teams with profile \'$MSTEAMS_PROFILE\'\\"\\nSet-Location \\"$OLD_USERPROFILE\\\\AppData\\\\Local\\\\Microsoft\\\\Teams\\"\\n\\n$teamsProcessStartInfo = New-Object System.Diagnostics.ProcessStartInfo\\n$teamsProcessStartInfo.FileName = \\"$OLD_USERPROFILE\\\\AppData\\\\Local\\\\Microsoft\\\\Teams\\\\Update.exe\\"\\n$teamsProcessStartInfo.Arguments = \\"--processStart \\"\\"Teams.exe\\"\\"\\"\\n$teamsProcessStartInfo.WorkingDirectory = \\"$OLD_USERPROFILE\\\\AppData\\\\Local\\\\Microsoft\\\\Teams\\"\\n$teamsProcessStartInfo.EnvironmentVariables[\\"USERPROFILE\\"] = $USERPROFILE\\n$teamsProcessStartInfo.UseShellExecute = $false\\n\\n[System.Diagnostics.Process]::Start($teamsProcessStartInfo) | Out-Null\\n\\n# Set the user profile back to the old user profile\\n$env:USERPROFILE = $OLD_USERPROFILE\\n\\n```\\n\\nWhen the script is ran, a new profile will be created for Microsoft Teams, and then opened. You can then use that second Microsoft Teams instance, to connect to another account or tenancy.\\n\\nTo make it easier, you could also look at turning this script into an [executable](https://github.com/MScholtes/PS2EXE \\"PS2EXE\\")."},{"id":"azure/failed-to-persist-terraform-state-using-an-azure-blob-storage-account","metadata":{"permalink":"/azure/failed-to-persist-terraform-state-using-an-azure-blob-storage-account","source":"@site/blog/2023-04-07-failed-to-persist-terraform-state-using-an-azure-blob-storage-account.md","title":"Failed to persist Terraform state using an Azure Blob Storage account","description":"When attempting to make changes with Terraform, and the state changes are in an Azure storage account, you may come across: Failed to save state.","date":"2023-04-07T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.47,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-04-07 00:00:00 +1200","title":"Failed to persist Terraform state using an Azure Blob Storage account","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/terraform_savestate.PNG"},"slug":"azure/failed-to-persist-terraform-state-using-an-azure-blob-storage-account"},"unlisted":false,"prevItem":{"title":"Open multiple Microsoft Teams instances using PowerShell","permalink":"/2023/05/03/Open-multiple-Microsoft-Teams-instances-using-PowerShell"},"nextItem":{"title":"Azure Quick Review","permalink":"/azure/azure-quick-review"}},"content":"When attempting to make changes with Terraform, and the [state changes are in an Azure storage account](https://learn.microsoft.com/azure/developer/terraform/store-state-in-azure-storage?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796 \\"Store Terraform state in Azure Storage\\"), you may come across: Failed to save state.\\n\\n> Error: Failed to save state\\n>\\n> Error saving state: blobs:Clien#GetProperties: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: error response cannot be parsed: {\\"\\" \'\\\\\\\\x00\' \'\\\\\\\\x00\'} error: EOF\\n\\nAnd: Error: Failed to persist state to backend.\\n\\nOr Error: Error releasing the state lock.\\n\\n![Terraform - Failed to save state](/uploads/terraform_savestate.PNG \\"Terraform - Failed to save state\\")\\n\\nRecently, encountered this issue when attempting a Terraform deployment; the state file kept locking midway through a deployment, this was traced to the Terraform storage account being in the Terraform code itself - with Public access set to Deny.\\n\\nSo the flow was looking like this: Script ran to allow Azure DevOps IP to the Storage account Firewall, then Terraform would start deploying and Deny access - preventing the state file from saving.\\n\\nThe first thing you need to do is break the lease on the state file.\\n\\n1. Navigate to your **Azure Storage** account that contains the state file\\n2. Navigate to the **Container** that contains the state file\\n3. Click on your state file and select **Break lease**\\n4. ![Azure Storage account - break lease](/uploads/azure_storageaccount_breaklease.png \\"Azure Storage account - break lease\\")\\n5. Once the lease is broken - make sure that your state file is \'Available\' and not \'Leased\'\\n6. Then check your Terraform to make sure that it wasn\'t changing your Storage account that contained the Terraform state file in any way, then re-run your deployment.\\n\\nNote: It may be wise, to make sure that the Azure storage account containing your state file is not managed by Terraform, to avoid unintentional mishaps."},{"id":"azure/azure-quick-review","metadata":{"permalink":"/azure/azure-quick-review","source":"@site/blog/2023-03-27-azure-quick-review.md","title":"Azure Quick Review","description":"There are a lot of workbooks that help with Microsoft Azure cost optimization, but when having discussions and looking into SLA/SLO and availability scenarios, there are fewer options to select from - today, we are going to look at the deployment and output of Azure Quick Review.","date":"2023-03-26T11:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.785,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-03-26T11:00:00.000Z","title":"Azure Quick Review","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/azurequickreview_excel_overview.png"},"slug":"azure/azure-quick-review"},"unlisted":false,"prevItem":{"title":"Failed to persist Terraform state using an Azure Blob Storage account","permalink":"/azure/failed-to-persist-terraform-state-using-an-azure-blob-storage-account"},"nextItem":{"title":"Create Azure Bastion with Shareable Link support with PowerShell","permalink":"/azure/create-azure-bastion-with-shareable-link-support-with-powershell"}},"content":"There are a lot of workbooks that help with Microsoft Azure cost optimization, but when having discussions and looking into SLA/[SLO](https://learn.microsoft.com/azure/cloud-adoption-framework/manage/monitor/service-level-objectives?WT.mc_id=AZ-MVP-5004796 \\"Cloud monitoring guide: Service Level Objectives\\") and availability scenarios, there are fewer options to select from - today, we are going to look at the deployment and output of Azure Quick Review.\\n\\n> Azure Quick Review (azqr) goal is to produce a high level assessment of an Azure Subscription or Resource Group providing the following information for each Azure Service:\\n\\n[Azure Quick Review](https://github.com/cmendible/azqr \\"Azure Quick Review\\") _(created by Microsoft Senior Cloud Solution Architect_ [Carlos Mendible](https://www.linkedin.com/in/carlosmendible/)_),_ can supplement other tools - to give you visibility into your Azure services and answer questions such as:\\n\\n* What is my expected SLA?\\n* Are my resources protected against zone failures?\\n* Am I collecting diagnostic logs for my resources?\\n* Is Defender for Cloud-enabled for all my resource types?\\n\\nUsing this tool is pretty simple _(and, as the name suggests, Quick)_, and today we will look at running it from a windows endpoint, but first, we need some prerequisites.\\n\\n* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?WT.mc_id=AZ-MVP-5004796 \\"How to install the Azure CLI\\")\\n* [azqr - windows release](https://github.com/cmendible/azqr/releases \\"cmendible/azqr/releases\\")\\n\\nInstall the Azure CLI and make sure you have Reader rights across the subscriptions you want to review; in this demo, we will scan all subscriptions I have access to.\\n\\nThe Azure Quick Review (azqr) windows binary is intended to be run from the command line, so let\'s run it.\\n\\n 1. Open your Windows Terminal\\n 2. Navigate to the location of the azqr binary\\n 3. ![Azure Quick Review](/uploads/windowsterminal_azqr_binary.png \\"Azure Quick Review\\")\\n 4. Login to Azure using the Azure CLI by typing:\\n\\n        az login\\n 5. Once you have authenticated, run the executable.\\n 6. ![Run azqr-windows-latest](/uploads/run_azurequickreview.gif \\"Run azqr-windows-latest\\")\\n 7. Once it has been completed, there will be an excel spreadsheet in the same folder as the Azure Quick Review executable, with an output that contains something similar to the below:\\n 8. ![Azure Quick Review - Overview](/uploads/azurequickreview_excel_overview.png \\"Azure Quick Review - Overview\\")\\n 9. ![Azure Quick Review - Recommedations](/uploads/azurequickreview_excel_recommdations.png \\"Azure Quick Review - Recommedations\\")\\n10. ![Azure Quick Review - Defender for Cloud](/uploads/azurequickreview_excel_defenderoverview.png \\"Azure Quick Review - Defender for Cloud\\")"},{"id":"azure/create-azure-bastion-with-shareable-link-support-with-powershell","metadata":{"permalink":"/azure/create-azure-bastion-with-shareable-link-support-with-powershell","source":"@site/blog/2023-03-16-create-azure-bastion-with-shareable-link-support-with-powershell.md","title":"Create Azure Bastion with Shareable Link support with PowerShell","description":"Azure Bastion is a service you deploy that lets you connect to a virtual machine using your browser and the Azure portal or via the native SSH or RDP client installed on your local computer.","date":"2023-03-16T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.825,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-03-16 00:00:00 +1300","title":"Create Azure Bastion with Shareable Link support with PowerShell","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azurebastion_shareablelinkheader.png"},"slug":"azure/create-azure-bastion-with-shareable-link-support-with-powershell"},"unlisted":false,"prevItem":{"title":"Azure Quick Review","permalink":"/azure/azure-quick-review"},"nextItem":{"title":"Azure Budget Filters: A Key Tool for Effective Cloud Cost Management","permalink":"/azure/azure-budget-filters-a-key-tool-for-effective-cloud-cost-management"}},"content":"[Azure Bastion](https://learn.microsoft.com/en-us/azure/bastion/bastion-overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Bastion?\\") is a service you deploy that lets you connect to a virtual machine using your browser and the Azure portal or via the native SSH or RDP client installed on your local computer.\\n\\n#### Overview\\n\\n> The Azure Bastion service is a fully platform-managed PaaS service you provision inside your virtual network. It provides secure and seamless RDP/SSH connectivity to your virtual machines directly from the Azure portal over TLS.\\n\\nBecause of this, if you don\'t have line-of-sight access to your Virtual Machines (_via express route, Site-to-Site VPN etc.)_, Bastion becomes your jump box, allowing secure access to your virtual machines without needing a public IP.\\n\\nThere is a downside, though. To connect to a Virtual Machine secured by Bastion, you need access to the Azure Portal, or command line connectivity to Azure, to create the tunnel; this means that you may need to grant people elevated rights and access they may not need to connect.\\n\\nAs of November 2022, Microsoft introduced shareable links into public preview, solving two key pain points:\\n\\n* Administrators will no longer have to provide full access to their Azure accounts to one-time VM users\u2014helping to maintain their privacy and security.\\n* Users without Azure subscriptions can seamlessly connect to VMs without exposing RDP/SSH ports to the public internet.\\n\\n> The Bastion **Shareable Link** feature lets users connect to a target resource (virtual machine or virtual machine scale set) using Azure Bastion without accessing the Azure portal.\\n\\nAt the time of this writing, there are some [scenarios](https://learn.microsoft.com/en-us/azure/bastion/shareable-link?WT.mc_id=AZ-MVP-5004796#considerations \\"Create a shareable link for Bastion\\") where shareable links won\'t work - particularly across Network peering across subscriptions and regions.\\n\\nBecause the service is in Public Preview - native PowerShell cmdlet support, enabling and configuring this feature isn\'t available - but you can easily allow it via the [Azure Portal](https://learn.microsoft.com/en-us/azure/bastion/shareable-link?WT.mc_id=AZ-MVP-5004796#enable-shareable-link-feature \\"Enable Shareable Link feature\\").\\n\\n![Create Azure Bastion with Shareable Link Support](/uploads/azurebastion_shareablelinkheader.png \\"Create Azure Bastion with Shareable Link Support\\")\\n\\nTo get around that, we will leverage the [Azure Rest API](https://learn.microsoft.com/rest/api/azure/?WT.mc_id=AZ-MVP-5004796 \\"Azure REST API reference\\") directly, using PowerShell to enable the Shareable Link feature and create and obtain a shareable link for a Virtual Machine.\\n\\n#### Create Azure Bastion\\n\\nI will assume there is already an Azure Virtual Network created; if not, you can follow the [Microsoft documentation](https://learn.microsoft.com/azure/virtual-network/quick-create-portal?WT.mc_id=AZ-MVP-5004796 \\"Quickstart: Create a virtual network using the Azure portal\\") to get it up and running!\\n\\nAlso, make sure you have the [Az Module](https://learn.microsoft.com/powershell/azure/new-azureps-module-az?view=azps-9.5.0&WT.mc_id=AZ-MVP-5004796 \\"Introducing the Azure Az PowerShell module\\") installed.\\n\\nThe PowerShell function we will run will require a few parameters to create the Azure Bastion resource and enable Shared Link functionality; these parameters are:\\n\\n|  |  |\\n| --- | --- |\\n| Parameters | Note |\\n| RGName | The Resource Group of your Virtual Network |\\n| VNetName | The Virtual Network name |\\n| addressPrefix | The address prefix for your new Bastion subnet. For Azure Bastion resources deployed on or after November 2, 2021, the minimum AzureBastionSubnet size is /26 or larger (/25, /24, etc.). |\\n| region | The region, that Azure Bastion is deployed into (this needs to match your Virtual Network) |\\n| BastionPubIPName | The name of the Public IP, used by the Azure Bastion resource (this is the Azure resource name, it doesn\'t have an external DNS alias, so doesn\'t need to be globally unique) |\\n| BastionResourceName | The name of your Azure Bastion resource |\\n\\n 1. Copy the script below into a file named: New-AzBastionSharedLinkEnabled.ps1\\n\\n        function New-AzBastionSharedLinkEnabled {\\n            <#\\n              .SYNOPSIS\\n              Creates an Azure Bastion resource with shared link enabled, on an already existing Azure Virtual Network.\\n            #>\\n            [CmdletBinding()]\\n            param\\n            (\\n              [Parameter(Mandatory = $false, Position = 0)]\\n              [System.String]\\n              $RGName = \\"BastionTest\\",\\n              \\n              [Parameter(Mandatory = $false, Position = 1)]\\n              [System.String]\\n              $VNetName = \'vnet-aue-dev\',\\n              \\n              [Parameter(Mandatory = $false, Position = 2)]\\n              [System.String]\\n              $addressPrefix = \'10.2.1.0/26\',\\n              \\n              [Parameter(Mandatory = $false, Position = 3)]\\n              [System.String]\\n              $region = \'AustraliaEast\',\\n              \\n              [Parameter(Mandatory = $false, Position = 4)]\\n              [System.String]\\n              $BastionPubIPName = \'VNet1-ip\',\\n              \\n              [Parameter(Mandatory = $false, Position = 5)]\\n              [Object]\\n              $BastionResourceName = \\"$VNetName-bastion\\"\\n            )\\n            \\n            # Set variable values for Resource Group name, Virtual Network name, address prefix, region, and bastion-related resources.\\n          \\n            # Connect to Azure using Get-AzAccount cmdlet.\\n            Connect-AzAccount\\n            \\n            # Use Get-AzSubscription cmdlet to get all the subscriptions that the account has access to and allow the user to choose one using Out-GridView.\\n            Get-AzSubscription | Out-GridView -PassThru | Select-AzSubscription\\n            $token = (Get-AzAccessToken).Token\\n            $subscription = Get-AzContext | Select-Object Subscription\\n            \\n            # Use Get-AzVirtualNetwork cmdlet to get the virtual network object and then use Add-AzVirtualNetworkSubnetConfig cmdlet to create a new subnet for Azure Bastion service. Finally, use Set-AzVirtualNetwork cmdlet to update the virtual network configuration.\\n            $VNET = Get-AzVirtualNetwork -ResourceGroupName $RGName -Name $VNetName \\n            Add-AzVirtualNetworkSubnetConfig -VirtualNetwork $VNET -Name \\"AzureBastionSubnet\\" -AddressPrefix $addressPrefix | Set-AzVirtualNetwork\\n            $VNET = Get-AzVirtualNetwork -ResourceGroupName $RGName -Name $VNetName \\n            \\n            # Note: If there is an error message, it could indicate that the address prefix for the new subnet overlaps with existing address ranges or is too small.\\n            \\n            # Use New-AzPublicIpAddress cmdlet to create a new public IP address resource for the Bastion service.\\n            $publicip = New-AzPublicIpAddress -ResourceGroupName $RGName -name $BastionPubIPName -location $region -AllocationMethod Static -Sku Standard\\n            $publicip = Get-AzPublicIpAddress -ResourceGroupName $RGName -Name $BastionPubIPName\\n            # Use New-AzBastion cmdlet to create a new Azure Bastion resource with the specified configuration, including the virtual network and public IP address resources created earlier.\\n            New-AzBastion -ResourceGroupName $RGName -Name $BastionResourceName -PublicIpAddressRgName $publicip.ResourceGroupName -PublicIpAddressName $publicip.Name  -VirtualNetwork $VNET -Sku \'Standard\' \\n            \\n            #Enable Shareable links for VMs in Azure Bastion.\\n            $BastionSubnet = Get-AzVirtualNetworkSubnetConfig -Name \'AzureBastionSubnet\' -VirtualNetwork $VNET\\n            \\n            $Body = [PSCustomObject]@{\\n              location   = $region\\n              properties = @{\\n                enableShareableLink = \\"true\\"\\n                ipConfigurations    = @(\\n                  @{\\n                    name       = \\"bastionHostIpConfiguration\\"\\n                    properties = @{\\n                      subnet          = @{\\n                        id = $BastionSubnet.id\\n                      }\\n                      publicIPAddress = @{\\n                        id = $publicip.Id\\n                      }\\n                    }\\n                  }\\n                )\\n              }\\n              \\n            }  | ConvertTo-Json -Depth 6\\n            \\n            $params = @{\\n              Uri         = \\"https://management.azure.com/subscriptions/\\" + $subscription.Subscription.Id + \\n              \\"/resourceGroups/$($RGName)/providers/Microsoft.Network/bastionHosts/$($BastionResourceName)?api-version=2022-07-01\\"\\n              Headers     = @{ \'Authorization\' = \\"Bearer $token\\" }\\n              Method      = \'Put\'\\n              Body        = $body\\n              ContentType = \'application/json\'\\n            }\\n            \\n            # Invoke the REST API and store the response\\n            Invoke-RestMethod @Params\\n          }\\n 2. Open a Terminal or PowerShell prompt, and navigate to the folder containing the script.\\n 3. Dot source the script so that you can run it from the session: **. .\\\\\\\\New-AzBastionSharedLinkEnabled.ps1**\\n 4. ![. .\\\\\\\\New-AzBastionSharedLinkEnabled.ps1](/uploads/windowsterminal_new-azbastionsharedlinkenabled.png \\". .\\\\New-AzBastionSharedLinkEnabled.ps1\\")\\n 5. Once it\'s imported - we can now run it; make sure you replace your parameters that match your environment:\\n\\n        New-AzBastionSharedLinkEnabled -RGName BastionTest -VNetName vnet-aue-dev -addressPrefix 10.2.1.0/26 -region AustraliaEast -BastionPubIPName VNet1-ip -BastionResourceName net-aue-dev-bastion\\n 6. The script will then prompt for your credentials to authenticate\\n 7. You will then need to select the Azure subscription containing your Azure Virtual Network, then select Ok\\n 8. ![Select Azure subscription](/uploads/select-azsubscription_outgridview.png \\"Select Azure subscription\\")\\n 9. The script will then go and provision Azure Bastion and enable Shared Links. It will take a few minutes to run while it provisions Bastion. Then you will get JSON output, indicating it has been completed.\\n10. ![Windows PowerShell - New Azure Bastion](/uploads/windowsterminal_new-azbastionsharedlinkenabledrun.png \\"Windows PowerShell - New Azure Bastion\\")\\n11. ![Azure Bastion - Shareable Link](/uploads/azureportal_azurebastion_shareablelinkconfig.png \\"Azure Bastion - Shareable Link\\")\\n\\n#### Create Shareable Link\\n\\nNow that we have an Azure Bastion instance and have Shareable Links enabled - it\'s time to create a Shareable Link for a Virtual Machine; this triggers 2 API endpoints - creating the shareable link and then retrieving the shareable link.\\n\\nThe same assumptions are made, so make sure you have the [Az Module](https://learn.microsoft.com/powershell/azure/new-azureps-module-az?view=azps-9.5.0&WT.mc_id=AZ-MVP-5004796 \\"Introducing the Azure Az PowerShell module\\") installed.\\n\\nThe script relies on the following parameters:\\n\\n| Parameters | Note |\\n| --- | --- |\\n| BastionResourceName | The name of your Azure Bastion resource |\\n| RGName | The Resource Group of your Bastion resource |\\n| VMRGName | The Resource Group of your Virtual Machine, you want a Shareable Link for |\\n| Vmname | The name of the Virtual Machine you want a shareable link for |\\n\\n 1. Copy the script below into a file named: New-AzBastionShareableLink.ps1\\n\\n        function New-AzBastionShareableLink {\\n          <#\\n            .SYNOPSIS\\n              Creates an Azure Bastion shareable link.\\n          #>\\n          [CmdletBinding()]\\n          param\\n          (\\n            [Parameter(Mandatory = $false, Position = 0)]\\n            [System.String]\\n            $BastionResourceName = \'vnet-aue-dev-bastion\',\\n            \\n            [Parameter(Mandatory = $false, Position = 1)]\\n            [System.String]\\n            $RGName = \\"BastionTest\\",\\n            \\n            [Parameter(Mandatory = $false, Position = 1)]\\n            [System.String]\\n            $VMRGName = \\"BastionTest\\",\\n        \\n            [Parameter(Mandatory = $false, Position = 2)]\\n            [System.String]\\n            $VMname = \\"2022ServerVM-2\\"\\n          )\\n          \\n          # Connect to Azure using Get-AzAccount\\n          Connect-AzAccount\\n          \\n          # Get all subscriptions that the account has access to\\n          Get-AzSubscription | Out-GridView -PassThru | Select-AzSubscription\\n          \\n          $subscription = Get-AzContext | Select-Object Subscription\\n          # Get the access token for the authenticated user\\n          $token = (Get-AzAccessToken).Token\\n          \\n          $ID = Get-AzVM -ResourceGroupName $VMRGName -Name $VMName | Select-Object Id -ExpandProperty id\\n          \\n          $body = @{\\n            \\n            vms = @(\\n              @{\\n                vm = @{\\n                  id = $ID.Id\\n                }\\n              }\\n            )\\n            \\n          }  | ConvertTo-Json -Depth 3\\n          \\n            #creates the shareable link for the VM\\n          $params = @{\\n            Uri         = \\"https://management.azure.com/subscriptions/\\" + $subscription.Subscription.Id + \\n            \\"/resourceGroups/$RGName/providers/Microsoft.Network/bastionHosts/$BastionResourceName/createShareableLinks?api-version=2022-07-01\\"\\n            Headers     = @{ \'Authorization\' = \\"Bearer $token\\" }\\n            Method      = \'POST\'\\n            Body        = $body\\n            ContentType = \'application/json\'\\n          }\\n          \\n          # Invoke the REST API and store the response\\n          Invoke-RestMethod @Params\\n          \\n          #Gets the shareable link for the VM\\n            \\n         $params = @{\\n            Uri         = \\"https://management.azure.com/subscriptions/\\" + $subscription.Subscription.Id + \\n            \\"/resourceGroups/$($RGName)/providers/Microsoft.Network/bastionHosts/$BastionResourceName/getShareableLinks?api-version=2022-09-01\\"\\n            Headers     = @{ \'Authorization\' = \\"Bearer $token\\" }\\n            Method      = \'POST\'\\n            # Body        = $body\\n            ContentType = \'application/json\'\\n          }\\n          \\n          # Invoke the REST API and store the response\\n          $ShareableLink = Invoke-RestMethod @Params\\n          Write-Output $ShareableLink.value.bsl \\n        }\\n 2. Open a Terminal or PowerShell prompt, and navigate to the folder containing the script.\\n 3. Dot source the script so that you can run it from the session: **. .\\\\\\\\New-AzBastionShareableLink.ps1**\\n 4. s\\n 5. Once it\'s imported - we can now run it; make sure you replace your parameters that match your environment:\\n\\n        New-AzBastionShareableLink -BastionResourceName net-aue-dev-bastion -RGName BastionTest -VMRGName BastionTest -VMname 2022ServerVM-2\\n 6. ![Azure Bastion - Create Shared Link](/uploads/windowsterminal_new-azbastionsharedlink.png \\"Azure Bastion - Create Shared Link\\")\\n 7. The script will then prompt for your credentials to authenticate\\n 8. You will then need to select the Azure subscription containing your Azure Virtual Network, then select Ok\\n 9. ![Select Azure subscription](/uploads/select-azsubscription_outgridview.png \\"Select Azure subscription\\")\\n10. The script will then go and collect the ID of the Virtual Machine, pass that through to the Create a Shareable Link, then wait 10 seconds for the Bastion Resource to update properly, then collect the Shareable Link and output it to the terminal.\\n11. ![Azure Bastion - Shared Link](/uploads/windowsterminal_azurebastionsharedlink.png \\"Azure Bastion - Shared Link\\")\\n12. You can also see the link created in the Azure Portal\\n13. ![Microsoft Azure Portal - Shareable Link](/uploads/azureportal_azurebastion_shareablelinks.png \\"Microsoft Azure Portal - Shareable Link\\")\\n14. I can then copy the URL into my favourite browser and connect to your Virtual Machine securely!\\n15. ![Microsoft Azure Bastion - Connect](/uploads/azurebastionshareablelink_vmconnect.gif \\"Microsoft Azure Bastion - Connect\\")\\n\\nThe scripts can also be found directly on GitHub here: [https://github.com/lukemurraynz/Azure](https://github.com/lukemurraynz/Azure \\"https://github.com/lukemurraynz/Azure\\")"},{"id":"azure/azure-budget-filters-a-key-tool-for-effective-cloud-cost-management","metadata":{"permalink":"/azure/azure-budget-filters-a-key-tool-for-effective-cloud-cost-management","source":"@site/blog/2023-03-13-azure-budget-filters-a-key-tool-for-effective-cloud-cost-management.md","title":"Azure Budget Filters: A Key Tool for Effective Cloud Cost Management","description":"Azure Budgets are a vital tool that can be used to keep on top of your Cloud financial management (FinOps) Microsoft Azure platform potential and actual costs.","date":"2023-03-13T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":11.795,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-03-13 00:00:00 +1300","title":"Azure Budget Filters: A Key Tool for Effective Cloud Cost Management","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/AzureSpringClean_AzureBudgets.png"},"slug":"azure/azure-budget-filters-a-key-tool-for-effective-cloud-cost-management"},"unlisted":false,"prevItem":{"title":"Create Azure Bastion with Shareable Link support with PowerShell","permalink":"/azure/create-azure-bastion-with-shareable-link-support-with-powershell"},"nextItem":{"title":"Azure for Students","permalink":"/azure/microsoft-student-education-hub"}},"content":"[Azure Budgets](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets?WT.mc_id=AZ-MVP-5004796 \\"Tutorial: Create and manage Azure budgets\\") are a vital tool that can be used to keep on top of your Cloud financial management (FinOps) Microsoft Azure platform potential and actual costs.\\n\\n#### The most effective Azure Budgets - are the ones that you use!\\n\\n![ Azure Budgets](/uploads/budgets.png \\" Azure Budgets\\")\\n\\n> In the realm of Cost Management, budgets play a pivotal role in facilitating the planning and implementation of organizational accountability. These tools enable proactive communication regarding expenses and support the management of costs by closely monitoring spending trends over extended periods.\\n>\\n> One can set up alerts based on current or projected expenditures to maintain adherence to the established organizational spending limit. Upon surpassing the budget thresholds, notifications are promptly triggered. Such occurrences neither impact any of the available resources nor interrupt any consumption processes.\\n>\\n> By leveraging budgets, it becomes possible to perform detailed cost analysis and track expenses effectively.\\n\\n![Azure Back to School - Azure Budget Filters](/uploads/AzureSpringClean_AzureBudgets.png \\"Azure Spring Clean - Azure Budget Filters\\")\\n\\nBe aware of the delay with the Cost & Usage data, as there may be a difference between what you end up seeing in the Portal and the Budget itself - so make sure you account for this to be advised as early as possible:\\n\\n> Cost and usage data is typically available within 8-24 hours and budgets are evaluated against these costs every 24 hours.\\n>\\n> Be sure to get familiar with [Cost and usage data update](https://learn.microsoft.com/azure/cost-management-billing/costs/understand-cost-mgt-data?WT.mc_id=AZ-MVP-5004796#cost-and-usage-data-updates-and-retention \\"Cost and usage data updates and retention\\") specifics. When a budget threshold is met, email notifications are normally sent within an hour of the evaluation.\\n\\nNote: Azure Budgets are not supported on Subscriptions, where you can\'t access Microsoft Cost Management, i.e. [Azure Sponsorship](https://www.microsoftazuresponsorships.com/ \\"Azure Sponsorship\\") subscriptions.\\n\\n##### A time to clean the windows - Budget Scopes\\n\\nWhen creating an Azure Budget, you can specify a Scope. A scope is the level of your hierarchy _(i.e., if it\'s a Resource Group Budget, it cannot report on resources at the Subscription, you would have to create a Subscription or Management Group scoped Budget)._\\n\\nWhen you create an Azure Budget, they can be made at the following Scopes:\\n\\n![Microsoft Azure Budget Scopes](/uploads/azurebudget_scope.png \\"Microsoft Azure Budget Scopes\\")\\n\\nMost people, when creating scopes, will create a Scope at the Subscription and/or Resource Group level - there is no right or wrong answer when it comes to your Azure Budget Scope - this needs to work for you and your organisation, ie if you have a Project per Resource Group - then it would make sense to create a Budget per Resource Group, the same for Subscriptions.\\n\\n**You can also have multiple Azure Budgets** at the same or different scopes, so a **combination of Budgets may be the most effective**. An example could be a Subscription Budget that may go to a Product Owner, but a Management Group could go to Finance or the Technology teams.\\n\\nKeep in mind, that Budgets on their own are just a forecasting and alerting tool, they won\'t stop resources from running, if it goes over an alert threshold, out of the box - the Budget doesn\'t touch your resources, merely gives you an opportunity to proactively react to them, before costs become a problem.\\n\\n##### Time to scrum the floor - Create an Azure Budget\\n\\nLet\'s go through the process of creating an Azure Budget, using the Azure Portal.\\n\\n 1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. In the Search bar above, search for **Budgets**\\n 3. Click on **Budgets**\\n 4. **Change** the **Scope**\\n 5. ![Cost Management - Azure Budget](/uploads/azureportal_costmanagement_root.png \\"Cost Management - Azure Budget\\")\\n 6. You can select a Management Group, Subscription, or Resource Group for the Scope, by clicking on each - in my example, I have a Management Group named: _mg-landingzones_, which I am going to select.\\n 7. Click **Select**\\n 8. Now that the Scope has been set, we can add our Budget to the specified Scope, click **+ Add**\\n 9. We will come back to Filters, in another section - but for the Budget details you will need:\\n    * **Name** this is the name of your Budget, make sure its something meaningful _(ie Monthly-Budget_MG-LandingZones)_\\n    * **Reset period** _(Monthly/Quarterly or Annual, this is the period that the Budget resets back to $0 - you can\'t go wrong with Monthly)_\\n    * **Creation date** _(the date that the Budget will start)_\\n    * **Expiration date** _(the date that the Budget will stop)_\\n10. **Budget Amount**_(this is the overall; amount that you are planning on your resources to spend)_\\n11. ![Azure Portal - Create budget](/uploads/azureportal_createbudget_scoping.png \\"Azure Portal - Create budget\\")\\n12. Once you have entered in your Budget details, click Next to configure your Alert conditions. The alert conditions are where you can specify, what you want to alert on ie 50% or 80% of the overall budget amount. Actual - is when it financially reaches that point and Forecasted - is when current consumption is forecasted to reach that budget.\\n13. Specify an email address to send the Alert to and click **Create**.\\n14. ![Azure Budget Conditions](/uploads/azureportal_createbudget_conditions.png \\"Azure Budget Conditions\\")\\n\\n#### Time to clean the dishes - Azure Budget Filters\\n\\nBy default, Scoping the Budget to the Subscription or Resource Group is good enough for 95% of the use cases - but using Budget filters, you can enable a bit more flexibility - for scenarios such as:\\n\\n* Product-centric alerts\\n* Service centric alerts\\n\\nWhile the scope is your level of the Azure hierarchy, your filter is your handrail to stop you from falling, currently, Microsoft offers the following filters:\\n\\n| Property | When to use | Notes |\\n| --- | --- | --- |\\n| Availability zones | Break down AWS costs by availability zone. | Applicable only to AWS scopes and management groups. Azure data doesn\'t include availability zone and will show as\xa0No availability zone. |\\n| Billing period | Break down PAYG costs by the month that they were, or will be, invoiced. | Use\xa0Billing period\xa0to get a precise representation of invoiced PAYG charges. Include two extra days before and after the billing period if filtering down to a custom date range. Limiting to the exact billing period dates won\'t match the invoice. Will show costs from all invoices in the billing period. Use\xa0Invoice ID\xa0to filter down to a specific invoice. Applicable only to PAYG subscriptions because EA and MCA are billed by calendar months. EA/MCA accounts can use calendar months in the date picker or monthly granularity to accomplish the same goal. |\\n| BillingProfileId | The ID of the billing profile that is billed for the subscription\'s charges. | Unique identifier of the EA enrollment, pay-as-you-go subscription, MCA billing profile, or AWS consolidated account. |\\n| BillingProfileName | Name of the EA enrollment, pay-as-you-go subscription, MCA billing profile, or AWS consolidated account. | Name of the EA enrollment, pay-as-you-go subscription, MCA billing profile, or AWS consolidated account. |\\n| Charge type | Break down usage, purchase, refund, and unused reservation and savings plan costs. | Reservation purchases, savings plan purchases, and refunds are available only when using actual costs and not when using amortized costs. Unused reservation and savings plan costs are available only when looking at amortized costs. |\\n| Department | Break down costs by EA department. | Available only for EA and management groups. PAYG subscriptions don\'t have a department and will show as\xa0No department\xa0or\xa0unassigned. |\\n| Enrollment account | Break down costs by EA account owner. | Available only for EA billing accounts, departments, and management groups. PAYG subscriptions don\'t have EA enrollment accounts and will show as\xa0No enrollment account\xa0or\xa0unassigned. |\\n| Frequency | Break down usage-based, one-time, and recurring costs. | Indicates whether a charge is expected to repeat. Charges can either happen once\xa0OneTime, repeat on a monthly or yearly basis\xa0Recurring, or be based on usage\xa0UsageBased. |\\n| Invoice ID | Break down costs by billed invoice. | Unbilled charges don\'t have an invoice ID yet and EA costs don\'t include invoice details and will show as\xa0No invoice ID. |\\n| InvoiceSectionId | Unique identifier for the MCA invoice section. | Unique identifier for the EA department or MCA invoice section. |\\n| InvoiceSectionName | Name of the invoice section. | Name of the EA department or MCA invoice section. |\\n| Location | Break down costs by resource location or region. | Purchases and Marketplace usage may be shown as\xa0unassigned, or\xa0No resource location. |\\n| Meter | Break down costs by usage meter. | Purchases and Marketplace usage will show as\xa0unassigned\xa0or\xa0No meter. Refer to\xa0Charge type\xa0to identify purchases and\xa0Publisher type\xa0to identify Marketplace charges. |\\n| Operation | Break down AWS costs by operation. | Applicable only to AWS scopes and management groups. Azure data doesn\'t include operation and will show as\xa0No operation\xa0- use\xa0Meter\xa0instead. |\\n| Pricing model | Break down costs by on-demand, reservation, or spot usage. | Purchases show as\xa0OnDemand. If you see\xa0Not applicable, group by\xa0Reservation\xa0to determine whether the usage is reservation or on-demand usage and\xa0Charge type\xa0to identify purchases. |\\n| PartNumber | The identifier used to get specific meter pricing. |  |\\n| Product | Name of the product. |  |\\n| ProductOrderId | Unique identifier for the product order |  |\\n| ProductOrderName | Unique name for the product order. |  |\\n| Provider | Break down costs by the provider type: Azure, Microsoft 365, Dynamics 365, AWS, and so on. | Identifier for product and line of business. |\\n| Publisher type | Break down Microsoft, Azure, AWS, and Marketplace costs. | Values are\xa0Microsoft\xa0for MCA accounts and\xa0Azure\xa0for EA and pay-as-you-go accounts. |\\n| Reservation | Break down costs by reservation. | Any usage or purchases that aren\'t associated with a reservation will show as\xa0No reservation\xa0or\xa0No values. Group by\xa0Publisher type\xa0to identify other Azure, AWS, or Marketplace purchases. |\\n| ReservationId | Unique identifier for the purchased reservation instance. | In actual costs, use ReservationID to know which reservation the charge is for. |\\n| ReservationName | Name of the purchased reservation instance. | In actual costs, use ReservationName to know which reservation the charge is for. |\\n| Resource | Break down costs by resource. | Marketplace purchases show as\xa0Other Marketplace purchases\xa0and Azure purchases, like Reservations and Support charges, show as\xa0Other Azure purchases. Group by or filter on\xa0Publisher type\xa0to identify other Azure, AWS, or Marketplace purchases. |\\n| Resource group | Break down costs by resource group. | Purchases, tenant resources not associated with subscriptions, subscription resources not deployed to a resource group, and classic resources don\'t have a resource group and will show as\xa0Other Marketplace purchases,\xa0Other Azure purchases,\xa0Other tenant resources,\xa0Other subscription resources,\xa0$system, or\xa0Other charges. |\\n| ResourceId | Unique identifier of the\xa0Azure Resource Manager\xa0resource. |  |\\n| Resource type | Break down costs by resource type. | Type of resource instance. Not all charges come from deployed resources. Charges that don\'t have a resource type will be shown as null or empty,\xa0Others, or\xa0Not applicable. For example, purchases and classic services will show as\xa0others,\xa0classic services, or\xa0No resource type. |\\n| ServiceFamily | Type of Azure service. For example, Compute, Analytics, and Security. |  |\\n| ServiceName | Name of the Azure service. | Name of the classification category for the meter. For example, Cloud services and Networking. |\\n| Service name\xa0or\xa0Meter category | Break down cost by Azure service. | Purchases and Marketplace usage will show as\xa0No service name\xa0or\xa0unassigned. |\\n| Service tier\xa0or\xa0Meter subcategory | Break down cost by Azure usage meter subclassification. | Purchases and Marketplace usage will be empty or show as\xa0unassigned. |\\n| Subscription | Break down costs by Azure subscription and AWS linked account. | Purchases and tenant resources may show as\xa0No subscription. |\\n| Tag | Break down costs by tag values for a specific tag key. | Purchases, tenant resources not associated with subscriptions, subscription resources not deployed to a resource group, and classic resources cannot be tagged and will show as\xa0Tags not supported. Services that don\'t include tags in usage data will show as\xa0Tags not available. Any remaining cases where tags aren\'t specified on a resource will show as\xa0Untagged. Learn more about\xa0tags support for each resource type. |\\n| UnitOfMeasure | The billing unit of measure for the service. For example, compute services are billed per hour. |  |\\n\\nOne or a [combination of these filters](https://learn.microsoft.com/azure/cost-management-billing/costs/group-filter?WT.mc_id=AZ-MVP-5004796 \\"Group and filter options in Cost analysis and budgets\\") can be used to create your own meaningful Budgets! You can target specific resources, an example is if you have resources in a Shared Resource Group - for example, Networking, and you have a VPN Gateway, that is used for a Site to Site VPN, for a specific application, that is sitting in another resource group or subscription - you can add the Resource directly into the filter of your Budget for the Azure Gateway, and then include a Tag - that may reference the rest of the application dependencies.\\n\\nBudgets can be created with all sorts of various tools, from the Azure Portal to:\\n\\n* [Quickstart: Create a budget with Bicep](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/quick-create-budget-bicep?tabs=CLI&WT.mc_id=AZ-MVP-5004796 \\"Quickstart: Create a budget with Bicep\\")\\n* PowerShell ([New-AzConsumptionBudget](https://learn.microsoft.com/en-us/powershell/module/az.billing/new-azconsumptionbudget?view=azps-9.5.0&WT.mc_id=AZ-MVP-5004796 \\"New-AzConsumptionBudget\\") & [Set-AzConsumptionBudget](https://learn.microsoft.com/powershell/module/az.billing/set-azconsumptionbudget?view=azps-9.5.0&WT.mc_id=AZ-MVP-5004796 \\"Set-AzConsumptionBudget\\"))\\n\\n#### Relaxing beverage time - Tips & Tricks\\n\\nFinished the day of cleaning! Now is the time to sit back and enjoy your favourite beverages, and read the labels on the bottles!\\n\\n* You can use the [**Azure Mobile Application**](https://azure.microsoft.com/get-started/azure-portal/mobile-app?WT.mc_id=AZ-MVP-5004796 \\"Get the Azure mobile app\\") **to display your Cost and Budgets** so keep on top of your consumption on the go!\\n* The Microsoft Cost Management team are working on new features all the time, including improvements to Cost Management and Budgets! If you like living on the edge - be sure to check out [**the Preview portal**](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/enable-preview-features-cost-management-labs?WT.mc_id=AZ-MVP-5004796#recommendationinsights \\"Enable preview features in Cost Management Labs\\") (and add your feedback)!\\n* You can use an [**Action Group**](https://learn.microsoft.com/azure/azure-monitor/alerts/action-groups?WT.mc_id=AZ-MVP-5004796#configure-basic-action-group-settings \\"Create and manage action groups in the Azure portal\\")**,** to trigger a Webhook or Azure **Automation** runbook - to resize or stop resources. Action Groups are currently only supported for subscription and resource group scopes, so you may need to have one Budget for Monitoring at a higher level and one Budget for running automation at a lower level."},{"id":"azure/microsoft-student-education-hub","metadata":{"permalink":"/azure/microsoft-student-education-hub","source":"@site/blog/2023-03-02-microsoft-student-education-hub.md","title":"Azure for Students","description":"Students learn in different ways through many possible avenues and experiences. Microsoft has assets to help students navigate through their journey.","date":"2023-03-02T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.67,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-03-02 00:00:00 +1300","title":"Azure for Students","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/student_developer_skillingjourney.png"},"slug":"azure/microsoft-student-education-hub"},"unlisted":false,"prevItem":{"title":"Azure Budget Filters: A Key Tool for Effective Cloud Cost Management","permalink":"/azure/azure-budget-filters-a-key-tool-for-effective-cloud-cost-management"},"nextItem":{"title":"Azure Availability Zone Peering","permalink":"/azure/azure-availability-zone-peering"}},"content":"Students learn in different ways through many possible avenues and experiences. Microsoft has assets to help students navigate through their journey.\\n\\nThis article aims to help to make access to student resources clearer.\\n\\n#### Skilling Journey\\n\\n![MS Student Developer - Skilling Journey](/uploads/student_developer_skillingjourney.png)\\n\\nThere are 3 phases:\\n\\n* Discover\\n* Engage\\n* Grow\\n\\nEach phase offers various ways of engage, learn and discover Microsoft Azure services and functionality.\\n\\nSome of the resources for each phase can be found below:\\n\\n| Discover | Engage | Grow |\\n| :--- | :--- | :--- |\\n| [Microsoft Learn Student Hub](https://learn.microsoft.com/training/student-hub/?WT.mc_id=AZ-MVP-5004796) | [Azure for Students](https://azure.microsoft.com/free/students/?WT.mc_id=AZ-MVP-5004796) | [Imagine Cup](https://imaginecup.microsoft.com/?WT.mc_id=AZ-MVP-5004796) |\\n| [Microsoft Reactor](https://developer.microsoft.com/reactor/?WT.mc_id=AZ-MVP-5004796) | Hackathons | [Events: Student Summit](https://learn.microsoft.com/events/browse?terms=student-summit&WT.mc_id=AZ-MVP-5004796) |\\n| [Microsoft Build](https://mybuild.microsoft.com/?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| [Curriculum on GitHub](https://microsoft.github.io/workshop-library/) | [Dev Tools for Teaching](https://azureforeducation.microsoft.com/en-us/Institutions?WT.mc_id=AZ-MVP-5004796) |  |\\n\\n##### Independent Learner\\n\\nIf you are an independent learner, the resources below can supplement your desire to learn:\\n\\n* [**Learn by Doing**](https://learn.microsoft.com/training/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Learn - Training\\"): Students can gain the skills to  \\n  apply to everyday situations through hands-on  \\n  personalized training at their own pace or with our  \\n  global network of learning partner\\n* [**Showcase Skills**](https://learn.microsoft.com/certifications/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Learn - Certifications\\"): Help advance their career by  \\n  completing challenges that demonstrate expertise.  \\n  Earn globally recognized and industry-endorsed  \\n  certifications and showcase them to their network.\\n* [**Code Samples**](https://learn.microsoft.com/samples/browse/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Learn - Code Samples\\"): Test out new capabilities in their  \\n  own projects faster and easier with code samples  \\n  that bring Microsoft technology to life.\\n\\n[**Build in the cloud-free with Azure for Students ($100 Azure Credit, No credit card required)**](https://azure.microsoft.com/en-us/free/students/?WT.mc_id=AZ-MVP-5004796 \\"Build in the cloud free with Azure for Students\\")\\n\\n_Use your university or school email to sign up and renew each year you\'re a student. Your school or university doesn\'t need to be enrolled in a program - your university and school email address is used to validate your student status._\\n\\n##### Group Learner\\n\\nIf you are a group learner, alongside the same content as the indepedant learning path, check out the [Microsoft Student Ambassador](https://studentambassadors.microsoft.com/?WT.mc_id=AZ-MVP-5004796) program to meet likeminded people, and don\'t forget to check out for your local [Azure meetups](https://www.meetup.com/pro/azuretechgroups/)!\\n\\n##### Education Hub\\n\\nLocated in the Azure Portal, the Education Hub enables easy access to Azure offers and other academic benefits Microsoft provides.\\n\\nUsing the [Education Hub](https://portal.azure.com/#view/Microsoft_Azure_Education/EducationMenuBlade/\\\\~/overview \\"Education Hub\\"), Students can:\\n\\n* Download free software provided by their Academic Institution\\n* Sign up for academic-specific Azure offers\\n* Launch self-guided role-based learning pathways\\n* Deploy academic-focused ARM templates\\n\\n![Microsoft Azure Education Hub](/uploads/azure_educationhub.png \\"Microsoft Azure Education Hub\\")\\n\\n##### FAQS\\n\\nA bit of a misleading heading! But there is no need to repeat FAQS that have already been answered by Microsoft or community members; the trick is finding where to go!\\n\\nMost Student FAQs _(Frequently Asked Questions)_ can be answered on the following page:  \\n\\n[Frequently asked questions about the Education Hub](https://learn.microsoft.com/en-us/azure/education-hub/azure-dev-tools-teaching/program-faq?WT.mc_id=AZ-MVP-5004796 \\"Frequently asked questions about the Education Hub\\"), but there may be other questions that don\'t fit in; if that\'s the case - make sure you check [Microsoft Q&A](https://learn.microsoft.com/en-us/search/?terms=students&category=QnA&WT.mc_id=AZ-MVP-5004796 \'\\"students\\" in Q&A\') - as most likely, what you need has been answered by Microsoft or a community member.\\n\\nMake sure you also check out my post on [How can I learn how to use Microsoft Azure?](https://luke.geek.nz/azure/How-can-I-learn-how-to-use-Microsoft-Azure/) and [AWESOME-Azure-Architecture](https://aka.ms/AwesomeAzureArchitecture) list."},{"id":"azure/azure-availability-zone-peering","metadata":{"permalink":"/azure/azure-availability-zone-peering","source":"@site/blog/2023-02-23-azure-availability-zone-peering.md","title":"Azure Availability Zone Peering","description":"In most regions (and odds are, if your area doesn\'t have Avalibility Zones, it\'s on the roadmap to be set up), Microsoft Azure has Availability Zones.","date":"2023-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":7.02,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-02-23 00:00:00 +1300","title":"Azure Availability Zone Peering","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/availability-zones.png"},"slug":"azure/azure-availability-zone-peering"},"unlisted":false,"prevItem":{"title":"Azure for Students","permalink":"/azure/microsoft-student-education-hub"},"nextItem":{"title":"Azure Architecture - Solution Requirement Consideration Checklist","permalink":"/azure/azure-architecture-solution-requirement-consideration-checklist"}},"content":"In most [regions ](https://learn.microsoft.com/azure/reliability/availability-zones-service-support?WT.mc_id=AZ-MVP-5004796 \\"Availability zone service and regional support\\")_(and odds are, if your area doesn\'t have Avalibility Zones, it\'s on the roadmap to be set up)_, Microsoft Azure has [Availability Zones](https://learn.microsoft.com/en-us/azure/reliability/availability-zones-overview?WT.mc_id=AZ-MVP-5004796 \\"What are Azure regions and availability zones?\\").\\n\\nEach Azure region features data centres deployed within a latency-defined perimeter. At a high level, these zones consist of 3 separate data centres with independent cooling, power, switching etc.\\n\\n![Azure availability zones](/uploads/availability-zones.png \\"Azure availability zones\\")\\n\\n> Azure availability zones are connected by a high-performance network with a round-trip latency of less than 2ms. They help your data stay synchronized and accessible when things go wrong. Each zone is composed of one or more datacenters equipped with independent power, cooling, and networking infrastructure. Availability zones are designed so that if one zone is affected, regional services, capacity, and high availability are supported by the remaining two zones.\\n>\\n> With availability zones, you can design and operate applications and databases that automatically transition between zones without interruption. Azure availability zones are highly available, fault tolerant, and more scalable than traditional single or multiple datacenter infrastructures.\\n\\n#### Availability Zone peering\\n\\nToday we are going to look into Availability Zone peering:\\n\\n> Each data centre is assigned to a physical zone. Physical zones are mapped to logical zones in your Azure subscription. Azure subscriptions are automatically assigned this mapping when a subscription is created.\\n\\n##### Physical Zones vs Logical Zones\\n\\nThere are a few things to be aware of here that I will call out:\\n\\n* **Physical zones** are mapped to **logical zones** in your Azure subscription.\\n* Azure subscriptions are automatically **assigned** this **mapping** **when** a **subscription** is **created**.\\n\\n**So what does this mean?**\\n\\nWe know we have three separate data centres within a single region:\\n\\n| Zone | Region |\\n| --- | --- |\\n| 1 | Australia East |\\n| 2 | Australia East |\\n| 3 | Australia East |\\n\\nWe can see these zones in the Azure Portal when we create resources:\\n\\n![Azure Avalibility Zone - Selection](/uploads/avalibilityzone_selection_azportal.png \\"Azure Avalibility Zone - Selection\\")\\n\\nThis is great for making your solutions redundant against a single data centre failure and spreading your workloads across different zones; services such as Virtual Networks are zone-redundant by default, allowing access to resources across multiple zones out of the box.\\n\\nOne reason you may have all your resources in a single zone could be latency.\\n\\nLets us go back to the paragraphs around physical and logical zones and mapping - what does this mean?\\n\\nWhat this means is that each of the three data centres is assigned a physical **AND** logical mapping, so your Azure datacentres look like this:\\n\\n| Zone (Physical) | Region | Zone (Logical) |\\n| --- | --- | --- |\\n| 1 | Australia East | 3 |\\n| 2 | Australia East | 2 |\\n| 3 | Australia East | 1 |\\n\\nWhen you **deploy a resource into an Azure Avalibility Zone and select Zone 1**, you choose **the Logical Zone, NOT** a **physical zone**.\\n\\nThis means that **FOR EACH** Microsoft **Azure subscription,** whether in the same Microsoft Entra ID tenancy or not, **Zone 1** can be a **different physical data centre**.\\n\\nSo if you have resources deployed across multiple subscriptions, and all your resources are deployed to Zone 1 - they **MAY NOT** be in the same physical data centre.\\n\\n| Azure Subscriptions | Region | Zone (Logical) | Zone (Physical) |\\n| --- | --- | --- | --- |\\n| Sub A | Australia East | 1 | 1 |\\n| Sub B | Australia East | 1 | 3 |\\n| Sub B | Australia East | 1 | 1 |\\n\\nIn an example like the above, you have three separate Azure subscriptions, and you have deployed your Virtual Machines and other resources across all Azure subscriptions into Zone 1, 2 of your subscriptions are using the same physical zone for zone 1, and another subscription is using a separate availability zone altogether.\\n\\n![AzureAvailabilityZones_Logical](/uploads/azureavailabilityzones_logical.png \\"AzureAvailabilityZones_Logical\\")\\n\\n**One of the reasons the logical and physical zones are different is due to capacity management**; out of habit, many people select Zone 1 - this would mean that certain zones become overpopulated while others are underutilized. The logical zones allow Microsoft some ability to spread the load.\\n\\nIt\'s worth noting that **mapping the Logical to Physical Zones of the Avalibility Zones within your region is done when the subscription is created**.\\n\\n##### Checking your Zone Peers using PowerShell and the Azure API\\n\\n**During normal business use - you don\'t need to know any of this; select a zone and deploy**; if you have resources across subscriptions and run into additional latency - this may be why, although each availability zone is connected through a dedicated regional low-latency network with a round-trip latency of less than 2ms.\\n\\nBut suppose you are curious or want to delve deeper into your Disaster Recovery and resiliency architecture within a single region. In that case, it can be helpful to know the mapping.\\n\\nThis information isn\'t fed into the Azure Portal. To find the mapping, we need to query the Azure API directly using the [Check Zone Peers](https://learn.microsoft.com/en-us/rest/api/resources/subscriptions/check-zone-peers?tabs=HTTP&WT.mc_id=AZ-MVP-5004796 \\"Subscriptions - Check Zone Peers\\") endpoint.\\n\\nTo do this, I have written a rough PowerShell script that will register the AvailabilityZonePeering Azure feature that you need to enable the lookup and query the API for the mappings.\\n\\n    # Connect to Azure using Get-AzAccount\\n    Connect-AzAccount\\n    \\n    # Set the region to \'Australia East\'\\n    $region = \'Australia East\'\\n    \\n    # Get all subscriptions that the account has access to\\n    $subscriptions = Get-AzSubscription | Select-Object -ExpandProperty SubscriptionId\\n    \\n    # Get the access token for the authenticated user\\n    $token = (Get-AzAccessToken).Token\\n    \\n    # Check if AvailabilityZonePeering feature is enabled and enable it if it\'s not\\n    $azFeature = Get-AzProviderFeature -ProviderNamespace Microsoft.Resources -FeatureName AvailabilityZonePeering\\n    if (!$azFeature.RegistrationState.Equals(\\"Registered\\")) {\\n        do {\\n            Register-AzProviderFeature -FeatureName AvailabilityZonePeering -ProviderNamespace Microsoft.Resources\\n            Start-Sleep -Seconds 5\\n            $azFeature = Get-AzProviderFeature -ProviderNamespace Microsoft.Resources -FeatureName AvailabilityZonePeering\\n        } until ($azFeature.RegistrationState.Equals(\\"Registered\\"))\\n        Write-Host \\"The AvailabilityZonePeering feature has been enabled.\\"\\n    } else {\\n        Write-Host \\"The AvailabilityZonePeering feature is already enabled.\\"\\n    }\\n    \\n    # Define the request body for the REST API call\\n    $body = @{\\n        subscriptionIds= $subscriptions | ForEach-Object { \'subscriptions/\' + $_ }\\n        location = $region\\n    } | ConvertTo-Json\\n    \\n    # Define the request parameters for the REST API call\\n    $params = @{\\n        Uri         = \\"https://management.azure.com/subscriptions/\\" + $subscriptions[0] + \\n                      \\"/providers/Microsoft.Resources/checkZonePeers/?api-version=2020-01-01\\"\\n        Headers     = @{ \'Authorization\' = \\"Bearer $token\\" }\\n        Method      = \'POST\'\\n        Body        = $body\\n        ContentType = \'application/json\'\\n    }\\n    \\n    # Invoke the REST API and store the response\\n    $availabilityZonePeers =  Invoke-RestMethod @Params\\n    \\n    # Initialize an empty array for the output\\n    $output = @()\\n    \\n    # Loop through each availability zone and its associated peers and add them to the output array\\n    foreach ($i in $availabilityZonePeers.availabilityZonePeers.availabilityZone) {\\n        foreach ($zone in $availabilityZonePeers.availabilityZonePeers[$i-1].peers ) {\\n            $output += New-Object PSObject -Property @{\\n                Zone = $i\\n                MatchesZone = $zone.availabilityZone\\n                SubscriptionId = $zone.subscriptionId\\n            }\\n        }\\n        $output += \\"\\"\\n    }\\n    \\n    # Output the results\\n    $output |  Format-Table\\n\\nOnce we have connected to Microsoft Azure and run the script, we will get an output like the one below, which I ran across my own three subscriptions:\\n\\n| SubscriptionId | MatchesZone | Zone |\\n| --- | --- | --- |\\n| 3bdfd67e-6280-43af-8121-4f04dc84706c | 2 | 1 |\\n| 8df7caa2-95cb-44d1-9ecb-e5220ec6a825 | 1 | 1 |\\n| 119bbbb7-3ab5-4eb3-ab21-3c65f562fbef | 1 | 1 |\\n| 3bdfd67e-6280-43af-8121-4f04dc84706c | 1 | 2 |\\n| 8df7caa2-95cb-44d1-9ecb-e5220ec6a825 | 2 | 2 |\\n| 119bbbb7-3ab5-4eb3-ab21-3c65f562fbef | 2 | 2 |\\n| 3bdfd67e-6280-43af-8121-4f04dc84706c | 3 | 3 |\\n| 8df7caa2-95cb-44d1-9ecb-e5220ec6a825 | 3 | 3 |\\n| 119bbbb7-3ab5-4eb3-ab21-3c65f562fbef | 3 | 3 |\\n\\nOn the right-hand side, we see the \'Zones\' - these are the Physical Zones, so Zone 1 to 3.\\n\\nFor each subscription, we can see the Logical Zone mapping as well.\\n\\nIn this example, my subscription of \'3bdfd67e-6280-43af-8121-4f04dc84706c\', if I were to deploy to Zone 2 in my Azure Portal, would deploy to the same physical datacenter as Zone 1 of: \'8df7caa2-95cb-44d1-9ecb-e5220ec6a825\'.\\n\\nAs you can also see, my Zone 3 matches the same Zone 3 logically and physically for all my subscriptions, but there are differences between Zone 2 and 1.\\n\\nAgain, during normal business as usual, you don\'t need to know this - but it\'s always good to know how this works. If you want confirmation of the resiliency of your architecture across Availability Zones, this is a great way to confirm whether your resources are physically located together - or not."},{"id":"azure/azure-architecture-solution-requirement-consideration-checklist","metadata":{"permalink":"/azure/azure-architecture-solution-requirement-consideration-checklist","source":"@site/blog/2023-02-14-azure-architecture-solution-requirement-consideration-checklist.md","title":"Azure Architecture - Solution Requirement Consideration Checklist","description":"Building a cloud solution on Azure can be an exciting yet daunting task.","date":"2023-02-14T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.915,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-02-14 00:00:00 +1300","title":"Azure Architecture - Solution Requirement Consideration Checklist","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/az_architecture_solutionchecklistheader.png"},"slug":"azure/azure-architecture-solution-requirement-consideration-checklist"},"unlisted":false,"prevItem":{"title":"Azure Availability Zone Peering","permalink":"/azure/azure-availability-zone-peering"},"nextItem":{"title":"Azure Deployment history cleanup with Azure DevOps","permalink":"/azure/azure-deployment-cleanup-with-azure-devops"}},"content":"Building a cloud solution on Azure can be an exciting yet daunting task.\\n\\nThe key to a successful implementation is carefully planning and considering solution requirements using the guidance of the [Microsoft Cloud Adoption](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Cloud Adoption Framework for Azure\\") and [Well Architecture frameworks](https://learn.microsoft.com/en-us/azure/architecture/framework/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Azure Well-Architected Framework\\").\\n\\nBut knowing what questions to ask and data to capture to give you the bigger picture - to not only consider the solution for the short term and long term, can be difficult. This is where the [Azure architecture solution requirements checklist](https://github.com/lukemurraynz/Azure_Checklists) comes in.\\n\\nLeaning on the great frameworks already in place to assist with the Cloud Adoption and Azure Well Architecture frameworks, the solution requirements checklist is intended to act as a way of asking and capturing the requirements of your solutions. It can be a great reminder to discover some of those requirements _(whether functional or non-functional)_ that you may have forgotten about!\\n\\nI am using the [Azure Review Checklist](https://github.com/Azure/review-checklists \\"Azure/review-checklists\\") - as a building block! I created a custom checklist intended to work alongside the review checklists - but aimed more at the discovery and requirements-gathering stage to assist with designing the proper outcomes for the business.\\n\\nAt the time of this article, there are 8 main categories, and various sub categories:\\n\\n* AI\\n* Business\\n* Data\\n* Governance\\n* Infrastructure\\n* Microservices\\n* Observability\\n* Resiliency\\n\\nExamples of some questions are:\\n\\n| Main Area | Sub Area  | Checklist item                                                                                    | Description (optional)                                                                                                                                                                                                       |\\n| --------- | --------- | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Business  | Goals     | Why are we moving the solution to Azure?                                                          | Understand the reasoning behind the decision to move to a cloud platform like Azure. Helps to validate the end result reaches this goal.                                                                                     |\\n| Business  | Goals     | What are the business objectives or quantifiable business goals?                                  | What is the business objectives (ie Increased sales revenue, cost reduction, customer satisaction, employee productivity                                                                                                     |\\n| Business  | Goals     | What outcomes will you achieve for customer?                                                      | What is the\xa0 objectives for the customer? What do they want to achieve using this solution                                                                                                                                   |\\n| Business  | Goals     | Is there a timeline for building the solution Azure?                                              | Asking about the timeline for building a solution in Azure is important to determine resource allocation, budgeting, prioritization, and setting stakeholder expectations.                                                   |\\n| Business  | Goals     | How many people will be accessing the solution?                                                   | Asking about the number of people accessing the solution helps to determine the necessary resources and scalability required to accommodate the expected traffic and usage.                                                  |\\n| Business  | Goals     | Is there a targeted event or date for an announcement about the solution\'s availability on Azure? | Timeline for architecture, deployment, testing can help determine what risks, resource requirements and cost and the delivery of solution.                                                                                   |\\n| Business  | Goals     | Does the solution impact a Team, Department or organization?                                      | Impact on a team, department, or organization helps determine the scope and potential consequences of the solution, ensuring that all relevant stakeholders are considered and accounted for in the decision-making process. |\\n| Business  | Customers | What are the customer expecations?                                                                | Customer expectations helps ensure that the solution meets the needs and desires of the end-users, and make sure business outcomes match customer expectations.                                                              |\\n| Business  | Customers | Is there a deal or customer opportunity associated with having the solution in Azure?             | Any associated deals or customer opportunities helps to understand the potential financial benefits, vendor offerings and growth opportunities of using Azure as a platform for the solution.                                |\\n\\n![Azure Architecture - Solution requirement considerations](/uploads/azurearchitecture_considerations.png \\"Azure Architecture - Solution requirement considerations\\")\\n\\n> The Azure Architecture - Solution Requirement Consideration checklist, is intended to be a living resource, I am not an expert in all fields so there may be gaps or questions you feel is relevant or missing! Feel free to open a Pull Request to contribute! This is for you!\\n>\\n> You can find the **latest version of the checklis**t on GitHub here: [lukemurraynz/**Azure_Checklists**](https://github.com/lukemurraynz/Azure_Checklists \\"lukemurraynz/Azure_Checklists\\")\\n\\nUsing this is simple! \\n\\n1. **Download** the [latest version of the excel](https://github.com/Azure/review-checklists/releases/latest/download/review_checklist.xlsm \\"Azure Review Checklists\\") spreadsheet from: [Azure/review-checklists](https://github.com/Azure/review-checklists \\"Azure Review Checklists\\").\\n2. **Download** the latest version of the [**Azure Architecture checklist**](https://github.com/lukemurraynz/Azure_Checklists/tree/main \\"azure_architecture_checklist.en.json\\")**.**\\n3. Open up the review_checklist excel document and click **Import checklist from JSON**\\n4. ![Import checklist from file](/uploads/import-azurearchitecturechecklistjson.png \\"Import checklist from file\\")\\n5. ![Import checklist from file](/uploads/select-azurearchitecturechecklist.png \\"Import checklist from file\\")\\n6. Select the downloaded: azure_architecture_checklist.en.json\\n\\nOnce imported, you can now save the excel document, and start adjusting the **Severity**, **Status** and add **Comments** - to capture the information, to then use to architect your solutions!\\n\\n> **Note: I do not create, edit or modify the Excel spreadsheet, created by for the Azure Reviews - I simply use it to run my custom checklist. Make sure to check out the** [**Azure Review Checklists**](https://github.com/Azure/review-checklists \\"Azure Review Checklists\\")**!**\\n\\nThere are some settings that you might need to change in your system to run **macro-enabled Excel spreadsheet**s. When initially opening the file you may see the following error, which prevents Excel from loading:\\n\\n> Excel cannot open the file \'review_checklist.xlsm\' because the file format or file extension is not valid. Verify that the file has not been corrupted and that the file extension matches the format of the file.\\n\\nIn other cases the file opens with the following message, which prevents you from being able to load the checklist items:\\n\\n### Unblock file or add an exception to Windows Security\\n\\n1. You might need to unblock the file from the file properties in the Windows File Explorer, so that you can use the macros required to import the checklist content from github.com:\\n2. Additionally, you might want to add the folder where you cloned this repo to the list of exceptions in Windows Security (in the Virus & Threat Protection section):"},{"id":"azure/azure-deployment-cleanup-with-azure-devops","metadata":{"permalink":"/azure/azure-deployment-cleanup-with-azure-devops","source":"@site/blog/2023-02-03-azure-deployment-cleanup-with-azure-devops.md","title":"Azure Deployment history cleanup with Azure DevOps","description":"Microsoft Azure has a limit of 800 deployments per resource group. This means that a single resource group can only contain 800 historical deployments at most.","date":"2023-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.99,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-02-03 00:00:00 +1300","title":"Azure Deployment history cleanup with Azure DevOps","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azdeploycleanupazdevopsheader.png"},"slug":"azure/azure-deployment-cleanup-with-azure-devops"},"unlisted":false,"prevItem":{"title":"Azure Architecture - Solution Requirement Consideration Checklist","permalink":"/azure/azure-architecture-solution-requirement-consideration-checklist"},"nextItem":{"title":"You Can\'t Touch This: How to Make Your Azure Backup Immutable and Secure","permalink":"/2023/01/18/you-can-t-touch-this-how-to-make-your-azure-backup-immutable-and-secure"}},"content":"Microsoft Azure has a limit of [800 deployments per resource group](https://learn.microsoft.com/azure/azure-resource-manager/management/azure-subscription-service-limits?WT.mc_id=AZ-MVP-5004796#resource-group-limits \\"Resource group limits\\"). This means that a single resource group can only contain 800 historical deployments at most.\\n\\n> A deployment in Azure refers to the process of creating or updating resources in a resource group.\\n\\nWhen deploying resources in Azure, it is essential to keep track of the number of [historic deployments](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/deployment-history?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796 \\"View deployment history with Azure Resource Manager\\") in a resource group to ensure that the limit is not exceeded. This is because new deployments will fail if the limit is exceeded, and creating or updating resources in that resource group will not be possible.\\n\\nIf you have CI/CD _(Continuous Integration and Continuous Deployment)_ set up to deploy or change your infrastructure or services with code, it can be easy to reach this limit. [Azure will attempt to do this automatically](https://learn.microsoft.com/azure/azure-resource-manager/troubleshooting/deployment-quota-exceeded?tabs=azure-cli&WT.mc_id=AZ-MVP-5004796 \\"Resolve error when deployment count exceeds 800\\") when reaching your limit. Still, you may want to pre-empt any problems if you make many deployments and the system hasn\'t had time to prune automatically, or this is [disabed](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/deployment-history-deletions?WT.mc_id=AZ-MVP-5004796&tabs=azure-powershell).\\n\\nThis came up in conversations on Microsoft Q&A, so I thought I would dig into it and put together a possible option.\\n\\nTo avoid exceeding the deployment limit, it may be necessary to clean up old deployments.\\n\\nThis can be done by using a script to remove deployments that are no longer needed.\\n\\nSo let\'s build an Azure DevOps pipeline that runs weekly to connect to our Microsoft Azure environment and clean up historical deployments.\\n\\n![Microsoft Azure Deployment History Cleanup with Azure DevOps](/uploads/azdeploycleanupazdevopsheader.png \\"Microsoft Azure Deployment History Cleanup with Azure DevOps\\")\\n\\nFor this article, I will assume you have an Azure DevOps repository setup and the permissions _(Owner)_ to make the necessary privileged actions to the Microsoft Azure environment to do the design.\\n\\n> Note: Scripts and pipeline are \\"[here](https://github.com/lukemurraynz/AzDeploymeantCleanup \\"lukemurraynz / AzDeploymeantCleanup\\")\\".\\n\\n#### Deploy and Configure\\n\\n##### Create Service Prinicipal\\n\\n 1. Navigate to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure\\")\\n 2. Click on [**Microsoft Entra ID**](https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/\\\\~/Overview \\"Microsoft Entra ID\\")\\n 3. Click on [**App Registrations**](https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/\\\\~/RegisteredApps \\"Azure App Registrations\\")\\n 4. Click on: **+ New Registration**\\n 5. Enter the following information:\\n    * **Name** (i.e. SPN.AzDeploymentCleanup)\\n 6. Click **Register**\\n 7. Copy the following for later when we add the SPN to Azure DevOps.\\n    * Application (client) ID\\n    * Directory (tenant ID)\\n 8. Click on **Certificates & Secrets**\\n 9. Press **+ New Client Secret**\\n10. Enter a relevant description and expiry date and click Add\\n11. **Copy** the **value** of the new secret (this is essentially your password), and you won\'t be able to see the matter again.\\n\\n##### Create Custom Role & Assign permissions\\n\\nNow that your service principal has been created, it is time to assign permissions because this script targets all subscriptions under a management group; we are going to set the permissions to that management group so that it flows to all subscriptions underneath it - and in the view of least privileged we will create a Custom Role to apply to our Service Principal.\\n\\n##### Create Custom Role\\n\\nFor the deployment history to be completed, we will need the following permissions:\\n\\n* Microsoft.Resources/deployments/delete\\n* Microsoft.Resources/subscriptions/resourceGroups/read\\n* Microsoft.Management/managementGroups/read\\n* Microsoft.Resources/subscriptions/read\\n* Microsoft.Management/managementGroups/descendants/read\\n* Microsoft.Management/managementGroups/subscriptions/read\\n* Microsoft.Resources/subscriptions/resourcegroups/deployments/operations/read\\n* Microsoft.Resources/subscriptions/resourcegroups/deployments/read\\n* Microsoft.Resources/subscriptions/resourcegroups/deployments/write\\n* Microsoft.Resources/deployments/read\\n\\n 1. Navigate to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure\\")\\n 2. In the search bar above, type in and navigate to [**Management Groups**](https://portal.azure.com/#view/Microsoft_Azure_ManagementGroups/ManagementGroupBrowseBlade/\\\\~/MGBrowse_overview \\"Management Groups\\")\\n 3. Click a management group, click on **Access Control (IAM)**\\n 4. Click **+ Add**\\n 5. Click **Add Custom Role**\\n 6. Type in a role name _(an example is: AzDeploymentHistoryCleanup)_\\n 7. Check Start from **Scratch** and next click\\n 8. Click **+ Add permissions** and the permissions above _(you can search for them)_. Feel free to import the role from a JSON file \\"[here](https://github.com/lukemurraynz/AzDeploymeantCleanup \\"AzDeploymeantCleanup\\")\\".\\n 9. Click **Next**\\n10. Add **Assignable Scopes** _(this is the scope you can use to assign a role to - this won\'t give it to the Service Principal; it will only open it up so we can post it)_. Make sure you set it at the management group level you are targetting.\\n11. Click **Review + Create**\\n12. Click **Create**\\n\\n##### Assign Permissions\\n\\nNow that the custom role has been created, it is time to assign it to the service principal we made earlier.\\n\\n 1. Navigate to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure\\")\\n 2. In the search bar above, type in and navigate to [**Management Groups**](https://portal.azure.com/#view/Microsoft_Azure_ManagementGroups/ManagementGroupBrowseBlade/\\\\~/MGBrowse_overview \\"Management Groups\\")\\n 3. Click on the management group you want to manage and click on **Access Control (IAM)**\\n 4. Click **Add**\\n 5. Click **Add Role Assignment**\\n 6. Select your custom role _(you can toggle the type column, so CustomRoles are first in the list)_\\n 7. ![Microsoft Azure - add role assignments](/uploads/microsoft-azure-add-roleassignments.png \\"Microsoft Azure - add role assignments\\")\\n 8. Click **Members**\\n 9. Make sure \'_User, group or service principal_\' is selected and click **+ Select Members**\\n10. ![Microsoft Azure - Add Roles](/uploads/azdeploycleanupaddrolemembers.png \\"Microsoft Azure - Add Roles\\").\\n11. Select your Service Principal created earlier _(i.e. SPN.AzDeploymentCleanup)_\\n12. Click **Select**\\n13. Click **Review + assign** to assign the role.\\n\\nNote: Copy the Management Group ID and name, as we will need the information, along with the Service Principal and tenant IDs from earlier, in the next step of setting up Azure DevOps.\\n\\n##### Configure Azure DevOps Service Endpoint\\n\\nNow that the Service Principal and permissions have been assigned in Azure, it\'s time to create the service connection endpoint that will allow Azure DevOps to connect to Azure.\\n\\n 1. Navigate to your [**Azure DevOps**](http://dev.azure.com/ \\"Azure DevOps\\") organisation.\\n 2. Create a **Project**, if you haven\'t already\\n 3. Click on **Project Settings**\\n 4. Navigate to **Service Connection**\\n 5. Click on **New service connection**\\n 6. Select **Azure Resource Manager**\\n 7. Click **Next**\\n 8. Select **Service principal (manual)**\\n 9. Click **Next**\\n10. For the scope, choose Group Management\\n11. Enter the **Management Group ID**, the **Management Group Name**\\n12. Time to enter in the Service Principal details copied earlier, for the Service Principal Id paste in the Application ID.\\n13. The Service Principal key, enter the secret client value and select the Tenant ID\\n14. Click **Verify** - to verify the connectivity to the azure platform from Azure DevOps\\n15. Select **Grant access permission to all pipelines** and click **Verify and save**\\n\\n##### Configure script and pipeline\\n\\nNow that we have our:\\n\\n* Azure Service Principal\\n* Custom role and assignment\\n* Service connection\\n\\nWe now need to import the script and pipeline.\\n\\nIf you haven\'t already done - [create a Repo](https://learn.microsoft.com/azure/devops/repos/git/create-new-repo?view=azure-devops&WT.mc_id=AZ-MVP-5004796 \\"Create a new Git repo in your project\\") for the AzHistoryCleanup writing.\\n\\nYou can clone _(or copy)_ the files in the [**AzDeploymentCleanup**](https://github.com/lukemurraynz/AzDeploymeantCleanup \\"AzDeploymeantCleanup\\") Repo to your own.\\n\\nFirst, we need to copy the name of the Service Principal.\\n\\n1. Click **Project settings**\\n2. Click **Service Connections**\\n3. Click on your **Service Connection** and copy the **name** _(i.e. SC.AzDeploymentCleanup)_\\n4. ![Azure DevOps Service Principal](/uploads/azuredevops_spn.png \\"Azure DevOps Service Principal\\")\\n5. Navigate back to your Repo, and click on **AzDeploymentCleanup.yml** (this will become your pipeline)\\n6. Click **Edit**\\n7. ![Edit AzDeploymentCleanup YML](/uploads/select-azure-devops-edityaml.png \\"Edit AzDeploymentCleanup YML\\")\\n8. Update the variable for **ConnectedServiceNameARM** to the name of your service connection\\n9. Here you can also edit the **Script Arguments** - for example, in my demo, I am targeting the **ManagementGroup** named: mg-landing zones and keeping the latest five **deployments**.\\n10. By default, I also have a [cron job](https://learn.microsoft.com/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796 \\"Configure schedules for pipelines\\") to schedule this pipeline at 6 AM UTC every Sunday, and you can remove or edit this.\\n11. Once your changes are made, click **Commit.**\\n12. Now that your pipeline has been updated, its time to create it - click on **Pipelines.**\\n13. Click **New Pipeline**\\n14. Select **Azure Repos Git (YAML)**\\n15. Select your **Repo**\\n16. ![Select Azure DevOps repo](/uploads/azdeploycleanupazdevopsselectrepo.png \\"Select Azure DevOps repo\\")\\n17. Select **Existing Azure Pipelines YAML file**\\n18. ![Select YAML](/uploads/azdeploycleanupazdevopsselectyaml.png \\"Select YAML\\")\\n19. Select your Pipeline YAML file and click **Continue**\\n20. Click **Save** to create the pipeline\\n21. Now it\'s time to run the pipeline! Click **Run pipeline**\\n22. ![Azure DevOps - Pipeline run](/uploads/azdeploycleanupazdevopsscriptrun.png \\"Azure DevOps - Pipeline run\\")\\n23. If successful, **your script will trigger and clean up the oldest deployment history**! This can take several minutes to run if you have a lot of deployments.\\n\\n![Azure Deployments - Cleanup - Comparison 1](/uploads/azdeploycleanupazdevopsheader_compare1.png \\"Azure Deployments - Cleanup - Comparison 1\\")\\n\\n![Azure Deployments - Cleanup - Comparison 2](/uploads/azdeploycleanupazdevopsheader_compare2.png \\"Azure Deployments - Cleanup - Comparison 2\\")"},{"id":"/2023/01/18/you-can-t-touch-this-how-to-make-your-azure-backup-immutable-and-secure","metadata":{"permalink":"/2023/01/18/you-can-t-touch-this-how-to-make-your-azure-backup-immutable-and-secure","source":"@site/blog/2023-01-18-you-can-t-touch-this-how-to-make-your-azure-backup-immutable-and-secure.md","title":"You Can\'t Touch This: How to Make Your Azure Backup Immutable and Secure","description":"With immutable vaults, Azure Backup ensures that recovery points that are once created cannot be deleted before their intended expiry time. Azure Backup does this by preventing any operations which could lead to the loss of backup data.","date":"2023-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.55,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-01-18 00:00:00 +1300","title":"You Can\'t Touch This: How to Make Your Azure Backup Immutable and Secure","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/can-ttouchthis_immutability.png"}},"unlisted":false,"prevItem":{"title":"Azure Deployment history cleanup with Azure DevOps","permalink":"/azure/azure-deployment-cleanup-with-azure-devops"},"nextItem":{"title":"Export icons to SVG from the Microsoft Azure Portal using Amazing Icon Downloader","permalink":"/2023/01/09/export-icons-to-svg-from-the-microsoft-azure-portal-using-amazing-icon-downloader"}},"content":"With [immutable vaults](https://learn.microsoft.com/en-us/azure/backup/backup-azure-immutable-vault-concept?tabs=recovery-services-vault&WT.mc_id=AZ-MVP-5004796 \\"Immutable vault for Azure Backup\\"), Azure Backup ensures that recovery points that are once created cannot be deleted before their intended expiry time. Azure Backup does this by preventing any operations which could lead to the loss of backup data.\\n\\nHence, this helps you protect your backups against ransomware attacks and malicious actors by disallowing operations such as deleting backups or reducing retention in backup policies.\\n\\n> [Immutable vaults is now Generally available](https://azure.microsoft.com/en-us/updates/azure-backup-immutable-vaults-ga?WT.mc_id=AZ-MVP-5004796 \\"Generally available: Immutable vaults for Azure Backup\\") in all regions (March 13th 2023).\\n\\nAn immutable vault can assist in safeguarding your backup data by prohibiting any actions that might result in the loss of recovery points.\\n\\n![Can\'t touch this](/uploads/can-ttouchthis_immutability.png)\\n\\nBy securing the immutable vault setting, it can be made irreversible, which can prevent any unauthorized individuals from disabling the immutability feature and erasing the backups.\\n\\nThe Immutable vault configuration supports both Recovery Services vaults and Backup vaults.\\n\\n> While Azure Backup stores data in isolation from production workloads, it allows performing management operations to help you manage your backups, including those operations that allow you to delete recovery points. However, in certain scenarios, you may want to make the backup data immutable by preventing any such operations that, if used by malicious actors, could lead to the loss of backups. The Immutable vault setting on your vault enables you to block such operations to ensure that your backup data is protected, even if any malicious actors try to delete them to affect the recoverability of data.\\n\\nEnabling immutability for the vault is a reversible operation. However, you can make it irreversible to prevent any malicious actors from disabling it _(after disabling it, they can perform destructive functions)_.\\n\\nThe type of operations enabling immutability on the Azure Backup vault can prevent and safeguard from is.\\n\\n| System | Operation type | Description |\\n| --- | --- | --- |\\n| Recovery Services Vault & Backup Vault | Stop protection with delete data | A protected item can\'t have its recovery points deleted before their respective expiry date. However, you can still stop protection of the instances while retaining data forever or until their expiry. |\\n| Recovery Services Vault | Modify backup policy to reduce retention | Any actions that reduce the retention period in a backup policy are disallowed on Immutable vault. However, you can make policy changes that result in the increase of retention. You can also make changes to the schedule of a backup policy. |\\n| Recovery Services Vault | Change backup policy to reduce retention | Any attempt to replace a backup policy associated with a backup item with another policy with retention lower than the existing one is blocked. However, you can replace a policy with the one that has higher retention. |\\n\\nThere are three current states for the immutability of the Backup and Recovery Services Vault:\\n\\n* Disabled\\n* Enabled _(soft immutability)_\\n* Enabled and locked _(hard immutability)_\\n\\n| State of Immutable vault setting | Description |\\n| --- | --- |\\n| Disabled | The vault doesn\'t have immutability enabled and no operations are blocked. |\\n| Enabled | The vault has immutability enabled and doesn\'t allow operations that could result in loss of backups. However, the setting can be disabled. |\\n| Enabled and locked | The vault has immutability enabled and doesn\'t allow operations that could result in loss of backups. As the Immutable vault setting is now locked, it can\'t be disabled. Note that immutability locking is irreversible, so ensure that you take a well-informed decision when opting to lock. |\\n\\nImmutable vaults and [multi-user authorization](https://learn.microsoft.com/en-us/azure/backup/multi-user-authorization-concept?tabs=recovery-services-vault&WT.mc_id=AZ-MVP-5004796 \\"Multi-user authorization using Resource Guard\\") can safeguard your backups from various human and technological accidents or disruptions.\\n\\nImmutable vaults will not affect live or hot backups, such as snapshots.\\n\\nUsing the Azure Portal, let us configure immutability on your Azure Backup Vault.\\n\\n 1. Navigate to your [**Recovery Services Vault**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.RecoveryServices%2Fvaults \\"Recovery Services vaults\\")\\n 2. Navigate to **Properties** _(under Settings)_\\n 3. ![Recovery Services Vault - Immutability](/uploads/azureportal_rsv_configureimmutable.png \\"Recovery Services Vault - Immutability\\")\\n 4. Under Immutable vault, select **Settings**\\n 5. Click the **box** to enable vault immutability\\n 6. ![Enable vault immutability](/uploads/azureportal_rsv_configureimmutablecheck.png \\"Enable vault immutability\\")\\n 7. Click **Apply**\\n 8. The Recovery Services vault will be adjusted, and the status has changed to **Enabled but not locked**; this means that your vault is now immutable and won\'t allow operations that will result in the loss of backups; however, you can reverse the change by unticking vault immutability.\\n 9. ![Immutable vault - soft](/uploads/azureportal_rsv_immutableenabledsoft.png \\"Immutable vault - soft\\")\\n10. To hard lock, your vault, navigate back into the Immutable vault settings, toggle Locked, and Apply. **This cannot be undone, so make this decision thought out, as it will stop the ability to reduce retention policies that will cause the deletion of recovery points, which could lead to increased costs in the longer term.**\\n\\nThe Azure Backup vault immutability can also be adjusted using Azure Bicep, reference below.\\n\\n    param vaults_name string = \'rsv\'\\n    \\n    resource vaults_name_resource \'Microsoft.RecoveryServices/vaults@2022-09-10\' = {\\n      name: vaults_rsv_name\\n      location: \'australiaeast\'\\n      sku: {\\n        name: \'RS0\'\\n        tier: \'Standard\'\\n      }\\n      properties: {\\n        securitySettings: {\\n          immutabilitySettings: {\\n            state: \'Unlocked\'\\n          }\\n        }\\n      }\\n    }\\n\\nThe immutabilitySettings states are:\\n\\n| State | Actions |\\n| --- | --- |\\n| Disabled | Immutability is Disabled |\\n| Locked | Enabled but locked |\\n| Unlocked | Enabled but unlocked |\\n\\n_Note: I was able to delete a Recovery Vault, with locked Immutability successfully, that didn\'t have any Recovery points._"},{"id":"/2023/01/09/export-icons-to-svg-from-the-microsoft-azure-portal-using-amazing-icon-downloader","metadata":{"permalink":"/2023/01/09/export-icons-to-svg-from-the-microsoft-azure-portal-using-amazing-icon-downloader","source":"@site/blog/2023-01-09-export-icons-to-svg-from-the-microsoft-azure-portal-using-amazing-icon-downloader.md","title":"Export icons to SVG from the Microsoft Azure Portal using Amazing Icon Downloader","description":"Have you ever wanted to export an icon from the Microsoft Azure Portal but found yourself having to screenshot the icon at a low definition to include in your documentation or presentations?","date":"2023-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.11,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-01-09 00:00:00 +1300","title":"Export icons to SVG from the Microsoft Azure Portal using Amazing Icon Downloader","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/azure-icon-downloader.png"}},"unlisted":false,"prevItem":{"title":"You Can\'t Touch This: How to Make Your Azure Backup Immutable and Secure","permalink":"/2023/01/18/you-can-t-touch-this-how-to-make-your-azure-backup-immutable-and-secure"},"nextItem":{"title":"Create Azure IP Groups based on the IP address ranges of countries","permalink":"/azure/create-azure-ip-groups-based-on-the-ip-address-ranges-of-countries"}},"content":"Have you ever wanted to export an icon from the Microsoft Azure Portal but found yourself having to screenshot the icon at a low definition to include in your documentation or presentations?\\n\\nWell - using the [Amazing Icon Downloader browser](https://github.com/mattl-msft/Amazing-Icon-Downloader \\"logo Amazing Icon Downloader\\") plugin, you can export a single or all icons on a specific page in high definition as an SVG (Scalable Vector Graphics) file.\\n\\n> Easily view all icons on a page, works with:\\n>\\n> * portal.azure.com\\n> * endpoint.microsoft.com\\n> * Search to filter down long lists of icons\\n> * Rename and download any single icon\\n> * Bulk download all icons as a .zip file\\n> * Works with either Chrome or Edge\\n\\nThe plugin is available on [Chrome](https://chrome.google.com/webstore/detail/amazing-icon-downloader/kllljifcjfleikiipbkdcgllbllahaob \\"Amazing Icon Downloader\\") or [Edge](https://microsoftedge.microsoft.com/addons/detail/amazing-icon-downloader/goanjjfecbakkdmbchgoooajnbiafong \\"Amazing Icon Downloader\\") browser stores and is intuitive.\\n\\nOnce you install it, it is a matter of browsing the page you want and clicking the extension to export.\\n\\n![Azure Icon Downloader](/uploads/azure-icon-downloader.png \\"Azure Icon Downloader\\")\\n\\nIf you are after Visio stencils and other files, make sure you also check out [David Summers Azure Stencil collection](https://github.com/David-Summers/Azure-Design \\"David-Summers / Azure-Design\\").\\n\\nYou may run into an issue where a new feature or icon is released, and the stencil collection hasn\'t been updated yet - which is where the Amazing Icon Downloader can come in handy."},{"id":"azure/create-azure-ip-groups-based-on-the-ip-address-ranges-of-countries","metadata":{"permalink":"/azure/create-azure-ip-groups-based-on-the-ip-address-ranges-of-countries","source":"@site/blog/2023-01-02-create-azure-ip-groups-based-on-the-ip-address-ranges-of-countries.md","title":"Create Azure IP Groups based on the IP address ranges of countries","description":"An IP Group in Microsoft Azure is a logical container of IP address ranges for private and public addresses.","date":"2023-01-02T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.875,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2023-01-02 00:00:00 +1300","title":"Create Azure IP Groups based on the IP address ranges of countries","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/azureipgroupblog.png"},"slug":"azure/create-azure-ip-groups-based-on-the-ip-address-ranges-of-countries"},"unlisted":false,"prevItem":{"title":"Export icons to SVG from the Microsoft Azure Portal using Amazing Icon Downloader","permalink":"/2023/01/09/export-icons-to-svg-from-the-microsoft-azure-portal-using-amazing-icon-downloader"},"nextItem":{"title":"Microsoft Dev Box wrapped in a bow","permalink":"/2022/12/11/microsoft-devbox-wrapped-in-a-bow"}},"content":"An [IP Group](https://learn.microsoft.com/azure/firewall/ip-groups?WT.mc_id=AZ-MVP-5004796 \\"IP Groups in Azure Firewall\\") in Microsoft Azure is a logical container of IP address ranges for private and public addresses.\\n\\n> IP Groups allow you to group and manage IP addresses for Azure Firewall rules in the following ways:\\n>\\n> * As a source address in DNAT rules\\n> * As a source or destination address in network rules\\n> * As a source address in application rules\\n>\\n> An IP Group can have a single IP address, multiple IP addresses, one or more IP address ranges or addresses and ranges in combination.\\n\\nThe IP Group allows you to define an IP address that can be used in conjunction with Azure Firewall, to allow or deny internal or external traffic from a perspective set of IP addresses.\\n\\n> The following IPv4 address format examples are valid to use in IP Groups:\\n>\\n> * Single address: 10.0.0.0\\n> * CIDR notation: 10.1.0.0/32\\n> * Address range: 10.2.0.0-10.2.0.31\\n\\nBy default, the Azure Firewall blocks outbound and inbound traffic; however, you may want to enable _(or block)_ traffic to and from specific countries - there is no built-in geo-filtering with Azure Firewall, as you can use other services, such as the Web Application Gateway and with the [Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Application Gateway?\\") and [Azure Front Door](https://learn.microsoft.com/azure/web-application-firewall/afds/waf-front-door-geo-filtering?WT.mc_id=AZ-MVP-5004796 \\"What is geo-filtering on a domain for Azure Front Door Service?\\") to block and allow access, and other third party services such as Cloudflare. This script can be adapted for any list of IP ranges; it doesn\'t need to be country IP addresses.\\n\\nHowever, you may want to control access to and from specific countries _(or other services)_ with Azure Firewall - this is where the IP Groups can be effective, and because we won\'t be editing the Firewall directly - we won\'t run into issues with delays without having to wait for the Azure Firewall policies to be updated.\\n\\nTo solve the issue of creating the IP groups and finding and keeping the IP groups up-to-date with various countries\' IP ranges - I have created a PowerShell function to retrieve supported [countries](https://www.ipdeny.com/ipblocks/data/aggregated/ \\"IP Deny aggregated list\\")\' IP CIDR ranges and create the relevant IP groups.\\n\\n![Azure IP Group - Country IP ranges](/uploads/azureipgroupscript.png \\"Azure IP Group - Country IP ranges\\")\\n\\nWith IP Groups, there are a few things to keep in mind:\\n\\n* You can have 200 IP Groups per firewall with a maximum of **5000 individual IP addresses or prefixes per each IP Group**.\\n\\nFor a country like New Zealand, the 5000 limit for the address ranges is acceptable - but for other countries, like the United States or United Kingdom, this can be an issue, where the total IP ranges can grow to over 20k - to deal with this, the script will create multiple IP Groups, and append a number to the end.\\n\\nSuppose IPs are manually added to the groups. In that case, they won\'t be added - the script will add in any different or new IP ranges, ignoring any current IP ranges (_this means it won\'t delete any IP ranges that are removed from the source IP list from IPDeny)_; however, I recommend that anything added outside of this script is kept in a separate IP group.\\n\\n_As with any script, I recommend this is tested in a test environment first._\\n\\nBefore we run it, we need a few prerequisites.\\n\\n* At least [PowerShell 3.0+](https://learn.microsoft.com/powershell/scripting/install/installing-powershell?view=powershell-7.3&WT.mc_id=AZ-MVP-5004796 \\"Install PowerShell on Windows, Linux, and macOS\\")\\n* Azure [Az PowerShell Modules](https://learn.microsoft.com/powershell/azure/new-azureps-module-az?view=azps-9.2.0&WT.mc_id=AZ-MVP-5004796 \\"Introducing the Azure Az PowerShell module\\")_(specifically Az.Network, Az.Resources)_\\n\\nThe function assumes you have [connected to Microsoft Azure and your relevant subscription](https://luke.geek.nz/azure/powershell/Using-PowerShell-to-connect-to-Azure/ \\"Using PowerShell to connect to Microsoft Azure\\").\\n\\nBefore we import the function, I am going to check if any IP groups already exist quickly _(this isn\'t required) -_ but it\'s a good opportunity to check that you are connected to your Azure subscription and that the AzIPGroup cmdlets exist -  and whether you have any IP groups already existing.\\n\\n    Get-AzIpGroup\\n\\n![Get-AzIpGroup](/uploads/checkexistingipgroups_empty.gif \\"Get-AzIpGroup\\")\\n\\nI have received no errors or existing IP groups in my subscription, so I will continue importing my function.\\n\\nThe function can be found here:\\n\\n```powershell title=\\"New-AzCountryIPGroup.p1\\"\\nfunction New-AzCountryIPGroup {\\n    <#\\n.SYNOPSIS\\nCreates an Azure IP group, with the IP address ranges for various countrues.\\nThe code does the following:\\n1. It downloads the IP address ranges for the country specified.\\n2. It checks if the IP Group already exists, if it does, it adds the IP addresses to the existing IP Group.\\n3. If the total number of IP addresses is less than 5000, it will add the IP addresses to the existing IP Group.\\n4. If the total number of IP addresses is over 5000, it will create a new IP Group, with the same name as the existing IP Group, and it will add the IP addresses to the new IP Group.\\n5. If the new IP Group is over 5000, it will create a new IP Group, with the same name as the existing IP Group, and it will add the IP addresses to the new IP Group.\\n6. It will continue to create new IP Groups until all of the IP addresses are added.\\n\\nThe code can be used to create IP Groups for multiple countries, and if the number of IP addresses is over 5000, it will create multiple IP Groups, with the same name, but with a counter after the name, so that it will be unique.\\n.EXAMPLE\\nNew-AzCountryIPGroup\\nNew-AzCountryIPGroup -CountryCode NZ -IPGroupName IP -IPGroupRGName NetworkRG -IPGroupLocation AustraliaEast\\n.AUTHOR\\nLuke Murray - https://luke.geek.nz/\\n\\n    #>\\n    [CmdletBinding()]\\n    param\\n    (\\n        [Parameter(Mandatory = $true, Position = 0)]\\n        [System.String]\\n        $CountryCode,\\n        [Parameter(Mandatory = $true, Position = 1)]\\n        [Object]\\n        $IPGroupName,\\n        [Parameter(Mandatory = $true, Position = 2)]\\n        [System.String]\\n        $IPGroupRGName,\\n        [Parameter(Mandatory = $true, Position = 3)]\\n        [System.String]\\n        $IPGroupLocation\\n    )\\n    \\n    \\n    $IPBlocks = Invoke-WebRequest -Uri (\'https://www.ipdeny.com/ipblocks/data/aggregated/{0}-aggregated.zone\' -f $CountryCode.ToLower()) \\n    #Exports the IPBlock content from the HTML request, into a String\\n    $IPBlock = $IPBlocks.Content \\n    #Spilts each IP block, into a seperate object\\n    $ipaddressranges = $IPBlock -split \'\\\\s+\' -replace \'\\\\r?\\\\n\\\\r?\', \'\'  | Where-Object { $_ -ne \'\' }\\n\\n    $Group = Get-AzIpGroup -Name $IPGroupName -ResourceGroupName $IPGroupRGName \\n\\n    if ($ipaddressranges.Length -lt 5000) {\\n\\n        If ($null -eq $Group) {\\n            Write-Host  \\"Group doesn\'t exist, creating a new IP Group called $IPGroupName in the following Azure Resource Group $IPGroupRGName and location $IPGroupLocation\\"\\n            $Group = New-AzIpGroup -Name $IPGroupName -ResourceGroupName $IPGroupRGName -Location $IPGroupLocation -Tag @{Country = $CountryCode } -Verbose\\n        \\n            If ($null -eq $Group) {\\n                New-AzResourceGroup -Name $IPGroupRGName -Location $IPGroupLocation -Tag @{Country = $CountryCode } \\n                $Group = New-AzIpGroup -Name $IPGroupName -ResourceGroupName $IPGroupRGName -Location $IPGroupLocation -Tag @{Country = $CountryCode } -Verbose\\n\\n            }\\n        \\n            ForEach ($ip in $ipaddressranges) {\\n                $Group.IpAddresses.Add($ip) \\n                Write-Host  \\"Adding $ip to $IPGroupName.\\"\\n            }\\n        \\n            $Group | Set-AzIPGroup -Verbose\\n        \\n        }\\n\\n        else {\\n            Write-Host \\"Group already exists called:$IPGroupName in the following Azure Resource Group $IPGroupRGName and location $IPGroupLocation. Adding IPs to the group... Please note that this script doesn\'t check already existing IP addresses, if identical IP addresses exist, it will overrite it, if IP addresses outside of the Country List exist, it will remain in the IP Group - but there is no checking, if there is pre-equisting IP addresses in the IP Group that will raise the Group Limit above 5000. I recommend keeping the Country IP group seperate.\\"\\n            $Group = Get-AzIpGroup -Name $IPGroupName -ResourceGroupName $IPGroupRGName \\n        \\n        \\n            ForEach ($ip in $ipaddressranges) {\\n                $Group.IpAddresses.Add($ip) \\n                Write-Host \\"Adding $ip to $IPGroupName\\"\\n            }\\n        \\n            $Group | Set-AzIPGroup -Verbose\\n        }\\n    }\\n    \\n    else {\\n\\n        Write-Host \\"Azure IP Groups only support IPAddresses of up-to 5000 (the country you have specified is: \\"$ipaddressranges.Length\\"), also please make sure the country code matches https://www.ipdeny.com/ipblocks/data/aggregated/\\"\\n\\n        $counter = [pscustomobject] @{ Value = 0 }\\n        $groupSize = 5000\\n        $groups = $ipaddressranges | Group-Object -Property { [math]::Floor($counter.Value++ / $groupSize) }\\n        $counter = 0\\n        ForEach ($group in $groups) {\\n            $countup = $counter + 1\\n        \\n            $azipgroup = Get-AzIpGroup -Name \\"$IPGroupName$countup\\" -ResourceGroupName $IPGroupRGName -Verbose \\n        \\n        \\n            If ($null -eq $azipgroup) {\\n                $countup = $counter + 1\\n                Write-Host  \\"$IPGroupName$countup doesn\'t exist. Creating... $IPGroupName$countup in the following Resource Group $IPGroupRGName and location $IPGroupLocation.\\" \\n                $azipgroup = New-AzIpGroup -Name \\"$IPGroupName$countup\\" -ResourceGroupName $IPGroupRGName -Location $IPGroupLocation -Tag @{Country = $CountryCode } -Verbose -Force\\n                $ipgroup = $group.Group\\n                ForEach ($IP in $ipgroup) {\\n                    $azipgroup.IpAddresses.Add($IP)\\n                    Write-Host \\"Adding $ip to $IPGroupName\\" \\n                }\\n\\n                $azipgroup | Set-AzIPGroup -Verbose\\n                $counter++\\n\\n            }\\n            else {\\n                $ipgroup = $group.Group\\n                ForEach ($IP in $ipgroup) {\\n                    $azipgroup.IpAddresses.Add($IP)\\n                    Write-Host \\"Adding $ip to $IPGroupName\\" \\n                }\\n\\n                $azipgroup | Set-AzIPGroup -Verbose\\n                $counter++\\n            }\\n        }\\n\\n    }\\n}\\n```\\n\\n**Note: Make sure your country matches the supported country shortcodes found here:** [**IPBlock Aggregated**](https://www.ipdeny.com/ipblocks/data/aggregated/ \\"Index of /ipblocks/data/aggregated/\\")**. IPDeny is the source for the IP address list.**\\n\\nOnce saved to your computer, it\'s time to import it into your active PowerShell terminal and run it _(after you have verified you have connected to the correct Azure subscription)_.\\n\\nSo I will navigate to the script and import it:\\n\\n    cd D:\\\\git\\n    . .\\\\New-AzCountryIPGroup.ps1\\n    New-AzCountryIPGroup\\n\\n![Import New-AzCountryIPGroup.ps1](/uploads/import_countryipgrpfunction.gif \\"Import New-AzCountryIPGroup.ps1\\")\\n\\nThe \'New-AzCountryIPGroup\' Azure function relies on 4 parameters:\\n\\n| Parameters | Values |\\n| --- | --- |\\n| CountryCode | NZ |\\n| IPGroupName | IPGrpNZ |\\n| IPGroupRGName | NetworkRG |\\n| IPGroupLocation | AustraliaEast |\\n\\nMake sure that the values change to your environment; in my example, I am specifying an IP Group and Resource Group that doesn\'t exist so that the script will create it for me - and the location I will be deploying to will be the Australia East region.\\n\\n    New-AzCountryIPGroup -CountryCode NZ -IPGroupName IPGrpNZ -IPGroupRGName NetworkRG -IPGroupLocation AustraliaEast\\n\\n![New-AzCountryIPGroup](/uploads/run_countryipgrpfunctionnz.gif \\"New-AzCountryIPGroup\\")\\n\\nAs you can see, the script created an Azure Resource Group and imported the New Zealand IP ranges to a new IP Group...\\n\\nNot required - but if I rerun it, it will simply override any IP addresses that are the same and add any new addresses to the same IP Group that already exists, as below:\\n\\n![Rerun New-AzCountryIPGroup](/uploads/rerun_countryipgrpfunctionnz.gif \\"Rerun New-AzCountryIPGroup\\")\\n\\nThe Azure IP Group is visible in the Azure Portal as below:\\n\\n![Azure Portal - Azure IP Group ](/uploads/azureportal_ipgrpnz.png \\"Azure Portal - Azure IP Group \\")\\n\\nAnd a Tag was added to include the country:\\n\\n![Azure IP Group - Tag](/uploads/azureportal_ipgrpnztag.png \\"Azure IP Group - Tag\\")\\n\\nAs New Zealand was under the 5000 limit, only one IP Group was needed, but if we change the Country Code to the US...\\n\\n![Run New-AzCountryIPGroup - US](/uploads/run_countryipgrpfunctionus.gif \\"Run New-AzCountryIPGroup - US\\")\\n\\nIt created 5 IP groups, each containing 5000 CIDR IP ranges, with the last containing the remaining IP address ranges.\\n\\nAs you can see, it\'s reasonably easy to create IP Groups containing a list of IP ranges for multiple countries quickly:\\n\\n![Azure Portal - Azure IP Groups](/uploads/azureportal_ipgrps.png \\"Azure Portal - Azure IP Groups\\")\\n\\nNote: The script can also be found in my Public Git Repo [here](https://github.com/lukemurraynz/Azure), feel free to recommend pull requests if you have anything to add or change."},{"id":"/2022/12/11/microsoft-devbox-wrapped-in-a-bow","metadata":{"permalink":"/2022/12/11/microsoft-devbox-wrapped-in-a-bow","source":"@site/blog/2022-12-11-microsoft-devbox-wrapped-in-a-bow.md","title":"Microsoft Dev Box wrapped in a bow","description":"Festive Tech Calender - Microsoft Dev Box","date":"2022-12-11T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":16.385,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-12-11 00:00:00 +1300","title":"Microsoft Dev Box wrapped in a bow","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/festivetechcalendar2022.png"}},"unlisted":false,"prevItem":{"title":"Create Azure IP Groups based on the IP address ranges of countries","permalink":"/azure/create-azure-ip-groups-based-on-the-ip-address-ranges-of-countries"},"nextItem":{"title":"Hosting your workloads in Azure vs On-premises","permalink":"/azure/hosting-your-workloads-in-azure-vs-on-premises"}},"content":"**![Festive Tech Calender - Microsoft Dev Box](/uploads/festivetechcalendar2022.png \\"Festive Tech Calender - Microsoft Dev Box\\")**\\n\\nIt\'s that time of year again! The time to be jolly and experience the Month of December by looking at the [Festive Tech Calendar](https://festivetechcalendar.com/ \\"Festive Tech Calender\\")!\\n\\n> This year the Festive Tech Calendar Team is **raising money** for the charity [@missingpeople](https://www.missingpeople.org.uk/).\\n>\\n> We believe its important to support charities that do great work. Without fundraising Missing People wouldn\u2019t be able to find vulnerable missing people and reunite families.\\n>\\n> #### If you would like to donate please visit our [Just Giving Page](https://www.justgiving.com/fundraising/festivetechcalendar2022)\\n\\nToday, we sent our present and took a peek inside the box - at Microsoft Dev Box!\\n\\n#### Overview\\n\\n[Microsoft Dev Box](https://azure.microsoft.com/products/dev-box/?WT.mc_id=AZ-MVP-5004796#overview \\"Microsoft Dev Box\\") provides self-service access for developers to high-performance, cloud-based workstations preconfigured and ready-to-code for specific projects - all while maintaining security and corporate governance. With Microsoft Dev Box, organizations can:\\n\\n* Maximize dev productivity with ready-to-code, self-service Dev Boxes.\\n* Central management of workstations running anywhere to maintain greater security, compliance, and cost efficiency.\\n* Customize dev boxes with everything developers need for their current projects.\\n\\nMicrosoft Dev Box supports any developer IDE, SDK, or tool that runs on Windows. Developers can target any development workload built from Windows, including desktop, mobile, IoT, and web applications. Microsoft Dev Box even supports building cross-platform apps thanks to Windows Subsystem for Linux and Windows Subsystem for Android. Remote access allows developers to securely access dev boxes from any device, whether it\'s Windows, macOS, Android, iOS, or a web browser.\\n\\n![High-level Azure Devbox workflow](/uploads/hl-devbox_workflow.png \\"High-level Devbox workflow\\")\\n\\n[Microsoft Dev Box](https://learn.microsoft.com/azure/dev-box/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Dev Box documentation\\") is a managed service that enables developers to create on-demand, high-performance, secure, ready-to-code, project-specific workstations in the cloud.\\n\\nMicrosoft Dev Box is available today as a [preview](https://azure.microsoft.com/en-us/updates/public-preview-microsoft-dev-box/?WT.mc_id=AZ-MVP-5004796 \\"Public preview: Microsoft Dev Box\\") from the Azure Portal. During this period, organizations get the first 15 hours of the dev box 8vCPU and 32 GB Memory SKU for free every month, along with the first 365 hours of the dev box Storage SSD 512 GB SKU.\\n\\nBeyond that, organizations pay only for what they use with a consumption-based pricing model. With this model, organizations are charged per hour depending on the number of Compute and Storage consumed.\\n\\nTo use Microsoft Dev Box, each user must be licensed for Windows 11 or 10 Enterprise, Microsoft Endpoint Manager, and Microsoft Entra ID P1. These licenses are included in M365 F3, E3, E5, A3, A5, Microsoft Business Premium and Microsoft 365 Education benefit plans.\\n\\n![Microsoft Dev Box](/uploads/microsoft_devbox_selfservicedevelopment.png \\"Microsoft Dev Box\\")\\n\\n> _Disclaimer: At the time of writing, this service is still in Public Preview, some services and license requirements may change by the time this becomes generally avaliable._\\n\\nSo, where does Microsoft Dev Box fit in?\\n\\nMicrosoft offers a plethora of services, from [Azure Virtual Desktop](https://azure.microsoft.com/products/virtual-desktop/?WT.mc_id=AZ-MVP-5004796 \\"Azure Virtual Desktop\\"), [Windows 365](https://www.microsoft.com/en-us/windows-365?WT.mc_id=AZ-MVP-5004796 \\"Windows 365 Cloud PC\\"), and now [Microsoft Dev Box](https://azure.microsoft.com/products/dev-box/?WT.mc_id=AZ-MVP-5004796#overview \\"Microsoft Dev Box\\") - where would you use Microsoft Dev Box over another service, such as Windows 365?\\n\\nGeneral scenarios at a high level are:\\n\\n| Scenario | Product |\\n| --- | --- |\\n| Production multi-session, supporting Windows Server and Client OS, Published Aps | Azure Virtual Desktop |\\n| Production dedicated personal PCs, for shift/party time users - or small environments | Windows 365 |\\n| Dev/Test Ondemand Windows machines for Testing and development with custom image support | Azure Dev Box |\\n\\n![Microsot Windows Experiances Strategy](/uploads/ms_microsoftwindowsexperiances_strategy.png \\"Microsoft Windows Experiances Strategy\\")\\n\\nMicrosoft Dev box can help project and development teams get up and running quickly, independent of what hardware a developer or contractor has, whether they prefer Mac, Windows, or Linux - the Microsoft Dev box can be used to get developers and contractors up and running in a secure environment that supports Intune!\\n\\n##### Concepts & Roles\\n\\n| Concepts | Notes |\\n| --- | --- |\\n| Dev center | A dev center is a collection of projects that require similar settings. Dev centers enable dev infrastructure managers to manage the images and SKUs available to the projects using dev box definitions and configure the networks the development teams consume using network connections. |\\n| Projects | A project is the point of access for the development team members. When you associate a project with a dev center, all the settings at the dev center level will be applied to the project automatically. Each project can be associated with only one dev center. |\\n| Dev box definition | A dev box definition specifies a source image and size, including compute size and storage size. You can use a source image from the marketplace, or a custom image. |\\n| Network connection | Network connections store configuration information like Active Directory join type and virtual network that dev boxes use to connect to network resources. |\\n| Dev box pool | A dev box pool is a collection of dev boxes that you manage together and to which you apply similar settings. |\\n| Dev box | A dev box is a preconfigured ready-to-code workstation that you create through the self-service developer portal. The new dev box has all the tools, binaries, and configuration required for a dev box user to be productive immediately |\\n\\n![DevBoxHierarchy](/uploads/festivetechcalendar2022_devboxhierarchy.png \\"DevBoxHierarchy\\")\\n\\nThe following are typical Azure Dev Box roles.\\n\\n| Role | Responsibilities | Permissions |\\n| --- | --- | --- |\\n| Dev Infra Admins | Providing developer infrastructure and tools to the development teas | Can create and manage dev centers, can create projects and define images that are used to create the dev boxes |\\n| Project Admins | Does administrative tasks for the Dev Box solution and assist with day to day tasks. | Can create and manage dev box pools across different regions |\\n| Dev Box Users | Members of your development teams | Can self-service and create one or more dev boxes, depending on the projects assigned. |\\n\\n#### Deployment\\n\\n##### Create Dev Center\\n\\nFirst, we need to create our [Dev Center](https://learn.microsoft.com/azure/dev-box/concept-dev-box-concepts?WT.mc_id=AZ-MVP-5004796#dev-center \\"Microsoft Dev Box Preview key concepts\\"). A Dev Center allows us to centrally manage our developer environments and enable development teams with self-service capability. Dev Center is used by more than just Microsoft DevBox - an example is [Azure Deployment Environments](https://azure.microsoft.com/products/deployment-environments/?WT.mc_id=AZ-MVP-5004796#overview \\"Azure Deployment Environments\\") - which allows devs to spin up templated _(ARM)_ application infrastructure quickly - but we will focus on components of Dev Center - used by Microsoft Dev Box.\\n\\n> Please confirm what [region](https://azure.microsoft.com/products/dev-box/?WT.mc_id=AZ-MVP-5004796#faq \\"Frequently asked questions about Microsoft Dev Box\\") you can deploy Dev Box. As this is in Public Preview at the time of writing - only certain regions are supported.\\n\\nLet\'s create a standard Dev Box environment in your favourite browser, starting with the Dev Center...\\n\\n1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. Click **+ Create a resource**\\n3. Search for: Dev center, select your Dev center and click **Create**\\n4. ![Microsoft Azure Portal - Dev center](/uploads/azuredevbox-createdevcentermarketplace.png \\"Microsoft Azure Portal - Dev center\\")\\n5. Select the **Subscription** and **ResourceGroup** you want to deploy your Dev Center; you can use this opportunity to create a new Resource Group.\\n6. Type in the **name__ of your** DevCenter **_(in my example, it is named DevCenter-Devs)_\\n7. Then select the **location** _(region)_ in which you want to deploy your DevCenter.\\n8. ![Azure Portal - Create a dev center](/uploads/azuredevbox-createdevcenterbasicspage.png \\"Azure Portal - Create a dev center\\")\\n9. Click **Review + Create,** then **Create**\\n\\n_Deployment of the Microsoft Dev Center will take a few minutes._\\n\\n##### Create Virtual Network\\n\\nTo use Microsoft Dev Boxes - like any Virtual Machine in Azure, you need a Virtual Network! The Dev Boxes can connect to existing Virtual Networks, which could be peered with other VNETs, have connectivity to on-premises - or have standalone secure connectivity through network links! In my demo, I don\'t currently have a Virtual Network - so I will create a Virtual Network from scratch.\\n\\n 1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Click **+ Create a resource**\\n 3. Search for: **Virtual Network**\\n 4. **Create** a **Virtual Network**\\n 5. **Select** your Virtual Network **name** and **region** _(make sure the region aligns with your workloads and Azure DevCenter location)_\\n 6. ![Azure Portal - Create VNET](/uploads/azuredevbox-createvnetbasicspage.png \\"Azure Portal - Create VNET\\")\\n 7. Click **Next: IP Addresses**\\n 8. I will leave the IP address space the default of 10.1.0.0/16 - but change the default subnet name to devbox-subnet.\\n 9. ![Azure Portal - Create VNET](/uploads/azuredevbox-createvnetipaddressespage.png \\"Azure Portal - Create VNET\\")\\n10. Click **Review + create**\\n11. Click **Create**\\n\\n##### Create a Virtual Network Connection and link to Dev Center\\n\\nNow that we have our Dev Center and our Virtual Network - it\'s time to make a Network connection - this connection will be used by Dev Center - to allow our Dev Boxes to connect to the Virtual Network - and to select your Virtual Machine identification _(i.e. Microsoft Entra ID, or Hybrid Microsoft Entra ID)_.\\n\\n 1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Click **+ Create a resource.**\\n 3. Type in: **Network Connection,** find and click **Create**\\n 4. ![Create Network Connection ](/uploads/azuredevbox-createnetworkconnectionmarketplace.png \\"Create Network Connection \\")\\n 5. As I will be using Microsoft Entra ID joined Virtual Machines, I will ensure that the Domain join type is: **Microsoft Entra ID join**.\\n 6. For the **Network connection name**, I will select: ProductionVNETAADJConnection\\n 7. I will select my Virtual Network and subnet, which the Dev Box will be placed into.\\n 8. ![Create Azure Network Connection ](/uploads/azuredevbox-createnetworkconnectionbasicspage.png \\"Create Azure Network Connection \\")\\n 9. Click **Review + Create,** and click **Create**\\n10. Now that we have created the Network connection - it is time to link it to our Dev Center - so it can be used.\\n11. **Navigate** to your **Dev Center**\\n12. Under Dev Box configuration, select **Networking**\\n13. Click **+ Add**\\n14. **Select** your Network **connection** that has just been created\\n15. ![Azure Dev Center - Link Network Connection](/uploads/azuredevbox-createnetworkconnectionlinkdevcenter.png \\"Azure Dev Center - Link Network Connection\\")\\n16. Click **Add**\\n17. The Network Connection will check all the network requirements for the Dev Box service, such as the Azure tenant and Intune configuration _(i.e. is there a restriction in Endpoint Management for Windows)._\\n\\n##### Create Dev box definitions\\n\\nIt\'s time to create our Dev box definition. The Dev box definition is the type of Virtual Machines -or Dev Boxes that are standard for your environment. A dev box definition will be used to define the image _(whether a_ [_custom_](https://luke.geek.nz/azure/capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin/ \\"Capturing Virtual Machine images and Snapshots in Azure using WVDAdmin\\") _or marketplace image),_ SKU of virtual machines _(Compute + Memory)_, and available storage. Note that if you want to use a Custom Image - you will need an [Azure Compute gallery](https://learn.microsoft.com/azure/virtual-machines/azure-compute-gallery?WT.mc_id=AZ-MVP-5004796 \\"Store and share resources in an Azure Compute Gallery\\"), and if you decide to go down this route, make sure you check out [Azure VM Image Builder](https://learn.microsoft.com/azure/virtual-machines/image-builder-overview?tabs=azure-powershell&WT.mc_id=AZ-MVP-5004796 \\"Azure VM Image Builder overview\\") to help automate and build your images. You can have multiple definitions per project.\\n\\n 1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Navigate to your **Dev center**\\n 3. Navigate to **Dev box definitions**\\n 4. Click **+ Create**\\n 5. For our **Image definition name**, I will go with Win11-VS\\n 6. For Images, there are a plethora of images available! For this guide, I will use Visual Studio 2019 Enterprise on Windows 11 Enterprise + Microsoft 365 Apps 22H2 image.\\n 7. ![Azure Dev Box definitions](/uploads/azuredevbox-createpdevboxdefinitionsimages.png \\"Azure Dev Box definitions\\")\\n 8. I will select the Latest image version and specify **4vCPU, 16GB of RAM**, and a **256 GB** SSD drive.\\n 9. ![Azure Dev Box definitions](/uploads/azuredevbox-createpdevboxdefinitions.png \\"Azure Dev Box definitions\\")\\n10. Click **Create**\\n\\nYou can edit a Dev box definition, change the image, Compute, and storage after it has been created; this could be useful if there are issues with the latest version of the image, you can roll back the version - so people can make their Dev Boxes while the image is worked on.\\n\\n##### Create and assign Project\\n\\nNow that we have our Dev Center and Virtual Network connection - it is time to create a Project. A Project is intended to be task specific - an example being the following user story _\\"As a developer working on a mobile game, I need access to a Windows 11 Development workstation with Visual Studio installed\\"_ - so all users working on that mobile game - will get an identical virtual machine setup with all the pre-requisites that the need to start development, a project team working on another mobile game, may need different software or dependencies - so will be part of another project.\\n\\n 1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Navigate to your **Dev center**\\n 3. Navigate to **Projects**\\n 4. Select **+ Create**\\n 5. Select your **Resource Group**\\n 6. Select your **Dev center**\\n 7. Please type in the name of our **Project** and enter a **description.**\\n 8. ![Azure DevBox - Create Project](/uploads/azuredevbox-createprojectbasics.png)\\n 9. You can use Tags to add additional information on billing for the project or the project administrator\'s contact details - but we will select **Review + create** and **Create**\\n10. Once the Project has been created, we need to assign assignees to use the project. I will give a DevBox User role to the project so I can make a Dev Box.\\n11. Within the Project, click on the **Access Control (IAM)**\\n12. Click **+ Add**\\n13. Select **Add Role assignment**\\n14. Select **DevCenter Dev Box User**\\n15. Click **Next**\\n16. Make sure User, Group, or Service principal is selected and click **+ Select Members.**\\n17. Ideally, you would assign the Dev Box User role to an Azure AD group - but in my demo, I will select an individual user.\\n18. Click **Next**\\n19. ![Azure Dev Box - Assign Project Members](/uploads/azuredevbox-assignprojectmembers.png \\"Azure Dev Box - Assign Project Members\\")\\n20. Once you have confirmed your users have been assigned, click on **Review + assign** to give your users or groups to the project, allowing them to create Dev Boxes.\\n\\n_Note: I have found it can take 5-10 minutes for access to be granted to the users before they can create Dev Boxes._\\n\\n##### Create Dev Box Pool\\n\\nNow that we have our project and dev box definitions - it\'s time to create our Dev Box Pool - which is what the Dev Boxes will be made from.\\n\\n 1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Navigate to your **Dev center**\\n 3. Navigate to **Projects**\\n 4. **Navigate** to **the project** you created earlier _(i.e. for me, its MobileGameDevelopment)_\\n 5. Click on the **Dev box pools**\\n 6. Click on **+ Create**\\n 7. ![Azure Dev Box - Create Dev Box Pools](/uploads/azuredevbox-createdevboxpools.png \\"Azure Dev Box - Create Dev Box Pools\\")\\n 8. Type in a **name** - _i.e. MobleDevelopmentWin11_\\n 9. **Select** your **Network connection**\\n10. Select your **definition**\\n11. Select your Creator **privileges** _(i.e. select whether your user will be a standard user or have Local administrator rights on their devbox)_\\n12. Configure Auto-stop or skip and confirm [licensing](https://learn.microsoft.com/azure/virtual-machines/windows/windows-desktop-multitenant-hosting-deployment?WT.mc_id=AZ-MVP-5004796 \\"How to deploy Windows 11 on Azure\\").\\n13. Click **Create**\\n\\nAfter 1-2 minutes, your Dev Box pool has been created.\\n\\n#### Create & Connect\\n\\nNow that your Dev Center, Network, and Dev Box project has been stood up - it\'s time to Create and connect to your new Dev box! Microsoft Dev Box - offers a few ways to connect to the DevBox; we will go through a few options now.\\n\\n##### Create Dev Box\\n\\nNow it\'s time to create our Dev Box! To do this, we need to go to the Dev box Developer portal _(as a Dev Center Devbox, User)_\\n\\n1. Navigate to the [**Microsoft Dev Box portal**](https://devbox.microsoft.com/ \\"Microsoft Dev Box\\")\\n2. Click on **+ New Dev Box**\\n3. Enter your **name** of the DevBox _(i.e. what you will name the Virtual Machine and see in the portal - make sure this is meaningful - as you may have more than one Dev Box)_\\n4. **Select** your assigned **Dev Box Pool,** and select your **Dev Box definition**\\n5. ![Microsoft Dev Box - Create Virtual Machine](/uploads/azuredevbox-createdevbox.png \\"Microsoft Dev Box - Create Virtual Machine\\")\\n6. Click **Create**\\n7. ![DevBox - Creating ](/uploads/devboxcreating.gif \\"DevBox - Creating \\")\\n\\n_Note: Dev box creation can take 30-90 minutes. **Dev boxes will automatically start upon creation.**_\\n\\n##### Connect Dev Box using Microsoft Dev Box Portal\\n\\n1. Navigate to the [**Microsoft Dev Box portal**](https://devbox.microsoft.com/ \\"Microsoft Dev Box\\")\\n2. **Click** on the Dev **Box** you want to connect to.\\n3. Select **Open in the browser**\\n4. ![Azure DevBox - HTML Client](/uploads/azuredevbox-desktophtml.png \\"Azure DevBox - HTML Client\\")\\n5. ![Azure Dev Box /Windows 365 Connection](/uploads/azuredevbox-desktophtmlconnecting.png \\"Azure Dev Box/Windows 365 Connection\\")\\n6. If prompted, then log in with your credentials.\\n7. ![Azure Dev Box - Sign In](/uploads/azuredevbox-desktophtml_login.png \\"Azure Dev Box - Sign In\\")\\n8. You will now be connected to your new Azure Dev Box!\\n9. ![Azure Dev Box](/uploads/azuredevbox-desktophtmldevbox.jpg \\"Azure Dev Box\\")\\n\\n##### Connect Dev Box using Remote Desktop Application\\n\\nLike Azure Virtual Desktop, you can connect to your Dev Box using the Remote Desktop client.\\n\\n1. Download and install the [**Remote Desktop Client**](https://learn.microsoft.com/azure/virtual-desktop/users/connect-windows?tabs=subscribe&WT.mc_id=AZ-MVP-5004796 \\"Remote Desktop Client\\")\\n2. [**Subscribe**](https://learn.microsoft.com/en-us/azure/virtual-desktop/users/connect-windows?tabs=subscribe-with-url&WT.mc_id=AZ-MVP-5004796#subscribe-to-a-workspace \\"Subscribe to a workspace\\") to the Azure workspace\\n3. As long as the Dev Box has been created, you can see your Dev Box and connect to it directly.\\n4. ![Remote Desktop Client - Microsoft Dev Box](/uploads/azuredevbox-remotedesktopapplication.png \\"Remote Desktop Client - Microsoft Dev Box\\")\\n\\n#### Configuration\\n\\nWe can configure a few extra things for the Azure DevBox environment.\\n\\n##### Auto-Stop\\n\\nAs the Dev Box is Pay As You Go, you can ensure that a Dev Box is shut down after hours.\\n\\n1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. Navigate to your **Dev center**\\n3. Navigate to **Projects**\\n4. **Navigate** to **the project** you created earlier _(i.e. for me, its MobileGameDevelopment)_\\n5. Click on the **Dev box pools**\\n6. Click on **Edit** on your pools\\n7. Click **Enable Auto-stop**\\n8. Select **Yes**\\n9. Configure your **Stop Time** and **time zone**\\n\\nYou may also have multiple pools - selected with the exact Dev box image definition - if your project runs across various timezones, so you can schedule a 7 PM Shutdown in New Zealand and a 7 PM shutdown for those developers in the United States.\\n\\n##### Delete or Stop\\n\\nNot as much as a Configuration item, but more of a quick - howto! As a Dev Box user, you can shut down your Dev Box or delete it from the [**Microsoft Dev Box portal**](https://devbox.microsoft.com/ \\"Microsoft Dev Box\\")**.**\\n\\n![Azure Dev Box - Stop or Delete](/uploads/azuredevbox-closedelete.png \\"Azure Dev Box - Stop or Delete\\")\\n\\n#### Additional Resources\\n\\nAs Microsoft Dev Box is still in Public Preview - at the time of writing - the experience may change before its GA (Generally available). \\n\\n##### Documentation\\n\\nAdditional reading on Microsoft Dev Box can be found below:\\n\\n* [Microsoft Dev Box](https://azure.microsoft.com/products/dev-box/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Dev Box\\")\\n* [Microsoft Dev Box pricing](https://azure.microsoft.com/pricing/details/dev-box/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Dev Box PREVIEW pricing\\")\\n* [Build developer environments fast with Microsoft Dev Box - Youtube](https://www.youtube.com/watch?v=kyeuSpR74W4)\\n\\n##### Azure Bicep\\n\\nBelow are some Azure Bicep samples for Azure Dev Box.\\n\\n###### Dev Center\\n\\n    param name string\\n    param location string\\n    param tags object\\n    \\n    resource name_resource \'Microsoft.DevCenter/devcenters@2022-08-01-preview\' = {\\n      name: name\\n      location: location\\n      tags: tags\\n      identity: {\\n        type: \'none\'\\n      }\\n    }\\n\\n###### DevBox Host Pool\\n\\n    param projects_MobileGameDevelopment_name string = \'MobileGameDevelopment\'\\n    \\n    resource projects_MobileGameDevelopment_name_MobleDevelopmentWin10Secure \'Microsoft.DevCenter/projects/pools@2022-09-01-preview\' = {\\n      name: \'${projects_MobileGameDevelopment_name}/MobleDevelopmentWin10Secure\'\\n      location: \'australiaeast\'\\n      properties: {\\n        devBoxDefinitionName: \'Win10-VS\'\\n        networkConnectionName: \'ProductionVNETAADJConnection\'\\n        licenseType: \'Windows_Client\'\\n        localAdministrator: \'Disabled\'\\n      }\\n    }\\n    \\n    resource projects_MobileGameDevelopment_name_MobleDevelopmentWin10Secure_default \'Microsoft.DevCenter/projects/pools/schedules@2022-09-01-preview\' = {\\n      parent: projects_MobileGameDevelopment_name_MobleDevelopmentWin10Secure\\n      name: \'default\'\\n      properties: {\\n        type: \'StopDevBox\'\\n        frequency: \'Daily\'\\n        time: \'11:15\'\\n        timeZone: \'Pacific/Auckland\'\\n        state: \'Enabled\'\\n      }\\n    }"},{"id":"azure/hosting-your-workloads-in-azure-vs-on-premises","metadata":{"permalink":"/azure/hosting-your-workloads-in-azure-vs-on-premises","source":"@site/blog/2022-12-05-hosting-your-workloads-in-azure-vs-on-premises.md","title":"Hosting your workloads in Azure vs On-premises","description":"Azure is a cloud computing platform that provides a wide range of services and capabilities for building and deploying applications and workloads in the cloud. In contrast, hosting your own datacenter involves setting up and managing a physical infrastructure, including servers, storage, networking, and other components, in a location controlled by the organization.","date":"2022-12-05T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.085,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-12-05 00:00:00 +1300","title":"Hosting your workloads in Azure vs On-premises","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/microsoft_azure.jpg"},"slug":"azure/hosting-your-workloads-in-azure-vs-on-premises"},"unlisted":false,"prevItem":{"title":"Microsoft Dev Box wrapped in a bow","permalink":"/2022/12/11/microsoft-devbox-wrapped-in-a-bow"},"nextItem":{"title":"Disable SFTP support on an Azure Storage account on a Schedule","permalink":"/azure/disable-sftp-support-on-an-azure-storage-account-on-a-schedule"}},"content":"[Azure](https://azure.microsoft.com/en-us/?WT.mc_id=AZ-MVP-5004796 \\"Do more with less\u2014 On Azure\\") is a cloud computing platform that provides a wide range of services and capabilities for building and deploying applications and workloads in the cloud. In contrast, hosting your own datacenter involves setting up and managing a physical infrastructure, including servers, storage, networking, and other components, in a location controlled by the organization.\\n\\nAzure and hosting your own datacenter are two different approaches for deploying and managing applications and workloads.\\n\\nThere are several key differences between Azure and hosting your own datacenter, including the following:\\n\\n* Capital expenditure and operational costs: Azure is a pay-as-you-go service, with no upfront costs or long-term commitments, while hosting your own datacenter involves a significant capital expenditure, and ongoing costs for maintenance, support, and infrastructure upgrades.\\n* Scalability and elasticity: Azure provides automatic scalability and elasticity, with the ability to scale up and down on demand, and pay only for the resources that are used, while hosting your own datacenter requires manual scaling and capacity planning, and may result in underutilized or overutilized resources.\\n* Security and compliance: Azure provides built-in security and compliance features, with the ability to deploy and manage applications and workloads in a secure and compliant manner, while hosting your own datacenter requires the implementation and maintenance of security and compliance controls, and may expose the organization to security and compliance risks.\\n* Integration and interoperability: Azure integrates well with other Azure services and technologies, as well as with on-premises environments, while hosting your own datacenter may require the use of complex integration and interoperability solutions, and may result in vendor lock-in and interoperability challenges.\\n\\nOverall, Azure and hosting your own datacenter are two different approaches, with different advantages and disadvantages. Azure can provide a more cost-effective, scalable, and secure solution, but may require the adoption of a different operating model and a learning curve, while hosting your own datacenter can provide more control and flexibility, but may require a larger investment and ongoing operational costs.\\n\\nThe [total cost of ownership (TCO) of Azure](https://azure.microsoft.com/pricing/tco/calculator/?WT.mc_id=AZ-MVP-5004796 \\"Total Cost of Ownership (TCO) Calculator\\") and on-premises can vary depending on several factors, including the specific services and resources that are used, the usage patterns and workloads, the pricing options and discounts, and the cost optimization strategies and techniques that are implemented.\\n\\nIn general, the TCO of Azure can be lower than the TCO of on-premises, for the following reasons:\\n\\n* Azure provides a pay-as-you-go pricing model, with no upfront costs or long-term commitments, and the ability to scale up and down on demand, and pay only for the resources that are used. This can help to reduce the overall TCO, compared to the upfront capital expenditure and ongoing operational costs of on-premises infrastructure.\\n* Azure provides built-in support and maintenance services, as part of the subscription fees, and the ability to choose from different support and maintenance plans, depending on the specific needs and requirements of the organization. This can help to reduce the TCO, compared to the costs of hiring and maintaining a dedicated IT staff for on-premises infrastructure.\\n* Azure provides built-in security and compliance features, and the ability to deploy and manage applications and workloads in a secure and compliant manner. This can help to reduce the TCO, compared to the costs of implementing and maintaining security and compliance controls for on-premises infrastructure.\\n\\nOverall, the TCO of Azure can be lower than the TCO of on-premises, due to the pay-as-you-go pricing model, the built-in support and maintenance services, and the built-in security and compliance features. However, the actual TCO may vary depending on the specific needs and requirements of the organization, and on the implementation and cost optimization strategies that are used."},{"id":"azure/disable-sftp-support-on-an-azure-storage-account-on-a-schedule","metadata":{"permalink":"/azure/disable-sftp-support-on-an-azure-storage-account-on-a-schedule","source":"@site/blog/2022-11-16-disable-sftp-support-on-an-azure-storage-account-on-a-schedule.md","title":"Disable SFTP support on an Azure Storage account on a Schedule","description":"Azure Storage account SFTP functionality support for Azure Blob Storage\\") has now gone GA (Generally Available) across most regions as part of the GA release - SFTP support for Azure Storage accounts was free while it was in preview - but now that the service is GA - there is an additional charge for SFTP (Secure File Transfer)_ functionality.","date":"2022-11-16T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.54,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Disable SFTP support on an Azure Storage account on a Schedule","authors":["Luke"],"tags":["Azure"],"date":"2022-11-16 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/azautomation_runbook_run.png"},"slug":"azure/disable-sftp-support-on-an-azure-storage-account-on-a-schedule"},"unlisted":false,"prevItem":{"title":"Hosting your workloads in Azure vs On-premises","permalink":"/azure/hosting-your-workloads-in-azure-vs-on-premises"},"nextItem":{"title":"Deploy Azure-Firewall-mon to a Static Web App","permalink":"/azure/deploy-azure-firewall-mon-to-a-static-web-app"}},"content":"Azure Storage account [SFTP functionality](https://learn.microsoft.com/en-us/azure/storage/blobs/secure-file-transfer-protocol-support?WT.mc_id=AZ-MVP-5004796#pricing-and-billing \\"SSH File Transfer Protocol (SFTP) support for Azure Blob Storage\\") has now gone GA _(Generally Available)_ across most regions as part of the GA release - SFTP support for Azure Storage accounts was free while it was in preview - but now that the service is GA - there is an additional charge for SFTP _(Secure File Transfer)_ functionality.\\n\\n> Enabling the SFTP endpoint has a cost of $0.30 per hour. We will start applying this hourly cost on or after December 1, 2022.\\n\\nThis service has worked for me without a hitch for months, but as with most resources in Microsoft Azure - you pay for what you use! Therefore, there may be instances where you do not need SFTP support 24 hours a day, seven days a week! This is where the following Azure Automation runbook can help.\\n\\n_Feel free to check out a_ [_previous article_](https://luke.geek.nz/azure/sftp-in-microsoft-azure-using-azure-blob-storage/ \\"SFTP in Microsoft Azure using Azure Blob Storage \\") _on setting up SFTP support for an Azure storage account._\\n\\n#### Overview\\n\\nUsing an [Azure Automation](https://learn.microsoft.com/en-us/azure/automation/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Automation?\\") PowerShell runbook and Schedules _(as part of the Azure Automation account)_  - we can turn on the SFTP endpoint - when we need it and disable it - the rest of the time - which is excellent from a security and cost perspective.\\n\\n#### Prerequisites\\n\\nTo do this, we will need an:\\n\\n* Azure Automation Account\\n* System Managed Identity set with Storage Account Contributor rights\\n* PowerShell runbook _(supplied below)_\\n\\nFor this article, I will assume you already have an Azure Automation account - if you do not - then follow the Microsoft documentation: [Create a standalone Azure Automation account](https://learn.microsoft.com/en-us/azure/automation/automation-create-standalone-account?tabs=azureportal&WT.mc_id=AZ-MVP-5004796 \\"Create a standalone Azure Automation account\\").\\n\\n#### Deploy & Configure\\n\\nNow that the Azure Automation account has been configured and set up - we need to add the Runbook, but before we can do that - there are some dependencies. For example, SFTP is a new service that the currently installed Az Modules in the Azure Automation don\'t have visibility on - so to configure the SFTP service - we need to update 2 Modules to the most recent version.\\n\\nThese modules are:\\n\\n* Az.Accounts _(\u2265 2.10.3)_\\n* Az.Storage\\n\\nAz.Accounts are a dependent service of the latest Az.Storage account, so let us import that first.\\n\\n##### Update Az.Accounts module\\n\\n1. In the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\"), navigate to [**Azure Automation accounts**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Automation%2FAutomationAccounts \\"Azure Automation Accounts\\").\\n2. Find your Azure Automation account and, click on it, navigate to **Modules** _(under Shared resources)_.\\n3. Select **Browse Gallery**\\n4. Search for: **Az.Accounts**\\n5. ![Import Az.Accounts](/uploads/azautomation_gallery_azaccounts.png \\"Az.Accounts\\")\\n6. Click \'**Az.Accounts**\' and select **Select.**\\n7. Set the **runtime** version to: **5.1** & select impor**t**\\n8. Wait for 5 minutes while the module imports.\\n\\n##### Update Az.Storage module\\n\\n_Note: the Az.The accounts module will need to finish its import before the Az.The storage module is updated._\\n\\n1. In the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\"), navigate to [**Azure Automation accounts**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Automation%2FAutomationAccounts \\"Azure Automation Accounts\\").\\n2. Find your Azure Automation account and, click on it, navigate to **Modules** _(under Shared resources)_.\\n3. Select **Browse Gallery**\\n4. Search for: **Az.Storage**\\n5. ![Import  Az.Storage](/uploads/azautomation_gallery_azstorage.png \\" Az.Storage\\")\\n6. Click \'**Az.Storage**\' and select **Select.**\\n7. Set the **runtime** version to: **5.1** & select impor**t**\\n8. Wait for 5 minutes while the module imports.\\n\\n##### Create System Managed Identity\\n\\nNow that the base Modules have been updated, we need to create a System Managed Identity - this Managed Identity will allow the Azure Automation runbook to authenticate to your Azure resources - and, in our example - make changes, such as Disabling or Enabling the SFTP service. This System Managed Identity will need Storage Account Contributor rights.\\n\\n1. In the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\"), navigate to [**Azure Automation accounts**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Automation%2FAutomationAccounts \\"Azure Automation Accounts\\").\\n2. Find your Azure Automation account and click on it; click on **Identity** _(under Account Settings)_\\n3. Select Status to: **On** and select **Save**\\n4. Click on: **Azure role assignments**\\n   1. Select your **Scope** _(in our example, we will go with Storage - to limit what changes this Azure Automation account can make)_\\n   2. Select the **Subscription** and **Storage account** Resource on which you want to disable or enable the SFTP service.\\n   3. For the role, select **Storage Account Contributor**.\\n   4. Click **Save**\\n\\n_You should now see the Azure automation account, listed as having Storage account contributor rights - under your Automation account\'s Access Control (IAM) blade._\\n\\n##### Import Runbook - Set-AzStgFTP.ps1\\n\\nNow that the AzAccounts, Az.Storage modules have been updated, and the Azure Automation account has been given permission - to enable and disable the SFTP service on the storage account- it\'s time to import the Runbook that will make this happen.\\n\\n 1. In the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\"), navigate to [**Azure Automation accounts**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Automation%2FAutomationAccounts \\"Azure Automation Accounts\\").\\n 2. Find your Azure Automation account and, click on it, navigate to **Runbooks** _(under Process Automation)_.\\n 3. Click **+ Create a Runbook**\\n 4. Enter your runbook **name** _(i.e. Set-AzSFTP)_\\n 5. Select the Runbook type as **PowerShell**\\n 6. Select the Runtime version as: **5.1**\\n 7. \\\\[Optional\\\\] Add a description of what this Runbook does and who to contact.\\n 8. Click **Create**\\n 9. Open the newly created blank Runbook, and select **Edit**\\n10. **Copy** the following PowerShell [**script**](https://github.com/lukemurraynz/Azure/blob/main/Azure%20Automation/Set-AzStgSFTP.ps1 \\"Set-AzStgSFTP.ps1\\") into the Edit pane:\\n\\n         param\\n            (\\n                [Parameter(Mandatory=$true,Position = 0, HelpMessage = \'Enter the Azure Resource Group, that contains your Azure Storage account\')]\\n                [string]\\n                $resourceGroupName,\\n            \\n                [Parameter(Position = 1, Mandatory = $true, HelpMessage = \'Enter the Azure Storage account name\')]\\n                [string]\\n                $storageAccountName,\\n            \\n                [Parameter(Mandatory = $true, HelpMessage = \'$True = Enable SFTP & $False = Disable SFTP\')][ValidateSet(\'False\',\'True\')]\\n                $enableSftp\\n            )\\n          \\n              <#\\n            .SYNOPSIS\\n            Disables or enables SFTP support on an Azure Storage Account.\\n            .DESCRIPTION\\n            Disables or enables SFTP support on an Azure Storage Account. The intention is for this script to be used in Azure Automation, alongside a Schedule to enable or disable SFTP support on an Azure Storage Account.\\n        \\n            .EXAMPLE\\n            Set-AzStgSFTP -resourceGroupName sftp_prod -storageAccountName sftpprod0 -EnableSFTP $true\\n          #>\\n        \\n          \\n          # Ensures you do not inherit an AzContext in your runbook\\n        Disable-AzContextAutosave -Scope Process\\n        \\n        Import-Module -Name Az.Storage\\n        # Connect to Azure with system-assigned managed identity\\n        $AzureContext = (Connect-AzAccount -Identity).context\\n        \\n        Write-Output -InputObject $AzureContext\\n        Write-Output -InputObject $AzureContext.Subscription\\n        Write-Output -InputObject $resourceGroupName \\n        Write-Output -InputObject $storageAccountName\\n        Write-Output -InputObject $EnableSFTP\\n        # set and store context\\n        $AzureContext = Set-AzContext -SubscriptionName $AzureContext.Subscription -DefaultProfile $AzureContext\\n        \\n          \\n          $SetSFTP = [System.Convert]::ToBoolean($enableSftp)\\n        \\n            $SFTPStatusBefore = Get-AzStorageAccount -DefaultProfile $AzureContext -ResourceGroupName $resourceGroupName -Name $storageAccountName | Select-Object -ExpandProperty EnableSftp\\n        \\n            $Status = $SFTPStatusBefore -replace \'True\', \'Enabled\' -replace \'False\', \'Disabled\'\\n        \\n            Write-Output -InputObject (\'SFTP for {0} currently has SFTP set to: {1} before update.\' -f $storageAccountName, $Status)\\n          \\n            Set-AzStorageAccount -DefaultProfile $AzureContext -ResourceGroupName $resourceGroupName -Name $storageAccountName -EnableSftp $SetSFTP \\n           \\n        \\n            $SFTPStatusAfter = Get-AzStorageAccount -DefaultProfile $AzureContext -ResourceGroupName $resourceGroupName -Name $storageAccountName | Select-Object -ExpandProperty EnableSftp\\n        \\n            $Status = $SFTPStatusAfter -replace \'True\', \'Enabled\' -replace \'False\', \'Disabled\'\\n        \\n            Write-Output -InputObject (\'SFTP for {0} currently has SFTP set to: {1} after update.\' -f $storageAccountName, $Status)\\n11. Click **Save**\\n12. Click **Publish**\\n\\n##### Run Runbook - Set-AzStgFTP\\n\\nNow that the Runbook is imported, we need to run it.\\n\\nThe Runbook uses the following parameters:\\n\\n| Parameters | Values |\\n| --- | --- |\\n| resourceGroupName | Enter the name of the Azure Resource Group, that contains your Azure Storage account. |\\n| storageAccountName | Enter the name of your Azure Storage account. |\\n| enableSftp | The following boolean values are accepted: False (Disable SFTP) and True (Enable SFTP). |\\n\\n1. Next, find your Runbook, and select **Start.**\\n2. Enter your parameters, **Resource Group**, **Storage Account** and **Enable SFTP.**\\n3. ![Start Azure Automation runbook](/uploads/azautomation_runbook_runparameters.png \\"Start Azure Automation runbook\\")\\n4. Click **Ok**\\n5. The Runbook will run, and as you can see - outputs its state Before the Runbook ran and after.\\n6. ![Azure Automation - Run](/uploads/azautomation_runbook_run.png \\"Azure Automation - Run\\")\\n\\n**Once working correctly, you can set up an** [**Azure Automation schedule**](https://learn.microsoft.com/en-us/azure/automation/shared-resources/schedules?WT.mc_id=AZ-MVP-5004796 \\"Azure Automation schedule\\") **to trigger the runbook to enable and disable the SFTP when needed only!**"},{"id":"azure/deploy-azure-firewall-mon-to-a-static-web-app","metadata":{"permalink":"/azure/deploy-azure-firewall-mon-to-a-static-web-app","source":"@site/blog/2022-11-14-deploy-azure-firewall-mon-to-a-static-web-app.md","title":"Deploy Azure-Firewall-mon to a Static Web App","description":"Azure-Firewall-mon is a near real-time Azure Firewall log viewer.","date":"2022-11-14T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":10.015,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-11-14 00:00:00 +1300","title":"Deploy Azure-Firewall-mon to a Static Web App","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azfirewallmon.png"},"slug":"azure/deploy-azure-firewall-mon-to-a-static-web-app"},"unlisted":false,"prevItem":{"title":"Disable SFTP support on an Azure Storage account on a Schedule","permalink":"/azure/disable-sftp-support-on-an-azure-storage-account-on-a-schedule"},"nextItem":{"title":"Official Microsoft Community Calls","permalink":"/2022/11/06/official-microsoft-community-calls"}},"content":"[Azure-Firewall-mon](https://github.com/nicolgit/azure-firewall-mon \\"Azure-Firewall-mon\\") is a near real-time [Azure Firewall](https://learn.microsoft.com/en-us/azure/firewall/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Firewall?\\") log viewer.\\n\\n> Azure-Firewall-mon provides an _alternative_and_opinable_ \ud83d\ude0a way to access and inspect Azure Firewall logs. The recommended approach for analysing Azure Firewall logs is to set up a Log Analytics Workspace to collect all the data and use Kusto _(KQL)_ queries to check what\'s happening.\\n>\\n> In Azure-Firewall-mon, the idea is to provide an approach much more like Sysinternals Process Monitor or Check Point\'s SmartView, where there is no queries or dashboards that you need to implement first to get working. Still, all events are available as a log stream. In addition, a full-text search at the top of the page lets you quickly filter the content displayed on the screen, helping you understand what is happening right now (or close to present).\\n\\n### Overview\\n\\nAzure-Firewall-mon _(AFM or Azure Firewall Monitor)_ is a custom solution _(currently in a functional beta)_ created by an Italian Microsoft Cloud Solution Architect called: [Nicola Delfino](https://nicolgit.github.io/ \\"Nicola Delfino\\"), its worth mentioning that although a Microsoft CSA makes AFM, **IT IS NOT A SUPPORTED MICROSOFT PRODUCT**.\\n\\nMonitoring Azure Firewall can be a pain - with trawling through logs - using the [Azure Firewall Workbook](https://learn.microsoft.com/en-us/azure/firewall/firewall-workbook?WT.mc_id=AZ-MVP-5004796 \\"Monitor logs using Azure Firewall Workbook\\") - helps fill in the gap - especially around the application and network rule traffic. Still, you may want something more straightforward and designed for real-time traffic to assist with in-the-moment troubleshooting.\\n\\nAzure-Firewall-mon is an open-source, [single Page Application](https://en.wikipedia.org/wiki/Single-page_application) written in [Angular](https://angular.io/) and hosted on an [Azure WebApp](https://az-firewall-mon.azurewebsites.net \\"az-firewall-mon\\") - so to use this, you don\'t need to deploy to your environment.. add in an Event Hub connection string, and away you go _(there is also a demo mode - so you can see what the experience will be like)_!\\n\\n![az-firewall-mon landing page](/uploads/az-firewall-mon.png \\"az-firewall-mon\\")\\n![az-firewall-mon landing page](/uploads/azfirewallmon.png \\"az-firewall-mon\\")\\n\\n**We can deploy it to an Azure** [**Static Web App**](https://azure.microsoft.com/en-us/products/app-service/static/?WT.mc_id=AZ-MVP-5004796 \\" Static Web Apps\\") **for those who would instead host it in our environment.**\\n\\n> \\"Azure Static Web Apps is a service that automatically builds and deploys full-stack web apps to Azure from a code repository. When you create an Azure Static Web Apps resource, Azure interacts directly with GitHub or Azure DevOps to monitor a branch of your choice. Every time you push commits or accept pull requests into the watched branch, a build is automatically run and your app and API is deployed to Azure.\\"\\n\\n![Azure Static WebApps - Overview](/uploads/azure-static-web-apps-overview.png \\"Azure Static WebApps - Overview\\")\\n\\n### Deployment\\n\\n#### Prerequisites\\n\\nToday, we are going to deploy Azure-Firewall-mon into an Azure Static Web App - to do this; we will need the following prerequisites:\\n\\n* A [GitHub](https://github.com/ \\"GitHub\\") account\\n* An [Azure](https://azure.microsoft.com/en-us/?WT.mc_id=AZ-MVP-5004796 \\"Do more with less. On Azure.\\") subscription_(with permissions to deploy Event Hub, deploy an Azure Static WebApp, and configure Diagnostics on the Azure Firewall)_\\n* Azure Firewall _(provisioned)_\\n\\n_Note: Also, ensure that your Event Hub is in the same region as your Azure Firewall so that you can use Diagnostics settings. Regarding the Azure Static WebApp - it doesn\'t matter; this is a global service  - and you will be entering the Event Hub listener._\\n\\nWe will use the Azure Portal and a browser to provision the workflow _(however, I will have added Azure Bicep to the bottom of the article for reference)_.\\n\\nThis article - assumes you have basic knowledge of GitHub and Microsoft Azure.\\n\\nFor this demo, I am using a [Hub & Spoke Azure](https://learn.microsoft.com/azure/architecture/reference-architectures/hybrid-networking/hub-spoke?tabs=cli&WT.mc_id=AZ-MVP-5004796 \\"Hub-spoke network topology in Azure\\") topology.\\n\\n![Azure Firewall Monitor - High Level Architecture](/uploads/AzMonArchitecture.png \\"Azure Firewall Monitor - High Level Architecture\\")\\n\\n#### Fork the GitHub repository\\n\\nThe first thing we need to do is clone the Azure-Firewall-mon repository; this repository holds the source control of Azure-Firewall-mon. First, however, we need a clone of it - to use in our Static Web App - this will also allow us to pull down and build the latest changes and updates of the Azure-Firewall-mon tool while having the stability of maintaining your version of the device.\\n\\n1. In your favourite browser of choice, navigate to: [https://github.com/nicolgit/azure-firewall-mon](https://github.com/nicolgit/azure-firewall-mon \\"https://github.com/nicolgit/azure-firewall-mon\\").\\n2. Click **Fork** _(top right of the repository)_\\n3. ![GitHub - Create a new fork](/uploads/github_azfirewall_mon_createfork.png \\"GitHub - Create a new fork\\")\\n4. Click **Create fork**\\n5. You have now created a fork of the \'azure-firewall-mon\' repository; when a new update of Azure-Firewall-mon comes out - you can also select \'Sync fork\' - to keep your fork up-to-date and trigger a new build.\\n\\n#### Create Static Web App and deploy azure-firewall-mon\\n\\nNow that you have created a fork, it\'s time to make your Azure Static WebApp!\\n\\n 1. Navigate to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Click **+ Create a resource**\\n 3. Type in: **Static Web App**\\n 4. Select and click **Create**\\n 5. Create or **select** a **Resource Group**\\n 6. **Type** in the **name** of your Static Web App\\n 7. For the plan type, we will go with **Free**\\n 8. Please select your **region** _(this is the staging environment used to create your resource so the Azure Static Web App can then be replicated geographically)_.\\n 9. Select **Source**, as **GitHub**\\n10. Click **Sign in** with **GitHub** _(and sign in with an account with access to the GitHub Repository fork of azure-firewall-mon created earlier)_.\\n11. **Authorise** the **Azure Static Web Apps** to have access to your repositories\\n12. **Select** your **organisation** and the azure-firewall-mon **repository** you forked earlier.\\n13. Select \'**main**\' for the branch\\n14. Under Build Presents, select **Angular**\\n15. For App location, enter \\"**/firewall-mon-app/**\\"\\n16. Leave the API location empty\\n17. For the Output location, enter: **\\"dist/firewall-mon-app\\"**\\n18. ![Azure Static WebApps - Angular Build](/uploads/github_azfirewall_mon_createbuildangular.png \\"Azure Static WebApps - Angular Build\\")\\n19. If you navigate to your own forked GitHub repository, you should see a new folder created under .github/workflows - and a new GitHub Actions workflow file!\\n20. ![Create Azure Static WebApp - Angular - Azure Portal](/uploads/create_azstaticwebapp_portal_azfw-mon.gif \\"Create Azure Static WebApp - Angular - Azure Portal\\")\\n21. If you select **Actions** in GitHub, you should see a deployment start.\\n22. After roughly 5 minutes, your **Azure Static App - will have deployed azure-firewall-mon**!\\n23. **Navigate** to your newly created [**Azure Static App**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2FStaticSites \\"Static Web Apps\\") in the Azure Portal\\n24. Click **Browse**\\n25. You should now see **azure-firewall-mon**!\\n26. ![azure-firewall-mon](/uploads/azure_azfirewall_mon_deployed.png \\"azure-firewall-mon\\")\\n\\n_Note: In GitHub, under Actions and the Build and Deploy Job, you may see a message about Note.js 12 actions being deprecated; you can set the node version to be higher._\\n\\nAdd the step to set the node version below submodules and above the Build and Deploy step:\\n\\n          - uses: actions/setup-node@v1\\n            with:\\n              node-version: \\"18.x\\"\\n\\nRefer to a copy of my Github Actions file here: [AzureStaticWebAppsCICD.yml](https://gist.github.com/lukemurraynz/3f300a5eb73b2693d3a9378261a023db \\"Azure Static Web Apps CI/CD\\") for a comparison of GitHub action - a setup-node step running on the latest version of 18.\\n\\n#### Create Event Hub namespace and shared access policy\\n\\nEven if you use the externally hosted version of [Azure Firewall Monitor](https://az-firewall-mon.azurewebsites.net/), you still need an Event Hub and Namespace to stream the events from our Azure Firewall to the Azure Firewall Monitor.\\n\\n 1. Navigate to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Click **+ Create a resource**\\n 3. Type in: **Event Hubs**\\n 4. Select and click **Create**\\n 5. Select your subscription and **Resource Group**;\\n 6. Type in the **Namespace** of the event hub _(i.e. AzureFirewallMonitor)_\\n 7. Select your **location** _(make sure this is the same region as your Azure Firewall)_\\n 8. Select your **Pricing Tier** _(in this example, I am going with Basic)_\\n 9. Click **Review + create**\\n10. Once the Namespace has been created, it\'s time to make our Event Hub; navigate to your newly created AzFirewallMonitor namespace.\\n11. Under **Entitles**, click **+ Event Hub**\\n12. Under the name, enter the Event Hub name _(i.e. AzMonitorCapture)_\\n13. Leave the defaults _(and Message retention to 1 day)_\\n14. Click **Review + Create**\\n15. Click **Create**\\n16. ![Create Azure Event Hub](/uploads/azure_azfirewall_mon_createeventhub.png \\"Create Azure Event Hub\\")\\n17. Now that the Event Hub is created, we need to create a Shared access policy; in the Event Hub namespace, click on **Shared access policies.**\\n18. Click **+ Add**\\n19. Type in a **Policy name**_(i.e. AzMonitorListener)_\\n20. Select **Send**\\n21. Click **Create**\\n22. ![Azure Event Hub - Create shared access policy](/uploads/azure_azfirewall_mon_sharedaccesspolicy.png \\"Azure Event Hub - Create shared access policy\\")\\n\\n    #### Configure Azure Firewall to stream to Event Hub and run Azure Firewall monitor\\n\\n    Now that we have an Event Hub configured and a Shared access policy set to Listen - it\'s time to configure the Azure Firewall to direct logs to the Namespace.\\n23. Navigate to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n24. Navigate to your [**Azure Firewall**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Network%2FazureFirewalls \\"Firewalls\\")\\n25. Select **Diagnostic Settings**\\n26. Click **+ Add diagnostic setting**\\n27. ![Azure Firewall - Diagnostic Settings](/uploads/azure_azfirewall_create_diagsettings.png \\"Azure Firewall - Diagnostic Settings\\")\\n28. Type in a Diagnostic setting name _(i.e. AzureFirewallMonitor)_\\n29. Select **All Logs**\\n30. Select **Stream to an event hub**\\n31. Select your subscription, event hub namespace, event hub and policy created earlier.\\n32. ![Azure Firewall - Diagnostic setting](/uploads/azure_azfirewall_configure_diagsettings.png \\"Azure Firewall - Configure Diagnostic setting\\")\\n33. Click **Save**\\n34. Please navigate back to your **Event Hub** namespace and select your **Event Hub entity**; now, we need to create a Shared access policy to **Listen** _(for the entity, not the Namespace)_\\n35. Click Shared access policies, and create a new Shared access policy with **Listen.**\\n36. Copy the **Connection string-primary key**\\n37. **Navigate** to your newly created [**Azure Static App**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2FStaticSites \\"Static Web Apps\\") in the Azure Portal\\n38. Click **Browse**\\n39. You should now see **azure-firewall-mon**, and enter in the **Connection string-primary key copied** earlier!\\n40. **Congratulations you have now set up Azure Firewall Monitor on an Azure Static Web App and can troubleshoot your Azure Firewall quickly in real-time!**\\n41. ![Run Azure Firewall Monitor](/uploads/run_azstaticwebapp_portal_azfw-mon.gif \\"Run Azure Firewall Monitor\\")\\n\\n#### **References: GitHub Action**\\n\\n    name: Azure Static Web Apps CI/CD\\n    \\n    on:\\n      push:\\n        branches:\\n          - main\\n      pull_request:\\n        types: [opened, synchronize, reopened, closed]\\n        branches:\\n          - main\\n    \\n    jobs:\\n      build_and_deploy_job:\\n        if: github.event_name == \'push\' || (github.event_name == \'pull_request\' && github.event.action != \'closed\')\\n        runs-on: ubuntu-latest\\n        name: Build and Deploy Job\\n        steps:\\n          - uses: actions/checkout@v2\\n            with:\\n              submodules: true\\n    \\n          - uses: actions/setup-node@v1\\n            with:\\n              node-version: \\"18.x\\"\\n          - name: Build And Deploy\\n            id: builddeploy\\n            uses: Azure/static-web-apps-deploy@v1\\n            with:\\n              azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_POLITE_CLIFF_06D4C2810 }}\\n              repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\\n              action: \\"upload\\"\\n              ###### Repository/Build Configurations - These values can be configured to match your app requirements. ######\\n              # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig\\n              app_location: \\"/firewall-mon-app/\\" # App source code path\\n              api_location: \\"\\" # Api source code path - optional\\n              output_location: \\"dist/firewall-mon-app\\" # Built app content directory - optional\\n    \\n              ###### End of Repository/Build Configurations ######\\n    \\n      close_pull_request_job:\\n        if: github.event_name == \'pull_request\' && github.event.action == \'closed\'\\n        runs-on: ubuntu-latest\\n        name: Close Pull Request Job\\n        steps:\\n          - name: Close Pull Request\\n            id: closepullrequest\\n            uses: Azure/static-web-apps-deploy@v1\\n            with:\\n              azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_POLITE_CLIFF_06D4C2810 }}\\n              action: \\"close\\"\\n\\n#### **References: Azure Bicep**\\n\\nBelow are some Azure Bicep references:\\n\\n##### Azure Static Web App\\n\\n    param staticSites_AzFw_Mon_name string = \'AzFw-Mon\'\\n    \\n    resource staticSites_AzFw_Mon_name_resource \'Microsoft.Web/staticSites@2022-03-01\' = {\\n      name: staticSites_AzFw_Mon_name\\n      location: \'Central US\'\\n      sku: {\\n        name: \'Free\'\\n        tier: \'Free\'\\n      }\\n      properties: {\\n        repositoryUrl: \'https://github.com/lukemurraynz/azure-firewall-mon\'\\n        branch: \'main\'\\n        stagingEnvironmentPolicy: \'Enabled\'\\n        allowConfigFileUpdates: true\\n        provider: \'GitHub\'\\n        enterpriseGradeCdnStatus: \'Disabled\'\\n      }\\n    }\\n\\n##### Event Hub\\n\\n    param namespaces_AzFirewallMonitor_name string = \'AzFirewallMonitor\'\\n    param location string = resourceGroup().location\\n    \\n    \\n    resource namespaces_AzFirewallMonitor_name_resource \'Microsoft.EventHub/namespaces@2022-01-01-preview\' = {\\n      name: namespaces_AzFirewallMonitor_name\\n      location: \'Australia East\'\\n      sku: {\\n        name: \'Basic\'\\n        tier: \'Basic\'\\n        capacity: 1\\n      }\\n      properties: {\\n        minimumTlsVersion: \'1.2\'\\n        publicNetworkAccess: \'Enabled\'\\n        disableLocalAuth: false\\n        zoneRedundant: true\\n        isAutoInflateEnabled: false\\n        maximumThroughputUnits: 0\\n        kafkaEnabled: false\\n      }\\n    }\\n    \\n    resource namespaces_AzFirewallMonitor_name_AzMonitorListner \'Microsoft.EventHub/namespaces/authorizationrules@2022-01-01-preview\' = {\\n      parent: namespaces_AzFirewallMonitor_name_resource\\n      name: \'AzMonitorListner\'\\n      location: location\\n      properties: {\\n        rights: [\\n          \'Listen\'\\n          \'Send\'\\n        ]\\n      }\\n    }\\n    \\n    \\n    resource namespaces_AzFirewallMonitor_name_azmonitorcapture \'Microsoft.EventHub/namespaces/eventhubs@2022-01-01-preview\' = {\\n      parent: namespaces_AzFirewallMonitor_name_resource\\n      name: \'azmonitorcapture\'\\n      location: location\\n      properties: {\\n        messageRetentionInDays: 1\\n        partitionCount: 2\\n        status: \'Active\'\\n      }\\n    }\\n    \\n    resource namespaces_AzFirewallMonitor_name_default \'Microsoft.EventHub/namespaces/networkRuleSets@2022-01-01-preview\' = {\\n      parent: namespaces_AzFirewallMonitor_name_resource\\n      name: \'default\'\\n      location: location\\n      properties: {\\n        publicNetworkAccess: \'Enabled\'\\n        defaultAction: \'Allow\'\\n        virtualNetworkRules: []\\n        ipRules: []\\n      }\\n    }\\n    \\n    resource namespaces_AzFirewallMonitor_name_azmonitorcapture_AzMonitor \'Microsoft.EventHub/namespaces/eventhubs/authorizationrules@2022-01-01-preview\' = {\\n      parent: namespaces_AzFirewallMonitor_name_azmonitorcapture\\n      name: \'AzMonitor\'\\n      location: location\\n      properties: {\\n        rights: [\\n          \'Listen\'\\n        ]\\n      }\\n      dependsOn: [\\n    \\n        namespaces_AzFirewallMonitor_name_resource\\n      ]\\n    }\\n    \\n    resource namespaces_AzFirewallMonitor_name_azmonitorcapture_Default \'Microsoft.EventHub/namespaces/eventhubs/consumergroups@2022-01-01-preview\' = {\\n      parent: namespaces_AzFirewallMonitor_name_azmonitorcapture\\n      name: \'$Default\'\\n      location: location\\n      properties: {\\n      }\\n      dependsOn: [\\n    \\n        namespaces_AzFirewallMonitor_name_resource\\n      ]\\n    }"},{"id":"/2022/11/06/official-microsoft-community-calls","metadata":{"permalink":"/2022/11/06/official-microsoft-community-calls","source":"@site/blog/2022-11-06-official-microsoft-community-calls.md","title":"Official Microsoft Community Calls","description":"You don\'t have a be a Microsoft MVP to engage with Microsoft product teams, and help give feedback! One of the best ways that the Microsoft product teams engage with the community- is through Public Community Calls!","date":"2022-11-06T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.275,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-11-06 00:00:00 +1300","title":"Official Microsoft Community Calls","authors":["Luke"],"tags":["Misc"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Deploy Azure-Firewall-mon to a Static Web App","permalink":"/azure/deploy-azure-firewall-mon-to-a-static-web-app"},"nextItem":{"title":"Microsoft Ignite 2022 - Azure Infrastructure Microsoft Documentation Updates","permalink":"/azure/microsoft-ignite-2022-azure-infrastructure-microsoft-documentation-updates"}},"content":"You don\'t have a be a [Microsoft MVP](https://mvp.microsoft.com/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft MVP\\") to engage with Microsoft product teams, and help give feedback! One of the best ways that the Microsoft product teams engage with the community- is through Public Community Calls!\\n\\nHere is a list of the community calls across [Microsoft Azure](https://azure.microsoft.com/en-us/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Azure\\") products.\\n\\n| Topic                            | Link                                                                                                                                                                                                                                                                                    | Notes                                                                                                                                                                                                                              |\\n| -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Azure Landing Zone               | [https://aka.ms/ALZ/CommunityCallAgenda](https://aka.ms/ALZ/CommunityCallAgenda)                                                                                                                                                                                                        | Azure Landing Zones news roundup and updates                                                                                                                                                                                       |\\n| Microsoft 365 Platform Community | [https://pnp.github.io/#community](https://pnp.github.io/#community)                                                                                                                                                                                                 | Not specifically Azure related, but related more to the M365/Modern workspace.                                                                                                                                                     |\\n| Windows Customer Connection      | [https://techcommunity.microsoft.com/t5/windows-it-pro-blog/join-the-windows-customer-connection-program/ba-p/3473775](https://techcommunity.microsoft.com/t5/windows-it-pro-blog/join-the-windows-customer-connection-program/ba-p/3473775?WT.mc_id=AZ-MVP-5004796) | Not specifically Azure related, but related more to the Windows OS (Operating System)                                                                                                                                              |\\n| Azure Development Community Call      | [https://github.com/Azure/azure-dev/discussions/categories/announcements](https://github.com/Azure/azure-dev/discussions/categories/announcements) | Azure Developers Community Call              |\\n| Azure Governance & Deployments   | [https://github.com/Azure/azure-policy#general-questions](https://github.com/Azure/azure-policy#general-questions)                                                                                                                                                   | Same as the ARM/Bicep community call. Also features Azure Policy.                                                                                                                                                                  |\\n| Cloud Security                   | [https://techcommunity.microsoft.com/t5/security-compliance-and-identity/join-our-security-community/ba-p/927888](https://techcommunity.microsoft.com/t5/security-compliance-and-identity/join-our-security-community/ba-p/927888?WT.mc_id=AZ-MVP-5004796)           | This is a \'Private\' community. Meaning that feedback for Cloud security products (Defender, Sentinel etc) is under NDA (Non-Disclosure Agreement). A great community to get early feedback and testing, to help the products grow. |\\n| Azure ARM/Bicep Community Calls  | [https://github.com/Azure/bicep/issues?q=label%3A%22Community+Call%22+](https://github.com/Azure/bicep/issues?q=label%3A%22Community+Call%22+)                                                                                                                       |                                                                                                                                                                                                                                    |\\n| Azure Arc                        | [https://github.com/microsoft/azure\\\\_arc\\\\_community](https://github.com/microsoft/azure_arc_community)                                                                                                                                                             |                                                                                                                                                                                                                                    |\\n| PowerShell Community Call        | [https://github.com/PowerShell/PowerShell-RFC/tree/master/CommunityCall](https://github.com/PowerShell/PowerShell-RFC/tree/master/CommunityCall)                                                                                                                                                                                                                   |                                                                                                                                       |\\n\\nThere may be other communities I have missed - so feel free to add links in the comments.\\n\\nUp-to-date links to additional public Microsoft community calls can also be found on the [AWESOME-Azure-Architecture list](https://aka.ms/AwesomeAzureArchitecture \\"AWESOME-Azure-Architecture\\")."},{"id":"azure/microsoft-ignite-2022-azure-infrastructure-microsoft-documentation-updates","metadata":{"permalink":"/azure/microsoft-ignite-2022-azure-infrastructure-microsoft-documentation-updates","source":"@site/blog/2022-10-14-microsoft-ignite-2022-azure-infrastructure-microsoft-documentation-updates.md","title":"Microsoft Ignite 2022 - Azure Infrastructure Microsoft Documentation Updates","description":"Microsoft Ignite \\"Microsoft Ignite\\") is an annual conference held by Microsoft for IT Professionals and Cloud builders - each year is a flurry of new announcements and updates! Keeping track of the changes can be a full-time job during the week (and beyond!)_!","date":"2022-10-14T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":21.695,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-10-14 00:00:00 +1300","title":"Microsoft Ignite 2022 - Azure Infrastructure Microsoft Documentation Updates","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/ms_ignite_web_1920x1080-logo-1.webp"},"slug":"azure/microsoft-ignite-2022-azure-infrastructure-microsoft-documentation-updates"},"unlisted":false,"prevItem":{"title":"Official Microsoft Community Calls","permalink":"/2022/11/06/official-microsoft-community-calls"},"nextItem":{"title":"Application cost analysis in Microsoft Azure with cm-resource-parent tag","permalink":"/azure/application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag"}},"content":"[Microsoft Ignite](https://ignite.microsoft.com/en-US/home?WT.mc_id=AZ-MVP-5004796] \\"Microsoft Ignite\\") is an annual conference held by Microsoft for IT Professionals and Cloud builders - each year is a flurry of new announcements and updates! Keeping track of the changes can be a full-time job during the week _(and beyond!)_!\\n\\nMake sure you check out the [Microsoft Ignite 2022 Book of News](https://news.microsoft.com/ignite-2022-book-of-news/?WT.mc_id=AZ-MVP-5004796] \\"Microsoft Ignite 2022 - Book of News\\")! For a consolidated list of features and releases!\\n\\nAlso, check out [AzureFeeds](https://azurefeeds.com/ \\"AzureFeeds\\") - for a consolidated feed across all Microsoft changes!\\n\\nAlong with the product pages, current and new Microsoft documentation have been updated to align with these new products, features and changes!\\n\\nThe pages that have been updated focussed on:\\n\\n* Azure Infrastructure\\n* M365\\n* Security\\n\\nIt can be found here:\\n\\n| Portfolio | Product/Service | Release Type | Content Type | URL | Title | Context |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/app-proxy/what-is-application-proxy](https://learn.microsoft.com/azure/active-directory/app-proxy/what-is-application-proxy?WT.mc_id=AZ-MVP-5004796)  |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/authentication/how-to-mfa-registration-campaign](https://learn.microsoft.com/azure/active-directory/authentication/how-to-mfa-registration-campaign?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/azuread-dev/videos](https://learn.microsoft.com/azure/active-directory/azuread-dev/videos?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/cloud-sync/concept-how-it-works](https://learn.microsoft.com/azure/active-directory/cloud-sync/concept-how-it-works?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/develop/active-directory-claims-mapping](https://learn.microsoft.com/azure/active-directory/develop/active-directory-claims-mapping?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/develop/custom-rbac-for-developers](https://learn.microsoft.com/azure/active-directory/develop/custom-rbac-for-developers?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/fundamentals/secure-with-azure-ad-single-tenant](https://learn.microsoft.com/azure/active-directory/fundamentals/secure-with-azure-ad-single-tenant?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/fundamentals/service-accounts-managed-identities](https://learn.microsoft.com/azure/active-directory/fundamentals/service-accounts-managed-identities?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/tutorial-linux-vm-access-cosmos-db](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/tutorial-linux-vm-access-cosmos-db?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/tutorial-vm-managed-identities-cosmos](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/tutorial-vm-managed-identities-cosmos?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Entra ID | Updated | Docs | [https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-cosmos-db](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-cosmos-db?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DDOS Protection | New | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-powershell-ip](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-powershell-ip?WT.mc_id=AZ-MVP-5004796) | DDoS IP Protection PowerShell quickstart article |  |\\n| Infrastructure | Azure DDOS Protection | New | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-cli-ip](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-cli-ip?WT.mc_id=AZ-MVP-5004796) | DDoS Protection: DDoS IP Protection quickstart CLI article |  |\\n| Infrastructure | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/types-of-attacks](https://learn.microsoft.com/azure/ddos-protection/types-of-attacks?WT.mc_id=AZ-MVP-5004796) | DDoS Protection: Types of DDoS attacks overview |  |\\n| Infrastructure | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/telemetry](https://learn.microsoft.com/azure/ddos-protection/telemetry?WT.mc_id=AZ-MVP-5004796) | DDoS Protection: Tutorial: View and configure DDoS protection telemetry |  |\\n| Infrastructure | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/alerts](https://learn.microsoft.com/azure/ddos-protection/alerts?WT.mc_id=AZ-MVP-5004796) | DDoS Protection: View and configure DDoS protection alerts |  |\\n| Infrastructure | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/fundamental-best-practices](https://learn.microsoft.com/azure/ddos-protection/fundamental-best-practices?WT.mc_id=AZ-MVP-5004796) | DDoS Protection: Fundamental best practices |  |\\n| Infrastructure | Azure DDOS Protection | New | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-protection-sku-comparison](https://learn.microsoft.com/azure/ddos-protection/ddos-protection-sku-comparison?WT.mc_id=AZ-MVP-5004796) | DDoS Protection: SKU comparison | Creating new document to identify differences between the SKUs. |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/alerts](https://learn.microsoft.com/azure/ddos-protection/alerts?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-disaster-recovery-guidance](https://learn.microsoft.com/azure/ddos-protection/ddos-disaster-recovery-guidance?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-faq](https://learn.microsoft.com/azure/ddos-protection/ddos-faq?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-protection-overview](https://learn.microsoft.com/azure/ddos-protection/ddos-protection-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-protection-partner-onboarding](https://learn.microsoft.com/azure/ddos-protection/ddos-protection-partner-onboarding?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-protection-reference-architectures](https://learn.microsoft.com/azure/ddos-protection/ddos-protection-reference-architectures?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | New | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-protection-sku-comparison](https://learn.microsoft.com/azure/ddos-protection/ddos-protection-sku-comparison?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-rapid-response](https://learn.microsoft.com/azure/ddos-protection/ddos-rapid-response?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/ddos-response-strategy](https://learn.microsoft.com/azure/ddos-protection/ddos-response-strategy?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/diagnostic-logging](https://learn.microsoft.com/azure/ddos-protection/diagnostic-logging?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/fundamental-best-practices](https://learn.microsoft.com/azure/ddos-protection/fundamental-best-practices?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/index](https://learn.microsoft.com/azure/ddos-protection/index?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/inline-protection-glb](https://learn.microsoft.com/azure/ddos-protection/inline-protection-glb?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-bicep](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-bicep?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-cli](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-cli?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | New | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-powershell-ip](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-powershell-ip?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-powershell](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-powershell?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-template](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection-template?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection](https://learn.microsoft.com/azure/ddos-protection/manage-ddos-protection?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/manage-permissions](https://learn.microsoft.com/azure/ddos-protection/manage-permissions?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/policy-reference](https://learn.microsoft.com/azure/ddos-protection/policy-reference?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/telemetry](https://learn.microsoft.com/azure/ddos-protection/telemetry?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/test-through-simulations](https://learn.microsoft.com/azure/ddos-protection/test-through-simulations?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure DDOS Protection | Updated | Docs | [https://learn.microsoft.com/azure/ddos-protection/types-of-attacks](https://learn.microsoft.com/azure/ddos-protection/types-of-attacks?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://review.docs.microsoft.com/azure/dns/dns-private-resolver-overview](https://review.docs.microsoft.com/azure/dns/dns-private-resolver-overview?WT.mc_id=AZ-MVP-5004796) | Azure DNS Private Resolver updates for GA | Removing public preview mentions and warnings, updating screenshots, etc. |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.comazure/dns/private-dns-privatednszone#restrictions](https://learn.microsoft.comazure/dns/private-dns-privatednszone#restrictions?WT.mc_id=AZ-MVP-5004796) | Azure DNS: Update private DNS zone restrictions | Table of zone names that are not allowed. These will be blocked the end of August \'22. |\\n| Infrastructure | Azure DNS | New | Docs | [https://learn.microsoft.com/azure/dns/private-resolver-reliability](https://learn.microsoft.com/azure/dns/private-resolver-reliability?WT.mc_id=AZ-MVP-5004796) | Resiliency in Azure DNS Private Resolver | This article describes reliability support in Azure DNS Private Resolver, and covers both regional resiliency with\xa0availability zones\xa0and cross-region resiliency with disaster recovery. |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.com/azure/dns/dns-private-resolver-get-started-portal](https://learn.microsoft.com/azure/dns/dns-private-resolver-get-started-portal?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.com/azure/dns/dns-private-resolver-get-started-powershell](https://learn.microsoft.com/azure/dns/dns-private-resolver-get-started-powershell?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.com/azure/dns/dns-private-resolver-overview](https://learn.microsoft.com/azure/dns/dns-private-resolver-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.com/azure/dns/private-resolver-endpoints-rulesets](https://learn.microsoft.com/azure/dns/private-resolver-endpoints-rulesets?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.com/azure/dns/private-resolver-hybrid-dns](https://learn.microsoft.com/azure/dns/private-resolver-hybrid-dns?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure DNS | Updated | Docs | [https://learn.microsoft.com/azure/dns/tutorial-dns-private-resolver-failover](https://learn.microsoft.com/azure/dns/tutorial-dns-private-resolver-failover?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure ExpressRoute | Updated | Docs | [https://learn.microsoft.com/azure/expressroute/expressroute-locations-providers](https://learn.microsoft.com/azure/expressroute/expressroute-locations-providers?WT.mc_id=AZ-MVP-5004796) | Azure ExpressRoute: Hybrid ExpressRoute (ExpressRoute Metro) | Documenting availability of locations that will soon have ExpressRoute Metro support. |\\n| Infrastructure | Azure Kubernetes Service | New | Docs | [https://learn.microsoft.com/azure/aks/vertical-pod-autoscaler](https://learn.microsoft.com/azure/aks/vertical-pod-autoscaler?WT.mc_id=AZ-MVP-5004796) | Vertical Pod Scaler add-on for Azure Kubernetes Service | New documentation supporting this addon\'s support by our service |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/cluster-configuration](https://learn.microsoft.com/azure/aks/cluster-configuration?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/index](https://learn.microsoft.com/azure/aks/index?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/intro-kubernetes](https://learn.microsoft.com/azure/aks/intro-kubernetes?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/monitor-aks-reference](https://learn.microsoft.com/azure/aks/monitor-aks-reference?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/monitor-aks](https://learn.microsoft.com/azure/aks/monitor-aks?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/openfaas](https://learn.microsoft.com/azure/aks/openfaas?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/operator-best-practices-identity](https://learn.microsoft.com/azure/aks/operator-best-practices-identity?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/quickstart-dapr](https://learn.microsoft.com/azure/aks/quickstart-dapr?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/use-cvm](https://learn.microsoft.com/azure/aks/use-cvm?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | New | Docs | [https://learn.microsoft.com/azure/aks/use-mariner](https://learn.microsoft.com/azure/aks/use-mariner?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Kubernetes Service | Updated | Docs | [https://learn.microsoft.com/azure/aks/use-multiple-node-pools](https://learn.microsoft.com/azure/aks/use-multiple-node-pools?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/app/java-standalone-profiler](https://learn.microsoft.com/azure/azure-monitor/app/java-standalone-profiler?WT.mc_id=AZ-MVP-5004796) | Java Illuminate - Profiling via Java Flight Recorder (JFR) | The Application Insights Java profiler uses the JFR profiler provided by the JVM to record profiling data. So that users may download the JFR recordings at a later time and analyze them to identify the cause of performance issues. |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-create-new-alert-rule](https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-create-new-alert-rule?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-types](https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-types?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-understand-migration](https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-understand-migration?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/alerts/prometheus-alerts](https://learn.microsoft.com/azure/azure-monitor/alerts/prometheus-alerts?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/app/java-in-process-agent](https://learn.microsoft.com/azure/azure-monitor/app/java-in-process-agent?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/autoscale/autoscale-understanding-settings](https://learn.microsoft.com/azure/azure-monitor/autoscale/autoscale-understanding-settings?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/best-practices-cost](https://learn.microsoft.com/azure/azure-monitor/best-practices-cost?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/change/change-analysis](https://learn.microsoft.com/azure/azure-monitor/change/change-analysis?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-cost](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-cost?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-custom-metrics](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-custom-metrics?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-enable-arc-enabled-clusters](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-enable-arc-enabled-clusters?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-gpu-monitoring](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-gpu-monitoring?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-log-query](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-log-query?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-metric-alerts](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-metric-alerts?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-onboard](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-onboard?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-overview](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-prometheus-metrics-addon](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-prometheus-metrics-addon?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-prometheus-monitoring-addon](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-prometheus-monitoring-addon?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-prometheus](https://learn.microsoft.com/azure/azure-monitor/containers/container-insights-prometheus?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/data-platform](https://learn.microsoft.com/azure/azure-monitor/data-platform?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/activity-log](https://learn.microsoft.com/azure/azure-monitor/essentials/activity-log?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/azure-monitor-workspace-overview](https://learn.microsoft.com/azure/azure-monitor/essentials/azure-monitor-workspace-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/data-collection-rule-edit](https://learn.microsoft.com/azure/azure-monitor/essentials/data-collection-rule-edit?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/data-platform-metrics](https://learn.microsoft.com/azure/azure-monitor/essentials/data-platform-metrics?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/metrics-supported](https://learn.microsoft.com/azure/azure-monitor/essentials/metrics-supported?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-multiple-workspaces](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-multiple-workspaces?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-overview](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-configuration-minimal](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-configuration-minimal?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-configuration](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-configuration?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-default](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-default?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-scale](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-scale?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-validate](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-scrape-validate?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-troubleshoot](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-metrics-troubleshoot?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | New | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-rule-groups](https://learn.microsoft.com/azure/azure-monitor/essentials/prometheus-rule-groups?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/essentials/resource-logs](https://learn.microsoft.com/azure/azure-monitor/essentials/resource-logs?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/faq](https://learn.microsoft.com/azure/azure-monitor/faq?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/index](https://learn.microsoft.com/azure/azure-monitor/index?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/logs/create-pipeline-datacollector-api](https://learn.microsoft.com/azure/azure-monitor/logs/create-pipeline-datacollector-api?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/monitor-reference](https://learn.microsoft.com/azure/azure-monitor/monitor-reference?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/observability-data](https://learn.microsoft.com/azure/azure-monitor/observability-data?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/overview](https://learn.microsoft.com/azure/azure-monitor/overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/resource-manager-samples](https://learn.microsoft.com/azure/azure-monitor/resource-manager-samples?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/service-limits](https://learn.microsoft.com/azure/azure-monitor/service-limits?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Monitor | Updated | Docs | [https://learn.microsoft.com/azure/azure-monitor/whats-new](https://learn.microsoft.com/azure/azure-monitor/whats-new?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Purview | Updated | Docs | [https://learn.microsoft.com/azure/purview/create-sensitivity-label](https://learn.microsoft.com/azure/purview/create-sensitivity-label?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Purview | Updated | Docs | [https://learn.microsoft.com/azure/purview/how-to-automatically-label-your-content](https://learn.microsoft.com/azure/purview/how-to-automatically-label-your-content?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Purview | Updated | Docs | [https://learn.microsoft.com/azure/purview/microsoft-purview-connector-overview](https://learn.microsoft.com/azure/purview/microsoft-purview-connector-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Purview | Updated | Docs | [https://learn.microsoft.com/azure/purview/register-scan-azure-cosmos-database](https://learn.microsoft.com/azure/purview/register-scan-azure-cosmos-database?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Security | Updated | Docs | [https://learn.microsoft.com/azure/security/develop/threat-modeling-tool-authorization](https://learn.microsoft.com/azure/security/develop/threat-modeling-tool-authorization?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Security | Updated | Docs | [https://learn.microsoft.com/azure/security/develop/threat-modeling-tool-input-validation](https://learn.microsoft.com/azure/security/develop/threat-modeling-tool-input-validation?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Security | Updated | Docs | [https://learn.microsoft.com/azure/security/develop/threat-modeling-tool-sensitive-data](https://learn.microsoft.com/azure/security/develop/threat-modeling-tool-sensitive-data?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Security | Updated | Docs | [https://learn.microsoft.com/azure/security/fundamentals/encryption-overview](https://learn.microsoft.com/azure/security/fundamentals/encryption-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Security | Updated | Docs | [https://learn.microsoft.com/azure/security/fundamentals/feature-availability](https://learn.microsoft.com/azure/security/fundamentals/feature-availability?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Security | Updated | Docs | [https://learn.microsoft.com/azure/security/fundamentals/ransomware-prepare](https://learn.microsoft.com/azure/security/fundamentals/ransomware-prepare?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Sentinel | Updated | Docs | [https://learn.microsoft.com/azure/sentinel/create-codeless-connector](https://learn.microsoft.com/azure/sentinel/create-codeless-connector?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Azure Sentinel | Updated | Docs | [https://learn.microsoft.com/azure/sentinel/customer-managed-keys](https://learn.microsoft.com/azure/sentinel/customer-managed-keys?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Azure Storage | Updated | Training | [https://learn.microsoft.com/training/modules/choose-the-right-disk-storage-for-vm-workload](https://learn.microsoft.com/training/modules/choose-the-right-disk-storage-for-vm-workload?WT.mc_id=AZ-MVP-5004796) | Refresh: Choose the right disk storage for your virtual machine workload | Need to include updates for the Premium SSD v2 to the list of disk options.<br>One of those offerings (premium v2) has will GA in the coming few months (date TBD)<br>Perhaps add a question to the Knowledge Check for the new disk type. |\\n| Infrastructure | Azure Virtual Network Manager | New | Docs | [https://learn.microsoft.com/azure/virtual-network-manager/concept-cross-tenant](https://learn.microsoft.com/azure/virtual-network-manager/concept-cross-tenant?WT.mc_id=AZ-MVP-5004796) | Cross-Tenant Support Concept |  |\\n| Infrastructure | Azure Virtual Network Manager | New | Docs | [https://learn.microsoft.com/azure/virtual-network-manager/how-to-configure-cross-tenant-portal](https://learn.microsoft.com/azure/virtual-network-manager/how-to-configure-cross-tenant-portal?WT.mc_id=AZ-MVP-5004796) | Cross Tenant HowTo - Portal |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/apply-security-baseline](https://learn.microsoft.com/azure/defender-for-cloud/apply-security-baseline?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/attack-path-reference](https://learn.microsoft.com/azure/defender-for-cloud/attack-path-reference?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/auto-deploy-azure-monitoring-agent](https://learn.microsoft.com/azure/defender-for-cloud/auto-deploy-azure-monitoring-agent?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/auto-deploy-vulnerability-assessment](https://learn.microsoft.com/azure/defender-for-cloud/auto-deploy-vulnerability-assessment?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/azure-devops-extension](https://learn.microsoft.com/azure/defender-for-cloud/azure-devops-extension?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/concept-agentless-data-collection](https://learn.microsoft.com/azure/defender-for-cloud/concept-agentless-data-collection?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/concept-attack-path](https://learn.microsoft.com/azure/defender-for-cloud/concept-attack-path?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/concept-cloud-security-posture-management](https://learn.microsoft.com/azure/defender-for-cloud/concept-cloud-security-posture-management?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/concept-defender-for-cosmos](https://learn.microsoft.com/azure/defender-for-cloud/concept-defender-for-cosmos?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/concept-easm](https://learn.microsoft.com/azure/defender-for-cloud/concept-easm?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/concept-regulatory-compliance](https://learn.microsoft.com/azure/defender-for-cloud/concept-regulatory-compliance?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/custom-dashboards-azure-workbooks](https://learn.microsoft.com/azure/defender-for-cloud/custom-dashboards-azure-workbooks?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/custom-security-policies](https://learn.microsoft.com/azure/defender-for-cloud/custom-security-policies?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-cloud-introduction](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-cloud-introduction?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-container-registries-introduction](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-container-registries-introduction?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-architecture](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-architecture?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-enable](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-enable?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-introduction?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-va-ecr](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-containers-va-ecr?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-databases-enable-cosmos-protections](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-databases-enable-cosmos-protections?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-devops-introduction](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-devops-introduction?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-servers-introduction](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-servers-introduction?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/defender-for-sql-usage](https://learn.microsoft.com/azure/defender-for-cloud/defender-for-sql-usage?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/deploy-vulnerability-assessment-byol-vm](https://learn.microsoft.com/azure/defender-for-cloud/deploy-vulnerability-assessment-byol-vm?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/deploy-vulnerability-assessment-tvm](https://learn.microsoft.com/azure/defender-for-cloud/deploy-vulnerability-assessment-tvm?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/deploy-vulnerability-assessment-vm](https://learn.microsoft.com/azure/defender-for-cloud/deploy-vulnerability-assessment-vm?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/detect-credential-leaks](https://learn.microsoft.com/azure/defender-for-cloud/detect-credential-leaks?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/enable-enhanced-security](https://learn.microsoft.com/azure/defender-for-cloud/enable-enhanced-security?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/enable-vulnerability-assessment-agentless](https://learn.microsoft.com/azure/defender-for-cloud/enable-vulnerability-assessment-agentless?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/enhanced-security-features-overview](https://learn.microsoft.com/azure/defender-for-cloud/enhanced-security-features-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/exempt-resource](https://learn.microsoft.com/azure/defender-for-cloud/exempt-resource?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/faq-data-collection-agents](https://learn.microsoft.com/azure/defender-for-cloud/faq-data-collection-agents?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/faq-general](https://learn.microsoft.com/azure/defender-for-cloud/faq-general?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/faq-vms](https://learn.microsoft.com/azure/defender-for-cloud/faq-vms?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/github-action](https://learn.microsoft.com/azure/defender-for-cloud/github-action?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/how-to-manage-attack-path](https://learn.microsoft.com/azure/defender-for-cloud/how-to-manage-attack-path?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/how-to-manage-cloud-security-explorer](https://learn.microsoft.com/azure/defender-for-cloud/how-to-manage-cloud-security-explorer?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/iac-vulnerabilities](https://learn.microsoft.com/azure/defender-for-cloud/iac-vulnerabilities?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/index](https://learn.microsoft.com/azure/defender-for-cloud/index?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/information-protection](https://learn.microsoft.com/azure/defender-for-cloud/information-protection?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/kubernetes-workload-protections](https://learn.microsoft.com/azure/defender-for-cloud/kubernetes-workload-protections?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/monitoring-components](https://learn.microsoft.com/azure/defender-for-cloud/monitoring-components?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/os-coverage](https://learn.microsoft.com/azure/defender-for-cloud/os-coverage?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/overview-page](https://learn.microsoft.com/azure/defender-for-cloud/overview-page?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/partner-integration](https://learn.microsoft.com/azure/defender-for-cloud/partner-integration?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/permissions](https://learn.microsoft.com/azure/defender-for-cloud/permissions?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-automate-connector-deployment](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-automate-connector-deployment?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-define-adoption-strategy](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-define-adoption-strategy?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-access-control-requirements](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-access-control-requirements?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-business-needs](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-business-needs?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-compliance-requirements](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-compliance-requirements?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-data-residency-requirements](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-data-residency-requirements?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-multicloud-dependencies](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-multicloud-dependencies?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-ownership-requirements](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-determine-ownership-requirements?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-get-started](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-get-started?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-other-resources](https://learn.microsoft.com/azure/defender-for-cloud/plan-multicloud-security-other-resources?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/policy-reference](https://learn.microsoft.com/azure/defender-for-cloud/policy-reference?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/powershell-onboarding](https://learn.microsoft.com/azure/defender-for-cloud/powershell-onboarding?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-aws](https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-aws?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-devops](https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-devops?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-gcp](https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-gcp?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-github](https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-github?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/recommendations-reference](https://learn.microsoft.com/azure/defender-for-cloud/recommendations-reference?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/regulatory-compliance-dashboard](https://learn.microsoft.com/azure/defender-for-cloud/regulatory-compliance-dashboard?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/release-notes-archive](https://learn.microsoft.com/azure/defender-for-cloud/release-notes-archive?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/release-notes](https://learn.microsoft.com/azure/defender-for-cloud/release-notes?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/security-center-planning-and-operations-guide](https://learn.microsoft.com/azure/defender-for-cloud/security-center-planning-and-operations-guide?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/security-policy-concept](https://learn.microsoft.com/azure/defender-for-cloud/security-policy-concept?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/supported-machines-endpoint-solutions-clouds-containers](https://learn.microsoft.com/azure/defender-for-cloud/supported-machines-endpoint-solutions-clouds-containers?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/supported-machines-endpoint-solutions-clouds-servers](https://learn.microsoft.com/azure/defender-for-cloud/supported-machines-endpoint-solutions-clouds-servers?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/troubleshooting-guide](https://learn.microsoft.com/azure/defender-for-cloud/troubleshooting-guide?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/tutorial-enable-pull-request-annotations](https://learn.microsoft.com/azure/defender-for-cloud/tutorial-enable-pull-request-annotations?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/tutorial-security-incident](https://learn.microsoft.com/azure/defender-for-cloud/tutorial-security-incident?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/tutorial-security-policy](https://learn.microsoft.com/azure/defender-for-cloud/tutorial-security-policy?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/update-regulatory-compliance-packages](https://learn.microsoft.com/azure/defender-for-cloud/update-regulatory-compliance-packages?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | Updated | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/workflow-automation](https://learn.microsoft.com/azure/defender-for-cloud/workflow-automation?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Security, Compliance & Identity Management | Microsoft Defender for Cloud | New | Docs | [https://learn.microsoft.com/azure/defender-for-cloud/working-with-log-analytics-agent](https://learn.microsoft.com/azure/defender-for-cloud/working-with-log-analytics-agent?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| M365 | Microsoft Teams | New | Digital Brochure | [https://aka.ms/TeamsIgnite2022Guide](https://aka.ms/TeamsIgnite2022Guide?WT.mc_id=AZ-MVP-5004796) | Your Guide to Microsoft Teams @Microsoft Ignite 2022 | Learn about resources, main messaging,\xa0and access our digital brochure with a list of all Microsoft Teams sessions |\\n| M365 | Microsoft Teams | Updated | Blog | [https://aka.ms/teamsignite2022MTCblog](https://aka.ms/teamsignite2022MTCblog?WT.mc_id=AZ-MVP-5004796) | What\'s New In Teams Blog:\xa0 Microsoft Ignite Edition<br> | Discover the innovations coming to Microsoft Teams at Microsoft Ignite |\\n| M365 | Microsoft Teams | Updated | Docs | [https://learn.microsoft.com/en-us/microsoftteams/operator-connect-mobile-plan](https://learn.microsoft.com/en-us/microsoftteams/operator-connect-mobile-plan?WT.mc_id=AZ-MVP-5004796) | Plan for Teams Phone Mobile | Updating existing text to show Operator Connect Mobile which will rebrand to Teams Phone Mobile after 10/12 |\\n| M365 | Microsoft Teams | Updated | Docs | [https://learn.microsoft.com/en-us/microsoftteams/operator-connect-mobile-configure](https://learn.microsoft.com/en-us/microsoftteams/operator-connect-mobile-configure?WT.mc_id=AZ-MVP-5004796) | Configure Teams Phone Mobile | Updating existing text to show Operator Connect Mobile which will rebrand to Teams Phone Mobile after 10/12 |\\n| M365 | Microsoft Teams | New | Blog | [https://aka.ms/TeamsPremiumBlog](https://aka.ms/TeamsPremiumBlog?WT.mc_id=AZ-MVP-5004796) | Teams Premium Announcement | New licensing for Teams announcment happening at Ignite 2022. Details in the blog and how to get updates, including public preview in December 2022 and GA in February 2023 |\\n| Infrastructure | Network Watcher | New | Docs | [https://learn.microsoft.com/azure/network-watcher/azure-monitor-agent-with-connection-monitor](https://learn.microsoft.com/azure/network-watcher/azure-monitor-agent-with-connection-monitor?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | New | Docs | [https://learn.microsoft.com/azure/network-watcher/connection-monitor-connected-machine-agent](https://learn.microsoft.com/azure/network-watcher/connection-monitor-connected-machine-agent?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/connection-monitor-create-using-portal](https://learn.microsoft.com/azure/network-watcher/connection-monitor-create-using-portal?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | New | Docs | [https://learn.microsoft.com/azure/network-watcher/connection-monitor-install-azure-monitor-agent](https://learn.microsoft.com/azure/network-watcher/connection-monitor-install-azure-monitor-agent?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/frequently-asked-questions](https://learn.microsoft.com/azure/network-watcher/frequently-asked-questions?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/index](https://learn.microsoft.com/azure/network-watcher/index?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/network-insights-overview](https://learn.microsoft.com/azure/network-watcher/network-insights-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | New | Docs | [https://learn.microsoft.com/azure/network-watcher/network-insights-topology](https://learn.microsoft.com/azure/network-watcher/network-insights-topology?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | New | Docs | [https://learn.microsoft.com/azure/network-watcher/network-insights-troubleshooting](https://learn.microsoft.com/azure/network-watcher/network-insights-troubleshooting?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/network-watcher-connectivity-overview](https://learn.microsoft.com/azure/network-watcher/network-watcher-connectivity-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/network-watcher-connectivity-portal](https://learn.microsoft.com/azure/network-watcher/network-watcher-connectivity-portal?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/network-watcher-monitoring-overview](https://learn.microsoft.com/azure/network-watcher/network-watcher-monitoring-overview?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Network Watcher | Updated | Docs | [https://learn.microsoft.com/azure/network-watcher/view-network-topology](https://learn.microsoft.com/azure/network-watcher/view-network-topology?WT.mc_id=AZ-MVP-5004796) |  |  |\\n| Infrastructure | Virtual Machines | Updated | Docs | [https://learn.microsoft.com/azure/virtual-machines/disks-metrics](https://learn.microsoft.com/azure/virtual-machines/disks-metrics?WT.mc_id=AZ-MVP-5004796) | Disks metrics - new VM burst metrics |  |"},{"id":"azure/application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag","metadata":{"permalink":"/azure/application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag","source":"@site/blog/2022-10-05-application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag.md","title":"Application cost analysis in Microsoft Azure with cm-resource-parent tag","description":"Cost Analysis in Microsoft Azure allows you to analyse the cost of your services; these services can be scoped into Resource Groups, Resources and Services; you can also group your services by Tags.","date":"2022-10-05T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.95,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-10-05 00:00:00 +1300","title":"Application cost analysis in Microsoft Azure with cm-resource-parent tag","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/azureportal_costanalysis_aznamingtool.png"},"slug":"azure/application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag"},"unlisted":false,"prevItem":{"title":"Microsoft Ignite 2022 - Azure Infrastructure Microsoft Documentation Updates","permalink":"/azure/microsoft-ignite-2022-azure-infrastructure-microsoft-documentation-updates"},"nextItem":{"title":"How to download or print a Microsoft exam certificate","permalink":"/misc/how-to-download-a-microsoft-exam-certificate"}},"content":"Cost Analysis in Microsoft Azure allows you to analyse the cost of your services; these services can be scoped into Resource Groups, Resources and Services; you can also group your services by [Tags](https://learn.microsoft.com/azure/azure-resource-manager/management/tag-resources?tabs=json&WT.mc_id=AZ-MVP-5004796 \\"Use tags to organize your Azure resources and management hierarchy\\").\\n\\nAzure tags are name-value pairs used to organize resources. You can apply tags for individual resources, display show back or ownership and can be used for automation - but what assigning parent/child relationships to your resources?\\n\\n[Tags](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/decision-guides/resource-tagging/?toc=%2Fazure%2Fazure-resource-manager%2Fmanagement%2Ftoc.json&WT.mc_id=AZ-MVP-5004796 \\"Resource naming and tagging decision guide\\") work well for most used cases, but there may be times when you want to get a more in-depth view of the service and dependencies - this is where the \\"_cm-resource-parent_\\" tag comes in.\\n\\nIntroduced in [Cost Analysis preview](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/enable-preview-features-cost-management-labs?WT.mc_id=AZ-MVP-5004796#group-related-resources-in-the-cost-analysis-preview \\"Group related resources in the cost analysis preview\\"), Q3 of 2022, the \'cm-resource-parent tag\' allows you to Group related resources together - to help give you a quick view of the solution\'s total cost in a parent/child relationship. The \'cm\' in the tag stands for: Cost Management.\\n\\n![cm-resource-parent Child Relationship](/uploads/parentchild.png \\"cm-resource-parent Child Relationship\\")\\n\\nTo use the cm-resource-parent tag, you must choose a parent resource _(an example may be an App Service or an Azure Virtual Desktop host pool)._ No changes will be made to this resource, but you need the ResourceID of the resource to apply to Child resources.\\n\\nTo find the ResourceID of the parent resource, you can use the [Azure Portal](https://portal.azure.com/#home \\"Azure Portal\\"), by\\n\\n1. **Open** the **resource** that you want to be the parent of.\\n2. Select **Properties** in the resource menu.\\n3. Find the **Resource ID** property and copy its value.\\n\\nYou can easily use PowerShell to find the ResourceID as well:\\n\\n    $ResourceName = \'Parent Resource\'\\n    Get-AzResource -Name $ResourceName | Select-Object ResourceId\\n\\nA resource ID looks like this _(you will need to copy the full thing, this will be used on your child\'s resources)_:\\n\\n    /subscriptions/4501c644-74a3-4bfc-a456-16425eccd2a4/resourceGroups/vm-preprod-rg/providers/Microsoft.Network/publicIPAddresses/VM-T01-ip\\n\\nOnce you have the Resource ID of your resource, it is time to tag your Child\'s resources.\\n\\nAs an Azure Tag is a Key/Value pair - the tags will be similar to:\\n\\n| Name | Value |\\n| --- | --- |\\n| cm-resource-parent | /subscriptions/4501c644-74a3-4bfc-a456-16425eccd2a4/resourceGroups/vm-preprod-rg/providers/Microsoft.Network/publicIPAddresses/VM-T01-ip |\\n\\nTo apply the ResourceID of the parent resource, you can use the [Azure Portal](https://portal.azure.com/#home \\"Azure Portal\\"), by\\n\\n1. **Open** the **resource** you want to be the child of the parent you selected above.\\n2. Click **Tags**\\n3. Add in: **cm-resource-parent** in the value and the resource ID of your parent as a value.\\n\\n_Note: You cannot have a multi-hierarchy, i.e. a Parent, then Child and Child off that - it is purely a Parent and Child relationship at this stage. Also, the Resource ID will change if the Parent resources are moved to another Resource group or subscription._\\n\\nThese Tags can work easily with other cost management tags you may be using _(but you cannot have more than one cm-resource-parent tag)_, so it doesn\'t replace but supplements your visibility. They are supported on any resource that is Tag-capable.\\n\\nYou may have to wait up to a day _(24 hours)_ before the changes are visible in Azure Cost Analysis.\\n\\nI have the [Azure Naming Tool](https://luke.geek.nz/azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container/ \\"Azure Naming Tool\\") deployed in my example and would like to see the overall cost; these resources are deployed across multiple resources in the same subscription.\\n\\nChoosing my WebApp as my parent resource, I tagged all child resources _(App Service Plan, Container Registry, Storage Account)_ with the relevant tag and resource ID as seen below:\\n\\n![Azure Portal - Child resource](/uploads/azureportal_cmtags_child.png \\"Azure Portal - Child resource\\")\\n\\nAfter 24 hours, I then went to the [**Cost Analysis (Preview)**](https://portal.azure.com/#view/Microsoft_Azure_CostManagement/Menu/\\\\~/costanalysisv3 \\"Cost analysis (preview)\\") and selected **Resources** and could view the current cost of my service after a few days of use.\\n\\n![Azure Cost Analysis](/uploads/azureportal_costanalysis_aznamingtool.png \\"Azure Cost Analysis\\")\\n\\nAnd another example can be seen below - where I have added a Public IP as a child resource of the Azure storage account, which, although the parent in this relationship, is a child in the Azure Naming Tool.\\n\\n![Azure Cost Analysis](/uploads/azureportal_costanalysis_example2.png \\"Azure Cost Analysis\\")\\n\\nAs you can see, the \'cm-resource-parent\' is another way to group related resources of different types from a cost analysis angle; still, in preview, this tag opens up the door for various other initiatives across observability, security stacks etc. \\n\\nAlthough not tested, you also should be able to output the resource ID of your Bicep code and add that as a variable for any resources deployed via Infrastructure as Code.\\n\\nIt\'s worth noting that this feature is still in Preview at the time of this article, so if you incur any bugs or have feature requests, you can use the \'Rate the Cost Analysis Preview\' feature in the Azure Portal to supply feedback to the product teams."},{"id":"misc/how-to-download-a-microsoft-exam-certificate","metadata":{"permalink":"/misc/how-to-download-a-microsoft-exam-certificate","source":"@site/blog/2022-09-07-how-to-download-a-microsoft-exam-certificate.md","title":"How to download or print a Microsoft exam certificate","description":"This was asked in the Microsoft forums; with the change of platform from being able to see and download certificates for the competition of your Microsoft exam with Pearsonvue to the new Microsoft Learn experience, you might find yourself lost when attempting to view (and print) your hard-earned Microsoft certificate!","date":"2022-09-07T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.785,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to download or print a Microsoft exam certificate","authors":["Luke"],"tags":["Misc"],"toc":false,"date":"2022-09-07 00:00:00 +1300","slug":"misc/how-to-download-a-microsoft-exam-certificate"},"unlisted":false,"prevItem":{"title":"Application cost analysis in Microsoft Azure with cm-resource-parent tag","permalink":"/azure/application-cost-analysis-in-microsoft-azure-with-cm-resource-parent-tag"},"nextItem":{"title":"InternalServerError when deploying Azure Firewall","permalink":"/azure/internalservererror-when-deploying-azure-firewall"}},"content":"This was asked in the Microsoft forums; with the change of platform from being able to see and download certificates for the competition of your Microsoft exam with Pearsonvue to the new Microsoft Learn experience, you might find yourself lost when attempting to view _(and print)_ your hard-earned Microsoft certificate!\\n\\nThe certificate format has also been refreshed.\\n\\n**Old**\\n\\n![Microsoft certificate](/uploads/001-mtc-cert_old.png \\"Microsoft certificate\\")\\n\\n**New**\\n\\n![Microsoft certificate](/uploads/mslearn_certificateformat.png \\"Microsoft certificate\\")\\n\\nTo view and print your certificate:\\n\\n1. Navigate to: [**https://learn.microsoft.com/en-us/users/me/activity**](https://learn.microsoft.com/en-us/users/me/activity \\"https://learn.microsoft.com/en-us/users/me/activity\\")**/**\\n2. Click on: **Certifications**\\n3. **Find** your certificate and click on **View certification details**.\\n4. Click a **Print** certification, then click Print and you can print and save it as a PDF or print _(remember to uncheck print Headers and footers, to remove any footers from the print)_.\\n5. ![Microsoft Learn - Certificate details](/uploads/microsoftlearn_certificatedetails.png \\"Microsoft Learn - Certificate details\\")\\n\\nIf you want to download it, you can print it to a PDF, to save the certificate to your computer."},{"id":"azure/internalservererror-when-deploying-azure-firewall","metadata":{"permalink":"/azure/internalservererror-when-deploying-azure-firewall","source":"@site/blog/2022-09-06-internalservererror-when-deploying-azure-firewall.md","title":"InternalServerError when deploying Azure Firewall","description":"When attempting to deploy an Azure Firewall, you may get an error: Conflict, DeploymentFailed error. This error can occur when you have an expressroute connection, and the Firewall is not deployed in Force Tunneled mode, as the routes from the BGP link will be replacing the default Azure internet route, required for the Azure Firewall.","date":"2022-09-06T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.01,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-09-06 00:00:00 +1200","title":"InternalServerError when deploying Azure Firewall","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/deploymentfailedazurefirewall.png"},"slug":"azure/internalservererror-when-deploying-azure-firewall"},"unlisted":false,"prevItem":{"title":"How to download or print a Microsoft exam certificate","permalink":"/misc/how-to-download-a-microsoft-exam-certificate"},"nextItem":{"title":"Azure Private DNS Resolver and Azure Point to Site VPN","permalink":"/azure/azure-point-to-site-vpn-and-private-dns-resolver"}},"content":"When attempting to deploy an [Azure Firewall](https://learn.microsoft.com/en-us/azure/firewall/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Firewall?\\"), you may get an error: Conflict, DeploymentFailed error. This error can occur when you have an expressroute connection, and the Firewall is not deployed in Force Tunneled mode, as the routes from the BGP link will be replacing the default Azure internet route, required for the Azure Firewall.\\n\\n    \\"code\\": \\"InternalServerError\\",\\n    \\"message\\": \\"An error occurred.\\"\\n\\n![Deployment Failed - Azure Firewall](/uploads/deploymentfailedazurefirewall.png \\"Deployment Failed - Azure Firewall\\")\\n\\nIf you click Input in the deployment, you may notice your vnetName, vnetAddressSpace and subnetAddressSpace are blank.\\n\\n![Azure Firewall deployment](/uploads/deploymentfailedazurefirewallinputs.png \\"Azure Firewall deployment\\")\\n\\nEven though the Azure Firewall will appear as deployed. You will notice that it won\'t have a Private IP assigned.\\n\\nTo resolve this error:\\n\\n1. **Delete** the **Azure Firewall** that has been **partially deployed**\\n2. **Create** a **[User Defined route](https://learn.microsoft.com/en-us/azure/virtual-network/manage-route-table?WT.mc_id=AZ-MVP-5004796 \\"Create, change, or delete a route table\\")** for the internet:\\n\\n| Name | Address Prefix | Next hop type |\\n| --- | --- | --- |\\n| Internet | 0.0.0.0/0 | Internet |\\n\\n1. Link it to the **AzureFirewallSubnet**\\n2. **Redeploy**\\n\\nThis error may occur as your internet route may flow via BGP routes from on-premises; the user-defined route will override this route."},{"id":"azure/azure-point-to-site-vpn-and-private-dns-resolver","metadata":{"permalink":"/azure/azure-point-to-site-vpn-and-private-dns-resolver","source":"@site/blog/2022-09-02-azure-point-to-site-vpn-and-private-dns-resolver.md","title":"Azure Private DNS Resolver and Azure Point to Site VPN","description":"You might access resources such as Azure SQL databases or Azure Storage accounts if you\'re connecting to a Microsoft Azure network externally (from a non-Azure VM or VPN); mainly if you operate Cloud-only services and don\'t have an external DNS provider, such as Active Directory - connecting to private link resources, you may have to edit your local host\'s file and override local DNS to point to the IP of the private endpoint for each service.","date":"2022-09-02T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":7.045,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-09-02 00:00:00 +1200","title":"Azure Private DNS Resolver and Azure Point to Site VPN","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azureprivatednsresolver.png"},"slug":"azure/azure-point-to-site-vpn-and-private-dns-resolver"},"unlisted":false,"prevItem":{"title":"InternalServerError when deploying Azure Firewall","permalink":"/azure/internalservererror-when-deploying-azure-firewall"},"nextItem":{"title":"IMS Payroll not opening as a published application in Azure Virtual Desktop","permalink":"/azure/ims-payroll-not-opening-as-a-published-application-in-azure-virtual-desktop"}},"content":"You might access resources such as [Azure SQL databases](https://azure.microsoft.com/products/azure-sql/database/?WT.mc_id=AZ-MVP-5004796 \\"Azure SQL Database\\") or [Azure Storage accounts](https://learn.microsoft.com/azure/storage/?WT.mc_id=AZ-MVP-5004796 \\"Azure Storage documentation\\") if you\'re connecting to a Microsoft Azure network externally _(_[_from a non-Azure VM_](https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-name-resolution-for-vms-and-role-instances?WT.mc_id=AZ-MVP-5004796#azure-provided-name-resolution \\"Name resolution for resources in Azure virtual networks\\") _or VPN)_; mainly if you operate Cloud-only services and don\'t have an external DNS provider, such as Active Directory - connecting to [private link](https://azure.microsoft.com/services/private-link/?WT.mc_id=AZ-MVP-5004796 \\"Private Link\\") resources, you may have to edit your local host\'s file and override local DNS to point to the IP of the [private endpoint](https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview?WT.mc_id=AZ-MVP-5004796) for each service.\\n\\nThis is not sustainable, not scalable, and you might end up throwing your hands in the air and switching back to public-facing services and just whitelisting other users\' Public IPs to gain access to Azure resources - which can lead to its own set of issues, such as unmanaged IPs left with access to resources after contractors or users leave or have finished their work, IP address changes if not managed correctly can allow any user or company to have a direct line of sight to your company resources.\\n\\n### Overview\\n\\n> Today we will concentrate on DNS resolution of Private Endpoints, using [Azure DNS Private Resolver](https://learn.microsoft.com/azure/dns/dns-private-resolver-overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure DNS Private Resolver?\\") as a DNS proxy when connecting to Azure using a [Point to Site VPN](https://learn.microsoft.com/azure/vpn-gateway/point-to-site-about?WT.mc_id=AZ-MVP-5004796 \\"About Point-to-Site VPN\\").\\n\\nFor this article, I assume you have an Azure Point to Site already set up; if you don\'t, you can refer to a previous article I wrote for [Creating an Azure Point to Site VPN using Microsoft Entra ID authentication](https://luke.geek.nz/azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication/ \\"Create Azure Point to Site VPN using Microsoft Entra ID authentication\\").\\n\\n_Disclaimer: Azure Private DNS Resolver is still in Public Preview at the time of this article (02/09/2022). If you aim to use this in a Production scenario, functionality and services may change. This also means there are current_ [_regional restrictions_](https://learn.microsoft.com/en-us/azure/dns/dns-private-resolver-overview?WT.mc_id=AZ-MVP-5004796#regional-availability \\"Regional availability\\")_, and Azure Private DNS Resolver is not currently available in all regions. Also bear in mind the [cost](https://azure.microsoft.com/en-us/pricing/details/dns/?WT.mc_id=AZ-MVP-5004796) of this service._\\n\\n#### So what is Azure DNS Resolver?\\n\\n> Azure DNS private resolver is a cloud-native, highly available, and DevOps-friendly service. It provides a simple, zero-maintenance, reliable, and secure Domain Name System (DNS) service to resolve and conditionally forward DNS queries from a virtual network, on-premises, and to other target DNS servers without the need to create and manage a custom DNS solution. Resolve DNS names hosted in Azure Private DNS Zones from on-premises networks as well as DNS queries for your own domain names. This will make your DNS infrastructure work privately and seamlessly across on-premises networks and enable key hybrid networking scenarios.\\n\\n![Azure Private DNS Resolver](/uploads/azureprivatednsresolver.png \\"Azure Private DNS Resolver\\")\\n\\nCustomers will no longer need to provision IaaS-based solutions on their virtual networks to resolve names registered on Azure Private DNS Zones and can do conditional forwarding of domains back to on-premises, across multi-cloud providers, and public DNS servers.\\n\\nThis solution can work with your Azure ExpressRoute, Azure VPN, or Azure Bastion setup.\\n\\n![Azure Private DNS Resolver](/uploads/dns-private-resolver.png \\"Azure Private DNS Resolver\\")\\n\\n#### Inbound or Outbound?\\n\\nName resolution queries for Azure workloads from the on-premises network are conditionally forwarded to the Azure DNS private resolver inbound endpoint, which enables you to perform name resolution of workloads registered on Azure Private DNS Zones from on-premises.\\n\\n| Endpoint          | Blurb                                                                                                                                     |\\n| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\\n| Inbound Endpoint  | Azure DNS private resolver inbound endpoint that receives the name resolution request from Azure & on-premises network and resolve names. |\\n| Outbound Endpoint | Azure DNS private resolver outbound endpoint conditionally forwards the request to on-premises or other target DNS servers.               |\\n\\nThe Azure DNS private resolver inbound endpoint has a private IP that is part of a subnet where the endpoint has been created. The IP address of the DNS private resolver inbound endpoint is then set as a DNS server on the on-premises network.\\n\\nAzure DNS private resolver outbound endpoint conditionally forwards the request to on-premises or other target DNS servers.\\n\\n**Today, we will connect to private endpoints to concentrate on the Inbound functionality of Azure Private DNS Resolver.**\\n\\n### Deployment\\n\\nTo deploy Azure Private DNS Resolver, we will need a few things.\\n\\n* A Virtual Network\\n* A [subnet](https://learn.microsoft.com/en-us/azure/dns/dns-private-resolver-overview#subnet-restrictions \\"Subnet restrictions\\") dedicated to resolving DNS queries _(/28)_\\n* A private endpoint _(i.e. Storage Account, SQL Database)_ is linked to the virtual network.\\n\\n#### Deploy DNS Private Resolver\\n\\n_I assume you already have a Virtual Network tied to your Virtual Network gateway as part of the \'_[_Point to Site VPN_](https://luke.geek.nz/azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication/ \\"Create Azure Point to Site VPN using Microsoft Entra ID authentication \\")_\' setup._\\n\\n 1. Open the **Azure Portal**\\n 2. Click on **+ Create a resource**\\n 3. Type in: **DNS Private Resolver**\\n 4. ![Azure DNS Private Resolver](/uploads/azureportal_creatednsprivateresolver.png \\"Azure DNS Private Resolver\\")\\n 5. Click **Create**\\n 6. Select your **Subscription**\\n 7. Select your **Resource Group** _(I recommend placing the DNS Private Resolver in the same resource group as your Virtual Network - but place this service in a Resource Group that makes sense for your environment, i.e. shared services or a specific network team resource group)_\\n 8. Type in a **name** for your DNS Private Resolver _(this is a regional service, but the name does not need a globally unique )_\\n 9. **Select** your **region** _(this needs to be the same region as the Virtual Network)_\\n10. Select your **Virtual Network** _(the same Virtual Network that has your Virtual Network Gateway for the Point to Site VPN and your Private endpoints)_\\n11. ![Create Azure Private DNS Resolver](/uploads/azureportal_creatednsprivateresolverinitialpane.png \\"Create Azure Private DNS Resolver\\")\\n12. Click **Next: Inbound Endpoints >**\\n13. Now its time to add our Inbound Endpoint and create the Private DNS Resolver Subnet, click **+ Add an endpoint**\\n14. Type in your **endpoint name** (_for example, InboundEndpoint)_\\n15. If you have already created a subnet, select it - else, click **Create New**\\n16. Enter in your subnet name and address range\\n17. Click **Save**\\n18. ![Private DNS Resolver create subnet](/uploads/azureportal_creatednsprivateresolversubnet.png \\"Private DNS Resolver create subnet\\")\\n19. Click **Review + Create**\\n20. Click **Create**\\n\\n#### Adjust Point to Site DNS\\n\\nNow that the DNS Resolver has been created, with an inbound endpoint, allowing the lookup of private endpoints, we need to add the Private Resolver DNS relay to our point-to-site VPN configuration; first, we need the newly created private IP of the inbound endpoint.\\n\\n 1. Navigate to the [**DNS Private Resolver**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Network%2FdnsResolvers \\"DNS Private Resolvers\\") in the Azure Portal\\n 2. **Open** your DNS Private **Resolver service**\\n 3. Click on **Inbound Endpoints**\\n 4. Make a note of the **private IP** of your inbound endpoint.\\n 5. ![Private DNS Resolver](/uploads/azureportal_creatednsprivateresolverinboundendpointip.png \\"Private DNS Resolver\\")\\n 6. Now that the Private Inbound resolver has been configured, we need to add the DNS relay into our Azure VPN configuration so that our DNS queries will respond with a private endpoint; you will need to modify the \'[azurevpnconfig.xml](https://learn.microsoft.com/en-us/azure/vpn-gateway/about-vpn-profile-download?WT.mc_id=AZ-MVP-5004796#generate \\"Generate profile files\\")\' file and reimport the VPN.\\n 7. Right-click \'azurevpnconfig.xml\' and edit in Notepad or Visual Studio Code\\n 8. Under: </serverlist>\\n 9. **Add** _(replace the IP listed below with the IP of your Inbound endpoint copied earlier)_:\\n\\n        <clientconfig>\\n        <dnsservers>\\n        <dnsserver>10.0.18.4</dnsserver>\\n        </dnsservers>\\n        </clientconfig>\\n10. **Save** and **reimport** to the Azure VPN Client\\n11. Once connected, ping a resource behind a private endpoint, and you should get the private IP of that resource back and should be able to connect to that resource privately.\\n12. ![Azure Private DNS Resolver ping](/uploads/azurevpn_testprivateendpoint.png \\"Azure Private DNS Resolver ping\\")\\n\\nAny future or current private endpoints linked to the same Virtual Network will instantly be accessible without additional changes on the Azure VPN client.\\nIf you have a Hub & Spoke topology, then you may place the DNS Private Resolver in the HUB, then use [forwarding rules](https://learn.microsoft.com/en-us/azure/dns/dns-private-resolver-get-started-portal?WT.mc_id=AZ-MVP-5004796#link-your-forwarding-ruleset-to-the-second-virtual-network) to link to other peered VNETs.\\n\\n### Additional resources\\n\\nThe third-party resources below include reading and learning about the Azure Private DNS Resolver.\\n\\n* [Quickstart: Create an Azure private DNS Resolver using the Azure portal](https://learn.microsoft.com/en-us/azure/dns/dns-private-resolver-get-started-portal?WT.mc_id=AZ-MVP-5004796 \\"Quickstart: Create an Azure private DNS Resolver using the Azure portal\\")\\n* [Intro to Azure DNS Private Resolver](https://learn.microsoft.com/en-us/learn/modules/intro-to-azure-dns-private-resolver/?WT.mc_id=AZ-MVP-5004796 \\"Intro to Azure DNS Private Resolver\\")\\n* [Azure DNS Private Resolver - MicroHack](https://github.com/dawlysd/azure-dns-private-resolver-microhack \\"Azure DNS Private Resolver - MicroHack\\")\\n* My Azure Private DNS Resolver Bicep export for reference:\\n\\n        param dnsResolvers_PrivateDNSResolver_name string = \'PrivateDNSResolver\'\\n        param virtualNetworks_vnettest_externalid string = \'/subscriptions/57627713-eff2-44fa-a546-a2c8fde3c6e3/resourceGroups/pointtositetest/providers/Microsoft.Network/virtualNetworks/vnettest\'\\n\\n        resource dnsResolvers_PrivateDNSResolver_name_resource \'Microsoft.Network/dnsResolvers@2020-04-01-preview\' = {\\n        name: dnsResolvers_PrivateDNSResolver_name\\n        location: \'australiaeast\'\\n        properties: {\\n        virtualNetwork: {\\n          id: virtualNetworks_vnettest_externalid\\n        }\\n      }\\n        }\\n                resource dnsResolvers_PrivateDNSResolver_name_InboundEndpoint \'Microsoft.Network/dnsResolvers/inboundEndpoints@2020-04-01-preview\' = {\\n      parent: dnsResolvers_PrivateDNSResolver_name_resource\\n      name: \'InboundEndpoint\'\\n      location: \'australiaeast\'\\n          }\\n        ]\\n      }\\n        }"},{"id":"azure/ims-payroll-not-opening-as-a-published-application-in-azure-virtual-desktop","metadata":{"permalink":"/azure/ims-payroll-not-opening-as-a-published-application-in-azure-virtual-desktop","source":"@site/blog/2022-08-23-ims-payroll-not-opening-as-a-published-application-in-azure-virtual-desktop.md","title":"IMS Payroll not opening as a published application in Azure Virtual Desktop","description":"Azure Virtual Desktop allows you to access an entire desktop or a published application with shortcuts and an appearance like it was running locally; depending on the requirements; I prefer published applications where possible to keep the user experience on the endpoint device and keep the cost down.","date":"2022-08-23T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.32,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"IMS Payroll not opening as a published application in Azure Virtual Desktop","authors":["Luke"],"tags":["Azure"],"date":"2022-08-23 00:00:00 +1300","toc":false,"header":{"teaser":"/uploads/imspayroll_avdpublishedapp.png"},"slug":"azure/ims-payroll-not-opening-as-a-published-application-in-azure-virtual-desktop"},"unlisted":false,"prevItem":{"title":"Azure Private DNS Resolver and Azure Point to Site VPN","permalink":"/azure/azure-point-to-site-vpn-and-private-dns-resolver"},"nextItem":{"title":"Microsoft Azure - Operational Cost Optimization Tasks","permalink":"/2022/08/18/microsoft-azure-cost-optimization-tasks"}},"content":"[Azure Virtual Desktop](https://azure.microsoft.com/en-us/services/virtual-desktop/?WT.mc_id=AZ-MVP-5004796 \\" Azure Virtual Desktop\\") allows you to access an entire desktop or a published application with shortcuts and an appearance like it was running locally; depending on the requirements; I prefer published applications where possible to keep the user experience on the endpoint device and keep the cost down.\\n\\nOne of the applications I published for a customer is [MYOB IMS Payroll](https://www.myob.com/nz/enterprise/ims-payroll \\" MYOB IMS Payroll \\").\\n\\nIMS Payroll worked well as a published application for months until one day; it didn\'t seem to open for the user, whether as a published application or in the Full Desktop.\\n\\nThe symptoms were that once the user clicked on the icon, it would appear to open _(visible on the Taskbar)_, but there was no window, and when you hovered over the preview thumbnail, it was blank. The cursor also appeared to be active with a circle, indicating it was trying to open.\\n\\nEven if you don\'t have IMS Payroll, you may experience applications with a similar experience, and hopefully, this article will help point you in the right direction.\\n\\n![Azure Virtual Desktop - Published Application](/uploads/imspayroll_avdpublishedapp.png)\\n\\nOne noticeable difference we found in our testing - was that it opened for us and other users using different accounts.\\n\\nAfter some discovery, we discovered that the user had gone to another branch office site and used a different monitor setup, and IMS Payroll was out of drawing range. Usually, windows would be able to snap this back into view; however, after comparing the registry keys for our user vs the user who had the issue, we discovered that IMS Payroll sets the location in the user registry.\\n\\n* Registry Key location: **\\\\\\\\HKEY_CURRENT_USER\\\\\\\\IMS Payroll Partner\\\\\\\\Layout**\\n\\nIn our case, the settings were as follows:\\n\\n    Windows Registry Editor Version 5.00\\n    \\n    [HKEY_CURRENT_USER\\\\IMS Payroll Partner\\\\Layout]\\n    \\"Left\\"=\\"684\\"\\n    \\"Top\\"=\\"310\\"\\n    \\"Height\\"=\\"713\\"\\n    \\"Width\\"=\\"1127\\"\\n    \\"StatusBar\\"=\\"1\\"\\n    \\"ActiveHelp\\"=\\"0\\"\\n    \\"EmployeePage\\"=\\"0\\"\\n    \\"PayrollPage\\"=\\"5\\"\\n    \\"CompanyPage\\"=\\"1\\"\\n    \\"SkipWelcome\\"=\\"1\\"\\n    \\"SkinName\\"=\\"lfUltraFlat\\"\\n    \\"LastPage\\"=\\"6\\"\\n\\nFor the users who couldn\'t see IMS Payroll, their settings looked more like this:\\n\\n    Windows Registry Editor Version 5.00\\n    \\n    [HKEY_CURRENT_USER\\\\IMS Payroll Partner\\\\Layout]\\n    \\"Left\\"=\\"-1444\\"\\n    \\"Top\\"=\\"310\\"\\n    \\"Height\\"=\\"713\\"\\n    \\"Width\\"=\\"1127\\"\\n    \\"StatusBar\\"=\\"1\\"\\n    \\"ActiveHelp\\"=\\"0\\"\\n    \\"EmployeePage\\"=\\"0\\"\\n    \\"PayrollPage\\"=\\"5\\"\\n    \\"CompanyPage\\"=\\"1\\"\\n    \\"SkipWelcome\\"=\\"1\\"\\n    \\"SkinName\\"=\\"lfUltraFlat\\"\\n    \\"LastPage\\"=\\"6\\"\\n\\nThe difference was that the Left entry had moved the Window too far, left out of view, so it could not be seen by the user when opening as a published app or on a Desktop.\\n\\nAfter the **Left entry was changed from -1444 to 684**. IMS became visible again as a published application and on the Full Desktop.\\n\\nDue to the hard-coded user registry entries, this specific issue would have occurred regardless of Azure Virtual Desktop, running in a Terminal Services environment, or even locally, when working with different monitor setups.\\n\\n_Note: Some applications may have configuration files stored in the user\'s AppData folders instead of the registry; if in doubt, raise a support ticket with the application vendor._"},{"id":"/2022/08/18/microsoft-azure-cost-optimization-tasks","metadata":{"permalink":"/2022/08/18/microsoft-azure-cost-optimization-tasks","source":"@site/blog/2022-08-18-microsoft-azure-cost-optimization-tasks.md","title":"Microsoft Azure - Operational Cost Optimization Tasks","description":"The Microsoft Azure platform is not a set-and-forget ecosystem like doing service on your car!","date":"2022-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":15.125,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-08-18 00:00:00 +1200","title":"Microsoft Azure - Operational Cost Optimization Tasks","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/cost_pillar_overview.png"}},"unlisted":false,"prevItem":{"title":"IMS Payroll not opening as a published application in Azure Virtual Desktop","permalink":"/azure/ims-payroll-not-opening-as-a-published-application-in-azure-virtual-desktop"},"nextItem":{"title":"Connect to Azure SQL Database in a Jupyter Notebook using Python","permalink":"/2022/08/17/connect-to-azure-sql-database-in-a-jupyter-notebook-using-python"}},"content":"The [Microsoft Azure](https://azure.microsoft.com/en-us/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Azure\\") platform is not a set-and-forget ecosystem like doing service on your car!\\n\\nThere are no one-size-fits when it comes to cost optimization, but some general tasks can be done or considered on a Monthly/Quarterly/Annual basis to keep on top of the resources you are running in Azure and to keep them lean.\\n\\n#### Overview\\n\\nAlthough Microsoft takes a lot of traditional infrastructure management and security concerns off your hand, you are still responsible for the spending and ensuring the value of the technologies and services you consume match your business goals and agility.\\n\\nToday we are going to go back to basics and look at the [Cost Optimization](https://learn.microsoft.com/en-us/azure/architecture/framework/?WT.mc_id=AZ-MVP-5004796#cost-optimization \\"Cost optimization\\") pillar of the Microsoft [Well-Architected Framework](https://learn.microsoft.com/en-us/azure/architecture/framework/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Azure Well-Architected Framework\\").\\n\\n> \u201cThe cost optimization pillar provides principles for balancing business goals with budget justification to create a cost-effective workload while avoiding capital-intensive solutions. Cost optimization is about looking at ways to reduce unnecessary expenses and improve operational efficiencies.\u201d\\n>\\n> \u201cUse the pay-as-you-go strategy for your architecture, and invest in scaling out, rather than delivering a large investment-first version. Consider opportunity costs in your architecture and the balance between first-mover advantage versus fast follow.\u201d\\n\\nThe right governance and oversight can help prevent Cloud sprawl and wasted consumption costs.\\n\\nTo help get you started, I have put together a list of some optimization opportunities, that should be run regularly, items such as reviewing unassociated public IPs should be done Monthly _(along with Azure Advisor checks)_, and [Azure Reservation reviews](https://learn.microsoft.com/en-us/azure/cost-management-billing/reservations/save-compute-costs-reservations?WT.mc_id=AZ-MVP-5004796 \\"What are Azure Reservations?\\") at least quarterly.\\n\\n> **_This is not an exhaustive list_**, and the use of [Azure Policy](https://learn.microsoft.com/en-us/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Policy?\\") and [Azure Advisor](https://learn.microsoft.com/en-us/azure/advisor/advisor-overview?WT.mc_id=AZ-MVP-5004796 \\"Introduction to Azure Advisor\\") help supplement these tasks.\\n>\\n> If you have other tasks that you run, feel free to share them with the community in the page comments below.\\n\\nThe [Microsoft Graph](https://learn.microsoft.com/en-us/graph/overview?WT.mc_id=AZ-MVP-5004796 \\"Overview of Microsoft Graph\\") and [KQL queries](https://luke.geek.nz/azure-resource-graph-explorer-and-the-powershell-azure-resource-graph \\"Azure Resource Graph Explorer and the PowerShell Azure Resource Graph \\") can also be used in conjunction with PowerShell to pull recommendations straight out of Advisor, which can then be fed into reports, and the use of community tools such as the [Azure Optimization Engine](https://luke.geek.nz/azure/azure-optimization-engine \\"Azure Optimization Engine \\") cannot be undervalued.\\n\\n#### Design\\n\\n![Azure - Monitor & optimize](/uploads/cost_pillar_overview.png \\"Azure - Monitor & optimize\\")\\n\\n##### Keep within the cost constraints\\n\\nEvery design choice has cost implications. Before choosing an architectural pattern, Azure service, or a price model for the service, consider the budget constraints set by the company. As part of the design, identify acceptable boundaries on scale, redundancy, and performance against cost. After estimating the initial cost, set budgets and alerts at different scopes to measure the cost.\\n\\nOne of the cost drivers can be unrestricted resources. These resources typically need to scale and consume more cost to meet demand.\\n\\n##### Aim for scalable costs\\n\\nA key benefit of the cloud is the ability to scale dynamically. The workload cost should scale linearly with demand.\\n\\nYou can save costs through automatic scaling. First, consider the usage metrics and performance to determine the number of instances. Then, choose smaller instances for a highly variable workload and scale out to get the required level of performance rather than up. This choice will enable you to make your cost calculations and estimates granular.\\n\\n##### Pay for the consumption\\n\\nAdopt a leasing model instead of owning infrastructure. Azure offers many SaaS and PaaS resources that simplify the overall architecture. The cost of hardware, software, development, operations, security, and data centre space is included in the pricing model. Also, choose pay-as-you-go over fixed pricing. That way, you\'re charged for only what you use as a consumer.\\n\\n##### Right resources, the right size\\n\\nChoose the right resources aligned with business goals and can handle the workload\'s performance.\\n\\nAn inappropriate or misconfigured service can impact the cost.\\n\\nFor example, building a multi-region service when the service levels don\'t require high availability or geo-redundancy will increase cost without any reasonable business justification. Specific infrastructure resources are delivered as fix-sized building blocks. Ensure that these blocks are adequately sized to meet capacity demand and deliver expected outcomes.\\n\\n##### Monitor and optimize\\n\\nTreat cost monitoring and optimization as a process rather than a point-in-time activity. Conduct regular cost reviews and measure and forecast the capacity needs so that you can provision resources dynamically and scale with demand. Review the cost management recommendations and take action.\\n\\nToday, we will focus on **Monitor and optimize**.\\n\\n#### Review Underutilized Resources\\n\\nOptimize and improve efficiency by identifying idle and underutilized resources across the Azure ecosystem.\\n\\n##### Review Azure App Service Plans\\n\\nReview [Azure App Service Plans](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2FserverFarms \\"Azure Portal - App Service plan\\") to determine if:\\n\\n1. The Azure App Service Plan is \u2018Standard\u2019 or \u2018Premium\u2019 pricing and has an associated application.\\n2. If the Azure App Service is getting utilized _(by looking at the Metrics/CPU)_ and doesn\u2019t need to be downscaled to a smaller plan.\\n\\n##### Review shutdown workloads\\n\\nBecause you pay for Azure Resources as \u2018Pay As You Go\u2019, a quick win can be to review [Virtual Machines](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Compute%2FVirtualMachines \\"Azure Portal - Virtual Machines\\") to determine if the workload needs to be 24/7!\\n\\nFor example, you have automation configured to automatically start up and shut down workloads based on the following schedule: 7 AM Start \u2013 7 PM Stop _(& off Weekends)_.\\n\\nYou can add servers to this automated schedule by adding the following Tag to the Virtual Machine or trigger automation when a workload is \u2018Shutdown\u2019 and not deallocated; see my article on \\"[Turn on an Azure Virtual Machine using Azure Automation](https://luke.geek.nz/azure/turn-on-a-azure-virtual-machine-using-azure-automation/ \\"Turn on a Azure Virtual Machine using Azure Automation \\")\\" for a potential place to start.\\n\\n##### Review Azure Advisor\\n\\nThe Azure Advisor is an inbuilt tool critical to optimizing the Azure Environment. The [Azure Advisor](https://portal.azure.com/#blade/Microsoft_Azure_Expert/AdvisorMenuBlade/Cost \\"Azure Portal - Azure Advisor\\") needs to be reviewed for Cost recommendations.\\n\\n1. The Azure Advisor will recommend Reserved Instances.\\n2. The Azure Advisor will recommend if a Virtual Machine runs on a VM size GREATER than what it needs _(based on CPU utilization under 5% in the last 14 days)_. If the Azure Advisor reports an overprovisioned machine, you need to investigate its use and resize it to a more suitable size.\\n\\n##### Review Azure SQL Databases\\n\\nReview [Azure SQL Databases](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Sql%2Fservers%2Fdatabases \\"Azure Portal - Azure SQL databases\\") to determine if:\\n\\n1. The SQL Database Pricing Tier is \u2018Standard\u2019 and uses the DTUs (usually found by looking at the Compute utilization on the databases); if not, downsize the DTU limit.\\n2. Check Geo-Replication to ensure that the SQL Database is not replicating across Regions if it doesn\u2019t need to be.\\n\\n#### Review Azure Reserved Instances\\n\\nAzure reserved instances significantly reduce costs\u2014up to 72 per cent compared to pay-as-you-go prices\u2014with one-year or three-year terms on Windows and Linux virtual machines (VMs). What\'s more, you can now improve budgeting and forecasting with a single upfront payment (i.e. Pay for a VM Upfront for 1/3 Year or 5 Years), making it easy to calculate your investments. Or lower your upfront cash outflow with monthly payment options at no additional cost.\\n\\n![Azure Reserved Instance](/uploads/azure-ri.png \\"Azure Reserved Instance\\")\\n\\nThe Azure Advisor is an inbuilt tool critical to optimizing the Azure Environment. The Azure Advisor needs to be reviewed for Reserved Instance recommendations.\\n\\n1. When reviewing Reserved Instances, you need to take into consideration:\\n2. What workloads are they used for?\\n3. Is there a project that may replace or resize the workloads next year?\\n4. Who is paying for the workloads?\\n\\n#### Review unused files and VHDs\\n\\nSave Azure costs by cleaning up unused VHDs in your Azure storage. Azure stores Azure Virtual Machine OS and data disks in Azure storage accounts.\\n\\nWhen a VM is deleted from the Azure portal, the underlying OS and data disks may not get deleted. Such disks continue to consume Azure storage and account for the cost of storing them. These disks are called Orphaned Disks.\\n\\nAs mentioned above, some Virtual Machines with unmanaged disks will keep the VHDs around when deleted.\\n\\nUsing a PowerShell [script](https://learn.microsoft.com/en-us/azure/virtual-machines/windows/find-unattached-disks?WT.mc_id=AZ-MVP-5004796 \\"Find and delete unattached Azure managed and unmanaged disks\\") _(provided by Microsoft),_ you can report on any disks that are not in use by a VM and then delete them.\\n\\n_Note: Be VERY cautious doing this; solutions such as Citrix and Azure Image Builder use unmanaged disks to create new Session hosts, etc., so context is key._\\n\\nWith the Azure Storage accounts using Blob data \u2013 such as Diagnostic Accounts, it is a good idea to implement [Azure Blob Storage Lifecycle](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796 \\"Azure Blob Storage Lifecycle\\") on the storage accounts, so we are only retaining recent and relevant data.\\n\\n![Azure Blob Storage Lifecycle Management](/uploads/azureblogstglifecyclemgmnt.PNG \\"Azure Blob Storage Lifecycle Management\\")\\n\\nThe lifecycle management policy lets you:\\n\\n1. Transition blobs to a cooler storage tier _(hot to cool, hot to archive, or cool to archive)_ to optimize for performance and cost\\n2. Delete blobs at the end of their lifecycles\\n3. Define rules to be run once per day at the storage account level.\\n\\n#### Review budgets\\n\\nBudgets in Cost Management help you plan for and drive organizational accountability. With budgets, you can account for the Azure services you consume or subscribe to during a specific period.\\n\\nBudgets help you inform others about their spending to proactively manage costs and monitor how spending progresses over time.\\n\\nWhen the budget thresholds you\'ve created are exceeded, notifications are triggered. None of your resources is affected, and your consumption isn\'t stopped; however, you can use Budget alerts as a trigger to run Azure Logic Apps or Functions to automate the shutdown and resize resources. You can use budgets to compare and track spending as you analyze costs.\\n\\n![Azure Budget](/uploads/azure_budget.png)\\n\\nEnsure Azure Budget notifications are configured to email Product Owners or other Stakeholders once a Resource Group or Subscription reaches a specific threshold.\\n\\nThis is set up in the Azure Portal, on the Resource Group under Budgets, and set to email the Application Owner.\\n\\nExamples of budgets that could be configured:\\n\\nGenerally, I recommend that three budgets should be configured to give enough notice:\\n\\n* 50%\\n* 60%\\n* 70%\\n\\n#### Review Tags\\n\\nYou apply tags to your Azure resources, resource groups, and subscriptions to logically organize them into a taxonomy. Each tag consists of a name and a value pair. For example, you can apply the name \\"Environment\\" and the value \\"Production\\" to all the resources in production.\\n\\nTags can be used to determine things like:\\n\\n* Who to bill?\\n* Who supports it?\\n\\n![Azure Portal - Tags](/uploads/operrationaltasksazuretags.png \\"Azure Portal - Tags\\")\\n\\nThe right tags can mean that the right owners get charged internally and have more ownership of their resource costs. Examples below:\\n\\n| Tag Name | Value | Comment |\\n| --- | --- | --- |\\n| Dept | Finance | Name of the department who owns the resources. |\\n| Environment | UAT | What environment the Resource is used for such as Production, UAT and Development |\\n| Application Owner | Luke Murray | The name of the Product Owner for the service sitting inside the Resource Group |\\n| Support Team | Platform Team | What team is responsible for the resources/site for support reasons |\\n| Billing Code | Operational | Purchase order or project billing code |\\n\\nFor further examples and a base tagging convention, check out a blog article I wrote on [Microsoft Azure Tagging conventions](https://luke.geek.nz/azure/microsoft-azure-tagging-conventions/ \\"Microsoft Azure Tagging Conventions \\").\\n\\n#### Review Hub (Hybrid Use Benefit)\\n\\nThe [Azure Hybrid Benefit](https://azure.microsoft.com/en-us/pricing/hybrid-benefit/?WT.mc_id=AZ-MVP-5004796 \\"Azure Hybrid Benefit\\") is a pricing benefit for customers with Software Assurance licenses, which helps maximize the value of existing on-premises Windows Server and/or SQL Server license investments when migrating to Azure.\\n\\nEligible customers can save up to 40% on Azure Virtual Machines _(infrastructure as a service, or IaaS)_, and save up to 55% on Azure SQL Database _(platform as a service, or PaaS)_ and SQL Server on Azure Virtual Machines _(IaaS)_ with Azure Hybrid Benefit, which increases to up to 80% when combined with Azure Reserved Instances.\\n\\n![Azure - Hybrid Use Benefit](/uploads/operationaltasks_hub.png \\"Azure - Hybrid Use Benefit\\")\\n\\nTo verify if a server is using the Azure Hybrid Benefit, Log in to the Azure Portal and navigate to the Virtual Machine Blade. Make sure that the: OS Licensing Benefit column is selected.\\n\\nIf a Virtual Machine Already has HUB, it will have: The azure hybrid benefit listed in the column, and any non-supported workloads _(such as Linux)_ will have \u2018Not Supported\u2019.\\n\\nIf any are eligible for HUB, click on the Virtual Machine\u2026\\n\\n1. Click the **Configuration blade**\\n2. Select **Licensing, Already have a Windows server license?**\\n3. **Yes** and **Save**\\n\\n_Note: This is a non-intrusive change that will take effect on the billing immediately and doesn\u2019t cause any impact on the Virtual Machine._\\n\\n#### Review Backups\\n\\n[Azure Backup](https://learn.microsoft.com/en-us/azure/backup/backup-overview?WT.mc_id=AZ-MVP-5004796 \\"Azure Backup\\") is simple because it\u2019s built into the platform. It has one-click backup support for SQL databases and virtual machines running in Azure.\\n\\nAzure Backup is cost-effective and less complex than other cloud backup solutions while keeping your data safe from ransomware and human errors. Sometimes there will be workloads backed up to migrate, test, or clone, and you no longer need to retain the data.\\n\\n> Note: This can be a tricky one as you will need to talk to product owners to confirm the workloads were just Dev/Test workloads, and not required, there may be legal implications for keeping workloads in the backup. But if someone stood up something to play with, particularly in a Sandbox or Development subscription there may not be a reason to keep it around.\\n\\nLog in to the Azure Portal and navigate the [Recovery Services Vault](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.RecoveryServices%2Fvaults \\"Azure Portal - Recovery Services\\") page. Navigate to each one and click on:\\n\\nBackup:\\n\\n1. Under **Usage**, click on **Backup Items**\\n2. Click on **Azure Virtual Machines**\\n3. **Sort** the **Backup** items by **Latest Restore Point** _(so the older restore points are at the top)_\\n\\n   Using the Latest Restore Point as a guide, IF any servers can have their Backups deleted:\\n4. Click on the **Name** of the Backup Item\\n5. Click on **Stop Backup**\\n6. Select **Delete Backup Data** _(this is non-reversible)_\\n7. Type in the name of the **Backup Item** and select **Stop Backup**\\n\\n#### Review unused Public IPs\\n\\nPublic IP addresses allow Internet resources to communicate inbound to Azure resources. Public IP addresses enable Azure resources to communicate to the Internet and public-facing Azure services.\\n\\nThis is also a great opportunity to inspect what Public IP addresses you have and make sure some resources have public IPs that does not need to be assigned! Tip setup an Azure Policy that prevents the creation of Public IPs.\\n\\nThe address is dedicated to the resource until it\u2019s unassigned by you. A resource without a public IP assigned can communicate outbound. Azure dynamically assigns an available IP address that isn\u2019t dedicated to the resource.\\n\\n![Azure Portal - Public IP Address](/uploads/operationaltasks_pip.png \\"Azure Portal - Public IP Address\\")\\n\\nWhen resources get created, sometimes they will create a Public IP; these can be removed as part of the build but left in the Resource Groups.\\n\\nWe want to remove unattached Public IP to save money.\\n\\n_Note: In some cases, the Product Owner may need to be consulted before any changes are made, as some of the resources may be inflight projects or required._\\n\\n1. Log in to the **Azure Portal** and navigate to the [Public IP Addresses](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Network%2FPublicIpAddresses \\"Azure Portal - Public IP Address\\") blade\\n2. Look in the \u2018**Associated to**\u2019 column, and if not required, click on the **Public IP**\\n3. Click **Delete**\\n\\n#### Review Azure Storage Accounts\\n\\nAn Azure storage account contains all your Azure Storage data objects: blobs, files, queues, tables, and disks. Your Azure storage account\'s data is durable, highly available, secure, and massively scalable.\\n\\nGeneral-purpose storage accounts may be configured for either of the following performance tiers:\\n\\n* A standard performance tier for storing blobs, files, tables, queues, and Azure virtual machine disks.\\n* A premium performance tier for storing unmanaged virtual machine disks. If a Storage account is Premium but only needs to be Standard _(or LRS instead of ZRS)_, this can save some money.\\n\\nNote: In some cases, the Product Owner may need to be consulted before any changes are made, as some of the resources may be inflight projects or required.\\n\\n1. Log in to the **Azure Portal** and navigate to the [**Storage Account**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts \\"Azure Portal - Storage account\\") blade\\n2. Click on **Manage View**, **Edit Columns**, and add in: **SKU**\\n3. **Review** the **Premium** Storage **Accounts** and determine if any accounts need to be downsized to Standard\\n4. To **change**, click on the **Storage Account**\\n5. Click on **Configuration** and change from **Premium** to **Standard**\\n\\nYou can also look at the Replication. Does that Storage Account need to be Geo-Redundant if the rest of the application that uses it isn\u2019t? Can the storage account be changed to Standard during off-hours or non-peak?\\n\\n#### - Download the PDF version of these Tasks\\n\\nFinally, if you prefer this in a more PDF/Visual format - you can download a PDF version of this directly from my Github \\"[here](https://github.com/lukemurraynz/presentations/blob/main/2022/Microsoft%20Azure%20-%20Cost%20Optimization_1.0.pdf \\"Azure Cost Optimization PDF\\")\\".\\n\\n#### - Azure Operational Checklist table\\n\\nThis is a very quick example of what an Azure Operational Checklist could look like; if you record what tasks you do, you can look at further automation around implementation and reporting.\\n\\n| **Azure Checklist**                 | ****   | **** | ****             |\\n|-------------------------------------|--------|------|------------------|\\n| **Action**                          | **Status** | **Date** | **Note/Opportunity** |\\n| **Review Azure App Service Plans**  |        |      |                  |\\n| **Review shutdown workloads**       |        |      |                  |\\n| **Review Azure Advisor**            |        |      |                  |\\n| **Review Azure SQL Databases**      |        |      |                  |\\n| **Review Azure Reserved Instances** |        |      |                  |\\n| **Review unused files and VHDs**    |        |      |                  |\\n| **Review budgets**                  |        |      |                  |\\n| **Review Tags**                     |        |      |                  |\\n| **Review Hub (Hybrid Use Benefit)** |        |      |                  |\\n| **Review Backups**                  |        |      |                  |\\n| **Review unused Public IPs**        |        |      |                  |\\n| **Review Azure Storage Accounts**   |        |      |                  |"},{"id":"/2022/08/17/connect-to-azure-sql-database-in-a-jupyter-notebook-using-python","metadata":{"permalink":"/2022/08/17/connect-to-azure-sql-database-in-a-jupyter-notebook-using-python","source":"@site/blog/2022-08-17-connect-to-azure-sql-database-in-a-jupyter-notebook-using-python.md","title":"Connect to Azure SQL Database in a Jupyter Notebook using Python","description":"Jupyter Notebooks, commonly used by Data Scientists and Students, allow you to run code, such as Python and PowerShell, inside a Notebook format and display the output inside the notebook; this is useful for teaching a subject or displaying up-to-date information.","date":"2022-08-17T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.405,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-08-17 00:00:00 +1200","title":"Connect to Azure SQL Database in a Jupyter Notebook using Python","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/juptyer_notebook.png"}},"unlisted":false,"prevItem":{"title":"Microsoft Azure - Operational Cost Optimization Tasks","permalink":"/2022/08/18/microsoft-azure-cost-optimization-tasks"},"nextItem":{"title":"Create a Public Holidays API using Microsoft Azure","permalink":"/2022/08/09/create-a-public-holidays-api-using-microsoft-azure"}},"content":"[Jupyter](https://jupyter.org/ \\"Jupyter\\") Notebooks, commonly used by Data Scientists and Students, allow you to run code, such as Python and PowerShell, inside a Notebook format and display the output inside the notebook; this is useful for teaching a subject or displaying up-to-date information.\\n\\nI am not a python or Jupyter expert, so this article will be brief on how I was able to connect to an Azure SQL Database using Microsoft Entra ID authentication and run a query.\\n\\nTo run a Jupyter Notebook, you can install [Anaconda](https://www.anaconda.com/products/distribution \\"Anaconda\\") and then use that to download Juypter to run the notebooks from a locally _(or server)_ hosted web-based interface.\\n\\nHowever, today I will be using Visual Studio Code with the [Jupyter extension](https://code.visualstudio.com/docs/datascience/jupyter-notebooks \\" Jupyter Notebooks in VS Code \\") on a windows endpoint.\\n\\nMake sure you install:\\n\\n* [Python](https://www.python.org/downloads/ \\"Python\\")\\n* [pyodbc](https://pypi.org/project/pyodbc/ \\"pyodbc \\") library\\n* [Microsoft ODBC Driver for SQL Server](https://learn.microsoft.com/en-us/sql/connect/odbc/microsoft-odbc-driver-for-sql-server \\"Microsoft ODBC Driver for SQL Server\\") _(has to be v17 or newer to support Microsoft Entra ID authentication)_.\\n* [Visual Studio Code](https://code.visualstudio.com/ \\"Visual Studio Code\\") + [Jupyter extension](https://code.visualstudio.com/docs/datascience/jupyter-notebooks \\" Jupyter Notebooks in VS Code\\")\\n\\n_Note: Jupyter notebook extensions end in \'*.ipynb\'._\\n\\nOnce all the prerequisites are installed, it\'s time to create the Notebook.\\n\\n1. Open **Visual Studio Code**\\n2. Click **File**, **New File**\\n3. Select **Jupyter Notebook**\\n4. Press **+ Code** _(to add a Code snippet)_\\n5. First, we need to import the pyodbc library:\\n\\n       #Libraries\\n       import pyodbc\\n\\nThen we need to **add the snippet to connect to the SQL database** _(this can be in a separate Codeblock or the same code block, as long as the import is run before the SQL connection is made - **make sure you update the server and database variables,** to match your environment!)_:\\n\\n    #Connection to SQL database\\n    \\n    \\n    server = \'tcp:SQLSERVER.database.windows.net\' \\n    database = \'DBNAME\' \\n    username = \'user@contoso.com\' \\n    password = \'password\' \\n    \\n    \\n    connection = pyodbc.connect(\'Driver={ODBC Driver 18 for SQL Server};Server=\'+server+\',1433;Database=\'+database+\';Uid=\'+username+\';Pwd=\'+password+\';Encrypt=yes;TrustServerCertificate=no;Connection Timeout=180;Authentication=ActiveDirectoryInteractive\')\\n    cursor = connection.cursor()\\n\\nThe \'Authentication=ActiveDirectoryInteractive\' parameter as part of the Connection string will prompt an interactive Microsoft Entra ID prompt to display and ask for credentials to be logged in; this includes MFA support. Using this method, the username and password variables are simply placeholders.\\n\\nIf you want to hardcode credentials into the Notebook _(not recommended)_, you can remove the \'_Authentication=ActiveDirectoryInteractive_\' section and enter the credentials into the username and password field.\\n\\nNow that we have connected to the database, let us **run a test query to obtain the SQL version**:\\n\\n    #Sample select query\\n    cursor.execute(\\"SELECT @@version;\\")\\n    row = cursor.fetchone()\\n    while row:\\n    print(row[0])\\n    row = cursor.fetchone()\\n\\n![Jupyter python SQL connection](/uploads/juptyer_notebook_query.png)\\n\\nCongratulations, you have successfully connected to an Azure SQL database and ran a query against the database.\\n\\nIf the connection to SQL appears to be stalling, check to make sure the Azure authentication window, isn\'t hidden behind another window.\\n\\n_A_ [_GIST_](https://gist.github.com/lukemurraynz/6636632309bc2bf2b1b37676ee0881ce \\"python.sqldb.text\\") _has been created, with the code as well, in case issues are copied from the website._"},{"id":"/2022/08/09/create-a-public-holidays-api-using-microsoft-azure","metadata":{"permalink":"/2022/08/09/create-a-public-holidays-api-using-microsoft-azure","source":"@site/blog/2022-08-09-create-a-public-holidays-api-using-microsoft-azure.md","title":"Create a Public Holidays API using Microsoft Azure","description":"Using a previous blog post I did on using a third-party API (Application Programming Interface) to start a Virtual Machine when it wasn\'t a Public Holiday, I had a thought on what could be an option if I wanted an API only accessible on an internal network or if I wanted to include custom Holidays such as Star Wars day or company holidays? Could I create and query my API using Microsoft Azure services? You can!","date":"2022-08-09T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":14.375,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Create a Public Holidays API using Microsoft Azure","authors":["Luke"],"tags":["Azure"],"date":"2022-08-09 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/azureportal_storagebrowser_api_table.png"}},"unlisted":false,"prevItem":{"title":"Connect to Azure SQL Database in a Jupyter Notebook using Python","permalink":"/2022/08/17/connect-to-azure-sql-database-in-a-jupyter-notebook-using-python"},"nextItem":{"title":"Microsoft Azure - ZonalAllocationFailed","permalink":"/azure/microsoft-azure-zonalallocationfailed"}},"content":"Using a previous [blog post](https://luke.geek.nz/azure/turn-on-a-azure-virtual-machine-using-azure-automation/ \\"Turn on a Azure Virtual Machine using Azure Automation\\") I did on using a third-party API _(Application Programming Interface)_ to start a Virtual Machine when it wasn\'t a Public Holiday, I had a thought on what could be an option if I wanted an API only accessible on an internal network or if I wanted to include custom Holidays such as Star Wars day or company holidays? Could I create and query my API using Microsoft Azure services? You can!\\n\\n### Overview\\n\\n> Today we will create a base Public Holidays API using several Microsoft Azure serverless services, such as Azure Function, Azure Storage Account and API Management.\\n\\n_Note: As this is a demonstration, I will be using a_ [_Consumption-based Azure Function_](https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale?WT.mc_id=AZ-MVP-5004796 \\"Azure Functions hosting options\\") _and Azure storage account, and although it is a good place to start - depending on your requirements, you may be better off with Azure Function Premium Plan to avoid cold-start times, and if you need a high amount of requests and writes (GET and POSTs) and resiliency, then replace the Storage account table with a_ [_Cosmos DB_](https://learn.microsoft.com/en-us/azure/cosmos-db/introduction?WT.mc_id=AZ-MVP-5004796 \\"Azure Cosmos DB\\")_._\\n\\nThe solution will be made up of the following:\\n\\n| Azure Service | Name | Plan | Note |\\n| --- | --- | --- | --- |\\n| Application Insights | ai-nzPublicHolidays-prd-ae |  |  |\\n| Azure API Management | apims-publicholidays-prd-ae | Developer (No SLA) |  |\\n| Azure Function | func-nzpublicHolidays-prd-ae | Function App - Consumption |  |\\n| Azure Storage Account | funcnzpublicholidaystgac | StorageV2 (general purpose v2) - Locally-redundant storage (LRS) | Contains \'PublicHolidays\' table |\\n| Azure Storage Account | rgnzpublicholidayspb4ed | Storage (general purpose v1)  - Locally-redundant storage (LRS) | Contains Azure Functions App Files |\\n| Resource Group | rg-publicholidays-prd-ae |  | Resource Group - containing above resources. |\\n\\n![Azure Resource Group - Diagram](/uploads/api_resourcegroupazviz.png \\"Azure Resource Group - Diagram\\")\\n\\n#### Pre-requisites\\n\\n* An Azure subscription _(with at least Contributor rights to a Resource Group)_.\\n* Azure PowerShell modules _(_[_Az.Accounts_](https://learn.microsoft.com/en-us/powershell/module/az.accounts/?view=azps-8.2.0&WT.mc_id=AZ-MVP-5004796 \\"Az.Accounts\\")_,_ [_Az.Storage_](https://learn.microsoft.com/en-us/powershell/module/az.storage/?view=azps-8.2.0&WT.mc_id=AZ-MVP-5004796 \\"Az.Storage\\") _&_ [_AzTables_](https://www.powershellgallery.com/packages/AzTable/ \\"AzTable\\")_)_\\n\\n_Note: AzTables is not part of the standard Az PowerShell module set and is a separate module you will need to install (Install-Module AzTables)._\\n\\nWe will use a mix of the Azure Portal and PowerShell to deploy this solution from start to finish; you can find the source data and code directly in the GitHub repository here: [lukemurraynz/PublicHoliday-API](https://github.com/lukemurraynz/PublicHoliday-API \\"PublicHoliday-API\\") for reference _(feel free to fork, raise pull requests etc.)._ In this guide, I will try not to assume preexisting knowledge _(other than general Azure and PowerShell knowledge)_.\\n\\n### Deployment\\n\\nThe deployment steps will be separated into different sections to help simplify implementation.\\n\\n> First, make sure you adjust the names of your resources and locations to suit your naming conventions and regional locations _(such as Australia East or West Europe)_. Your deployments may fail if a name is already in use. See \\"[Microsoft Azure Naming Conventions](https://luke.geek.nz/azure/microsoft-azure-naming-conventions/ \\"Microsoft Azure Naming Conventions \\")\\" for more about Naming conventions.\\n\\n#### Create Resource Group\\n\\nThe Resource Group will contain all resources related to the API that we will deploy today.\\n\\n_However, I recommend you consider what resources might be shared outside of this API - such as API Management, and put them in a separate Shared or Common Resource Group, to keep the li.e.ecycle of your resources together (ie API resources all in one place, so if it gets decommissioned, it is as easy a deleting the Resource Group)._\\n\\n1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. Click **Click on the burger and click** [**Resource groups**](https://portal.azure.com/#view/HubsExtension/BrowseResourceGroups \\"Resource Groups\\")\\n3. Click **+ Create**\\n4. Select your **Subscription**\\n5. Type in a name for your **Resource Group** _(like \'rg-publicholidays-prd-ae\')_\\n6. Select your **Region** and click **Next: Tags**\\n7. Enter in applicable **tags** _(i.e. Application: Public Holidays API)_\\n8. Click **Next: Review + create**\\n9. Click **Create**\\n\\n![Create a resource group](/uploads/azureportal_creatergapi.png \\"Create a resource group\\")\\n\\nIf you prefer PowerShell, you can deploy a new Resource Group with the below:\\n\\n    New-AzResourceGroup -Name \'rg-publicholidays-prd-ae\' -Location \'Australia East\' -Tag @{Application=\\"Public Holidays API\\"}\\n\\n#### Create Storage Account\\n\\nNow that the Resource Group has been created, it\'s time to import our Storage Account - which will hold our Table of data around Public Holidays.\\n\\n 1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Click **Click on the burger and click** [**Storage Accounts**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts \\"Storage accounts\\")\\n 3. Click **+ Create**\\n 4. Select the **Subscription** and **Resource Group** you created earlier\\n 5. Enter in a **Name** for your **Storage Account** (_like \'funcnzpublicholidaystgac\')_\\n 6. Select your **Region** _(i.e. Australia East)_\\n 7. For **Performance**, I am going to select: **Standard**\\n 8. For **Redundancy**, as this is a demo, I will select **Locally-redundant storage (LRS). However, if** you plan on running this in production, you may consider ZRS for zone redundancy.\\n 9. If you plan on locking down the Storage Account to your Virtual Network or specific IP addresses, continue to the Networking Tab; we can accept the defaults and click: **Review**.\\n10. Click **Create**\\n\\nIf you prefer PowerShell, you can deploy a new Storage account with the below:\\n\\n    New-AzStorageAccount -ResourceGroupName \'rg-publicholidays-prd-ae\' -Name \'funcnzpublicholidaystgac\' -Location \'Australia East\' -SkuName \'Standard_LRS\' -Kind StorageV2\\n\\n#### Import Public Holiday data\\n\\n##### Create Azure Storage Account Table\\n\\nNow that we have the Storage account that will hold our Public Holiday time to import the data.\\n\\nMost of this task will be done with PowerShell, but first, we need to create the Table that will hold our Public Holidays.\\n\\n1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. Click **Click on the burger and click** [**Storage Accounts**](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts \\"Storage accounts\\")\\n3. **Navigate** to your created **Storage** account\\n4. In the Navigation blade, click **Tables**\\n5. Click **+ Table**\\n6. For a Table Name, I will go with **PublicHolidays**\\n7. Click **Ok**\\n\\n![Create Azure Storage Account Table](/uploads/azureportal_createpublicholidaystable.png \\"Create Azure Storage Account Table\\")\\n\\nYou can use PowerShell to create the Table below:\\n\\n    $storageAccount = Get-AzStorageAccount -ResourceGroupName \'rg-publicholidays-prd-ae\' -Name \'funcnzpublicholidaystgac\'\\n    $storageContext = $storageAccount.Context\\n    New-AzStorageTable -Name \'PublicHolidays\' -Context $storageContext\\n\\n##### Import Public Holiday Data into Table\\n\\nNow that we have the Azure storage account and PublicHolidays table, it\'s time to import the data.\\n\\nIf you want to do this manually, the Azure Table will have the following columns:\\n\\n| Date | Country | Type | Name | Day | Year | Comments |\\n\\nWe could enter the data manually, but I will leverage the Nager API to download and parse a CSV file for a few countries. You can find the source data and code directly in the GitHub repository here: [lukemurraynz/PublicHoliday-API](https://github.com/lukemurraynz/PublicHoliday-API \\"PublicHoliday-API\\") for reference.\\n\\nTo do this, we will need PowerShell, so assuming you have logged into PowerShell and set the context to your Azure subscription, let us continue.\\n\\nI have created a CSV _(Comma-separated values)_ file with a list of countries _(i.e. US, NZ, AU)_ called \'SourceTimeDate.CSV\', but you can adjust this to suit your requirements and place it in a folder on my C:\\\\\\\\ drive called: Temp\\\\\\\\API.\\n\\nOpen **PowerShell** and **run** the following:\\n\\n    $Folder = \'C:\\\\Temp\\\\API\\\\\'\\n    $Csv = Import-csv \\"$Folder\\\\DateTimeSource\\\\SourceTimeDate.csv\\"\\n    $CurrentYear = (Get-Date).Year\\n    \\n       ForEach ($Country in $Csv)\\n      {\\n    $CountryCode = $Country.Country\\n    Invoke-WebRequest -Uri \\"https://date.nager.at/PublicHoliday/Country/$CountryCode/$CurrentYear/CSV\\" -OutFile \\"$FolderAPI\\\\DateTimeSource\\\\Country$CountryCode$CurrentYear.csv\\" \\n    }\\n\\nThese cmdlets will download a bunch of CSV files into the API folder, with the Public Holidays for each Country for this year, and then you can adjust the $CurrentYear variable for future years _(i.e. 2025)_.\\n\\nOnce you have all the CSV files for your Public Holidays and before we import the data into the Azure storage table, now is the time to create a new Custom Holidays CSV; you can easily use an existing one to create a new CSV containing your company\'s public holidays or other days that may be missing from the standard list, make sure it matches the correct format and save it into the same folder.\\n\\n![Custom Public Holidays API](/uploads/customholidays_api.png \\"Custom Public Holidays API\\")\\n\\nNow that you have all your CSV files containing the Public Holidays in your Country or countries, it\'s time to import them into the Azure Table. First, we import the data using a PowerShell session logged into Azure.\\n\\n    # Imports Public Holiday into Azure Storage table\\n    # Requires AzTable Module (not part of the normal Az cmdlets)\\n    Import-Module AzTable\\n    \\n    #Imports data from CSV files into $GLobalHolidays variable\\n    $Folder = \'C:\\\\Temp\\\\API\\\\\'\\n    \\n    $GlobalHolidays = Get-ChildItem \\"$Folder\\\\DateTimeSource\\\\*.csv\\" | Foreach-Object {\\n      $basename = $_.BaseName\\n      import-csv $_ \\n    }\\n    \\n    #Connect-AzAccount\\n    #Connects to Azure Storage Account\\n    $storageAccountName = \'funcnzpublicholidaystgac\'\\n    $resourceGroupName = \'rg-publicHolidays-prd-ae\'\\n    $tableName = \'PublicHolidays\'\\n    $storageAccount = Get-AzStorageAccount -ResourceGroupName $resourceGroupName -Name $storageAccountName\\n    $storageContext = $storageAccount.Context\\n    $cloudTable = (Get-AzStorageTable -Name $tableName -Context $storageContext).CloudTable\\n    \\n      \\n    #Imports CSV data into Azure Table\\n    $counter = 0\\n    ForEach ($Holiday in $GlobalHolidays)\\n    \\n    {\\n      $Date = [DateTime]($Holiday.Date)\\n      $Dayofweek = $Date.DayOfWeek | Out-String\\n      $Year = $Date.Year\\n      $HolidayDate = Get-Date $Date -format \\"dd-MM-yyyy\\"\\n    \\n     Add-AzTableRow `\\n      -table $cloudTable `\\n      -partitionKey \'1\' `\\n      -rowKey ((++$counter)) -property @{\\"Date\\"=$HolidayDate;\\"Country\\"=$Holiday.CountryCode;\\"Type\\"=$Holiday.Type;\\"Name\\"=$Holiday.LocalName;\\"Day\\"=$Dayofweek;\\"Year\\"=$Year;\\"Comments\\"=$Holiday.Counties}\\n    \\n    }\\n    \\n    \\n    #Validate the data in the Storage table\\n    Get-AzTableRow -table $cloudTable\\n\\n![Import CSV to Azure Storage Account](/uploads/import-csvtoazuretables.gif \\"Import CSV to Azure Storage Account\\")\\n\\n![Validate Azure Storage Account Table](/uploads/import-csvtoazuretablesvalidate.gif \\"Validate Azure Storage Account Table\\")\\n\\nNow the Public Holidays are imported into the Azure storage account table with additional information, such as the Day it falls, and the Date format has been changed to suit the NZ format _(DD-MM-YYYY)_.\\n\\nIf we log in to the Azure Portal, navigate to the Storage account and under Storage Browser, we can now see our Table is full of Public Holidays.\\n\\n![](/uploads/azureportal_storagebrowser_api_table.png)\\n\\n#### Create API\\n\\nThat we have our Table with Public Holiday data, it\'s time to create our Azure Function to act as the API that will talk to the azure storage account!\\n\\n##### Create Azure Function\\n\\n 1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Click **Click on the burger and click** [**Resource groups**](https://portal.azure.com/#view/HubsExtension/BrowseResourceGroups \\"Resource groups\\")\\n 3. Navigate to your **resource group** and click **+ Create**\\n 4. Search for: **Function**\\n 5. Select **Function App**, and click **Create**\\n 6. Enter your **Function App Name** _(i.e. \'func-nzpublicHolidays-prd-ae\')_\\n 7. For Runtime Stack, select **PowerShell Core**\\n 8. Select the latest version (at this time, it\'s **7.2**)\\n 9. Select your **Region**\\n10. Select **Windows**\\n11. Set your **Plan** _(in my example, its Consumption (Serverless))_\\n12. Click **Review + Create**\\n13. Click **Create**\\n\\n![Azure Function - Create](/uploads/azureportal_createazfunctionapiwin.png \\"Azure Function - Create\\")\\n\\n##### Configure Environment Variables\\n\\nNow that the Function App has been created before creating the GetPublicHoliday function, we need to add a few environment variables that the Function will use; these variables will contain the ResourceGroup and Storage account name.\\n\\n1. **Navigate** to your **Azure Function**\\n2. Click **Configuration**\\n3. Click **+ New application setting**\\n4. Under the name, add: **PublicHolidayRESOURCEGROUPNAME**\\n5. For value, type in the name of your resource group.\\n6. Add a second application setting named: **PublicHolidaySTORAGEACCNAME**\\n7. For value, type in the name of your storage account that contains the Public Holiday table.\\n8. Click **Save** _(to save the variables)_.\\n\\n![Azure Function - Variables](/uploads/azureportal_funcapp_api_variables.png \\"Azure Function - Variables\\")\\n\\n##### Configure Managed Identity\\n\\nNext, we need to give the Function App the ability to read the Azure storage account. To do this, we need to configure a System assigned managed identity.\\n\\n 1. **Navigate** to your **Azure Function**\\n 2. Click **Identity**\\n 3. Under the System assigned heading, toggle the status to **On**\\n 4. Click **Save**\\n 5. Select **Yes**, to enable the System assigned managed identity\\n 6. Under Permissions, click **Azure role assignments**\\n 7. Click **+ Add role assignment**\\n 8. For Scope, select **Storage**\\n 9. Select your Subscription and storage account containing your Public Holiday data\\n10. For role, select **Contributor** _(Storage Table Data Reader is not enough)._\\n11. Click **Save**\\n\\n##### Configure Requirements\\n\\nThe Azure function app will rely on a few PowerShell Modules; for the FunctionApp to load them, we need to add them to the requirements.psd1 file.\\n\\n1. **Navigate** to your **Azure Function**\\n2. Click **App files**\\n3. Change the dropdown to **requirements.psd1**\\n4. In the hash array, comment out the #Az module line _(as this will load the entire Az Module set, which will cause an increased delay in the startup as those extra modules aren\'t needed)_, and add the following:\\n\\n       # This file enables modules to be automatically managed by the Functions service.\\n       # See https://aka.ms/functionsmanageddependency for additional information.\\n       #\\n       @{\\n           # For latest supported version, go to \'https://www.powershellgallery.com/packages/Az\'. \\n           # To use the Az module in your function app, please uncomment the line below.\\n           #\'Az\' = \'8.*\'\\n           \'Az.Accounts\'  = \'2.*\'\\n           \'Az.Storage\'   = \'4.*\'\\n           \'Az.Resources\' = \'2.*\'\\n           \'AzTable\'      = \'2.*\'\\n       \\n       }\\n5. Click **Save**\\n\\n##### Create Function PublicHolidays\\n\\nNow that the Function App has been configured, it is time to create our Function.\\n\\n1. **Navigate** to your **Azure Function**\\n2. Click **Functions**\\n3. Click **+ Create**\\n4. Change Development environment to **Develop in Portal**\\n5. Select Template, an **HTTP trigger**\\n6. For the New Function name, I will go with **GetPublicHoliday**\\n7. Change Authorization level to **Anonymous** _(if you aren\'t going to implement API Management, select Function and look at whitelisting your IP only, we will be locking it down to API Management later)_.\\n8. Click **Create**\\n\\n![Create Azure Function App](/uploads/azureportal_createfunctionpublicholidays.png \\"Create Azure Function App\\")\\n\\n1. Click **Code + Test**\\n2. Copy the following Code into the run.ps1 file, this code is core to the Function that will read the HTTP request and bring back a PowerShell object with the Public Holiday information as part of a GET request:\\n\\n       <# The code above does the following, explained in English:\\n       1. Read the query parameters from the request.\\n       2. Read the body of the request.\\n       3. Write to the Azure Functions log stream.\\n       4. Interact with query parameters or the body of the request.\\n       5. Associate values to output bindings by calling \'Push-OutputBinding\'. \\n       https://luke.geek.nz/ #>\\n       \\n       using namespace System.Net\\n       \\n       # Input bindings are passed in via param block.\\n       param([Parameter(Mandatory = $true)]$Request, [Parameter(Mandatory = $true)]$TriggerMetadata)\\n       \\n       # Write to the Azure Functions log stream.\\n       Write-Host \'GetPublicHoliday function processed a request.\'\\n       \\n       \\n       # Interact with query parameters or the body of the request.\\n       $date = $Request.Query.Date\\n       $country = $Request.Query.CountryCode\\n       \\n       $resourceGroupName = $env:PublicHolidayRESOURCEGROUPNAME\\n       $storageAccountName = $env:PublicHolidaySTORAGEACCNAME\\n       $tableName = \'PublicHolidays\'\\n       \\n       $ClientIP = $Request.Headers.\\"x-forwarded-for\\".Split(\\":\\")[0]\\n            \\n       try {   \\n           \\n         $storageAccount = Get-AzStorageAccount -ResourceGroupName $resourceGroupName -Name $storageAccountName\\n         $storageContext = $storageAccount.Context\\n         $cloudTable = (Get-AzStorageTable -Name $tableName -Context $storageContext).CloudTable\\n         Import-Module AzTable   \\n             \\n         $Tables = Get-AzTableRow -table $cloudTable \\n       \\n       \\n         ForEach ($table in $Tables)\\n         {\\n       \\n       \\n           [string]$Filter1 = [Microsoft.Azure.Cosmos.Table.TableQuery]::GenerateFilterCondition(\\"Country\\", [Microsoft.Azure.Cosmos.Table.QueryComparisons]::Equal, $country)\\n           [string]$Filter2 = [Microsoft.Azure.Cosmos.Table.TableQuery]::GenerateFilterCondition(\\"Date\\", [Microsoft.Azure.Cosmos.Table.QueryComparisons]::Equal, $date)\\n           [string]$finalFilter = [Microsoft.Azure.Cosmos.Table.TableQuery]::CombineFilters($Filter1, \\"and\\", $Filter2)     \\n           $object = Get-AzTableRow -table $cloudTable -CustomFilter $finalFilter\\n          \\n          \\n           $body = @()\\n            \\n           $System = New-Object -TypeName PSObject\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name CountryCode -Value   $object.Country\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name HolidayDate -Value   $object.Date\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name HolidayYear -Value   $object.Year\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name HolidayName -Value   $object.Name\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name HolidayType -Value   $object.Type\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name Comments -Value      $object.Comments\\n           Add-Member -InputObject $System -MemberType NoteProperty -Name RequestedIP -Value   $ClientIP\\n       \\n           $body += $System\\n           $System = New-Object -TypeName PSObject\\n            \\n           $status = [Net.HttpStatusCode]::OK\\n       \\n         }\\n         \\n       \\n       }\\n       catch {\\n         $body = \\"Failure connecting to table for state data, $_\\"\\n         $status = [Net.HttpStatusCode]::BadRequest\\n       }\\n       #$body =  $TriggerMetadata\\n       \\n       \\n       # Associate values to output bindings by calling Push-OutputBinding\'\\n       Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{\\n           StatusCode = $status\\n           Body       = $body\\n         }\\n       )\\n3. Click **Save**\\n\\n##### Test Function - PublicHolidays\\n\\nBefore proceeding with the next step, it\'s time to test the function.\\n\\n1. **Navigate** to your **Azure Function**\\n2. Click **Functions**\\n3. Click **GetPublicHoliday**\\n4. Click **Code + Test**\\n5. Click **Test/Run**\\n6. Change HTTP method to **Get**\\n7. Under Query, add Country value and Date value.\\n\\nNote: Make sure the date and country formats match what is in the Azure storage account.\\n\\nYou can also Invoke the function app directly with PowerShell, with the Date and Country as Parameters at the end:\\n\\n    Invoke-RestMethod -URI \\"https://func-nzpublicholidays-prd-ae.azurewebsites.net/api/GetPublicHoliday?Date=25-12-2023&CountryCode=NZ\\"\\n\\n![Test Public Holiday API](/uploads/testpublicholidayapi.gif \\"Test Public Holiday API\\")\\n\\nCongratulations! You have now created a Public Holiday API that you can call for automation! You can lock down the Function App to only certain IPs or proceed to configure Azure API Management.\\n\\n##### Configure Azure API Management\\n\\nNow that the Function App responds to requests, we can expose the HTTP endpoint through [Azure API Management](https://learn.microsoft.com/en-us/azure/azure-functions/functions-openapi-definition?WT.mc_id=AZ-MVP-5004796 \\"Expose serverless APIs from HTTP endpoints using Azure API Management\\"). Azure API Management will give greater flexibility and security over API endpoints, particularly when dealing with more than one API. Azure API Management also offers inbuilt shared cache functionality and integration into [Azure Cache for Redis](https://azure.microsoft.com/en-us/services/cache/?WT.mc_id=AZ-MVP-5004796 \\" Azure Cache for Redis\xae2\\").\\n\\n1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. **Navigate** to your **Azure Function**\\n3. On the Navigation blade, select **API Management**\\n4. Click **Create New**\\n5. Select your subscription, **Region**, and organisation **name**.\\n6. Select a [**Pricing Tier**](https://azure.microsoft.com/en-us/pricing/details/api-management/?WT.mc_id=AZ-MVP-5004796 \\" API Management pricing\\")\\n7. Click **Review + Create**\\n8. Click **Create**\\n\\n![Create Azure API Management](/uploads/azureportal_createapimanagement.png)\\n\\n1. Wait for 10 minutes to half an hour for provisioning to take place, and Azure API Management will be in an activating state.\\n2. Once API Management has been provisioned, you can **copy** the **Virtual IP (VIP)** addresses of API Management and **restrict** your **function** app to **only** allow **inbound** access from that **IP**.\\n3. Once you have done that, add the GetPublicHoliday function app into Azure API Management, add the paths to add a version, and then, using the subscription key, you run the following command to pull data.\\n\\n    Invoke-RestMethod -uri \\"https://apims-nzpublicholidays-prd-ae.azure-api.net/v1/GetPublicHoliday?Date=4/05/2022&CountryCode=NZ&Ocp-Apim-Subscription-Key=$KEY\\""},{"id":"azure/microsoft-azure-zonalallocationfailed","metadata":{"permalink":"/azure/microsoft-azure-zonalallocationfailed","source":"@site/blog/2022-07-27-microsoft-azure-zonalallocationfailed.md","title":"Microsoft Azure - ZonalAllocationFailed","description":"Error code: AllocationFailed or ZonalAllocationFailed","date":"2022-07-27T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.465,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Azure - ZonalAllocationFailed","authors":["Luke"],"tags":["Azure"],"date":"2022-07-27 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/zonalallocationfailed.png"},"slug":"azure/microsoft-azure-zonalallocationfailed"},"unlisted":false,"prevItem":{"title":"Create a Public Holidays API using Microsoft Azure","permalink":"/2022/08/09/create-a-public-holidays-api-using-microsoft-azure"},"nextItem":{"title":"Azure Virtual Machine and a custom MAC address","permalink":"/azure/azure-virtual-machine-and-mac-address-software-licensing"}},"content":"> **Error code**: AllocationFailed or ZonalAllocationFailed\\n>\\n> **Error message**: \\"Allocation failed. We do not have sufficient capacity for the requested VM size in this region. Read more about improving likelihood of allocation success at [https://aka.ms/allocation-guidance](https://aka.ms/allocation-guidance?WT.mc_id=AZ-MVP-5004796 \\"https://aka.ms/allocation-guidance\\")\\"\\n\\nWhen you create a virtual machine _(VM)_, start stopped _(deallocated)_ VMs, or resize a VM, Microsoft Azure allocates compute resources to your subscription.\\n\\n![ZonalAllocationFailed](/uploads/zonalallocationfailed.png \\"ZonalAllocationFailed\\")\\n\\nMicrosoft is continually investing in additional infrastructure and features to ensure that they always have all VM types available to support customer demand. However, you may occasionally experience resource allocation failures because of unprecedented growth in demand for Azure services in specific regions.\\n\\nThese tips, also apply to the \'Following SKUs have failed for Capacity Restrictions\' error.\\n\\nThis error could also be caused by a parameter issue with your Infrastructure as Code deployments, if you are to restrictive and it attempts to create a resource that isn\'t supported - an example is a SKU that doesn\'t support Accelerated Networking, or an attempt to deploy an Ultra SSD disk for a SKU that doesn\'t deploy it.\\n\\n#### **Waiting for more Compute to be added to the Azure server clusters may not be an option, so what can you do?**\\n\\n##### Raise a Support Case\\n\\n* Take a screenshot of the error\\n* Copy the Activity/Deployment ID\\n* Take note of the Region\\n* Take note of the Availability Zone.\\n\\nLet [Azure Support](https://azure.microsoft.com/en-us/support/?WT.mc_id=AZ-MVP-5004796 \\" Azure Support\\") know; that Microsoft may already be aware, but raising a support request helps identify potentially impacted customers. If you know of other SKUs you need to deploy, you can let them know.\\n\\n##### Purchase On-demand Capacity Reservation\\n\\n[On-demand Capacity Reservation](https://learn.microsoft.com/en-us/azure/virtual-machines/capacity-reservation-overview?WT.mc_id=AZ-MVP-5004796 \\"On-demand Capacity Reservation\\") enables you to reserve Compute capacity in an Azure region or an Availability Zone for any duration of time. \\n\\nUnlike [Reserved Instances](https://azure.microsoft.com/en-us/pricing/reserved-vm-instances/?WT.mc_id=AZ-MVP-5004796 \\"Reserved Instances\\"), you do not have to sign up for a 1-year or a 3-year term commitment.\\n\\nOnce the Capacity Reservation is created, the capacity is available immediately and is exclusively reserved for your use until the reservation is deleted.\\n\\nCapacity Reservations are priced at the same rate as the underlying VM size. \\n\\nFor example, if you create a reservation for the D2s_v3 VMs, you will start getting billed for the D2s_v3 VMs, even if the reservation is not being used.\\n\\nSo why would you purchase On-demand Capacity reservations?\\n\\n* You are operating Azure workloads that scale out and run off a fresh image, like a Citrix farm and want to ensure the capacity is available for the minimum workloads you need.\\n* You have a project coming up where you need the capacity to be available.\\n\\n##### Redeploy to another Availability Zone\\n\\nThe server cluster that ARM (Azure Resource Manager) attempted to deploy your workload may not have the necessary capacity, but another Availability Zone _(datacenter)_ might.\\n\\nMake sure your Virtual Machine is not in a Proximtry or Avalibility Group and do the following.\\n\\n1. **Take note of the Availability Zone that your deployment failed** _(i.e. Availability Zone 1)_\\n2. **Remove any resources** that may have been created as part of the original failed deployment.\\n3. **Redeploy** your workload and **select** another Availability **Zone**, such as _(2 - if your failed deployment was in Zone 1)_\\n\\n##### Change the Virtual Machine version  \\n\\nBy version, I don\'t mean [Generation 1 and Generation 2](https://learn.microsoft.com/en-us/azure/virtual-machines/generation-2?WT.mc_id=AZ-MVP-5004796#features-and-capabilities \\"Generation 1 vs. generation 2 features\\") Virtual Machines; I mean the version of underlying Compute; when you look at a VM SKU size, you will see:\\n\\n> Standard_DC24s_**v3**\\n>\\n> \\\\[Family\\\\] + _\\\\[Sub-family\\\\]_* + \\\\[# of vCPUs\\\\] + _\\\\[Constrained vCPUs\\\\]_* + \\\\[Additive Features\\\\] + _\\\\[Accelerator Type\\\\]_* + **\\\\[Version\\\\]**\\n\\nYou can read more about Virtual Machine Naming conversions \\"[here](https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions?WT.mc_id=AZ-MVP-5004796 \\"Azure virtual machine sizes naming conventions\\")\\".\\n\\nThe version of the VM series links to the underlying hardware associated with the Virtual Machine series; with most new hardware releases, the version changes; an example is: from v3 to v4.  \\n\\n> #Tip: Microsoft may run a promotion on the pricing for early adopters from time to time, to move to the new version; they can be seen from the Azure Portal with \\"Promo\\" in the name.\\n\\n1. You can **change the version of the SKU** by looking in the Azure Portal, **Sizing**, and you should be **select** different **versions** of the same SKU; _if you are at v5, try resizing to v4 - or the other way around_.  \\n\\nRemember that changing the VM SKU will force the Virtual Machine to deallocate _(stop)_, as it triggers ARM to stand up the Virtual Machine on different server clusters/hardware.\\n\\n_I have found that there are no noticeable decreases in performance for most workloads, but keep in mind you may be returning on older hardware - but it should get you going, and then you can update the SKU to the latest version later._\\n\\n\\n##### Increase regional vCPU quotas\\n\\nAzure Resource Manager enforces two types of vCPU quotas for virtual machines:\\n\\n* standard vCPU quotas\\n* spot vCPU quotas\\nStandard vCPU quotas apply to pay-as-you-go VMs and reserved VM instances. They are enforced at two tiers, for each subscription, in each region:\\n\\n* The first tier is the total regional vCPU quota.\\n* The second tier is the VM-family vCPU quota such as D-series vCPUs.\\n\\nCheck your subscription quotas and if necessary, raise a request to increase them by following the guide here: [Increase regional vCPU quotas]{https://learn.microsoft.com/azure/quotas/regional-quota-requests?WT.mc_id=AZ-MVP-5004796}"},{"id":"azure/azure-virtual-machine-and-mac-address-software-licensing","metadata":{"permalink":"/azure/azure-virtual-machine-and-mac-address-software-licensing","source":"@site/blog/2022-07-19-azure-virtual-machine-and-mac-address-software-licensing.md","title":"Azure Virtual Machine and a custom MAC address","description":"You may need an Azure Virtual Machine to install or license software bound to a media access control address (MAC address).","date":"2022-07-19T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.005,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-07-19 00:00:00 +1200","title":"Azure Virtual Machine and a custom MAC address","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/azure-virtual-machine-and-mac-address-software-licensing"},"unlisted":false,"prevItem":{"title":"Microsoft Azure - ZonalAllocationFailed","permalink":"/azure/microsoft-azure-zonalallocationfailed"},"nextItem":{"title":"Deploy Azure Naming Tool into an Azure WebApp as a container","permalink":"/azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container"}},"content":"You may need an Azure Virtual Machine to install or license software bound to a media access control address _(MAC address)_.\\n\\nIn Microsoft Azure, you can make changes to the Primary Network interface; these changes include manually setting the IP settings to changing the MAC address - these settings are managed by the underlying Network Interface and Azure host.\\n\\n> _If you do inadvertently make changes to this, you will lose connection to the Azure Virtual Machine, however, don\'t panic! Until its rebooted and the configuration is reset by the Azure fabric._\\n\\nThis causes issues when the software is licensed to a specific MAC address; you could reissue the license to the new MAC address OR create a Secondary Interface in Microsoft Azure and update the MAC address on the secondary network interface.\\n\\nYou can easily [create a new Network Interface](https://learn.microsoft.com/en-us/azure/virtual-network/network-overview?WT.mc_id=AZ-MVP-5004796#network-interfaces \\"Network interfaces\\") from the Azure Portal and then attach it to the Virtual Machine (_the virtual machine needs to be off to allow the NIC to be attached)_.\\n\\n##### Change Network Adapter MAC using PowerShell\\n\\nOnce the NIC is created and attached, run the following PowerShell command in the Azure Virtual Virtual machine _(assuming this is a Windows OS, but the same process should work for_ [_Linux_](https://www.linuxshelltips.com/change-mac-address-linux/ \\"How to Change Network MAC Address in Linux\\")_)_:\\n\\n    Get-NetAdapter\\n\\nYou want to make sure you are targeting the right Network Adapter; in my example, it is the Hyper-V Interface #2 _(with #1 being my Primary NIC)_.\\n\\nAdd the new MAC address into the $MACAddress variable, and make sure you update the InterfaceDescription to match the Network Adapter you are targeting _(note the wildcard before the #2, this targets any network adapter with #2 at the end)_.\\n\\n    $MACAddress = \'000000000000\'\\n    $NetAdapter = Get-NetAdapter -InterfaceDescription \\"*#2\\"\\n    Set-NetAdapter $NetAdapter.Name -MacAddress $MACAddress\\n\\n##### Change Network Adapter MAC using Device Manager\\n\\nYou can also use Device Manager to check and update the MAC address:\\n\\n1. Open the Device Manager.\\n2. Expand the Network Adapters section.\\n3. Right-click on your adapter.\\n4. Click the Advanced tab.\\n5. Enter your new MAC address.\\n6. Reboot your computer to enable the changes.\\n7. Check that the changes took effect.\\n\\n> Finally - make sure you document this MAC address somewhere with the reasons WHY the change was made. You can also Tag the secondary MAC address in Azure with notes, such as the reason why it exists, who created it etc."},{"id":"azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container","metadata":{"permalink":"/azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container","source":"@site/blog/2022-07-11-deploy-azure-naming-tool-into-an-azure-webapp-as-a-container.md","title":"Deploy Azure Naming Tool into an Azure WebApp as a container","description":"Organising your cloud workloads to support governance, operational management, and accounting requirements can take a lot of effort before the first resource is created.","date":"2022-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":13.82,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-07-11 00:00:00 +1200","title":"Deploy Azure Naming Tool into an Azure WebApp as a container","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azurenamingtool_referencepage.png"},"slug":"azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container"},"unlisted":false,"prevItem":{"title":"Azure Virtual Machine and a custom MAC address","permalink":"/azure/azure-virtual-machine-and-mac-address-software-licensing"},"nextItem":{"title":"Add Custom DNS servers and set Azure Point to Site VPN to Connect automatically","permalink":"/azure/add-custom-dns-servers-and-set-azure-point-to-site-vpn-to-connect-automatically"}},"content":"Organising your cloud workloads to support governance, operational management, and accounting requirements can take a lot of effort before the first resource is created.\\n\\nWell-defined naming and metadata tagging conventions help to locate and manage resources quickly. These conventions also help associate cloud usage costs with business teams via chargeback and show-back accounting mechanisms, along with rapidly identifying what services are used across services.\\n\\nA useful naming convention composes resource names from important information about each resource. A well-chosen name helps you quickly identify the resource\'s type, associated workload, deployment environment, and the Azure region hosting it. Some resource names, such as PaaS services with public endpoints or virtual machine DNS labels, have global scopes, so they must be unique across the Azure platform.\\n\\nThere\'s no one size fits Azure naming convention; it needs to suit your organisation. However, it is worth noting that there are limitations to naming rules for Azure resources.\\n\\n* [Naming rules and restrictions for Azure resources](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-name-rules?WT.mc_id=AZ-MVP-5004796 \\"Naming rules and restrictions for Azure resources\\")\\n* [Define your naming convention](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming?WT.mc_id=AZ-MVP-5004796 \\"Define your naming convention\\")\\n\\nWith rules around naming resources that are Global, specific to Resource Groups or that have maximum character limits that can\'t contain specific characters - it can become a project on its own, the world of Cloud where resources are treated as [cattle and not pets](http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/ \\"The History of Pets vs Cattle and How to Use the Analogy Properly\\") - the effort to develop a proper naming convention, used across teams or even companies can be quite complex.\\n\\nThis is where the Azure Naming Tool, as part of the Microsoft Cloud Adoption framework, comes into play.\\n\\n### Overview\\n\\nThe Naming Tool _(v2 as of June 2022)_ was developed using a naming pattern based on [Microsoft\'s best practices](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/naming-and-tagging). Once the organisational components have been defined by an administrator, users can use the tool to generate a name for the desired Azure resource.\\n\\n![Azure \\\\[naming-tool\\\\]](/uploads/azurenamingtoollogo.png \\"Azure [naming-tool]\\")\\n\\nThis tool sitting in the [Azure Naming Tool](https://github.com/mspnp/AzureNamingTool) GitHub repository runs as a standalone Web _(.NET 6 Blazor application)_ application using stateless JSON files for its Configuration and offers users the ability to generate and customise their own [Microsoft Azure Naming convention](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming?WT.mc_id=AZ-MVP-5004796 \\"Define your naming convention\\") taking all the restrictions into account. In addition, Azure Naming Tool - also provides a Swagger API that can be used in your Infrastructure as Code deployments to generate the names of resources on the fly.\\n\\n![Azure Naming Tool - Reference](/uploads/azurenamingtool_referencepage.png \\"Azure Naming Tool - Reference\\")\\n\\nThis information is straight from the project README.md:\\n\\n> **Project Components**\\n>\\n> * UI/Admin\\n> * API\\n> * JSON configuration files\\n> * Dockerfile\\n>\\n> **Important Notes**\\n>\\n> The following are important notes/aspects of the Azure Naming Tool:\\n>\\n> * The application is designed to run as a stand-alone solution, with no internet/Azure connection.\\n> * The application can be run as a .NET 6 site, or as a Docker container.\\n> * The site can be hosted in any environment, including internal or in a public/private cloud.\\n> * The application uses local JSON files to store the configuration of the components.\\n> * The application requires persistent storage. If running as a container, a volume is required to store configuration files.\\n> * The application contains a _repository_ folder, which contains the default component configuration JSON files. When deployed, these files are copied to the _settings_ folder.\\n> * The Admin interface allows configurations to be \\"reset\\", if needed. This process copies the configuration from the _repository_ folder to the _settings_ folder.\\n> * The API requires an API Key for all executions. A default API Key (guid) will be generated on first launch. This value can be updated in the Admin section.\\n> * On first launch, the application will prompt for the Admin password to be set.\\n\\n### Deployment\\n\\n#### Prerequisites\\n\\nToday, we will deploy the Azure Naming Tool into an Azure WebApp, running as a Container.![Azure Naming Tool - High-Level Architecture](/uploads/azurenamingtool_architecture.png \\"Azure Naming Tool - High-Level Architecture\\")\\n\\nThe Azure resources we will create are:\\n\\n* [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/?WT.mc_id=AZ-MVP-5004796 \\" Azure Container Registry\\")\\n* [Azure Storage Account](https://azure.microsoft.com/en-us/products/category/storage/?WT.mc_id=AZ-MVP-5004796 \\"Storage\\") _(with File Share - to store our persistent data and Configuration)_\\n* [Azure App Service Plan and App Service for Containers](https://azure.microsoft.com/en-us/services/app-service/containers/?WT.mc_id=AZ-MVP-5004796 \\" Web App for Containers\\") _(to run our Azure Naming Tool)_\\n\\nYou need Contributor rights in at least a Resource Group to deploy these Azure resources.\\n\\nWe will be using a mix of services such as:\\n\\n* [Docker](https://www.docker.com/ \\"Docker\\")\\n* [PowerShell](https://learn.microsoft.com/en-us/powershell/?WT.mc_id=AZ-MVP-5004796 \\"PowerShell\\") & [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/?WT.mc_id=AZ-MVP-5004796 \\"Azure Command-Line Interface (CLI) documentation\\")\\n* [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/?WT.mc_id=AZ-MVP-5004796 \\"Bicep documentation\\")\\n\\nTo reduce the need to set up these dependencies on individual workstations, we will use a mix of the Azure Cloud Shell and Azure Portal. If you haven\'t set up your Azure Cloud Shell, you can refer to an article I wrote previously \\"[here](https://luke.geek.nz/2022/03/07/setup-azure-cloud-shell/ \\"Setup Azure Cloud Shell \\")\\" for this remainder of this article I am going to assume you have it set up already.\\n\\nNote: I will connect to the Cloud Shell using the [Windows Terminal](https://learn.microsoft.com/en-us/shows/it-ops-talk/azure-cloud-shell-in-the-windows-terminal?WT.mc_id=AZ-MVP-5004796 \\"Azure Cloud Shell in the Windows Terminal\\") so that any screenshots will be of the Terminal, but it\'s the same behaviour if I used the browser experience.\\n\\n#### Clone the Git Repository\\n\\nNow is time to clone the git repository into our Cloud Shell so that we can build the docker image definition.\\n\\n1. **Log in** to the [Microsoft Azure Portal](https://portal.azure.com/#home \\"Microsoft Azure - Portal\\") and open up the **Azure Cloud Shell** _(make sure you are in PowerShell (not Bash))._\\n2. **Run** the following **commands** and wait for the Repository to be cloned directly into the CloudShell virtual instance:\\n\\n       git clone https://github.com/mspnp/AzureNamingTool\\n\\n![Azure Naming Tool - Clone Repo](/uploads/AzNamingTool_RepoClone.gif \\"Azure Naming Tool - Clone Repo\\")\\n\\n#### Create Resource Group & Azure Container Registry\\n\\nNow that we have our Repository, it\'s time to create our Resource Group and Container Registry _(Public)_; we will use a few PowerShell cmdlets to develop the resources; make sure you change the name of your Container Registry and Resource Group to match your environment.\\n\\n1. Log in to the [Microsoft Azure Portal](https://portal.azure.com/#home \\"Microsoft Azure - Portal\\") and open up the **Azure Cloud Shell** _(make sure you are in PowerShell (not Bash))._\\n2. **Run** the following **commands** to **create** the **Resource Group** and the **Azure Container Registry**:\\n\\n**Remember to change the name of the Container Registry - this is a globally unique resource, so if someone else has already created a registry with the same name, yours won\'t deploy.**\\n\\n       $ResourceGroup = New-AzResourceGroup -Name \'AzNamingTool-PROD-RG\' -Location \'Australia East\'\\n       $registry = New-AzContainerRegistry -ResourceGroupName \'AzNamingTool-PROD-RG\' -Name \\"ContainerRegistryAzNamingTool\\" -EnableAdminUser -Sku Basic\\n       Connect-AzContainerRegistry -Name $registry.Name\\n\\n![AzureNaming Tool - Create Resource Group & Azure Container Registry](/uploads/AzNamingTool_ContainerRegistryImageBuild.gif \\"AzureNaming Tool - Create Resource Group & Azure Container Registry\\")\\n\\n#### Build your image to the Azure Container Registry\\n\\nThe Azure Container Registry will be stored to host and build your image definition, as Docker support is not native to the Azure Cloud Shell; now that we have created it is time to build the image and push it to the registry. Ensure you are in the AzNamingTool folder _(CloudAdoptionFramework/ready/AzNamingTool/)_.\\n\\n1. **Run** the following **Azure CLI** command:\\n\\n       az acr build --image azurenamingtool:v1 --registry $registry.Name --file Dockerfile .\\n\\n![AzureNaming Tool - Azure Container Registry](/uploads/AzNamingTool_ContainerRegistryImageBuild.gif \\"AzureNaming Tool - Azure Container Registry\\")\\n\\n#### Deploy Azure App Service and WebApp\\n\\nFor the following, we will use a mix of Azure Bicep and the Azure Portal _(I ran into an Access Key error and PowerShell_ [_issue_](https://github.com/Azure/azure-powershell/issues/10645 \\"webapp: New-AzWebApp does not set LinuxFxVersion from ContainerImageName\\") _when attempting to map the share using Bicep and PowerShell - if you managed to complete the setup feel free to add a comment in the comments below)_.\\n\\nAzure Bicep will be used to create the App Service and Storage account + file share, and then we will use the Azure Portal to complete the setup _(Azure WebApp as a Container and mapping the persistent file share)_.\\n\\nFirst, we need to install Azure Bicep and import the Bicep file into Cloud Shell; we could Upload the file straight from the Portal or clone a [repo](https://github.com/lukemurraynz/Azure-Bicep \\"Azure-Bicep\\") with the file - but because I am using Azure Cloud Shell from the Terminal because Azure Cloud Shell runs on Linux - I am going to use \'nano\' to create the Bicep file manually - feel free to do any of the above options to get the Azure Bicep into Cloud Shell.\\n\\n##### Install Azure Bicep\\n\\n1. To **install** Azure **Bicep,** run:\\n\\n       az bicep install\\n\\n![Azure Naming Tool - Install Azure Bicep](/uploads/install_azbicep.gif \\"Azure Naming Tool - Install Azure Bicep\\")\\n\\n##### Create Azure Bicep File\\n\\nWe will use Nano, copy the Azure Bicep file and Paste it into Nano, and make sure you adjust the parameters to suit your environment before deploying.\\n\\n1. In the **Azure Cloud Shell**, let us create the file by typing.\\n\\n       nano AzNamingTool_main.bicep\\n\\n2. **Paste** the Azure **Bicep file** and do any final edits\\n3. Now we need to save the file; press **Ctrl+X** on your keyboard\\n4. Press **Y** to save the file\\n5. Verify the file name and press **Enter** to accept the filename.\\n\\n   ![Azure Naming Tool - Create Bicep file](/uploads/createazbicepfile.gif \\"Azure Naming Tool - Create Bicep file\\")\\n\\n**Remember to edit the Azure Bicep parameters, the Resource Names need to be globally unique, so you may run into problems if someone has deployed using the same name!**\\n\\n```bicep title=\\"AzNamingTool_main.bicep\\"\\n//Related to a Blog Article: https://luke.geek.nz for setting up Azure Naming Tool.\\n///Parameter Setting\\nparam location string = resourceGroup().location\\n\\n//Adjust Parameter values to match your naming conventions\\n\\nparam serverfarms_AzNamingTool_ASP_Prod_name string = \'AzNamingTool-ASP-Prod\'\\nparam storageAccounts_aznamingstgacc_name string = \'aznaming\'\\n\\n// The following Parameters are used add Tags to your deployed resources. Adjust for your own needs.\\n\\nparam dateTime string = utcNow(\'d\')\\nparam resourceTags object = {\\n  Application: \'Azure Naming Tool\'\\n  Version: \'v2.0\'\\n  CostCenter: \'Operational\'\\n  CreationDate: dateTime\\n  Createdby: \'Luke Murray (luke.geek.nz)\'\\n}\\n\\n/// Deploys Resources\\n\\n//Deploys Azure Storage Account for Azure File Share for AzNamingtool persistant data\\n\\nresource storageAccounts_aznamingstgacc_name_resource \'Microsoft.Storage/storageAccounts@2021-09-01\' = {\\n  name: \'${storageAccounts_aznamingstgacc_name}${uniqueString(resourceGroup().id)}\'\\n  location: location\\n  tags: resourceTags\\n  sku: {\\n    name: \'Standard_LRS\'\\n  }\\n  kind: \'StorageV2\'\\n  properties: {\\n    dnsEndpointType: \'Standard\'\\n    defaultToOAuthAuthentication: false\\n    publicNetworkAccess: \'Enabled\'\\n    allowCrossTenantReplication: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowBlobPublicAccess: true\\n    allowSharedKeyAccess: true\\n    networkAcls: {\\n      bypass: \'AzureServices\'\\n      defaultAction: \'Allow\'\\n    }\\n    supportsHttpsTrafficOnly: true\\n    encryption: {\\n      requireInfrastructureEncryption: false\\n      services: {\\n        file: {\\n          keyType: \'Account\'\\n          enabled: true\\n        }\\n        blob: {\\n          keyType: \'Account\'\\n          enabled: true\\n        }\\n      }\\n      keySource: \'Microsoft.Storage\'\\n    }\\n    accessTier: \'Hot\'\\n  }\\n}\\n// Deploys Azure File Share from the Storage Account above.\\n\\nresource Microsoft_Storage_storageAccounts_fileServices_storageAccounts_aznamingstgacc_name_default \'Microsoft.Storage/storageAccounts/fileServices@2021-09-01\' = {\\n  parent: storageAccounts_aznamingstgacc_name_resource\\n  name: \'default\'\\n\\n  properties: {\\n\\n    shareDeleteRetentionPolicy: {\\n      enabled: true\\n      days: 7\\n    }\\n  }\\n}\\n\\nresource storageAccounts_aznamingstgacc_name_default_aznamingtool \'Microsoft.Storage/storageAccounts/fileServices/shares@2021-09-01\' = {\\n  parent: Microsoft_Storage_storageAccounts_fileServices_storageAccounts_aznamingstgacc_name_default\\n  name: \'aznamingtool\'\\n  properties: {\\n    accessTier: \'TransactionOptimized\'\\n    shareQuota: 100\\n    enabledProtocols: \'SMB\'\\n  }\\n}\\n\\n//Deploys the App Service PLan for AzNamingTool\\n\\nresource serverfarms_AzNamingTool_ASP_Prod_name_resource \'Microsoft.Web/serverfarms@2021-03-01\' = {\\n  name: serverfarms_AzNamingTool_ASP_Prod_name\\n  tags: resourceTags\\n  location: location\\n  sku: {\\n    name: \'B1\'\\n    tier: \'Basic\'\\n    size: \'B1\'\\n    family: \'B\'\\n    capacity: 1\\n  }\\n  kind: \'linux\'\\n  properties: {\\n    perSiteScaling: false\\n    elasticScaleEnabled: false\\n    maximumElasticWorkerCount: 1\\n    isSpot: false\\n    reserved: true\\n    isXenon: false\\n    hyperV: false\\n    targetWorkerCount: 0\\n    targetWorkerSizeId: 0\\n    zoneRedundant: false\\n  }\\n}\\n```\\n\\n##### Deploy Azure Bicep\\n\\nNow it\'s time to create the Azure App Service Plan and Storage account **_(remove the -what if flag at the end, when you confirmed there are no errors)_**.\\n\\n1. **Run** the following **command** to deploy the **App Service and Storage** account into your Resource Group:\\n\\n       New-AzResourceGroupDeployment -Name \'AzNamingTool-WebApp\' -ResourceGroupName \'AzNamingTool-PROD-RG\' -TemplateFile .\\\\AzNamingTool_main.bicep -WhatIf\\n\\n![Azure Naming Tool - Deploy Azure Bicep resources](/uploads/deployaznamingtoolbicepresources.gif \\"Azure Naming Tool - Deploy Azure Bicep resources\\")\\n\\nYour resources _(App Service, Storage account with File Share)_ should now be deployed, and we can now close our trusty Cloud Shell.\\n\\n##### Deploy and configure WebApp as a Container\\n\\n 1. Log in to the [Microsoft Azure Portal](https://portal.azure.com/#home \\"Microsoft Azure - Portal\\")\\n 2. Click **+ Create a Resource**\\n 3. Search for: Web App and click **Create a Web App**\\n 4. Select your **Subscription** and **Resource Group**\\n 5. **Select** a **name** for your Web App _(AzNamingTool-AS-Prod)_\\n 6. In **Publish**, select **Docker Container**\\n 7. For Operating system: Select **Linux**\\n 8. Select the **Region** that your App Service plan was deployed to\\n 9. **Select** the App Service **Plan** created earlier, then Select **Next: Docker**\\n10. ![Azure Naming Tool - Web App Deployment](/uploads/azure-naming-tool-webapp-deployment-general.png \\"Azure Naming Tool - Web App Deployment\\")\\n11. Under Options, select **Single Container**\\n12. Change Image Source to **Azure Container Registry**\\n13. Select your **Registry** and **Azure Naming Too**l image, then select **Next: Networking**\\n14. ![Azure Naming Tool - Registry](/uploads/azure-naming-tool-webapp-deployment-docker.png \\"Azure Naming Tool - Registry\\")\\n15. If you want to enable Network injection, by placing it on your Virtual Network, you can configure this, and we are just going head to **Monitoring.** \\n16. Application Insights isn\'t required, but it is recommended _- even if it is just for Availability alerting)_; I always enable it, so select **Yes** and **Next Tags.**\\n17. ![Azure App Deployment - Application Insights](/uploads/azure-naming-tool-webapp-deployment-app-insights.png \\"Azure App Deployment - Application Insights\\")\\n18. Enter in any applicable **Tags** and finally click **Review + Create**\\n19. Click **Create**\\n20. Now that your container is running, we need to mount the Azure file share, so any persistent data is saved.\\n21. **Open** your newly created **App Service.**\\n22. Navigate to **Configuration**, under Settings in the navigation bar\\n23. Click on **Path mappings**\\n24. Click **+ New Azure Storage Mount**\\n25. **Give** the **mount** a **name**: i.e. _a naming tool-stg-mnt_\\n26. Select **Basic Configuration**\\n27. Select the **Storage account** created earlier _(as part of the Bicep deployment)_ and select **Azure File share**\\n28. Select your Storage container and enter in**/app/settings** to the mount path and click **Ok**\\n29. ![Azure App Service - Mount Azure File Share](/uploads/azure-naming-tool-webapp-deployment-storage-mount.png \\"Azure App Service - Mount Azure File Share\\")\\n30. Then select Save to **Save** the Path Mappings\\n\\n##### Optional: Azure App Service Tweaks\\n\\nBy now, your Azure Naming Tool should be accessible,  you don\'t need to do any of the following, but I recommend them at a bare minimum _(environment and use case depending)_.\\n\\n###### Enable [**Always On**](https://learn.microsoft.com/en-us/azure/app-service/configure-common?tabs=portal&WT.mc_id=AZ-MVP-5004796 \\"Configure an App Service app\\")\\n\\n1. In your App Service, select **Configuration**, then **General Settings**\\n2. Check **\'On**\' under \'Always On\'\\n3. Click **Save**\\n\\n###### Configure Firewall\\n\\nYour App Service will be publically accessible by default, and although you may want to link it to your network via a Private Endpoint, locking down by Public IP may be suitable in some scenarios _(such as this demo environment)_. \\n\\n1. To lock it down to a specific Public IP, in your App Service, Select **Networking**, then **Access restriction.**\\n2. Add in your **Public IP** to restrict it from being accessible from your network and click **Ok.**\\n3. Make sure you select the scm instance and select: **Same restrictions** so that the SCM instance isn\'t also publically accessible.\\n\\n### Let\'s take a look!\\n\\nNow that you have successfully deployed the Azure Naming Tool let\'s take a look.\\n\\nTo open your **Azure Naming Tool**, navigate to your **App Service** and select **Browse** (or copy the URL).\\n\\nWhen you open it the first time, you will have the option to create an Admin password, set your Password and select Save; if the Azure File Share wasn\'t mounted to the Web App - then your Password won\'t be saved if the App Services crashes or gets reloaded to another node.\\n\\n![Azure Naming Tool](/uploads/azure-naming-tool.png)\\n\\nClick on **Generate**\\n\\nYou can immediately generate a naming standard out of the box _(and it already contains the prefix for the NZ North Azure region!)_.\\n\\n![Azure Naming Standard - Generate](/uploads/azurenamingtool-generate.png \\"Azure Naming Standard - Generate\\")\\n\\nIf you click **Reference,** you can see the reference criteria that Azure Naming Tool works with generating your Naming schema; for example, for ApiManagement APS, we can see that the short name is: API; it supports up to 256 characters but cannot have a \'#\', and does not need a globally unique name.\\n\\n![Azure Naming Tool - Reference API Management](/uploads/azurenamingstandard-referenceapi.png \\"Azure Naming Tool - Reference API Management\\")\\n\\nIf you navigate to: **Configuration**, this is where you can specify any Custom changes to suit your Organisation or Organisations _(yes, you can use this as a Cloud Architect or Consultant to generate names of multiple organisations)_. If you don\'t like the default prefixes for the Resources, Regions, Environment or even Delimiters, you can adjust them here.\\n\\n![Azure Naming Tool - Configuration](/uploads/azurenamingtool-configuration.png \\"Azure Naming Tool - Configuration\\")\\n\\nYou can also **Export** and **Import** a configuration from a previous install on the Configuration pane.\\n\\nThere is also an Azure Naming Tool Swagger API that you can leverage _(the API key can be found under Admin)_ in your Infrastructure as Code or script deployments.\\n\\n![Azure Naming Tool - API](/uploads/azurenamingtool-api.png \\"Azure Naming Tool - API\\")"},{"id":"azure/add-custom-dns-servers-and-set-azure-point-to-site-vpn-to-connect-automatically","metadata":{"permalink":"/azure/add-custom-dns-servers-and-set-azure-point-to-site-vpn-to-connect-automatically","source":"@site/blog/2022-07-10-add-custom-dns-servers-and-set-azure-point-to-site-vpn-to-connect-automatically.md","title":"Add Custom DNS servers and set Azure Point to Site VPN to Connect automatically","description":"The Azure Point to Site VPN will take the DNS servers from the Virtual Network, that the Gateway is peering into by default, but due to VNET Peering or custom configuration if you may want to point this to custom DNS servers.","date":"2022-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.045,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-07-10 00:00:00 +1200","title":"Add Custom DNS servers and set Azure Point to Site VPN to Connect automatically","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/add-custom-dns-servers-and-set-azure-point-to-site-vpn-to-connect-automatically"},"unlisted":false,"prevItem":{"title":"Deploy Azure Naming Tool into an Azure WebApp as a container","permalink":"/azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container"},"nextItem":{"title":"Migrating resources between regions using Azure Resource Mover","permalink":"/azure/migrating-resources-between-regions-using-azure-resource-mover"}},"content":"The Azure Point to Site VPN will take the DNS servers from the Virtual Network, that the Gateway is peering into by default, but due to VNET Peering or custom configuration if you may want to point this to custom DNS servers.\\n\\nTo do this, you need to edit the \'azurevpnconfig.xml\' file and reimport the VPN connection.\\n\\n1. Open: azurevpnconfig.xml in your favourite editor _(ie Visual Studio Code or Notepad)_\\n2. Underneath the <name> _(which you can also change, as this is the name that users will see in Windows)_ add: < clientconfig>.\\n\\nFor example:\\n\\n      <name>Luke\'s Azure Point to Site VPN</name>\\n      <clientconfig>\\n     \x3c!-- need to specify always on = true for the VPN to connect automatically --\x3e \\n     <AlwaysOn>true</AlwaysOn>\\n      \x3c!-- Add custom DNS Servers --\x3e \\n               <dnsservers>\\n                 <dnsserver>10.100.1.1</dnsserver>\\n                 <dnsserver>10.100.1.2</dnsserver>\\n             </dnsservers>\\n    \x3c!-- Add custom DNS suffixes --\x3e \\n              <dnssuffixes>\\n              <dnssuffix>.luke.geek.nz</dnssuffix>\\n        </dnssuffixes>\\n    </clientconfig>\\n\\nSave your azurevpnconfig.xml and import it into the Azure VPN client.\\n  \\nOnce the VPN has been re-established your Custom DNS settings and suffxies should take effect.\\nIf you included the <AlwaysOn> this will reconnect automatically, after your first connection and after computer reboots.\\n  \\nIf you need assistance setting up a Point to Site VPN, check out my article here: [Create Azure Point to Site VPN using Microsoft Entra ID authentication ](https://luke.geek.nz/azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication/)"},{"id":"azure/migrating-resources-between-regions-using-azure-resource-mover","metadata":{"permalink":"/azure/migrating-resources-between-regions-using-azure-resource-mover","source":"@site/blog/2022-07-06-migrating-resources-between-regions-using-azure-resource-mover.md","title":"Migrating resources between regions using Azure Resource Mover","description":"With over 70+ Microsoft Azure regions across the globe and new regions popping up all the time (for example new New Zealand North region coming in 2023)_!","date":"2022-07-06T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.345,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-07-06 00:00:00 +1200","title":"Migrating resources between regions using Azure Resource Mover","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azureresourcemover_header.png"},"slug":"azure/migrating-resources-between-regions-using-azure-resource-mover"},"unlisted":false,"prevItem":{"title":"Add Custom DNS servers and set Azure Point to Site VPN to Connect automatically","permalink":"/azure/add-custom-dns-servers-and-set-azure-point-to-site-vpn-to-connect-automatically"},"nextItem":{"title":"Microsoft Azure Portal - Global & Advanced Filters","permalink":"/azure/microsoft-azure-portal-global-advanced-filters"}},"content":"With over 70+ Microsoft Azure [regions](https://azure.microsoft.com/en-us/global-infrastructure/geographies/?WT.mc_id=AZ-MVP-5004796#overview \\" Azure geographies\\") across the globe and new regions popping up all the time _(for example new New Zealand North region coming in 2023)_!\\n\\nMigrating resources between regions is something that you may want to consider. Let\'s look at migrating workloads between them, by using [Azure Resource Mover](https://learn.microsoft.com/en-us/azure/resource-mover/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Resource Mover?\\").\\n\\n### Overview\\n\\n![Azure Resource Mover](/uploads/azureresourcemover_header.png \\"Keep calm and migrate it in Azure\\")\\n\\nAzure Resource Mover helps you to move Azure resources between Azure regions, offering a single pane of glass to migrate different resource types, such as Virtual Machines or Azure SQL databases from a single portal without having to know how to migrate the individual underlying resources, or trying to work out the dependencies for each resource.\\n\\n#### Why would you migrate resources between Regions?\\n\\nLet\u2019s start with why would you migrate resources between regions? Common scenarios include:\\n\\n* Taking advantage of new Azure region expansions to be closer to customers and reduce latency _(such as migrating from Australia East to New Zealand North)_.\\n* Increasing availability and resilience by moving to Azure Availability Zones, from regions that don\u2019t currently support it.\\n* Meeting data residential and compliance requirements.\\n* Consolidating workloads for mergers and acquisitions.\\n* The cost of resources in one region may also be cheaper than in another.\\n\\n#### So, what are the risks of migrating resources?\\n\\nWhatever your reason, moving your applications from Point A to Point B is often no easy task. Here are common reasons why:\\n\\n* Moving resources can pose a risk of an outage.\\n* Workloads are often made up of multiple services, each requiring its own method and tools to move.\\n* Interdependencies are often not understood.\\n* Testing and rollback of these complex scenarios can be daunting.\\n\\n#### So, what are the benefits of using Azure Resource Mover?\\n\\nSo, what are some of the benefits that Azure Resource Mover offers us?\\n\\n* The ability to plan with ease, reducing the time and complexity of your move.\\n* Streamline your move process by identifying dependencies.\\n* Plan and test your move multiple times.\\n* Stage your move as part of scheduled downtime.\\n* Azure Resource Mover helps you orchestrate seamlessly with a consistent experience across common Azure resources\u200b:\\n* Move multiple resources through a single pane of glass.\\n* Reduce manual touchpoints, which could increase the change of services being missed.\\n* Reduce overall time for your move from months to weeks or days* (based on the service and data being consumed).\\n* Azure Resource Mover helps you move with confidence by planning, testing, and moving related resources together and validating and testing your move before final commitment _(by testing your migrated services, while having the peace of mind that your source resources are left intact until you commit to the migration)_.\\n\\n> So to recap, Azure Resource Mover offers you a unified experience to move multiple resource types across regions while validating dependencies between services and giving you the flexibility to adjust resources such as the Names, SKUs and Availability Zones during the migration to the destination region.\\n\\n#### What can Azure Resource Mover move?\\n\\nThe currently supported resources _(as of July 2022)_ are:\\n\\n* Azure Virtual Machines\\n* Azure SQL Database\\n* Azure Virtual Network\\n* SQL elastic pools\\n* Azure Load balancer\\n* Public IP\\n* Resource group\\n* Network security group\\n* Network interfaces\\n* Azure Availability Sets\\n\\nAnd Azure Storage account region replication support is scheduled in the next 6-9 months to be released so the storage account migration should be ready by the time the NZ North comes live.\\n\\nAn updated list of Resources currently supported by Azure Resource Mover can be found here: [What resources can I move across regions using Resource Mover?](https://learn.microsoft.com/en-us/azure/resource-mover/common-questions?WT.mc_id=AZ-MVP-5004796#what-resources-can-i-move-across-regions-using-resource-mover \\"https://learn.microsoft.com/en-us/azure/resource-mover/common-questions?WT.mc_id=AZ-MVP-5004796#what-resources-can-i-move-across-regions-using-resource-mover\\")\\n\\n#### Azure Resource Mover - The 6-Step Process!\\n\\n![Azure Resource Mover - 6 Step Process](/uploads/azure-resource-mover-6-step-process.png \\"Azure Resource Mover - 6 Step Process\\")\\n\\nAzure Resource Mover uses a 6-step process.\\n\\n1. The first step is to **select** the **resources** you\xb4d like to transfer! _A tip is to just pick the Virtual Machine object if you are migrating Virtual Machines, the dependencies will be identified by the Azure Resource Mover service itself!_\\n2. The **dependency check** will be performed, identifying that you need to move other resources along with your virtual machine _(Resource Group, NIC, Managed Disks etc.)_\\n3. Start the **preparation**. This step initiates the preparation while creating a resource group with a dedicated Storage Account and a Recovery Services Vault to perform the move. The prepare step also creates the underlying ARM template deployments for the destination region.\\n4. **Move** initiation starts the process of transferring the resources to the target region. Certain dependencies should be \'committed\' before preparation can be initiated, on other resources. If your resource is stateless such as a Network interface, a new ARM deployment will occur, but if your machine is stateful such as a Virtual Machine, Azure Site Recovery will start to copy the disk of your source machine to the target region.  \\n   **ATTENTION! Resources might be temporarily not available \u2013 perform these steps out of business hours**\\n5. **Commit** your move or discard the move! Depending on if you want to complete the move process you can decide whether you want to keep or remove the replicated resources in the destination region.\\n6. **Delete** the **source** is the cleanup step required to remove the source resources from the region you have transferred from to finish your migration.\\n\\n### Let\'s see Azure Resource Mover in action\\n\\n![Azure Resource Mover - In Action](/uploads/azure-resource-mover-arm-in-action.png \\"Azure Resource Mover - In Action\\")\\n\\nSo enough talking, let us see Azure Resource Mover in action?\\n\\n#### Demo\\n\\nFor our demo, we are going to migrate from Australia East to West US3.\\n\\n> Make sure you review your [quota and subscription limits](https://learn.microsoft.com/en-us/azure/networking/check-usage-against-limits?WT.mc_id=AZ-MVP-5004796 \\"Check resource usage against limits\\"), for the other region before you look to migrate them.\\n\\n![Azure Regions  - Australia East to West US3](/uploads/azure-resource-mover-australia-to-westus3.png \\"Azure Regions  - Australia East to West US3\\")\\n\\nSo what resources are we going to migrate?\\n\\n* Virtual Network\\n* Azure SQL Database\\n* Azure Virtual Machine & associated dependencies _(Resource Groups, Network Interfaces, Managed Disks)._\\n\\n_Note: There is no Audio in the demo video below, but it will guide you through Azure Resource Mover and some of the options._\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/Y7szAjZu2yc\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n#### Some items to note\\n\\n* You can\'t select individual disks as resources to move across regions. However, disks are moved as part of a VM move.\\n* You can migrate encrypted Virtual Machines but needs manual intervention to copy the keys\\n* You can move resources to another subscription AFTER moving resources to the destination region.\\n* You cannot move peered Virtual Networks across subscriptions, you need to remove the peering first, then re-add it back in the destination region.\\n* Make sure your quota and required services have been registered and increased for the additional region\\n* Azure Resource Mover can be used to migrate Azure Virtual Desktop session hosts across regions.\\n* DNS records can be key to reducing the complexity and interruption to end users as part of your migration.\\n* There are PowerShell cmdlets _(i.e., New-AzResourceMoverMoveCollection)_\\n\\n### Additional Resources\\n\\nTo learn more about Azure Resource Mover, visit the Azure Resource Mover page.\\n\\n* [azure.microsoft.com/services/resource-mover](https://azure.microsoft.com/en-us/services/resource-mover/?WT.mc_id=AZ-MVP-5004796)\\n\\nAzure Resource Mover videos:\\n\\n* [Microsoft Azure Resource Region 2 Region Migration - Luke Murray](https://www.youtube.com/watch?v=Wd1Egke6ESs)\\n* [Azure Unblogged - Azure Resource Mover](https://techcommunity.microsoft.com/t5/itops-talk-blog/azure-unblogged-azure-resource-mover/ba-p/2050036?WT.mc_id=AZ-MVP-5004796)\\n* [Azure Friday Video](https://twitter.com/azurefriday/status/1388863346255015943?s=20)\\n* [Azure Resource Mover - Move resources between regions, subscriptions and resource groups -John Savill](https://www.youtube.com/watch?v=6FslxGE9YJM)"},{"id":"azure/microsoft-azure-portal-global-advanced-filters","metadata":{"permalink":"/azure/microsoft-azure-portal-global-advanced-filters","source":"@site/blog/2022-06-12-microsoft-azure-portal-global-advanced-filters.md","title":"Microsoft Azure Portal - Global & Advanced Filters","description":"We\'ve all been there! In the Azure portal, looking for a resource or subscription and cannot find it! Once permissions are ruled out, you are left with the Portal itself, and the filter.","date":"2022-06-12T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.5,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Azure Portal - Global & Advanced Filters","authors":["Luke"],"tags":["Azure"],"date":"2022-06-12 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/azureportal_advanced_subscription_filter.png"},"slug":"azure/microsoft-azure-portal-global-advanced-filters"},"unlisted":false,"prevItem":{"title":"Migrating resources between regions using Azure Resource Mover","permalink":"/azure/migrating-resources-between-regions-using-azure-resource-mover"},"nextItem":{"title":"Microsoft Azure Portal - Recent Resources","permalink":"/2022/06/12/microsoft-azure-portal-recent-resources"}},"content":"We\'ve all been there! In the Azure portal, looking for a resource or subscription and cannot find it! Once permissions are ruled out, you are left with the Portal itself, and the filter.\\n\\nYou may see a checkbox or message like the below:\\n\\n> **Show only subscriptions selected in the global subscriptions filter.**\\n>\\n> ![Show only subscriptions selected in the global subscriptions filter](/uploads/azureportal_globalfilter.png \\"Show only subscriptions selected in the global subscriptions filter\\")\\n\\nThis is because the Microsoft Azure portal has a default filter, which is very handy in hiding subscriptions and resources you don\'t want to see or use all the time.\\n\\nThe following Microsoft document \'[Manage Azure portal settings and preferences](https://learn.microsoft.com/en-us/azure/azure-portal/set-preferences?WT.mc_id=AZ-MVP-5004796 \\"Manage Azure portal settings and preferences\\")\' is a great place to start, but let us take a look ourselves.\\n\\n### Azure Portal Filters\\n\\n#### Global Subscription Filter\\n\\nLet us take a look at the Global subscription filter.\\n\\n1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. Click on \'**Settings**\' on the top right-hand navigation bar\\n3. ![Azure Portal - Settings](/uploads/azureportal_settings.png)\\n4. Click on the **dropdown list** under the Default subscription filter\\n5. Here you can **select** or **de-select** the **subscriptions** you want to display by default in the Microsoft Azure Portal.\\n6. ![Azure Portal - Global Filter](/uploads/azureportal_default_subscription_filter.png)\\n7. There is no Save button, the changes will automatically take effect.\\n\\n#### Advanced Filters\\n\\nLet us take a look at the Advanced subscription filters.\\n\\n1. Log in to the Microsoft [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n2. Click on \'**Settings**\' on the top right-hand navigation bar\\n3. ![Azure Portal - Settings](/uploads/azureportal_settings.png)\\n4. Toggle **Advanced Filters**\\n5. Click **Continue** to reload the Azure Portal, your Global subscription filter will be changed to an advanced filter.\\n6. Click **Modify Advanced filters**\\n7. Click **+ Create a filter**\\n8. Here you have the ability to create a filter or filters, to help match your requirements. You can create filters based on subscription ID and subscription name.\\n\\n| Filter Type | Operator | Value | Note |\\n| --- | --- | --- | --- |\\n| Subscription ID | == | Subscription ID array | Equal |\\n| Subscription name | != | Subscription ID array | Does not Equal |\\n| Subscription state | contains | String | Contains |\\n|  | !contains | String | Does not Contain |\\n|  | startswith | String | Starts with |\\n|  | !startswith | String | Does not start with |\\n|  | endswith | String | Ends with |\\n|  | !endswith | String | Does not end with |\\n\\n| Subscription State | Description |\\n| --- | --- |\\n| Activate/Enabled | Your Azure subscription is active. You can use the subscription to deploy new resources and manage existing ones. |\\n| Deleted | Your Azure subscription has been deleted along with all underlying resources/data. |\\n| Disabled | Your Azure subscription is disabled and can no longer be used to create or manage Azure resources. While in this state, your virtual machines are de-allocated, temporary IP addresses are freed, storage is read-only and other services are disabled. |\\n| Expired | Your Azure subscription is expired because it was canceled. You can reactivate an expired subscription. |\\n| Past Due | Your Azure subscription has an outstanding payment pending. Your subscription is still active but failure to pay the dues may result in subscription being disabled. |\\n| Warned | Your Azure subscription is in a warned state and will be disabled shortly if the warning reason isn\'t addressed. A subscription may be in warned state if its past due, canceled by user, or if the subscription has expired. |\\n\\n1. Using the logic above, we can easily create filters based on the state of a subscription and name, an example is, creating a filter that displays all subscriptions with \'dev\' in its name:\\n2. ![Azure Portal - Advanced Subscription Filter](/uploads/azureportal_advanced_subscription_filter.png)\\n\\nYou can only have one Filter displayed at once in the Azure Portal, but you can easily switch between them, by clicking **Activate**, next to the filter name.\\n\\nIf you wish to disable Advanced Filters, and go back to the Global Filter, you can deselect the Toggle for Advanced Filters.\\n\\n### Additional Resources\\n\\n#### Microsoft Docs\\n\\nTo get the most out of your Azure Portal experience, the below Microsoft documentation is worth a read _(in no particular order)_.\\n\\n* [Manage Azure portal settings and preferences](https://learn.microsoft.com/en-us/azure/azure-portal/set-preferences?WT.mc_id=AZ-MVP-5004796 \\"Manage Azure portal settings and preferences\\")\\n* [Azure subscription states](https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/subscription-states?WT.mc_id=AZ-MVP-5004796 \\"Azure subscription states\\")\\n* [Add, remove, and rearrange favorites](https://learn.microsoft.com/en-us/azure/azure-portal/azure-portal-add-remove-sort-favorites?WT.mc_id=AZ-MVP-5004796 \\"Add, remove, and rearrange favorites\\")\\n* [View and filter Azure resource information](https://learn.microsoft.com/en-us/azure/azure-portal/manage-filter-resource-views?WT.mc_id=AZ-MVP-5004796 \\"View and filter Azure resource information\\")\\n\\n#### Send Feedback to Microsoft\\n\\nOn the last note, Microsoft has made it easy to create Feedback, that will get fed back to the Azure Portal and product teams straight in the Microsoft Azure Portal, if you ever see anything that may need changing, or a link out of date don\'t hesitate to send your feedback to Microsoft, by pressing the little Feedback button on the top right of your navigation bar. \\n\\n![Azure Portal - Feedback](/uploads/azureportal_feedback.png)\\n\\nThe Microsoft Azure portal is in development all the time and is now built with [Azure Resource Graph](https://learn.microsoft.com/en-us/azure/governance/resource-graph/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Resource Graph?\\") capabilities, it is very easy not to try and see new functionality, so I recommend you keep your eyes out and try new features."},{"id":"/2022/06/12/microsoft-azure-portal-recent-resources","metadata":{"permalink":"/2022/06/12/microsoft-azure-portal-recent-resources","source":"@site/blog/2022-06-12-microsoft-azure-portal-recent-resources.md","title":"Microsoft Azure Portal - Recent Resources","description":"The Microsoft Azure Portal displays a list of Recent resources (whether they are subscriptions or Resources) you have accessed, usually when you first log in to the portal itself.","date":"2022-06-12T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.815,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Azure Portal - Recent Resources","authors":["Luke"],"tags":["Azure"],"date":"2022-06-12 00:00:00 +1300","toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Microsoft Azure Portal - Global & Advanced Filters","permalink":"/azure/microsoft-azure-portal-global-advanced-filters"},"nextItem":{"title":"You don\u2019t have authorization to perform action \'Microsoft.Resources/deployments/validate/action\'","permalink":"/azure/you-don-t-have-authorization-to-perform-action-microsoft.resources-deployments-validate-action"}},"content":"The Microsoft Azure Portal displays a list of Recent resources _(whether they are subscriptions or Resources)_ you have accessed, usually when you first log in to the portal itself.\\n\\nThis capability makes it quick to access resources you use the most often, but sometimes you may want to view the resources in a list for easy access or clear the recent resources _(ie if you are going to do a presentation)_ - this is how you can do it.\\n\\nThe Azure portal has a service called: Recent, to access it.\\n\\n1. Log in to the [**Microsoft Azure Portal**](https://portal.azure.com/?l=en.en-nz#home \\"Microsoft Azure Portal\\")\\n2. In the search bar type in: **Recent**\\n3. ![Azure Portal - Recent](/uploads/azureportal_searchrecent.png \\"Azure Portal - Recent\\")\\n4. Select **Recent**\\n5. You will be taken to the Recent Resources view, where you can select Clear to clear your Recent Resources, or you can view all your recent resources for easy access.\\n6. ![Azure Portal - Clear Recent](/uploads/azureportal_recent.png \\"Azure Portal - Clear Recent\\")"},{"id":"azure/you-don-t-have-authorization-to-perform-action-microsoft.resources-deployments-validate-action","metadata":{"permalink":"/azure/you-don-t-have-authorization-to-perform-action-microsoft.resources-deployments-validate-action","source":"@site/blog/2022-06-06-you-don-t-have-authorization-to-perform-action-microsoft.resources-deployments-validate-action.md","title":"You don\u2019t have authorization to perform action \'Microsoft.Resources/deployments/validate/action\'","description":"You may be attempting to deploy an Azure Landing Zone, such as the Enterprise Scale Landing Zone and receive the following error:","date":"2022-06-06T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.64,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-06-06 00:00:00 +1200","title":"You don\u2019t have authorization to perform action \'Microsoft.Resources/deployments/validate/action\'","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/you-don-t-have-authorization-to-perform-action-microsoft.resources-deployments-validate-action"},"unlisted":false,"prevItem":{"title":"Microsoft Azure Portal - Recent Resources","permalink":"/2022/06/12/microsoft-azure-portal-recent-resources"},"nextItem":{"title":"Turn on a Azure Virtual Machine using Azure Automation","permalink":"/azure/turn-on-a-azure-virtual-machine-using-azure-automation"}},"content":"You may be attempting to deploy an Azure Landing Zone, such as the Enterprise Scale Landing Zone and receive the following error:\\n\\n> You don\u2019t have authorization to perform action \'Microsoft.Resources/deployments/validate/action\'.\\n\\nThis is because by default, even if you have Owner right on an Azure subscription, and are a Global Administer, you are unable to assign rights at the root \'/\' tenant level, to be able to create new Management Groups and move subscriptions between them.\\n\\nHowever, users who have the Global Microsoft Entra ID role can elevate rights to do this.  There are a few steps to enabling this, including using [Azure PowerShell](https://learn.microsoft.com/en-us/powershell/azure/what-is-azure-powershell?WT.mc_id=AZ-MVP-5004796 \\"Azure PowerShell\\") to assign rights.\\n\\nWith an account with Global Administrator rights, do the following:\\n\\n 1. Sign in to the **Azure Portal**\\n 2. Open **Microsoft Entra ID**\\n 3. Click **Properties**\\n 4. Toggle the \'**Access management for Azure resources**\' to \'**Yes**\'\\n 5. Click **Save**\\n 6. Open **PowerShell**\\n 7. **Run:**\\n\\n        Connect-AzAccount\\n 8. Login with your **account**, and make sure you are in the correct directory _(if you aren\'t you can use Connect-Az Account - tenantid \'tenantidhere\')_.\\n 9. **Type**:\\n\\n        Get-AzADUser\\n10. **Copy** the **ID** of the user you are logged in as, and **run** the following _(replace the ObjectId to match the ID of your user)_:\\n\\n        $user =  Get-AzADUser -ObjectId f53eaa59-0fc0-4103-b9cb-1650e3069da8\\n11. Once the user ID has been stored in a variable, its finally time to assign the rights, **run** the following:\\n\\n        New-AzRoleAssignment -Scope \'/\' -RoleDefinitionName \'Owner\' -ObjectId $user.Id\\n12. Give Microsoft Entra ID **10-15 minutes** to **replicate** the Azure AD changes, **log out** and **back in** and you should now be able to **deploy** the **Landing Zone**.\\n\\n**Note: Remember to go back and change the toggle to \'Allow management of Azure resources\' to \'No\', or all Global Administrators of Microsoft Entra ID will be able to manage all Azure resources.**\\n\\n**Once the Landing Zone is deployed, you should also remove your role assignment at the root level by running:**\\n\\n    Remove-AzRoleAssignment -Scope \'/\' -RoleDefinitionName \'Owner\' -ObjectId $user.Id"},{"id":"azure/turn-on-a-azure-virtual-machine-using-azure-automation","metadata":{"permalink":"/azure/turn-on-a-azure-virtual-machine-using-azure-automation","source":"@site/blog/2022-06-05-turn-on-a-azure-virtual-machine-using-azure-automation.md","title":"Turn on a Azure Virtual Machine using Azure Automation","description":"Turning off a Virtual Machine in Microsoft Azure on a schedule can quickly be done using the built-in Shutdown controls in the Virtual Machine blade (part of Azure Lab Services, but not a requirement)_, but what about starting it?","date":"2022-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":10.915,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-06-05 00:00:00 +1200","title":"Turn on a Azure Virtual Machine using Azure Automation","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/turn-on-a-azure-virtual-machine-using-azure-automation"},"unlisted":false,"prevItem":{"title":"You don\u2019t have authorization to perform action \'Microsoft.Resources/deployments/validate/action\'","permalink":"/azure/you-don-t-have-authorization-to-perform-action-microsoft.resources-deployments-validate-action"},"nextItem":{"title":"Microsoft Azure Naming Conventions","permalink":"/azure/microsoft-azure-naming-conventions"}},"content":"Turning off a Virtual Machine in Microsoft Azure on a schedule can quickly be done using the built-in Shutdown controls in the Virtual Machine blade _(part of_ [_Azure Lab Services_](https://azure.microsoft.com/en-us/services/lab-services/?WT.mc_id=AZ-MVP-5004796 \\" Azure Lab Services\\")_, but not a requirement)_, but what about starting it?\\n\\nYou have a few options, Logic Apps, PowerShell, Functions and Runbooks; most of the time, these will run on a standard 7 AM to 5 PM Monday to Friday schedule _(meaning the Virtual Machine is off during off-peak hours and weekends, reducing compute cost)_.\\n\\nThis works fine for most scenarios, but what happens if a Bank or Public Holiday falls during the week? With the normal schedule, your Virtual Machine starts.\\n\\nBecause all your users are on Holiday, it wastes money while you and your users drink snicker cocktails at the beach?\\n\\nThis is where using a third party timezone API like \'[AbstractApi](https://www.abstractapi.com/ \\"Automate routine dev work with Abstract\'s suite of APIs\\")\' comes in handy; incorporating a lookup to check if it\'s a Public Holiday before starting that Virtual Machine can help reduce unnecessary costs.\\n\\n[Virtual Machines](https://azure.microsoft.com/en-us/overview/what-is-a-virtual-machine/?WT.mc_id=AZ-MVP-5004796#overview \\" What is a virtual machine (VM)?\\") in Microsoft Azure have different states and, depending on what state the Virtual Machine is in, will determine whether you get billed or not _(for the Compute, storage and network adapters are still billed)_.\\n\\n| Power state | Description | Billing |\\n| --- | --- | --- |\\n| Starting | Virtual Machine is powering up. | Billed |\\n| Running | Virtual Machine is fully up. This is the standard working state. | Billed |\\n| Stopping | This is a transitional state between running and stopped. | Billed |\\n| Stopped | The Virtual Machine is allocated on a host but not running. Also called PoweredOff state or Stopped (Allocated). This can be result of invoking the PowerOff API operation or invoking shutdown from within the guest OS. The Stopped state may also be observed briefly during VM creation or while starting a VM from Deallocated state. | Billed |\\n| Deallocating | This is the transitional state between running and deallocated. | Not billed |\\n| Deallocated | The Virtual Machine has released the lease on the underlying hardware and is completely powered off. This state is also referred to as Stopped (Deallocated). | Not billed |\\n\\nI have written a base runbook that does precisely that, every time the runbook runs, it checks if it is a public Holiday. If it is - then the Virtual Machine isn\'t started; if it isn\'t, then the virtual machine is started.\\n\\n### Overview\\n\\nToday, we are going to set up an Azure Automation runbook, triggered by a scheduled will go through the following steps:\\n\\n1. On a schedule _(7 AM, it will trigger an Azure Automation runbook)_\\n2. The Azure Automation runbook will do a lookup to an external API, in this case, AbstractApi.\\n3. The runbook will check the date and detect if it falls on a Public Holiday; if it is a Public Holiday, it will exit the Azure Automation runbook; if it is a standard workday, it will start the Virtual Machine.\\n\\nTo do this, we need a few resources.\\n\\n* [Azure Automation](https://learn.microsoft.com/en-us/azure/automation/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Automation?\\") Account\\n* Azure Automation [runbook](https://learn.microsoft.com/en-us/azure/automation/automation-runbook-types?WT.mc_id=AZ-MVP-5004796 \\"Azure Automation runbook types\\") _(I will supply a PowerShell runbook below)_\\n* [AbstractAPI ](https://www.abstractapi.com/ \\"Automate routine dev work with Abstract\'s suite of APIs\\") API Key\\n\\nAnd, of course, \'Contributor\' rights to the Microsoft Azure subscription to create the resources and the schedule, along with setting up the System Managed identity to grant the Azure Automation account access to start the Virtual Machine.\\n\\nWe will set up this from scratch using the Azure Portal and use an already created PowerShell Azure Automation runbook.\\n\\n### Deploy Start VM Solution\\n\\n#### Setup Azure Automation Account\\n\\n##### Create Azure Automation Account\\n\\nFirst, we need an [Azure Automation](https://learn.microsoft.com/en-us/azure/automation/automation-create-standalone-account?tabs=azureportal&WT.mc_id=AZ-MVP-5004796 \\"Create a standalone Azure Automation account\\") resource.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Click **+ Create a resource.**\\n 3. Type in **automation**\\n 4. Select **Create** under Automation, and select **Automation.**\\n 5. ![Create Azure Automation Account](/uploads/azureportal-create-automation.jpg \\"Create Azure Automation Account\\")\\n 6. Select your **subscription**\\n 7. Select your **Resource Group** or Create one if you don\'t already have one _(I recommend placing your automation resources in an Azure Management or Automation resource group, this will also contain your Runbooks)_\\n 8. Select your **region**\\n 9. ![Create Azure Automation Account](/uploads/azureportal-create-automation_basics.jpg \\"Create Azure Automation Account\\")\\n10. Select **Next**\\n11. Make sure: **System assigned** is selected for Managed identities _(this will be required for giving your automation account permissions to deallocate your Virtual Machine, but it can be enabled later if you already have an Azure Automation account)_.\\n12. Click **Next**\\n13. Leave Network connectivity as default (**Public access**)\\n14. Click **Next**\\n15. **Enter** in appropriate **tags**\\n16. ![Create Azure Automation Account](/uploads/azureportal-create-automation_tags.jpg \\"Create Azure Automation Account\\")\\n17. Click **Review + Create**\\n18. After validation has passed, select **Create**\\n\\n##### Configure System Identity\\n\\nNow that we have our Azure Automation account, its time to set up the System Managed Identity and grant it the following roles:\\n\\n* Virtual Machine Contributor _(to deallocate the Virtual Machine)_\\n\\nYou can set up a custom role to be least privileged and use that instead. But in this article, we will stick to the built-in roles.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click on: **Identity**\\n 4. Make sure that the **System assigned** toggle is: **On** and click **Azure role assignments.**\\n 5. ![Azure Automation Account managed identity](/uploads/azureportal-automation_managedidentity.jpg \\"Azure Automation Account managed identity\\")\\n 6. Click **+ Add role assignments**\\n 7. Select the **Subscription** _(make sure this subscription matches the same subscription your Virtual Machines are in)_\\n 8. Select Role: **Virtual Machine Contributor**\\n 9. Click **Save**\\n10. Click **Refresh** _(it may take a few seconds to update the Portal, so if it is blank - give it 10 seconds and try again)_.\\n11. You have now set up the System Managed identity and granted it the roles necessary to execute the automation.\\n\\n##### Setup Abstract API Key\\n\\nNow we need to create an API key, which will be used in the runbook to start the Virtual Machine, the API key will allow connections to the Abstract API to retrieve public Holliday information.\\n\\n1. Create an [**Abstract API**](https://www.abstractapi.com/ \\"Abstract API\\") account\\n2. **Log in** to the newly created account\\n3. On the left-hand navigation bar, click on **Holidays**\\n4. ![](/uploads/abstractapi_navigation.png)\\n5. Click on \'**Try it out**\\n6. **Copy** the **API key**\\n7. ![Abstract API - API Key](/uploads/abstractapi_key.png \\"Abstract API - API Key\\")\\n8. Copy the API key, as we will need it for the next steps.\\n\\n##### Import Runbook\\n\\nNow that the modules have been imported into your Azure Automation account, it is time to import the Azure Automation runbook.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click on **Runbooks**\\n 4. Click **+ Create a runbook**\\n 5. Specify a **name** _(i.e. Start-AzureVirtualMachine)_\\n 6. Select Runbook type of **PowerShell**\\n    1. A select Runtime version of: **5.1** _(7.1 works as well)._\\n 7. Type in a **Description** that explains the runbook _(this isn\'t mandatory, but like Tags is recommended, this is an opportunity to indicate to others what it is for and who set it up)_\\n 8. ![](/uploads/azure-createazurerunbook.png)\\n 9. Click **Create**\\n10. Now you will be greeted with a blank edit pane; paste in the Runbook from below:\\n\\n```powershell title=\\"Start-AzureVirtualMachine.ps1\\"\\n#requires -Version 3.0 -Modules Az.Accounts, Az.Resources\\n<#\\n    .SYNOPSIS\\n    PowerShell Azure Automation Runbook for Starting/Stopping Virtual Machines. \\n    .AUTHOR\\n    Luke Murray (https://github.com/lukemurraynz/)\\n    .VERSION\\n    1.0 - 28/04/22 - script versioned to \'1.0\'.\\n    .DESCRIPTION\\n    1. The script first checks if today is a holiday by making a call to the Abstract API.\\n    The Abstract API returns a JSON object containing the holiday name and (optional) description.\\n    The script checks if the name property is null. If it is not null, the script displays a message indicating that today is a holiday.\\n    If the name property is null, the script displays a message indicating that today is not a holiday.\\n    2. The script then checks if the virtual machine is running or not. If it is running, the script will stop the virtual machine.\\n    If it is not running, the script will start the virtual machine, depending on the Shutdown tag value\\n#>\\n\\nParam(\\n  [Parameter(Mandatory = $true)]\\n  [String]\\n  $TagName,\\n  [Parameter(Mandatory = $true)]\\n     \\n  [String]\\n  $TagValue,\\n  [Parameter(Mandatory = $true)]\\n  [Boolean]\\n  $Shutdown\\n)\\n\\n$CountryCode = \'NZ\'\\n\\n$tDate =(Get-Date).ToUniversalTime()\\n$tz = [System.TimeZoneInfo]::FindSystemTimeZoneById(\\"New Zealand Standard Time\\")\\n$Date  = [System.TimeZoneInfo]::ConvertTimeFromUtc($tDate, $tz)\\n\\n\\n$API = Get-AutomationVariable -Name AbstractApiKey\\n$Holiday = Invoke-WebRequest -Uri (\'https://holidays.abstractapi.com/v1/?api_key={0}&country={1}&year={2}&month={3}&day={4}\' -f $API, $CountryCode, $Date.Year, $Date.Month, $Date.Day)\\n\\n$Holidays = $Holiday.Content\\n$Holidays = $Holidays | ConvertFrom-Json\\n\\nIF ($null -ne $Holidays.name) \\n{\\n  Write-Output -InputObject (\\"Today is a holiday. The Holiday today is: {0}. The Azure Virtual Machine won\'t be started.\\" -f $Holidays.name)\\n}\\nELSE \\n{\\n  Write-Output -Message \'No holiday today. The Virtual Machine will be started.\'\\n\\n  # Ensures you do not inherit an AzContext in your runbook\\n  Disable-AzContextAutosave -Scope Process\\n  # Connect to Azure with system-assigned managed identity (Azure Automation account, which has been given VM Start permissions)\\n  $AzureContext = (Connect-AzAccount -Identity).context\\n  Write-Output -InputObject $AzureContext\\n  # set and store context\\n  $AzureContext = Set-AzContext -SubscriptionName $AzureContext.Subscription -DefaultProfile $AzureContext\\n  Write-Output -InputObject $AzureContext\\n\\n  $vms = Get-AzResource -TagName $TagName -TagValue $TagValue | Where-Object -FilterScript {\\n    $_.ResourceType -like \'Microsoft.Compute/virtualMachines\' \\n  }\\n\\n  Foreach ($vm in $vms) \\n  {\\n    if ($Shutdown -eq $true) \\n    {\\n      Write-Output -InputObject \\"Stopping $($vm.Name)\\"        \\n      Stop-AzVM -Name $vm.Name -ResourceGroupName $vm.ResourceGroupName -Force\\n    }\\n    else \\n    {\\n      Write-Output -InputObject \\"Starting $($vm.Name)\\"        \\n      Start-AzVM -Name $vm.Name -ResourceGroupName $vm.ResourceGroupName\\n    }\\n  }\\n}\\n```\\n\\n12. **Change** the **country code** to align with your own country. You can use the IP [geolocation API](https://app.abstractapi.com/api/ip-geolocation/tester \\"IP geolocation API\\") in Abstract API to do a live test, which will give you your country code. Feel free to amend the Write-Output messages to make sense for your environment.\\n13. Click **Save**\\n14. ![Azure Runbook - PowerShell](/uploads/azurerunbook_publishedsource.png \\"Azure Runbook - PowerShell\\")\\n15. Click **Publish** _(so the runbook is actually in production and can be used)_\\n16. You can select View or Edit at any stage, but you have now imported the Azure Automation runbook!\\n\\n##### Setup Variables\\n\\nNow that the Azure runbook has been imported, we need to set up the variables, which include the API key.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click **Variables**\\n 4. Click **+ Add a variable**\\n 5. **Create** a Variable named: **AbstractApiKey** _(this needs to match the variable name as part of the \'Get-AutomationVariable\' cmdlet)_.\\n 6. Enter in a **description**\\n 7. Select **String**\\n 8. **Enter** in the **API** key you retrieved earlier from Abstract API.\\n 9. ![Azure Automation - Variables](/uploads/azure-azautomate_variables.png)\\n10. Click **Save**\\n\\n##### Setup Schedule\\n\\nNow that the variables have been set up, we need to set up the schedule. This is the schedule that will be used to start the Virtual Machine. In the example below we are going to use a Standard Monday -> Friday work week, but adjust the time and date for when you need to start the virtual machine up.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click **Schedule**\\n 4. Click **+ Add a schedule**\\n 5. **Type** in a **name** for the **schedule** _(ie Azure Virtual Machine - Start)_.\\n 6. Type in a **Description**\\n 7. **Select** the **Start Date** to match when you want to start the Schedule _(ie first Monday of the week)_.\\n 8. **Select** your **Timezone**, so that the script runs on the right time/date which makes your timezone.\\n 9. For Recurrance, specify: **Recurring**\\n10. Set it to Recur every: **1 Day**\\n11. Check **Monday**, **Tuesday**, **Wednesday**, **Thursday**, and **Friday**\\n12. Leave Saturday and Sunday unchecked.\\n13. Click **Create**\\n14. Now that the Schedule has been created, we need to bind it to a Runbook\\n15. On the Automation account blade, click on **Runbooks**\\n16. Click on your \'Start Azure Virtual Machine\' runbook\\n17. Select **Schedules**\\n18. Click **Add a Schedule**\\n19. Press: **Link a schedule to your runbook**\\n20. Select **your** newly created **Schedule**\\n21. For the Tag Name, select **Shutdown**\\n22. For the Tag Value name, Select **Yes**\\n23. For Shutdown, select **false**\\n24. Click **Ok**\\n25. Your schedule has now been configured and the runbook will run the next time it matches your scheduled date and time.\\n\\n##### Configure Tags\\n\\nThe runbook is written, so it doesn\'t need to be adjusted for future machines and making changes on the fly, this relies on each Virtual Machine that you want started to be started using the Runbook to be tagged.\\n\\n1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n2. Navigate to your Azure **Virtual Machine**\\n3. Click **Tags**\\n4. **Add the following tag**:\\n\\n| Tag Key | Tag Value |\\n| --- | --- |\\n| Shutdown | Yes |\\n\\nCongratulations, the next time your schedule triggers, every runbook with the Shutdown tag will be started, according to your schedule, and workday. If it\'s a Public Holiday or a Weekend, the Virtual Machine will remain off - saving cost."},{"id":"azure/microsoft-azure-naming-conventions","metadata":{"permalink":"/azure/microsoft-azure-naming-conventions","source":"@site/blog/2022-06-02-microsoft-azure-naming-conventions.md","title":"Microsoft Azure Naming Conventions","description":"Accurately representing and naming your resources is essential for security purposes.","date":"2022-06-02T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":7.94,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-06-02 00:00:00 +1200","title":"Microsoft Azure Naming Conventions","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azurenaming.png"},"slug":"azure/microsoft-azure-naming-conventions"},"unlisted":false,"prevItem":{"title":"Turn on a Azure Virtual Machine using Azure Automation","permalink":"/azure/turn-on-a-azure-virtual-machine-using-azure-automation"},"nextItem":{"title":"Microsoft Azure Tagging Conventions","permalink":"/azure/microsoft-azure-tagging-conventions"}},"content":"Accurately representing and naming your resources is essential for security purposes.\\n\\nIn a security incident, it is critical to identify affected systems quickly, what functions those systems support, and the potential business impact.\\n\\nA useful naming convention composes resource names from important information about each resource. A well-chosen name helps you quickly identify the resource\'s type, its associated workload, its deployment environment, and the Azure region hosting it.\\n\\nSome resource names, such as PaaS services with public endpoints or virtual machine DNS labels, have global scopes, so they must be unique across the Azure platform.\\n\\nThere\'s no one size fits all to Azure naming conventions, it needs to suit your organisation, however, it is worth noting that there are limitations to naming rules to Azure resources.\\n\\n* [Naming rules and restrictions for Azure resources](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-name-rules?WT.mc_id=AZ-MVP-5004796 \\"Naming rules and restrictions for Azure resources\\")\\n* [Define your naming convention](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/resource-naming?WT.mc_id=AZ-MVP-5004796 \\"Define your naming convention\\")\\n* [Deploy Azure Naming Tool into an Azure WebApp as a container](https://luke.geek.nz/azure/deploy-azure-naming-tool-into-an-azure-webapp-as-a-container/)\\n\\nThe use of these limitations and scopes have been used to determine the following naming conventions across associated resources.\\n\\n| Casing | Name Format |\\n| --- | --- |\\n| Lowercase | {organizationName}-{component}-{resourceTypeshortCode}-{regionShortCode}-{environmentlongcode} |\\n\\n### Naming Convention Examples\\n\\n| Environment | Application Name | Azure Region | Azure Service | Example Name |\\n| --- | --- | --- | --- | --- |\\n| Production | application1 | Australia East | App Service | company-application1-asvc-au-e-prod |\\n| Production | application1 | Australia East | App Service Environment | company-application1-ase-au-e-prod |\\n| Production | application1 | Australia East | App Service Plan | company-application1-asp-au-e-prod |\\n| Production | application1 | Australia East | Application Gateway | company-application1-agw-au-e-prod |\\n| Production | application1 | Australia East | Automation Account | company-application1-aum-au-e-prod |\\n| Production | application1 | Australia East | Availability Set | company-application1-avs-au-e-prod |\\n| Production | application1 | Australia East | Azure Arc enabled Kubernetes cluster | company-application1-arck-au-e-prod |\\n| Production | application1 | Australia East | Azure Arc enabled server | company-application1-arcs-au-e-prod |\\n| Production | application1 | Australia East | Azure Cosmos DB database | company-application1-cosmos-au-e-prod |\\n| Production | application1 | Australia East | Azure Data Factory | company-application1-adf-au-e-prod |\\n| Production | application1 | Australia East | Azure Search | company-application1-srch-au-e-prod |\\n| Production | application1 | Australia East | Azure SQL Database | company-application1-sqldb-au-e-prod |\\n| Production | application1 | Australia East | Azure SQL Elastic Pool | company-application1-sqlep-au-e-prod |\\n| Production | application1 | Australia East | Azure SQL Server | company-application1-sql-au-e-prod |\\n| Production | application1 | Australia East | Container registry | company-application1-cr-au-e-prod |\\n| Production | application1 | Australia East | Cosmos DB | company-application1-cdb-au-e-prod |\\n| Production | application1 | Australia East | Function App | company-application1-func-au-e-prod |\\n| Production | application1 | Australia East | Gateway connection | company-application1-cn-au-e-prod |\\n| Test | application1 | Australia East | IoT Central | company-application1-iotc-au-e-test |\\n| Test | application1 | Australia East | Key Vault | company-application1-kv-au-e-test |\\n| Test | application1 | Australia East | Load Balancer | company-application1-lb-au-e-test |\\n| Test | application1 | Australia East | Local Network Gateway | company-application1-lgw-au-e-test |\\n| Test | application1 | Australia East | Log Analytics workspace | company-application1-la-au-e-test |\\n| Production | application1 | Australia East | MySQL database | company-application1-mysql-au-e-prod |\\n| Production | application1 | Australia East | Network Interface | company-application1-nic-au-e-prod |\\n| Production | application1 | Australia East | Network Security Group | company-application1-nsg-au-e-prod |\\n| Production | application1 | Australia East | Network Security Group Rule | company-application1-nsg-au-e-prod |\\n| Production | application1 | Australia East | Public IP Address | company-application1-pip-au-e-prod |\\n| Production |  | Australia East | Recovery Services vault | company-rsv-au-e-prod |\\n| Production | application1 | Australia East | Recovery Services Vault - Backup policies | company-application1-rsvp-au-e-prod |\\n| Production | application1 | Australia East | Resource Group | company-application1-rg-au-e-prod |\\n| Production | application1 | Australia East | Route table | company-application1-route-au-e-prod |\\n| Production | application1 | Australia East | Runbooks | company-application1-run-au-e-prod |\\n| Production | application1 | Australia East | Service Bus - Namespace | company-application1-sbns-au-e-prod |\\n| Production | application1 | Australia East | SQL Data Warehouse | company-application1-sqldw-au-e-prod |\\n| Production | application1 | Australia East | SQL Managed Instance | company-application1-sqlmi-au-e-prod |\\n| Production | App1 | Australia East | Storage Account | company-pp1-stg-au-e-prod |\\n| Production | application1 | Australia East | Subnet | company-application1-snet-au-e-prod |\\n| Production | application1 | Australia East | Subscription | company-application1-sub-prod |\\n| Production | application1 | Australia East | Traffic Manager Profile | company-application1-tmp-au-e-prod |\\n| Production | application1 | Australia East | User defined route (UDR) | company-application1-udr-au-e-prod |\\n| Production | application1 | Australia East | Virtual machine scale set | company-application1-vmss-au-e-prod |\\n| Production | application1 | Australia East | Virtual Network | company-application1-vn-au-e-prod |\\n| Production | application1 | Australia East | Virtual Network Gateway | company-application1-vngw-au-e-prod |\\n\\n![Azure Naming - Global](/uploads/azurenaming.png \\"Azure Naming - Global\\")\\n\\n### Resource Group\\n\\n| Environment | Application Name | Azure Region | Azure Service | Example Name |\\n| --- | --- | --- | --- | --- |\\n| Production | application1 | Australia East | Resource Group | company-application1-rg-au-e-prod |\\n\\n### Resource Type Codes\\n\\n| Resource Type | Short Code | Scope | Character Limit |\\n| --- | --- | --- | --- |\\n| App Service | asvc | Global | 40 |\\n| App Service Environment | ase | Resource Group | 38 |\\n| App Service Plan | asp | Resource Group | 40 |\\n| Application Gateway | agw | Resource Group | 80 |\\n| Automation Account | aum | Resource Group | 50 |\\n| Availability Set | avs | Resource Group | 80 |\\n| Azure Arc enabled Kubernetes cluster | arck | Resource Group | 63 |\\n| Azure Arc enabled server | arcs | Resource Group | 15 |\\n| Azure Cosmos DB database | cosmos | Global | 63 |\\n| Azure Data Factory | adf | Global | 63 |\\n| Azure Search | srch | Global | 60 |\\n| Azure SQL Database | sqldb | Server | 128 |\\n| Azure SQL Elastic Pool | sqlep | Server | 128 |\\n| Azure SQL Server | sql | Global | 63 |\\n| Container registry | cr | Global | 50 |\\n| Cosmos DB | cdb | Global | 50 |\\n| Function App | func | Global | 40 |\\n| Gateway connection | cn | Resource Group | 80 |\\n| IoT Central | iotc | Global | 63 |\\n| Key Vault | kv | Global | 24 |\\n| Load Balancer | lb | Resource Group | 80 |\\n| Local Network Gateway | lgw | Resource Group | 80 |\\n| Log Analytics workspace | la | Global | 24 |\\n| MySQL database | mysql | Global | 63 |\\n| Network Interface | nic | Resource Group | 80 |\\n| Network Security Group | nsg | Resource Group | 80 |\\n| Network Security Group Rule | nsg | Resource Group | 80 |\\n| Public IP Address | pip | Resource Group | 80 |\\n| Recovery Services vault | rsv | Resource Group | 50 |\\n| Recovery Services Vault - Backup policies | rsvp | vault | 50 |\\n| Resource Group | rg | Global | 64 |\\n| Route table | route | Resource Group | 80 |\\n| Runbooks | run | Automation Account | 63 |\\n| Service Bus - Namespace | sbns | Global | 50 |\\n| SQL Data Warehouse | sqldw | Global | 63 |\\n| SQL Managed Instance | sqlmi | Global | 63 |\\n| Storage Account | stg | Global | 24 |\\n| Subnet | snet | Virtual Network | 80 |\\n| Subscription | sub | Account | 64 |\\n| Traffic Manager Profile | tmp | Resource Group | 63 |\\n| User defined route (UDR) | udr | Resource Group | 80 |\\n| Virtual Machine | vm | Resource Group | 15 |\\n| Virtual machine scale set | vmss | Resource Group | 15 |\\n| Virtual Network | vn | Resource Group | 63 |\\n| Virtual Network Gateway | vngw | Resource Group | 80 |\\n\\n### Environment Names\\n\\n| Environment Name | Long Code |\\n| --- | --- |\\n| Development | dev |\\n| Test | test |\\n| Staging | stg |\\n| Production | prod |\\n\\n### Azure Regions\\n\\n| Azure Region | Geo Short Code | Datacentre Short Code | Short Code |\\n| --- | --- | --- | --- |\\n| East US | us | e | us-e |\\n| East US 2 | us | e2 | us-e2 |\\n| Central US | us | c | us-c |\\n| North Central US | us | cn | us-cn |\\n| West Central US | us | cw | us-cw |\\n| West US | us | w | us-w |\\n| West US 2 | us | w2 | us-w2 |\\n| Australia East | au | e | au-e |\\n| Australia Southeast | au | se | au-se |\\n| Australia Central | au | c | au-c |\\n| New Zealand North | nz | n | nz-n |"},{"id":"azure/microsoft-azure-tagging-conventions","metadata":{"permalink":"/azure/microsoft-azure-tagging-conventions","source":"@site/blog/2022-06-02-microsoft-azure-tagging-conventions.md","title":"Microsoft Azure Tagging Conventions","description":"Organizing cloud-based resources is a crucial task for IT unless you only have simple deployments. Use naming and tagging standards to organize your resources for these reasons:","date":"2022-06-01T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.535,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-06-01T12:00:00.000Z","title":"Microsoft Azure Tagging Conventions","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/microsoft-azure-tagging-conventions"},"unlisted":false,"prevItem":{"title":"Microsoft Azure Naming Conventions","permalink":"/azure/microsoft-azure-naming-conventions"},"nextItem":{"title":"My path to the Microsoft MVP Award","permalink":"/2022/05/31/my-path-to-the-mvp-award"}},"content":"Organizing cloud-based resources is a crucial task for IT unless you only have simple deployments. Use naming and tagging standards to organize your resources for these reasons:\\n\\n* **Resource management**: Your IT teams will need to quickly locate resources associated with specific workloads, environments, ownership groups, or other important information. Organizing resources is critical to assigning organizational roles and access permissions for resource management.\\n* **Cost management and optimization**: Making business groups aware of cloud resource consumption requires IT to understand each team\'s resources and workloads.\\n* **Operations management**: Visibility for the operations management team regarding business commitments and SLAs is an essential aspect of ongoing operations.\\n* **Security:** Classification of data and security impact is a vital data point for the team when breaches or other security issues arise.\\n* **Governance and regulatory compliance:** Maintaining consistency across resources helps identify deviation from agreed-upon policies.\\n* **Automation:** In addition to making resources easier for IT to manage, a proper organizational scheme allows you to take advantage of automation as part of resource creation, operational monitoring, and the result of DevOps processes.\\n\\n**Workload optimization:** Tagging can help identify patterns and resolve broad issues. A tag can also help determine the assets required to support a single workload. Tagging all assets associated with each workload enables a more profound analysis of your mission-critical workloads to make sound architectural decisions.\\n\\n* [Use tags to organize your Azure resources and management hierarchy](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources?tabs=json&WT.mc_id=AZ-MVP-5004796 \\"Use tags to organize your Azure resources and management hierarchy\\")\\n\\n### Tagging Types\\n\\nThe common tagging patterns listed below provide examples of how tagging can be used to organize cloud assets. These patterns are not meant to be exclusive and can be used in parallel, providing multiple ways of organizing assets based on your company\'s needs.\\n\\n| Tag type | Examples | Description |\\n| --- | --- | --- |\\n| Functional | app = catalogsearch1 tier = web webserver = apache env = prod env = staging env = dev | Categorize resources in relation to their purpose within a workload, what environment they have been deployed to, or other functionality and operational details. |\\n| Classification | confidentiality = private SLA = 24hours | Classifies a resource by how it is used and what policies apply to it. |\\n| Accounting | department = finance program = business-initiative region = northamerica | Allows a resource to be associated with specific groups within an organization for billing purposes. |\\n| Partnership | owner = jsmith contactalias = catsearchowners stakeholders = user1; user2; user3 | Provides information about what people (outside of IT) are related or otherwise affected by the resource. |\\n| Purpose | businessprocess = support businessimpact = moderate revenueimpact = high | Aligns resources to business functions to better support investment decisions. |\\n\\n### Tagging Baselines\\n\\nTag at the Resource Group level and then have an Azure policy implemented that tags the resources in that Resource Group with the appropriate tags.\\n\\n| Tag Name | Value | Tag Type | Description | Example |\\n| --- | --- | --- | --- | --- |\\n| Environment | Production Development Sandbox | Functional | Tags the resources with the Environment Tag. This can be used to determine if a resource is Production, Development or Sandbox. | Environment: Production |\\n| Creator | {CreatorName} | Partnership | Tags the resource with the name of who created the resource. This can be used to determine who created the resource to be able to get more information. | Creator: Luke Murray |\\n| CreatedDate | {CreatedDate} | Purpose | Tags the resource with the Date/Time when the resource was created. This can be used to determine how old a resource is, which can be used to look at new functionality on created resources or check if resources are still required. | CreatedDate: 10:00 PM 03/06/2022 NZT |\\n| Criticality | P1 P2 P3 | Purpose | Tags the resources with the criticality of the resources, i.e., if critical, then it is P1. This can be used to determine whether resources need to be highly available, whether changes can be made during or out of business hours. | Criticality:P1 |\\n| SupportedBy | {TeamName} | Partnership | Tags the resources with the team/person or company who supports the resources, whether it is internally supported by the company or outsourced. | SupportedBy:Company |\\n| RequesterName | {Requestor}-{CompanyName) | Partnership | Tags the resources with the user that requested the creation of the resources. | RequesterName:Project Manager |\\n| BillTo | {BillTo} | Accounting | Tags the resources with the cost centre or project codes who will pay for the resources. | BillTo:AppTransformationProject1 |\\n| AutoShutDown | Yes | Functional | This is an Automation functional tag, i.e., tag the resource (Virtual Machine) with a tagging code which will automatically Shut down and Start-up the Virtual Machine at specified times. | AutoShutDown:Yes |\\n| ApplicationName | {ProjectName} | Partnership | Tags the resource with the name of the project or what the resources in the resource group are for. | ApplicationName:AzureVirtualDesktopSH |\\n| Business Unit | {BusinessUnit} | Partnership | Tags the resource with the name of the Business Unit or Company that owns the resources. | BusinessUnit:Finance |\\n| Snapshot | True | Functional | This is an Automation functional tag, i.e., tag the resource (Disk) with a tagging code which can create daily snapshots of disks. | Snapshot:True |"},{"id":"/2022/05/31/my-path-to-the-mvp-award","metadata":{"permalink":"/2022/05/31/my-path-to-the-mvp-award","source":"@site/blog/2022-05-31-my-path-to-the-mvp-award.md","title":"My path to the Microsoft MVP Award","description":"The path to becoming a Microsoft MVP (Most Valuable Professional) is not as linear as some might think, others have a goal to receive the Microsoft MVP Award, and others have a passion for technology that shows in community activities such as speaking or user groups, helping others on forums, and helping to maintain documentation and helping others be up-to-speed with the ever changing ecosystem that is the Microsoft stack - it is not a one size fits all, just as there are multiple ways of learning, there are multiple ways to the MVP Award.","date":"2022-05-31T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.73,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-05-31 00:00:00 +1200","title":"My path to the Microsoft MVP Award","authors":["Luke"],"tags":["Misc"],"toc":false,"header":{"teaser":"/uploads/MVPBuzzChat.png"}},"unlisted":false,"prevItem":{"title":"Microsoft Azure Tagging Conventions","permalink":"/azure/microsoft-azure-tagging-conventions"},"nextItem":{"title":"Architecture in the Cloud","permalink":"/azure/architecture-in-the-cloud"}},"content":"The path to becoming a [Microsoft MVP](https://mvp.microsoft.com) _(Most Valuable Professional)_ is not as linear as some might think, others have a goal to receive the Microsoft MVP Award, and others have a passion for technology that shows in community activities such as speaking or user groups, helping others on forums, and helping to maintain documentation and helping others be up-to-speed with the ever changing ecosystem that is the Microsoft stack - it is not a one size fits all, just as there are multiple ways of learning, there are multiple ways to the MVP Award.\\n\\nI join [Christian Buckley](https://www.linkedin.com/in/ACoAAAAAGE0BlHRIKs-kft9wriNFsr-4V92iu-w) for **episode 167** of his **#MVPbuzChat** Podcast/Video chat, to tell my story, feel free to check it out _(if you can ignore the bad camera angle!)_ and other MVPs talk about their journey to the Microsoft MVP Award.\\n\\n* [#MVPbuzzChat with Luke Murray](https://www.buckleyplanet.com/2022/05/mvpbuzzchat-with-luke-murray.html \\"#MVPbuzzChat with Luke Murray\\")"},{"id":"azure/architecture-in-the-cloud","metadata":{"permalink":"/azure/architecture-in-the-cloud","source":"@site/blog/2022-05-30-architecture-in-the-cloud.md","title":"Architecture in the Cloud","description":"Solution architecture is concerned with the planning, design, implementation, and ongoing improvement of a technology system.","date":"2022-05-30T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.79,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-05-30 00:00:00 +1200","title":"Architecture in the Cloud","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/microsoft_azure.jpg"},"slug":"azure/architecture-in-the-cloud"},"unlisted":false,"prevItem":{"title":"My path to the Microsoft MVP Award","permalink":"/2022/05/31/my-path-to-the-mvp-award"},"nextItem":{"title":"How to contribute to Microsoft documentation","permalink":"/2022/05/27/how-to-contribute-to-microsoft-documentation"}},"content":"Solution architecture is concerned with the planning, design, implementation, and ongoing improvement of a technology system.\\n\\nThe architecture of a system must balance and align the business requirements with the technical capabilities that are needed to execute those requirements.\\n\\nThe finished architecture is a balance of risk, cost, and capability throughout the system and its components.\\n\\nRunning a solution in the cloud does not reduce the need for requirements to be clear. In fact, the flexibility and power provided by the cloud mean that it is even more important to have clear requirements from business stakeholders; otherwise, you could end up solving problems that don\'t exist, missing an important design decision, or going beyond the available budget by adding unnecessary resiliency.\\n\\n![Requirements and Architecture](/uploads/requirementsandarchitecture.png \\"Requirements and Architecture\\")\\n\\n#### Non-functional requirements _(NFRs)_\\n\\nBelow is a short list of NFRs (not exhaustive) that may be provided by the business to help inform the design of a solution.\\n\\n##### **Reliability requirements**\\n\\n* Service level agreement (SLA)\\n* Uptime objective\\n* Recovery time objective (RTO)\\n* Recovery point objective (RPO)\\n* Recoverability\\n\\n##### **Security requirements**\\n\\n* Geographical location\\n* Compliance and legislation\\n* Identity and access management\\n* Privacy\\n* Data Integrity\\n* Public or private endpoints (or both)\\n* OWASP\\n* Hybrid connectivity\\n* DDOS\\n\\n##### **Performance requirements**\\n\\n* Peak throughput, e.g., Requests per minute (RPM), active users\\n* Business plan for growth\\n* UX metrics (e.g., Page load time)\\n* Asynchronous vs Synchronous operations\\n* Workload profile (predictable, unpredictable, peak time of day)\\n* Scalability\\n* Data estate size and growth rate\\n* Time-to-live (TTL) of reports and views (real-time vs eventual consistency)\\n\\n##### **Operational requirements**\\n\\n* Prod and non-prod environments (Dev/Test, QA, Pre-prod, Prod)\\n* Release frequency (hours / days / months)\\n* Time to onboard (new customer)\\n* Licensing\\n* Cost _(Management)_\\n* Manageability\\n\\n##### **Cost optimization**\\n\\n* Cost per user\\n* Target hosting costs as a percentage of revenue\\n* Pricing model\\n* Tenancy model\\n\\n##### **Azure SLAs**\\n\\n* Familiarize yourself with [Azure service-level agreements](https://azure.microsoft.com/en-au/support/legal/sla/?WT.mc_id=AZ-MVP-5004796 \\" Service-level agreements\\")\\n* An Azure Service-level Agreement (SLA) can also be read as a minimum service-level objective (SLO).\\n* An SLA is a financial guarantee, not an absolute guarantee\\n* Read the SLA details carefully, particularly the definition of \\"downtime\\" for each service, which gives important hints about failure modes\\n\\nFor example, in the [SLA for Azure SQL Database](https://azure.microsoft.com/en-au/support/legal/sla/azure-sql-database/v1_8/?WT.mc_id=AZ-MVP-5004796 \\" SLA for Azure SQL Database\\"), \\"downtime\\" is defined as:\\n\\n_\\"The total accumulated Deployment Minutes across all Databases in a given Microsoft Azure subscription during which the Database is unavailable. A minute is considered unavailable for a given Database if all continuous attempts by Customer to establish a connection to the Database within the minute fail.\\"_\\n\\nThe Azure SQL Database team expect almost all outages to be transient (brief and non-recurring). Therefore, the retry pattern should be used to continuously retry for up to a minute. This is typical in cloud services; retry has been the default behaviour in ADO.NET since .NET Framework 4.6.1.\\n\\n#### External Resources\\n\\nFinally, resources such as the Azure Architecture Center, Cloud Adoption and Well-Architected Framework can help with thinking around the design and building blocks of your architecture\\n\\n* [**Azure Architecture Center**](https://learn.microsoft.com/en-us/azure/architecture/?WT.mc_id=AZ-MVP-5004796 \\"Azure Architecture Center\\")\\n* [**Microsoft Azure Well-Architected Framework**](https://learn.microsoft.com/en-us/azure/architecture/framework/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Azure Well-Architected Framework\\")\\n* [**Microsoft Cloud Adoption Framework for Azure**](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Cloud Adoption Framework for Azure\\")"},{"id":"/2022/05/27/how-to-contribute-to-microsoft-documentation","metadata":{"permalink":"/2022/05/27/how-to-contribute-to-microsoft-documentation","source":"@site/blog/2022-05-27-how-to-contribute-to-microsoft-documentation.md","title":"How to contribute to Microsoft documentation","description":"Did you know you can contribute to Microsoft documentation (ms docs)?","date":"2022-05-27T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.08,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to contribute to Microsoft documentation","authors":["Luke"],"tags":["Misc"],"date":"2022-05-27 00:00:00 +1300","toc":false,"header":{"teaser":"/uploads/msdocs_contribute_issue.png"}},"unlisted":false,"prevItem":{"title":"Architecture in the Cloud","permalink":"/azure/architecture-in-the-cloud"},"nextItem":{"title":"Deallocate \'Stopped\' Virtual Machines using Azure Automation","permalink":"/2022/05/12/deallocate-stopped-virtual-machines-using-azure-automation"}},"content":"Did you know you can contribute to Microsoft documentation _(ms docs)_?\\n\\nSuppose you see something not quite right, technically or even if the document\'s readability doesn\'t look right! Then, in true community style, you can contribute!\\n\\n> Tip: You can edit it straight from the Github webpage directly, or pressing \\"**.**\\" in a Github repository will open up Visual Studio Code in Dev spaces with the markdown linter to help check against best practices from your browser.\\n\\nSee the image below for an example:\\n\\n![Update Microsoft documentation](/uploads/updatemsdocs.gif)\\n\\nOnce the pull request is made, it will be reviewed by designated technical document reviewers/product owners at Microsoft. Then your changes will be merged live if successful _(and if not, the reviewers will let you know why and what changes could be made)_!\\n\\n* [Microsoft Docs contributor guide overview](https://learn.microsoft.com/en-us/contribute/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Docs contributor guide overview\\")\\n\\nIf you don\'t want to make the edit yourself, you can also raise an issue and give your feedback by linking to the document, and this will then be worked on by someone to review, contact the relevant product owners, and amended.\\n\\n![MS Docs  - GitHub Raise an Issue](/uploads/msdocs_contribute_issue.png \\"MS Docs  - GitHub Raise an Issue\\")\\n\\nTry to be as concise as possible, as people reading it may not have the same experience as you!"},{"id":"/2022/05/12/deallocate-stopped-virtual-machines-using-azure-automation","metadata":{"permalink":"/2022/05/12/deallocate-stopped-virtual-machines-using-azure-automation","source":"@site/blog/2022-05-12-deallocate-stopped-virtual-machines-using-azure-automation.md","title":"Deallocate \'Stopped\' Virtual Machines using Azure Automation","description":"Virtual Machines?\\") in Microsoft Azure have different states and, depending on what state the Virtual Machine is in, will determine whether you get billed or not (for the Compute, storage and network adapters are still billed)_.","date":"2022-05-12T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":12.81,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-05-12 00:00:00 +1200","title":"Deallocate \'Stopped\' Virtual Machines using Azure Automation","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"How to contribute to Microsoft documentation","permalink":"/2022/05/27/how-to-contribute-to-microsoft-documentation"},"nextItem":{"title":"Hidden Tags in Azure","permalink":"/azure/hidden-tags-in-azure"}},"content":"[Virtual Machines](https://azure.microsoft.com/en-us/overview/what-is-a-virtual-machine/?WT.mc_id=AZ-MVP-5004796#overview \\" What is a virtual machine (VM)?\\") in Microsoft Azure have different states and, depending on what state the Virtual Machine is in, will determine whether you get billed or not _(for the Compute, storage and network adapters are still billed)_.\\n\\n| Power state | Description | Billing |\\n| --- | --- | --- |\\n| Starting | Virtual Machine is powering up. | Billed |\\n| Running | Virtual Machine is fully up. This is the standard working state. | Billed |\\n| Stopping | This is a transitional state between running and stopped. | Billed |\\n| Stopped | The Virtual Machine is allocated on a host but not running. Also called PoweredOff state or Stopped (Allocated). This can be result of invoking the PowerOff API operation or invoking shutdown from within the guest OS. The Stopped state may also be observed briefly during VM creation or while starting a VM from Deallocated state. | Billed |\\n| Deallocating | This is the transitional state between running and deallocated. | Not billed |\\n| Deallocated | The Virtual Machine has released the lease on the underlying hardware and is completely powered off. This state is also referred to as Stopped (Deallocated). | Not billed |\\n\\nSuppose a Virtual Machine is not being used. In that case, turning off a Virtual Machine from the Microsoft Azure Portal _(or programmatically via_ [_PowerShell_](https://learn.microsoft.com/en-us/powershell/azure/?WT.mc_id=AZ-MVP-5004796 \\"Azure PowerShell Documentation\\")_/_[_Azure CLI_](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?WT.mc_id=AZ-MVP-5004796 \\"How to install the Azure CLI\\")_)_ is recommended to ensure that the Virtual Machine is deallocated and its affinity on the host has been released.\\n\\n![Microsoft Azure - Virtual Machine Power States](/uploads/azvm-power-states.png \\"Microsoft Azure - Virtual Machine Power States\\")\\n\\nHowever, you need to know this, and those new to Microsoft Azure, or users who don\'t have [Virtual Machine Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles?WT.mc_id=AZ-MVP-5004796 \\"Azure built-in roles\\") rights to deallocate a Virtual Machine, may simply shut down the operating system, leaving the Virtual Machine in a \'Stopped\' state, but still tied to an underlying Azure host and incurring cost.\\n\\nOur solution can help; by triggering an Alert when a Virtual Machine becomes unavailable due to a user-initiated shutdown, we can then start an [Azure Automation](https://learn.microsoft.com/en-us/azure/automation/overview?WT.mc_id=AZ-MVP-5004796) runbook to deallocate the Virtual Machine.\\n\\n### Overview\\n\\nToday, we are going to set up an Azure Automation runbook, triggered by a Resource Health alert that will go through the following steps:\\n\\n1. User shutdowns Virtual Machine from within the Operating System\\n2. The Virtual Machine enters an unavailable state\\n3. A Resource Alert is triggered when the Virtual Machine becomes unavailable (after being available) by a user initiated event\\n4. The Alert triggers a Webhook to an Azure Automation runbook\\n5. Using permissions assigned to the Azure Automation account through a System Managed Identity connects to Microsoft Azure and checks the VM state; if the Virtual Machine state is still \'Stopped\', then deallocate the virtual machine.\\n6. Then finally, resolve the triggered alert.\\n\\nTo do this, we need a few resources.\\n\\n* Azure Automation Account\\n* Az.AlertsManagement module in the Azure Automation account\\n* Az.Accounts module _(updated in the Azure Automation account)_\\n* Azure Automation runbook _(I will supply this below)_\\n* Resource Health Alert\\n* Webhook _(to trigger to the runbook and pass the JSON from the alert)_\\n\\nAnd, of course, \'Contributor\' rights to the Microsoft Azure subscription to provide the resources and the alerts and resources and set up the system managed identity.\\n\\nWe will set up this from scratch using the Azure Portal and an already created PowerShell Azure Automation runbook.\\n\\n### Deploy Deallocate Solution\\n\\n#### Setup Azure Automation Account\\n\\n##### Create Azure Automation Account\\n\\nFirst, we need an [Azure Automation](https://learn.microsoft.com/en-us/azure/automation/automation-create-standalone-account?tabs=azureportal&WT.mc_id=AZ-MVP-5004796 \\"Create a standalone Azure Automation account\\") resource.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Click **+ Create a resource.**\\n 3. Type in **automation**\\n 4. Select **Create** under Automation, and select **Automation.**\\n 5. ![Create Azure Automation Account](/uploads/azureportal-create-automation.jpg \\"Create Azure Automation Account\\")\\n 6. Select your **subscription**\\n 7. Select your **Resource Group** or Create one if you don\'t already have one _(I recommend placing your automation resources in an Azure Management or Automation resource group, this will also contain your Runbooks)_\\n 8. Select your **region**\\n 9. ![Create Azure Automation Account](/uploads/azureportal-create-automation_basics.jpg \\"Create Azure Automation Account\\")\\n10. Select **Next**\\n11. Make sure: **System assigned** is selected for Managed identities _(this will be required for giving your automation account permissions to deallocate your Virtual Machine, but it can be enabled later if you already have an Azure Automation account)_.\\n12. Click **Next**\\n13. Leave Network connectivity as default (**Public access**)\\n14. Click **Next**\\n15. **Enter** in appropriate **tags**\\n16. ![Create Azure Automation Account](/uploads/azureportal-create-automation_tags.jpg \\"Create Azure Automation Account\\")\\n17. Click **Review + Create**\\n18. After validation has passed, select **Create**\\n\\n##### Configure System Identity\\n\\nNow that we have our Azure Automation account, its time to set up the System Managed Identity and grant it the following roles:\\n\\n* Virtual Machine Contributor _(to deallocate the Virtual Machine)_\\n* Monitoring Contributor _(to close the Azure Alert)_\\n\\nYou can set up a custom role to be least privileged and use that instead. But in this article, we will stick to the built-in roles. \\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click on: **Identity**\\n 4. Make sure that the **System assigned** toggle is: **On** and click **Azure role assignments.**\\n 5. ![Azure Automation Account managed identity](/uploads/azureportal-automation_managedidentity.jpg \\"Azure Automation Account managed identity\\")\\n 6. Click **+ Add role assignments**\\n 7. Select the **Subscription** _(make sure this subscription matches the same subscription your Virtual Machines are in)_\\n 8. Select Role: **Virtual Machine Contributor**\\n 9. Click **Save**\\n10. Now we repeat the same process for **Monitoring Contributor**\\n11. lick **+ Add role assignments**\\n12. Select the **Subscription** _(make sure this subscription matches the same subscription your Virtual Machines are in)_\\n13. Select Role: **Monitoring Contributor**\\n14. Click **Save**\\n15. Click **Refresh** _(it may take a few seconds to update the Portal, so if it is blank - give it 10 seconds and try again)_.\\n16. You have now set up the System Managed identity and granted it the roles necessary to execute the automation.\\n\\n##### Import Modules\\n\\nWe will use the Azure Runbook and use a few Azure PowerShell Modules; by default, Azure Automation has the base Azure PowerShell modules, but we will need to add [Az.AlertsManagement](https://learn.microsoft.com/en-us/powershell/module/az.alertsmanagement/?WT.mc_id=AZ-MVP-5004796 \\"Az.AlertsManagement\\"), and update the Az.Accounts as required as a pre-requisite for Az.AlertsManagement.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click on **Modules**\\n 4. Click on **+ Add a module**\\n 5. Click on **Browse from Gallery**\\n 6. Click: **Click here to browse from the gallery**\\n 7. Type in: **Az.Accounts**\\n 8. Press **Enter**\\n 9. Click on **Az.Accounts**\\n10. Click **Select**\\n11. ![Import Az.Accounts module](/uploads/azureportal-automation_modules_az-accounts.jpg \\"Import Az.Accounts module\\")\\n12. Make sure that the Runtime version is: **5.1** \\n13. Click **Import**\\n14. Now that the Az.Accounts have been updated, and it\'s time to import Az.AlertsManagement!\\n15. Click on **Modules**\\n16. Click on **+ Add a module**\\n17. Click on **Browse from Gallery**\\n18. Click: **Click here to browse from the gallery**\\n19. Type in: **Az.AlertsManagement** _(note its Alert**s)**_\\n20. Click **Az.AlertsManagement**\\n21. ![Az.AlertsManagement module](/uploads/azureportal-automation_modules_az-alertsmanagement.jpg \\"Az.AlertsManagement module\\")\\n22. Click **Select**\\n23. Make sure that the Runtime version is: **5.1** \\n24. Click **Import** _(if you get an error, make sure that Az.Accounts has been updated, through the Gallery import as above)_\\n25. Now you have successfully added the dependent modules!\\n\\n##### Import Runbook\\n\\nNow that the modules have been imported into your Azure Automation account, it is time to import the Azure Automation runbook.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click on **Runbooks**\\n 4. Click **+ Create a runbook**\\n 5. Specify a **name** _(i.e. Deallocate-AzureVirtualMachine)_\\n 6. Select Runbook type of: **PowerShell**\\n 7. Select Runtime version of: **5.1**\\n 8. Type in a **Description** that explains the runbook _(this isn\'t mandatory, but like Tags is recommended, this is an opportunity to indicate to others what it is for and who set it up)_\\n 9. ![Create Azure Runbook](/uploads/azureportal-runbook-create.jpg \\"Create Azure Runbook\\")\\n10. Click **Create**\\n11. Now you will be greeted with a blank edit pane; paste in the Runbook from below:\\n\\n```powershell title=\\"Deallocate-AzureVirtualMachine.ps1\\"\\n#requires -Version 3.0 -Modules Az.Accounts, Az.AlertsManagement\\n<#\\n    .SYNOPSIS\\n    PowerShell Azure Automation Runbook for Stopping Virtual Machines, that have been Shutdown within the Windows Operating System (Stopped and not Deallocated). \\n    .AUTHOR\\n    Luke Murray (https://github.com/lukemurraynz/)\\n#>\\n\\n[OutputType(\'PSAzureOperationResponse\')]\\nparam ( \\n    [Parameter(Mandatory = $true, HelpMessage = \'Data from the WebHook/Azure Alert\')][Object]$WebhookData\\n)\\n\\nImport-Module Az.AlertsManagement\\n$ErrorActionPreference = \'stop\'\\n\\n# Get the data object from WebhookData\\n$WebhookData = $WebhookData.RequestBody\\nWrite-Output -InputObject $WebhookData \\n$Schema = $WebhookData | ConvertFrom-Json\\n\\n#Sets the Webhook data into object\\n$Essentials = [object] ($Schema.data).essentials\\nWrite-Output -InputObject $Essentials \\n\\n# Get the first target only as this script doesn\'t handle multiple and and export variables for the resource.\\n$alertIdArray = (($Essentials.alertId)).Split(\'/\')\\n$alertTargetIdArray = (($Essentials.alertTargetIds)[0]).Split(\'/\')\\n$alertid =  ($alertIdArray)[6]\\n$SubId = ($alertTargetIdArray)[2]\\n$ResourceGroupName = ($alertTargetIdArray)[4]\\n$ResourceType = ($alertTargetIdArray)[6] + \'/\' + ($alertTargetIdArray)[7]\\n$ResourceName = ($alertTargetIdArray)[-1]\\n$status = $Essentials.monitorCondition\\nWrite-Output -InputObject $alertTargetIdArray\\nWrite-Output  -InputObject \\"status: $status\\" -Verbose\\n\\n#Sets VM shutdown\\nif (($status -eq \'Activated\') -or ($status -eq \'Fired\')) {\\n    $status = $Essentials.monitorCondition\\n    Write-Output -InputObject \\"resourceType: $ResourceType\\" -Verbose\\n    Write-Output  -InputObject \\"resourceName: $ResourceName\\" -Verbose\\n    Write-Output  -InputObject \\"resourceGroupName: $ResourceGroupName\\" -Verbose\\n    Write-Output  -InputObject \\"subscriptionId: $SubId\\" -Verbose\\n\\n    # Determine code path depending on the resourceType\\n    if ($ResourceType -eq \'Microsoft.Compute/virtualMachines\') {\\n        # This is an Resource Manager VM\\n        Write-Output  -InputObject \'This is an Resource Manager VM.\' -Verbose\\n\\n        # Ensures you do not inherit an AzContext in your runbook\\n        Disable-AzContextAutosave -Scope Process\\n\\n        # Connect to Azure with system-assigned managed identity\\n        $AzureContext = (Connect-AzAccount -Identity).context\\n\\n        # set and store context\\n        $AzureContext = Set-AzContext -SubscriptionName $AzureContext.Subscription -DefaultProfile $AzureContext\\n        Write-Output   -InputObject $AzureContext \\n        #Checks Azure VM status\\n        $VMStatus = Get-AzVM -ResourceGroupName $ResourceGroupName -Name $ResourceName -Status\\n\\n        Write-Output  -InputObject $VMStatus\\n        If ($VMStatus.Statuses[1].Code -eq \'PowerState/stopped\') {\\n            Write-Output  -InputObject \\"Stopping the VM, it was Shutdown without being Deallocated - $ResourceName - in resource group - $ResourceGroupName\\" -Verbose\\n            Stop-AzVM -Name $ResourceName -ResourceGroupName $ResourceGroupName -DefaultProfile $AzureContext -Force -Verbose\\n      \\n            #Check VM Status after deallocation\\n            $VMStatus = Get-AzVM -ResourceGroupName $ResourceGroupName -Name $ResourceName -Status -Verbose\\n            \\n            Write-Output  -InputObject $VMStatus\\n\\n            If ($VMStatus.Statuses[1].Code -eq \'PowerState/deallocated\') {\\n                #Closes Alert\\n                Write-Output  -InputObject $VMStatus.Statuses[1].Code\\n                Write-Output  -InputObject $alertid \\n                Get-AzAlert -AlertId $alertid  -verbose -DefaultProfile $AzureContext\\n                Get-AzAlert -AlertId $alertid  -verbose -DefaultProfile $AzureContext | Update-AzAlertState -State \'Closed\' -Verbose -DefaultProfile $AzureContext\\n            }\\n        }\\n             \\n        Elseif ($VMStatus.Statuses[1].Code -eq \'PowerState/deallocated\') {\\n            Write-Output  -InputObject \'Already deallocated\' -Verbose\\n        }\\n\\n        Elseif ($VMStatus.Statuses[1].Code -eq \'PowerState/running\') {\\n            Write-Output  -InputObject \'VM running. No further actions\' -Verbose\\n        }\\n\\n        # [OutputType(PSAzureOperationResponse\\")]\\n    }\\n}\\nelse {\\n    # The alert status was not \'Activated\' or \'Fired\' so no action taken\\n    Write-Output  -InputObject (\'No action taken. Alert status: \' + $status) -Verbose\\n}\\n```\\n\\n13. Click **Save**\\n14. ![Azure Automation runbook](/uploads/azureportal-runbook-import.jpg \\"Azure Automation runbook\\")\\n15. Click **Publish** _(so the runbook is actually in production and can be used)_\\n16. You can select View or Edit at any stage, but you have now imported the Azure Automation runbook!\\n\\n##### Setup Webhook\\n\\nNow that the Azure runbook has been imported, we need to set up a Webhook for the Alert to trigger and start the runbook.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to your Azure **Automation account**\\n 3. Click on **Runbooks**\\n 4. **Click** on the **runbook** you just imported _(i.e. Deallocate-AzureVirtualMachine)_\\n 5. Click on **Add webhook**\\n 6. Click **Create a new webhook**\\n 7. **Enter** a **name** for the webhook\\n 8. Make sure it is **Enabled**\\n 9. You can edit the expiry date to match your security requirements; make sure you **record** the expiry date, as it will need to be renewed before it expires.\\n10. **Copy** the **URL** and paste it somewhere safe _(you won\'t see this again! and you need it for the next steps)_\\n11. ![Create Azure webhook](/uploads/azureportal-webhook-create.jpg \\"Create Azure webhook\\")\\n12. Click **Ok**\\n13. Click on **Configure parameters and run settings.**\\n14. Because we will be taking in dynamic data from an Azure Alert, enter in: **\\\\[EmptyString\\\\]**\\n15. Click **Ok**\\n16. Click **Create**\\n17. You have now set up the webhook _(make sure you have saved the URL from the earlier step as you will need it in the following steps)!_\\n\\n#### Setup Alert & Action Group\\n\\nNow that the Automation framework has been created with the Azure Automation account, runbook and webhook, we now need a way to detect if a Virtual Machine has been Stopped; this is where a Resource Health alert will come in.\\n\\n 1. Log into the [**Microsoft Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\").\\n 2. Navigate to: [**Monitor**](https://portal.azure.com/#blade/Microsoft_Azure_Monitoring/AzureMonitoringBrowseBlade/overview)\\n 3. Click on **Service Health**\\n 4. Select **Resource Health**\\n 5. Select **+ Add resource health alert**\\n 6. Select your **subscription**\\n 7. Select **Virtual machine** for Resource Type\\n 8. You can target specific Resource Groups for your alert _(and, as such, your automation)_ or **select all.**\\n 9. Check **Include all future resource groups**\\n10. Check **include all future resources**\\n11. Under the Alert conditions, make sure **Event Status** is: **All selected**\\n12. Set Current resource status to **Unavailable**\\n13. Set Previous resource status to **All selected**\\n14. For reason type, select: **User initiated** and **unknown**\\n15. ![Create Azure Resource Health Alert](/uploads/azureportal-alert-create.jpg \\"Create Azure Resource Health Alert\\")\\n16. Now that we have the Alert rule configured, we need to set up an Action group. That will get triggered when the alert gets fired.\\n17. Click **Select Action groups.**\\n18. Click **+ Create action group**\\n19. **Select** your subscription and **resource group** _(this is where the Action alert will go, I recommend your Azure Management/Monitoring resource group that may have a Log Analytics workspace as an example)_.\\n20. Give your Action Group a **name**, i.e. AzureAutomateActionGroup\\n21. The display name will be automatically generated, but feel free to adjust it to suit your naming convention\\n22. Click **Next: Notifications**\\n23. Under **Notifications**, you can trigger an **email alert**, which can be handy in determining how often the runbook runs. This can be modified and removed if it is running, especially during testing.\\n24. Click **Next: Actions**\\n25. Under Action Type, select **Webhook**\\n26. **Paste** in the **URI** created earlier when setting up the Webhook\\n27. Select **Yes** to enable the **common alert schema** (_this is required as the JSON that the runbook is parsing is expecting it to the in the schema, if it isn\'t the runbook will fail)_\\n28. ![Create Azure Action Group](/uploads/azureportal-actiongroup-webhook.jpg \\"Create Azure Action Group\\")\\n29. Click **Ok**\\n30. Give the **webhook** a **name**.\\n31. Click **Review + create** \\n32. Click **Create**\\n33. Finally, enter in an Alert **name** and **description**, specify the resource group for the Alert to go into and click **Save.**\\n\\n### Test Deallocate Solution\\n\\nSo now we have stood up our:\\n\\n* Azure automation account\\n* Alert\\n* Action Group\\n* Azure automation runbook \\n* Webhook\\n\\nIt is time to test! I have a VM called: VM-D01, running Windows _(theoretically, this runbook will also run against Linux workloads, as its relying on the Azure agent to send the correct status to the Azure Logs, but in my testing, it was against Windows workloads)_ in the same subscription that the alert has been deployed against.\\n\\nAs you can see below, I shut down the Virtual Machine. After a few minutes _(be patient, Azure needs to wait for the status of the VM to be triggered)_, an Azure Alert was fired into Azure Monitor, which triggered the webhook and runbook, and the Virtual Machine was deallocated, and the Azure Alert was closed.\\n\\n![Azure deallocate testing](/uploads/deallocatevm.gif \\"Azure deallocate testing\\")"},{"id":"azure/hidden-tags-in-azure","metadata":{"permalink":"/azure/hidden-tags-in-azure","source":"@site/blog/2022-05-07-hidden-tags-in-azure.md","title":"Hidden Tags in Azure","description":"Tags in Microsoft Azure are pivotal to resource management, whether it\'s used for reporting or automation.","date":"2022-05-07T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.495,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Hidden Tags in Azure","authors":["Luke"],"tags":["Azure"],"date":"2022-05-07 00:00:00 +1300","toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/hidden-tags-in-azure"},"unlisted":false,"prevItem":{"title":"Deallocate \'Stopped\' Virtual Machines using Azure Automation","permalink":"/2022/05/12/deallocate-stopped-virtual-machines-using-azure-automation"},"nextItem":{"title":"Azure Arc Bridge - Implementation and Testing","permalink":"/2022/04/20/azure-arc-bridge-implementing-and-testing"}},"content":"[Tags](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources?tabs=json&WT.mc_id=AZ-MVP-5004796 \\"Use tags to organize your Azure resources and management hierarchy\\") in Microsoft Azure are pivotal to resource management, whether it\'s used for reporting or automation.\\n\\nBut sometimes, you need that extra bit of information to help discover what resources are for, or you may way to add information to a resource that isn\'t directly displayed in the portal, especially when complex tags are in use that might be used in automation initiatives.\\n\\nThis is where \'hidden\' Azure Tags come in handy.\\n\\nTags starting with the prefix of _\'hidden-\'_ will not be displayed under Tags in the Azure Portal; however, they will be displayed in the resource metadata and utilised by PowerShell and Azure CLI for automation initiatives.\\n\\nExamples are:\\n\\n| Tags | Value |\\n| --- | --- |\\n| hidden-title | Web Server |\\n| hidden-ShutdownAutomation | Yes |\\n\\n### hidden-title\\n\\nAs I mentioned above, every tag with \'hidden-\' in front of it will be hidden in the Azure Portal. However, \'hidden-title\' behaves differently.\\n\\nYou may have noticed that some resources in Azure, especially if the Azure ARM _(Azure Resource Manager)_ creates them and the name is GUID based, has a \'(name)\' around them after the resource name; this is because of the hidden-title tag.\\n\\nThe hidden-title tag is especially useful for being able to pick resources that belong to a specific service or application.\\n\\nAn example is below:\\n\\n![Azure Portal - Hidden Title Tag](/uploads/azureportal_hiddentitle.png \\"Azure Portal - Hidden Title Tag\\")\\n\\nIn this case, I have used the _hidden-title_ of \'Web Server\', allowing me to quickly view what resources may be mapped to my Web Server.\\n\\nYou may notice that the Test-Virtual Machines title, is displayed in the Resource Groups search blade and not in the actual Resource Group, there are some areas of the Portal that will not display the hidden-title tag currently.\\n\\nIf I navigate to my Virtual Machine and click on the Tags blade, all I see is my CreatedBy tag.\\n\\n![Azure Portal - Tags](/uploads/azureportal-hiddentitle-vmtags.png \\"Azure Portal - Tags\\")\\n\\nHowever, if I navigate to the Overview page and click on JSON View, I can see the hidden tags in the resource metadata.\\n\\n![Azure Portal - Resource Tags](/uploads/azureportal-hiddentitle-vmtagsjson.png \\"Azure Portal - Resource Tags\\")\\n\\n### hidden tags\\n\\n#### Azure Portal\\n\\nYou can use the Azure Portal directly to add the Tags to apply hidden tags.\\n\\n![Azure Portal - Add Tags](/uploads/azureportal_hiddentagsadd.png \\"Azure Portal - Add Tags\\")\\n\\nYou can remove the Tag by adding the hidden-tag again and keeping the value empty _(ie blanking out the hidden-title will remove the title)_, but it will still be against the metadata _(Resource Graph)_ as a Tag that exists (as seen in the screenshot below) - it is much cleaner to use PowerShell.\\n\\n![Azure - Resource Tags](/uploads/azureportal_hiddentagsremove.png \\"Azure - Resource Tags\\")\\n\\n#### PowerShell\\n\\nGet-AzTag and Remove-AzTag, do not display the hidden tags, to add and remove the tags, you need to add them through \'Update-AzTag\' and \'Replace\' or \'Merge\' to overwrite the tags, which requires the Resource targetted by Resource ID.\\n\\nA handy snippet to use to add/remove the Tags on individual or multiple resources is:\\n\\n    $replacedTags = @{\\"hidden-title\\" = \\"Web Server\\"; \\"hidden-ShutdownAutomation\\" = \\"Yes\\"}\\n    $resouceGroup = \'vm-dev-rg\'\\n    Get-AzResource -ResourceGroupName $resouceGroup | Select-Object ResourceId | Out-GridView -PassThru | Update-AzTag -Tag $replacedTags -Operation Merge\\n\\nThis will snippet will gather all the resources in your Resource Group, then select their Resource IDs; the script will then prompt with a GUI allowing you to select which resources or resources you want to update your tags on, then once you click Ok, it will update the Tags on the resources you selected.\\n\\n![PowerShell - Add Azure Tags](/uploads/powershell_hiddentagsadd.png \\"PowerShell - Add Azure Tags\\")\\n\\nYou may be wondering if the Hidden tags are useful for automation, but if the \'Get-AzTag\' cmdlet doesn\'t work, how can I retrieve the resources? It\'s a good question, and that is where \'Get-AzResource\' comes to the rescue.\\n\\nExamples are:\\n\\n    Get-AzResource -TagName hidden-ShutdownAutomation\\n\\n    Get-AzResource -TagValue Yes\\n\\n    $TagName = \'hidden-title\'\\n    $TagValue = \'Web Server\'\\n    Get-AzResource -TagName $TagName -TagValue $TagValue | Where-Object -FilterScript {\\n        $_.ResourceType -like \'Microsoft.Compute/virtualMachines\' \\n    }\\n\\n#### Azure Bicep\\n\\nYou can also add the Tags, with Azure Bicep.\\n\\nExample is:\\n\\n    param resourceTags object = {\\n       hidden-title: \'Web Server\'\\n       hidden-ShutdownAutomation: \'Yes\'\\n    }\\n    \\n    tags: resourceTags"},{"id":"/2022/04/20/azure-arc-bridge-implementing-and-testing","metadata":{"permalink":"/2022/04/20/azure-arc-bridge-implementing-and-testing","source":"@site/blog/2022-04-20-azure-arc-bridge-implementing-and-testing.md","title":"Azure Arc Bridge - Implementation and Testing","description":"Azure Arc Bridge(currently in preview)_ is part of the core Azure Arc Hybrid Cloud platform.","date":"2022-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":7.73,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Arc Bridge - Implementation and Testing","authors":["Luke"],"tags":["Azure"],"date":"2022-04-20 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/azure_arc_vmware_portal_createresourcebridge.png"}},"unlisted":false,"prevItem":{"title":"Hidden Tags in Azure","permalink":"/azure/hidden-tags-in-azure"},"nextItem":{"title":"Create a Site to Site VPN to Azure with a Ubiquiti Dream Machine Pro","permalink":"/2022/03/27/create-a-site-to-site-vpn-to-azure-with-a-ubiquiti-dream-machine-pro"}},"content":"[Azure Arc Bridge](https://learn.microsoft.com/en-us/azure/azure-arc/resource-bridge/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Arc resource bridge \\")_(currently in preview)_ is part of the core Azure Arc Hybrid Cloud platform.\\n\\n### Overview\\n\\nThe Azure Arc resource bridge allows for VM _(Virtual Machine)_ self-servicing and managing on-premises Azure Stack HCI and VMWare virtualised workloads, supporting Linux and Windows.\\n\\nAlong with standard integration of Azure Arc workloads, such as support for Azure Policy and Azure extensions, Azure Update Management and Defender for Cloud support. The Azure Arc resource bridge offers the following self-service functionality direct from the Microsoft Azure portal, offering a single pane of a glass of your workloads, whether they exist on-premises or in Azure:\\n\\n* Start, stop and restart a virtual machine\\n* Control access and add Azure tags\\n* Add, remove, and update network interfaces\\n* Add, remove, and update disks and update VM size _(CPU cores and memory)_\\n* Enable guest management\\n* Install extensions\\n* Azure Stack HCI - You can provision and manage on-premises Windows and Linux virtual machines _(VMs)_ running on Azure Stack HCI clusters.\\n\\n> The resource bridge is a packaged virtual machine, which hosts a _management_ Kubernetes cluster that requires no user management. This virtual appliance delivers the following benefits:\\n>\\n> * Enables VM self-servicing from Azure without having to create and manage a Kubernetes cluster\\n> * It is fully supported by Microsoft, including update of core components.\\n> * Designed to recover from software failures.\\n> * Supports deployment to any private cloud hosted on Hyper-V or VMware from the Azure portal or using the Azure Command-Line Interface (CLI).\\n>\\n> All management operations are performed from Azure, no local configuration is required on the appliance.\\n\\n![Azure Arc - Overview](/uploads/arc-bridge-architecture-overview.png)\\n\\n> Azure Arc resource bridge currently supports the following Azure regions:\\n>\\n> * East US\\n> * West Europe\\n\\nThese regions hold the Resource Bridge metadata for the resources.\\n\\nToday, we will stand up an Azure Arc Bridge that supports VMWare vSphere.\\n\\nI will be running vSphere 6.7 on a single host in my home lab, connected to my Visual Studio subscription.\\n\\n### Prerequisites\\n\\n#### Private cloud environments\\n\\nThe following private cloud environments and their versions are officially supported for the Azure Arc resource bridge:\\n\\n* VMware vSphere version 6.7\\n* Azure Stack HCI\\n\\n_Note: You are unable to set this up on vSphere 7.0.3, as it is not currently supported - I tried!_\\n\\n#### Permissions\\n\\n* Contributor rights to the Resource Group that the Azure Arc bridge resource will be created.\\n* vSphere account _(with at least Read and modify VM rights)_\\n\\n#### Required Azure resources\\n\\n* Resource Group for your Azure Arc Resource Bridge\\n\\n#### Required On-premises resources\\n\\n* Resource pool with a reservation of at least 16 GB of RAM and four vCPUs. It should also have access to a datastore with at least 100 GB of free disk space.\\n* A workstation with rights to run PowerShell and install Python and the Azure CLI, with a line of sight to vCenter.\\n\\n#### Networking\\n\\n* The Arc resource bridge communicates outbound securely to Azure Arc over TCP port 443\\n* At least one free IP _(Internet Protocol)_ address on the on-premises network _(or three if there isn\'t a DHCP server)._ Make sure this isn\'t a used IP; you will need to enter this during the bridge provisioning script.\\n\\n### Create Azure Arc Resource Bridge\\n\\n#### Create Resource Bridge\\n\\n 1. Log in to the **Azure Portal**\\n 2. In the search box up the top, type in: **Azure Arc**\\n 3. Click **Azure Arc**\\n 4. Click on: **VMware vCenters (preview)**\\n 5. Click **Add**\\n 6. ![Azure Arc - Resource Bridge](/uploads/azure_arc_vmware_portal.png \\"Azure Arc - Resource Bridge\\")\\n 7. Click: **Create a new resource bridge**\\n 8. ![Azure Arc - Resource Bridge](/uploads/azure_arc_vmware_portal_createresourcebridge.png \\"Azure Arc - Resource Bridge\\")\\n 9. Click **Next: Basics**\\n10. **Enter** the **following** information to suit your environment:\\n\\n* **Name** _(of the Resource Bridge resource)_\\n* Select the **region** for your Metadata\\n* Create a [**Custom Location**](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/custom-locations?WT.mc_id=AZ-MVP-5004796 \\"Create and manage custom locations on Azure Arc-enabled Kubernetes\\")_(that matches your on-premises location, where your resources are stored, i.e. could be a data centre prefix that matches your naming convention)_\\n* Enter in the **name** of your **vCenter** resource _(this will represent your vCenter in Azure, so make sure it is easily identifiable)_\\n\\n 1. ![Azure Arc - vCenter](/uploads/azure_arc_vmware_portal_createresourcesbridge.png \\"Azure Arc - vCenter\\")\\n 2. Click **Next: Tags**\\n 3. A list of default tags has been supplied; feel free to enter or change these to suit your environment.\\n 4. ![Azure Arc - vCenter](/uploads/azure_arc_vmware_portal_createresourcesbridgetags.png \\"Azure Arc - vCenter\\")\\n 5. Click **Next: Download and run the script.**\\n 6. Click on **Register** to register the Azure Arc Provider to your subscription. Please wait for this process to complete _(it may take a minute or two, you will see: Successfully register your subscription(s) when completed)_.\\n 7. Once completed, **download** the **onboarding** PowerShell **script**\\n 8. **Run** the PowerShell **script** from a computer that has access to Azure and vCenter. This script will download the necessary dependencies _(Azure CLI, Python)_ and, if necessary, authenticate to Azure.\\n\\n        Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\\n        ./resource-bridge-onboarding-script.ps1\\n 9. When the script runs, you will be prompted for the following information.\\n    * **Proxy** information _(if the Workstation is behind a proxy)_\\n    * **UAC** _(User Access Control)_ **approval** for the script to install Azure CLI/Python on the workstation\\n    * **Azure authentication**\\n    * **vCenter** FQDN/**Address**\\n    * **vCenter Username** & **Password**\\n    * **vCenter datastore**\\n    * **vCenter folder** _(to place the template in)_\\n    * **IP address**\\n10. ![Azure Arc - vCenter Onboarding](/uploads/deploy-azurearcbridge.gif \\"Azure Arc - vCenter Onboarding\\")\\n\\n#### Link vCenter to Azure Arc\\n\\nYou may not need to do the below, but my Bridge was in a \'running\' state but hadn\'t added in the connection to vCenter.\\n\\n 1. Log in to the **Azure Portal**\\n 2. In the search box up the top, type in: **Azure Arc**\\n 3. Click **Azure Arc**\\n 4. Click on: **Resource bridges (preview)**\\n 5. Click on your Azure Arc Bridge and **verify** the **status** is \'**Running**\' _(if it is not, make sure it has been started on-premises)_\\n 6. In the Azure Portal, click on **VMWare vCenters (preview)**\\n 7. Click **Add**\\n 8. Click **Use an existing resource bridge**\\n 9. Click **Next: Basics**\\n10. Create your **Custom Location**, then **enter** in the on-premises **vCenter** details\\n11. ![Azure Arc - vCenter Onboarding](/uploads/azure_arc_vmware_portal_bridge_vcenter.png \\"Azure Arc - vCenter Onboarding\\")\\n12. On the next blade, enter in your appropriate **Tags,** then click **Create**\\n13. Wait for the deployment to complete; this could take 2-5 minutes.\\n14. In the search box up the top, type in: **Azure Arc**\\n15. Click **Azure Arc**\\n16. Click on: **VMware vCenters (preview)**\\n17. You should now see your vSphere instance in a Connected state.\\n18. ![Azure Arc - vCenter](/uploads/azure_arc_vmware_portal_bridge_vcenterdeployed.png \\"Azure Arc - vCenter\\")\\n\\n#### Enable vCenter resources to be managed in Microsoft Azure\\n\\nNow that the Bridge has been created, we need to allow resources _(such as Virtual Machines, Datastores,  Networks)_.\\n\\n 1. Log in to the **Azure Portal**\\n 2. In the search box up the top, type in: **Azure Arc**\\n 3. Click **Azure Arc**\\n 4. Click on: **VMware vCenters (preview)**\\n 5. Click on your **vCenter instance**\\n 6. Under vCenter Inventory, select **Virtual Machines**\\n 7. ![Azure Arc - vCenter](/uploads/azure_arc_vsphere_vm.png \\"Azure Arc - vCenter\\")\\n 8. Select the Virtual Machines you want to enable for management in Azure and click \'**Enable in Azure**\'\\n 9. Select your applicable Subscription and Resource Group (this is where the Azure Arc VM resources will be placed)\\n10. Make sure \'**Enable Guest management**\' is selected.\\n11. Enter in your Administrator _(this is the Admin Username and password of the workloads you want to install the Azure guest management too)_\\n12. ![Azure Arc - On-premises VM](/uploads/azure_arc_vsphere_vmguestagent.png \\"Azure Arc - On-premises VM\\")\\n13. Click **Enable**\\n14. It can take a few minutes to onboard these clients. If it fails, pick a single Virtual Machine and attempt to onboard that.\\n15. You can now repeat the process to onboard Networks, Resource Pools etc.\\n\\n### Manage Virtual Machines in Microsoft Azure\\n\\nNow that you have set up an Azure Arc Bridge and onboarded vCenter resources. You can now see and manage your vCenter Virtual Machines in Azure, examples below.\\n\\n* Ensure that you have VMWare Tools installed and up-to-date to help full functionality, such as Restart, or there may be issues managing these.\\n\\n#### Stop/Stop Virtual Machines\\n\\n![Azure Arc - Start/Stop VM](/uploads/startstopvm-azurearcbridge.gif \\"Azure Arc - Start/Stop VM\\")\\n\\n#### Resize Virtual Machines - CPU/Memory\\n\\n![Azure Arc - Resize VM](/uploads/resizevm-azurearcbridge.gif \\"Azure Arc - Resize VM\\")\\n\\n#### Resize Virtual Machines - Disk\\n\\n![Azure Arc - Resize Disk](/uploads/resizevmdisk-azurearcbridge.gif \\"Azure Arc - Resize Disk\\")\\n\\n### Troubleshooting\\n\\n* The \'resource-bridge-onboarding-script.ps1\' script contains an output file, named: arcvmware-output.log. This log file exists in the same directory as the script and is useful for investigating any errors.\\n* If you get no Folders, listed when the script prompts you to select a folder _(i.e. Please select folder)_:\\n\\n1. Right-click the Datacenter in vSphere\\n2. Select New Folder\\n3. Select New VM and Templates folder\\n4. Create a folder\\n\\n* If your Center becomes unavailable, it is most likely because you specified the same IP for the Azure Arc Appliance; if this is the case, log in to the host containing your Azure Arc Bridge and stop/delete the resources from the disk and remove from inventory. Then rerun deployment, this time selecting an appropriate IP."},{"id":"/2022/03/27/create-a-site-to-site-vpn-to-azure-with-a-ubiquiti-dream-machine-pro","metadata":{"permalink":"/2022/03/27/create-a-site-to-site-vpn-to-azure-with-a-ubiquiti-dream-machine-pro","source":"@site/blog/2022-03-27-create-a-site-to-site-vpn-to-azure-with-a-ubiquiti-dream-machine-pro.md","title":"Create a Site to Site VPN to Azure with a Ubiquiti Dream Machine Pro","description":"The Ubiquiti Dream Machine Pro has a lot of functionality built-in, including IPsec Site-to-site VPN(Virtual Private Network) support.","date":"2022-03-27T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.885,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-03-27 00:00:00 +1300","title":"Create a Site to Site VPN to Azure with a Ubiquiti Dream Machine Pro","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Azure Arc Bridge - Implementation and Testing","permalink":"/2022/04/20/azure-arc-bridge-implementing-and-testing"},"nextItem":{"title":"Azure Optimization Engine","permalink":"/2022/03/16/azure-optimisation-engine"}},"content":"The Ubiquiti [Dream Machine Pro](https://store.ui.com/collections/unifi-network-unifi-os-consoles/products/udm-pro \\"Dream Machine Pro\\") has a lot of functionality built-in, including IPsec Site-to-site VPN_(Virtual Private Network)_ support.\\n\\nI recently installed and configured a UDM-PRO at home, so now it\'s time to set up a site-to-vpn to my Microsoft Azure network.\\n\\nI will create Virtual Network and Gateway resources using Azure Bicep, but please skip ahead.\\n\\nMy address range is as follows _(so make sure you adjust to match your setup and IP ranges)_:\\n\\n| On-premises | Azure |\\n| --- | --- |\\n| 192.168.1.0/24 | 10.0.0.0/16 |\\n\\n#### Prerequisites\\n\\n* The latest [Azure PowerShell](https://learn.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.1.0?WT.mc_id=AZ-MVP-5004796) modules and [Azure Bicep/Azure CLI](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install?WT.mc_id=AZ-MVP-5004796) for local editing\\n* An Azure subscription that you have at least contributor rights to\\n* Permissions to the UDM Pro to set up a new network connection\\n\\nI will be using PowerShell [splatting](https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_splatting?WT.mc_id=AZ-MVP-5004796 \\"Splatting\\") as it\'s easier to edit and display. You can easily take the scripts here to make them your own.\\n\\n#### Deploy - Azure Network and Virtual Network Gateway\\n\\nI will assume that you have both [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install#windows?WT.mc_id=AZ-MVP-5004796 \\"Azure Bicep - Install\\") and[PowerShell Azure](https://learn.microsoft.com/en-us/powershell/azure/install-az-ps?WT.mc_id=AZ-MVP-5004796 \\"PowerShell - Azure\\") modules installed and the know-how to connect to Microsoft Azure.\\n\\nAzure Bicep deployments _(like ARM)_ have the following command: \'TemplateParameterObject\'. \'TemplateParameterObject\' allows Azure Bicep to accept parameters from PowerShell directly, which can be pretty powerful when used with a self-service portal or pipeline.\\n\\nI will first make an Azure Resource Group using PowerShell for my Azure Virtual Network, then use the New-AzResourceGroupDeployment cmdlet to deploy my Virtual Network and subnets from my bicep file.\\n\\nAlong with the Virtual Network, we will also create 2 other Azure resources needed for a Site to Site VPN, a [Local Network Gateway](https://learn.microsoft.com/en-us/azure/vpn-gateway/tutorial-site-to-site-portal?WT.mc_id=AZ-MVP-5004796 \\"Tutorial: Create a site-to-site VPN connection in the Azure portal\\") _(this will represent your on-premises subnet and external IP to assist with routing)_and a [Virtual Network Gateway](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways?WT.mc_id=AZ-MVP-5004796 \\"What is VPN Gateway?\\") _(which is used to send encrypted traffic over the internet between your on-premises site(s) and Azure)_.\\n\\nUpdate the parameters of the PowerShell script below, to match your own needs, and you may need to edit the Bicep file itself to add/remove subnets and change the IP address space to match your standards.\\n\\nThe shared key will be used between the UDM Pro and your Azure network; make sure this is unique.\\n\\n    #Connects to Azure\\n    Connect-AzAccount\\n    #Resource Group Name\\n    $resourcegrpname = \'network_rg\'\\n    #Creates a resource group for the storage account\\n    New-AzResourceGroup -Name $resourcegrpname -Location \'AustraliaEast\'\\n    # Parameters splat, for Azure Bicep\\n    # Parameter options for the Azure Bicep Template, this is where your Azure Bicep parameters go\\n    $paramObject = @{\\n    \'sitecode\' = \'luke\'\\n    \'environment\' = \'prod\'\\n    \'contactEmail\' = \'email@luke.geek.nz\'\\n    \'sharedkey\' = \'18d5b51a17c68a42d493651bed88b73234bbaad0\'\\n    \'onpremisesgwip\' = \'123.456.789.101\'\\n    \'onpremisesaddress\' = \'192.168.1.0/24\'\\n    }\\n    # Parameters for the New-AzResourceGroupDeployment cmdlet goes into.\\n    $parameters = @{\\n    \'Name\' = \'AzureNetwork-S2S\'\\n    \'ResourceGroupName\' = $resourcegrpname\\n    \'TemplateFile\' = \'c:\\\\temp\\\\Deploy-AzVNETS2S.bicep\'\\n    \'TemplateParameterObject\' = $paramObject\\n    \'Verbose\' = $true\\n    }\\n    #Deploys the Azure Bicep template\\n    New-AzResourceGroupDeployment @parameters -WhatIf\\n\\nNote: The _\'_[_-whatif_](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/deploy-what-if?tabs=azure-powershell%2CCLI?WT.mc_id=AZ-MVP-5004796 \\"Bicep deployment what-if operation\\")\' parameter has been added as a safeguard, so once you know the changes are suitable, then remove and rerun.\\n\\nThe Virtual Network Gateway can take 20+ minutes to deploy, leave the Terminal/PowerShell window open, you can also check the Deployment in the Azure Portal _(Under Deployments panel in the Resource Group)_.\\n\\n![Azure Portal - Resource Group Deployments](/uploads/vnet-deployments2svpnazportal.png \\"Azure Portal - Resource Group Deployments\\")\\n\\nThe Azure Bicep file is located here:\\n\\n```bicep title=\\"Deploy-AzVNETS2S.bicep\\"\\ntargetScope = \'resourceGroup\'\\n\\n///Parameter and Variable Setting\\n\\n@minLength(3)\\n@maxLength(6)\\nparam sitecode string = \'\'\\n\\nparam environment string = \'\'\\nparam contactEmail string = \'\'\\n\\nparam resourceTags object = {\\n  Application: \'Azure Infrastructure Management\'\\n  CostCenter: \'Operational\'\\n  CreationDate: dateTime\\n  Environment: environment\\n  CreatedBy: contactEmail\\n  Notes: \'Created on behalf of: ${sitecode} for their Site to Site VPN.\'\\n}\\n\\nparam dateTime string = utcNow(\'d\')\\nparam location string = resourceGroup().location\\n\\nparam sharedkey string = \'\'\\nparam onpremisesaddress string = \'\'\\nparam onpremisesgwip string = \'\'\\n\\n//Resource Naming Parameters\\nparam virtualNetworks_vnet_name string = \'${sitecode}-vnet\'\\nparam connections_S2S_Connection_Home_name string = \'S2S_Connection_Home\'\\nparam publicIPAddresses_virtualngw_prod_name string = \'${sitecode}-pip-vngw-${environment}\'\\nparam localNetworkGateways_localngw_prod_name string = \'${sitecode}-localngw-${environment}\'\\nparam virtualNetworkGateways_virtualngw_prod_name string = \'${sitecode}-virtualngw-${environment}\'\\n\\nresource localNetworkGateways_localngw_prod_name_resource \'Microsoft.Network/localNetworkGateways@2020-11-01\' = {\\n  name: localNetworkGateways_localngw_prod_name\\n\\n  location: location\\n  properties: {\\n    localNetworkAddressSpace: {\\n      addressPrefixes: [\\n        onpremisesaddress\\n      ]\\n    }\\n    gatewayIpAddress: onpremisesgwip\\n  }\\n}\\n\\nresource publicIPAddresses_virtualngw_prod_name_resource \'Microsoft.Network/publicIPAddresses@2020-11-01\' = {\\n  name: publicIPAddresses_virtualngw_prod_name\\n  tags: resourceTags\\n  location: location\\n  sku: {\\n    name: \'Standard\'\\n    tier: \'Regional\'\\n  }\\n  properties: {\\n    publicIPAddressVersion: \'IPv4\'\\n    publicIPAllocationMethod: \'Static\'\\n    idleTimeoutInMinutes: 4\\n    ipTags: []\\n  }\\n}\\n\\nresource virtualNetworks_vnet_name_resource \'Microsoft.Network/virtualNetworks@2020-11-01\' = {\\n  name: virtualNetworks_vnet_name\\n  location: location\\n  tags: resourceTags\\n  properties: {\\n    addressSpace: {\\n      addressPrefixes: [\\n        \'10.0.0.0/16\'\\n      ]\\n    }\\n    subnets: [\\n      {\\n        name: \'GatewaySubnet\'\\n        properties: {\\n          addressPrefix: \'10.0.0.0/26\'\\n          delegations: []\\n          privateEndpointNetworkPolicies: \'Enabled\'\\n          privateLinkServiceNetworkPolicies: \'Enabled\'\\n        }\\n      }\\n      {\\n        name: \'AzureBastionSubnet\'\\n        properties: {\\n          addressPrefix: \'10.0.0.64/27\'\\n          delegations: []\\n          privateEndpointNetworkPolicies: \'Enabled\'\\n          privateLinkServiceNetworkPolicies: \'Enabled\'\\n        }\\n      }\\n      {\\n        name: \'AzureFirewallSubnet\'\\n        properties: {\\n          addressPrefix: \'10.0.0.128/26\'\\n          delegations: []\\n          privateEndpointNetworkPolicies: \'Enabled\'\\n          privateLinkServiceNetworkPolicies: \'Enabled\'\\n        }\\n      }\\n      {\\n        name: \'appservers\'\\n        properties: {\\n          addressPrefix: \'10.0.2.0/24\'\\n          delegations: []\\n          privateEndpointNetworkPolicies: \'Enabled\'\\n          privateLinkServiceNetworkPolicies: \'Enabled\'\\n        }\\n      }\\n    ]\\n    virtualNetworkPeerings: []\\n    enableDdosProtection: false\\n  }\\n}\\n\\nresource virtualNetworks_vnet_name_appservers \'Microsoft.Network/virtualNetworks/subnets@2020-11-01\' = {\\n  parent: virtualNetworks_vnet_name_resource\\n  name: \'appservers\'\\n  properties: {\\n    addressPrefix: \'10.0.2.0/24\'\\n    delegations: []\\n    privateEndpointNetworkPolicies: \'Enabled\'\\n    privateLinkServiceNetworkPolicies: \'Enabled\'\\n  }\\n}\\n\\nresource virtualNetworks_vnet_name_AzureBastionSubnet \'Microsoft.Network/virtualNetworks/subnets@2020-11-01\' = {\\n  parent: virtualNetworks_vnet_name_resource\\n  name: \'AzureBastionSubnet\'\\n  properties: {\\n    addressPrefix: \'10.0.0.64/27\'\\n    delegations: []\\n    privateEndpointNetworkPolicies: \'Enabled\'\\n    privateLinkServiceNetworkPolicies: \'Enabled\'\\n  }\\n}\\n\\nresource virtualNetworks_vnet_name_AzureFirewallSubnet \'Microsoft.Network/virtualNetworks/subnets@2020-11-01\' = {\\n  parent: virtualNetworks_vnet_name_resource\\n  name: \'AzureFirewallSubnet\'\\n  properties: {\\n    addressPrefix: \'10.0.0.128/26\'\\n    delegations: []\\n    privateEndpointNetworkPolicies: \'Enabled\'\\n    privateLinkServiceNetworkPolicies: \'Enabled\'\\n  }\\n}\\n\\nresource virtualNetworks_vnet_name_GatewaySubnet \'Microsoft.Network/virtualNetworks/subnets@2020-11-01\' = {\\n  parent: virtualNetworks_vnet_name_resource\\n  name: \'GatewaySubnet\'\\n  properties: {\\n    addressPrefix: \'10.0.0.0/26\'\\n    delegations: []\\n    privateEndpointNetworkPolicies: \'Enabled\'\\n    privateLinkServiceNetworkPolicies: \'Enabled\'\\n  }\\n}\\n\\nresource connections_S2S_Connection_Home_name_resource \'Microsoft.Network/connections@2020-11-01\' = {\\n  name: connections_S2S_Connection_Home_name\\n  location: location\\n  properties: {\\n    virtualNetworkGateway1: {\\n      id: virtualNetworkGateways_virtualngw_prod_name_resource.id\\n    }\\n    localNetworkGateway2: {\\n      id: localNetworkGateways_localngw_prod_name_resource.id\\n    }\\n    connectionType: \'IPsec\'\\n    connectionProtocol: \'IKEv2\'\\n    routingWeight: 0\\n    sharedKey: sharedkey\\n    enableBgp: false\\n    useLocalAzureIpAddress: false\\n    usePolicyBasedTrafficSelectors: false\\n    ipsecPolicies: []\\n    trafficSelectorPolicies: []\\n    expressRouteGatewayBypass: false\\n    dpdTimeoutSeconds: 0\\n    connectionMode: \'Default\'\\n  }\\n}\\n\\nresource virtualNetworkGateways_virtualngw_prod_name_resource \'Microsoft.Network/virtualNetworkGateways@2020-11-01\' = {\\n  name: virtualNetworkGateways_virtualngw_prod_name\\n  location: location\\n  properties: {\\n    enablePrivateIpAddress: false\\n    ipConfigurations: [\\n      {\\n        name: \'default\'\\n        properties: {\\n          privateIPAllocationMethod: \'Dynamic\'\\n          publicIPAddress: {\\n            id: publicIPAddresses_virtualngw_prod_name_resource.id\\n          }\\n          subnet: {\\n            id: virtualNetworks_vnet_name_GatewaySubnet.id\\n          }\\n        }\\n      }\\n    ]\\n    sku: {\\n      name: \'VpnGw2\'\\n      tier: \'VpnGw2\'\\n    }\\n    gatewayType: \'Vpn\'\\n    vpnType: \'RouteBased\'\\n    enableBgp: false\\n    activeActive: false\\n    bgpSettings: {\\n      asn: 65515\\n      bgpPeeringAddress: \'10.0.0.62\'\\n      peerWeight: 0\\n\\n      \\n    }\\n    vpnGatewayGeneration: \'Generation2\'\\n  }\\n}\\n```\\n\\nOnce deployed, run the following command to capture and copy the Gateway Public IP: \\n\\n    Get-AzPublicIPAddress | Select-Object Name, IpAddress \\n\\nCopy the Public IP, we will need this for configuring the UDM Pro, this would have been generated dynamically.\\n\\n#### Configure - Ubiquiti Dream Machine Pro\\n\\n 1. **Login** to the **UDM-Pro**\\n 2. ![Unifi OS](/uploads/udm-pro_unifi-os.png \\"UDM Pro Unifi OS\\")\\n 3. Click on **Network** _(under Applications heading)_\\n 4. Click **Settings** _(Gear icon)_\\n 5. ![Unifi OS - Network](/uploads/udm-pro_networksettings.png \\"UDM Pro Unifi OS\\")\\n 6. Click **VPN**\\n 7. ![UDM Pro Unifi OS - VPN](/uploads/udm-pro_vpn_s2svpn.png \\"UDM Pro Unifi OS - VPN\\")\\n 8. Scroll down and click **+ Create Site-to site-VPN**\\n 9. Fill in the following information:\\n    * **Network Name**_(ie Azure - SYD)_\\n    * **VPN Protocol** _(select Manual IPsec)_\\n    * **Pre-shared Key** _(enter in the SAME key that was used by Azure Bicep to create the Connection - if you have lost it, it can be updated in Azure, under Shared key on the connection attached to the Virtual network gateway, but will stop any other VPN connections using the old key)_\\n    * **Server Address** _(make sure you select the interface for your WAN/External IP)_\\n    * **Remote** Gateway/**Subnets** _(Enter in the Address Prefix of your Azure virtual network or subnets, remember to add any peered virtual networks and Press Enter)_\\n    * **Remote IP Address** _(Enter in the Public IP of the Virtual Network gateway, the same IP retrieved by Get-AzPublicIPAddress cmdlet )_\\n10. ![UDM Pro - Azure S2S VPN](/uploads/udmpro_s2svpnsettings1.png \\"UDM Pro - Azure S2S VPN\\")\\n11. Select **Manual**\\n12. ![UDM Pro - Azure S2S VPN](/uploads/udmpro_s2svpnsettings2.png \\"UDM Pro - Azure S2S VPN\\")\\n\\n    Select **IPSec Profile**, and select **Azure Dynamic Routing**\\n13. Click **Apply Changes**\\n\\nAfter a few minutes, the VPN should become connected and you should be able to connect to devices on the Azure Network using their private IP address.\\n\\nIf you have problems, make sure that the Gateway IPs line up and are correct, along with the pre-shared key.  You can also Pause the Network from the UDM-Pro and Resume to reinitiate the connection.\\n\\nYou can also troubleshoot the VPN connection, from the Azure Portal, by navigating the Virtual network gateway and selecting VPN Troubleshoot.\\n\\n![Azure Portal - VPN Troubleshoot](/uploads/azureportal_vpntroubleshoot.png \\"Azure Portal - VPN Troubleshoot\\")"},{"id":"/2022/03/16/azure-optimisation-engine","metadata":{"permalink":"/2022/03/16/azure-optimisation-engine","source":"@site/blog/2022-03-16-azure-optimisation-engine.md","title":"Azure Optimization Engine","description":"This post is a part of Azure Spring Clean, which is a community event focused on Azure management topics from March 14-18, 2022.","date":"2022-03-16T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":20.89,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Optimization Engine","authors":["Luke"],"tags":["Azure"],"date":"2022-03-16 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/azurespringclean_2022_aoe.png"}},"unlisted":false,"prevItem":{"title":"Create a Site to Site VPN to Azure with a Ubiquiti Dream Machine Pro","permalink":"/2022/03/27/create-a-site-to-site-vpn-to-azure-with-a-ubiquiti-dream-machine-pro"},"nextItem":{"title":"Setup Azure Cloud Shell","permalink":"/azure/setup-azure-cloud-shell"}},"content":"This post is a part of [Azure Spring Clean](https://www.azurespringclean.com/), which is a community event focused on Azure management topics from March 14-18, 2022. \\n\\nThanks to [Joe Carlyle](https://twitter.com/wedoazure) and [Thomas Thornton](https://twitter.com/tamstar1234) for putting in the time and organising this event.\\n\\nThis article, along with others of its kind _(Articles, Videos etc.),_ cover Azure Management topics such as Azure Monitor, Azure Cost Management, Azure Policy, Azure Security Principles or Azure Foundations!\\n\\nToday I will be covering the [Azure Optimization Engine](https://github.com/helderpinto/AzureOptimizationEngine \\"Azure Optimization Engine\\").\\n\\n![#AzureSpringClean - Azure Optimization Engine](/uploads/azurespringclean_2022_aoe.png \\"#AzureSpringClean - Azure Optimization Engine\\")\\n\\n### Overview\\n\\n> The Azure Optimization Engine _(AOE)_ is an extensible solution designed to generate optimization recommendations for your Azure environment, like a fully customizable Azure Advisor.\\n>\\n> The first custom recommendations use-case covered by this tool was augmenting Azure Advisor Cost recommendations, particularly Virtual Machine right-sizing, with a fit score based on VM _(Virtual Machine)_ metrics and properties.\\n\\nThe Azure Optimization Engine can\u2026\\n\\n* Enable new custom recommendation types\\n* Augment Azure Advisor recommendations with richer details that better drive action\\n* Add fit score to recommendations.\\n* Add historical perspective to recommendations _(the older the recommendation, the higher the chances to remediate it)_\\n* Drive continuous automated optimisation\\n\\nAzure Optimisation Engine combines multiple data sources to give you better data-driven decisions and recommendations, outside of that usually deployed by the inbuilt Azure Advisor, example use-cases and data sources can be seen below:\\n\\n* Azure Resource Graph _(Virtual Machine and Managed Disks properties)_\\n* Azure Monitor Logs _(Virtual Machine performance metrics)_\\n* Azure Consumption _(consumption/billing usage details events)_\\n* Extracts data periodically to build a recommendations history\\n* Joins and queries data in an analytics-optimised repository _(Log Analytics)_\\n* Virtual Machine performance metrics collected with Log Analytics agent\\n* Can leverage existing customer setup\\n* Requires only a few metrics collected with a frequency >= 60 seconds\\n\\n> Besides collecting **all Azure Advisor recommendations**, AOE includes other custom recommendations that you can tailor to your needs:\\n>\\n> * Cost\\n>   * Augmented Advisor Cost VM right-size recommendations, with fit score based on Virtual Machine guest OS metrics _(collected by Log Analytics agents)_ and Azure properties\\n>   * Underutilized VM Scale Sets\\n>   * Unattached disks\\n>   * Standard Load Balancers without backend pool\\n>   * Application Gateways without backend pool\\n>   * VMs deallocated since a long time ago (_forgotten VMs)_\\n>   * Orphaned Public IPs\\n> * High Availability\\n>   * Virtual Machine high availability _(availability zones count, availability set, managed disks, storage account distribution when using unmanaged disks)_\\n>   * VM Scale Set high availability _(availability zones count, managed disks)_\\n>   * Availability Sets structure _(fault/update domains count)_\\n> * Performance\\n>   * VM Scale Sets constrained by lack of compute resources\\n> * Security\\n>   * Service Principal credentials/certificates without expiration date\\n>   * NSG rules referring to empty or non-existing subnets\\n>   * NSG rules referring to orphan or removed NICs\\n>   * NSG rules referring to orphan or removed Public IPs\\n> * Operational Excellence\\n>   * Load Balancers without backend pool\\n>   * Service Principal credentials/certificates expired or about to expire\\n>   * Subscriptions close to the maximum limit of RBAC _(Role Based Access Control)_ assignments\\n>   * Management Groups close to the maximum limit of RBAC assignments\\n>   * Subscriptions close to the maximum limit of resource groups\\n>   * Subnets with low free IP space\\n>   * Subnets with too much IP space wasted\\n>   * Empty subnets\\n>   * Orphaned NICs\\n\\nFeel free to skip to the Workbook and PowerBI sections to look at some of the outs of box data and recommendations.\\n\\nThe Azure Optimisation Engine is battle-tested\\n\\n* Providing custom recommendations since Nov 2019\\n* Serving Azure customers worldwide\\n* From smaller 50-500 VMs customers to larger ones with more than 5K VMs\\n* Several customer-specific developments (custom collectors and recommendation algorithms)\\n* Flexibility options include _(multi-subscription and multi-tenant capability)_\\n* Based on cheap services _(Azure Automation, Storage, small SQL Database_)\\n\\nA few hours after setting up the engine, you will get access to a Power BI dashboard and Log Analytic Workbooks with all Azure optimisation opportunities, coming from both Azure Advisor and tailored recommendations included in the engine.\\n\\nThese recommendations are then updated every seven days.\\n\\n> It is worth noting that Azure Optimisation Engine is **NOT** an official **Microsoft Product,** and as such is under no offical support, it was created and maintened by: H\xe9lder Pinto, a Senior Customer Engineer for Microsoft and would like to take the opportunity to thank H\xe9lder the amazing work he is doing with this product on a continous basis, and giving me his blessing to write this article, on which he has already done an amazing job documenting on Github.\\n\\n#### Architecture\\n\\n![Azure Optimization Engine Architecture](/uploads/architecture.jpg \\"Azure Optimization Engine Architecture\\")\\n\\nAzure Optimization Engine runs on top of Azure Automation _(Runbooks for each data source)_ and Log Analytics. It is supplemented by a storage account to store JSON and Azure SQL database to help control ingestion _(last processed blob and lines processed)_.\\n\\n### Install\\n\\n#### Prerequisites\\n\\nTaken directly from the Git repository readme, the prerequisite for Azure Optimization Engine are:\\n\\n* A supported Azure subscription _(see the_ [_FAQs_](https://github.com/helderpinto/AzureOptimizationEngine#faq)_on Github)_\\n* [Azure Powershell 6.6.0+](hhttps://learn.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.5.0&WT.mc_id=AZ-MVP-5004796 \\"Install the Azure Az PowerShell module\\")_(Azure Bicep support is not currently available but is being worked on)_.\\n* [Microsoft.Graph.Authentication](https://learn.microsoft.com/en-us/powershell/microsoftgraph/installation?view=graph-powershell-beta&WT.mc_id=AZ-MVP-5004796 \\" Microsoft.Graph.Authentication \\") and [Microsoft.Graph.Identity.DirectoryManagement](https://learn.microsoft.com/en-us/powershell/microsoftgraph/installation?view=graph-powershell-beta&WT.mc_id=AZ-MVP-5004796 \\"Microsoft.Graph.Identity.DirectoryManagement\\") PowerShell modules\\n* A user account with Owner permissions over the chosen subscription, so that the Automation Managed Identity is granted the required privileges over the subscription (Reader) and deployment resource group _(Contributor)_\\n* _(Optional)_ A user account with at least Privileged Role Administrator permissions over the Azure AD tenant, so that the Managed Identity is granted the required privileges over Azure AD _(Global Reader)_\\n\\nDuring deployment, you\'ll be asked several questions. It would be best if you planned for the following:\\n\\n* Whether you\'re going to reuse an existing Log Analytics Workspace or create a new one. **IMPORTANT**: you should ideally reuse a workspace where you have VMs onboarded and already sending performance metrics _(`Perf` table)_; otherwise, you will not fully leverage the augmented right-size recommendations capability. If this is not possible/desired for some reason, you can still manage to use multiple workspaces _(see_ [_Configuring Log Analytics workspaces_](https://github.com/helderpinto/AzureOptimizationEngine/blob/master/docs/configuring-workspaces.md)_)_.\\n* An Azure subscription to deploy the solution _(if you\'re reusing a Log Analytics workspace, you must deploy into the same subscription the workspace is in)._\\n* A unique name prefix for the Azure resources being created (if you have specific naming requirements, you can also choose resource names during deployment)\\n* Azure region\\n\\nIf the deployment fails for some reason, you can repeat it, as it is idempotent _(i.e. they can be applied multiple times without changing the result)_. The exact process is used to upgrade a previous deployment with the latest version. You have to keep the same deployment options, so make sure you document them.\\n\\nWe will now go through and install the prerequisites from scratch; as in this article, I will be deploying the Azure Optimization Engine from our local workstation.\\n\\nYou can also install from the [Azure Cloud Shell,](https://luke.geek.nz/azure/setup-azure-cloud-shell/ \\"Azure Cloud Shell\\")\\n\\n##### Install Azure PowerShell & Microsoft Graph modules\\n\\n1. Open Windows PowerShell\\n2. Type in:\\n\\n       Install-Module -Name Az,Microsoft.Graph.Authentication,Microsoft.Graph.Identity.DirectoryManagement -Scope CurrentUser -Repository PSGallery -Force\\n\\n#### Install\\n\\nNow that we have the prerequisites installed! Let\'s set up Azure Optimization Engine!\\n\\n 1. In your favourite web browser, **navigate** to the [**AzureOptimizationEngine**](https://github.com/helderpinto/AzureOptimizationEngine \\"https://github.com/helderpinto/AzureOptimizationEngine\\") GitHub repository.\\n 2. Select **Code**, **Download Zip**\\n 3. ![Azure Optimization Engine - GitHub](/uploads/2022-03-12-09_23_49-helderpinto_azureoptimizationengine_-the-azure-optimization-engine-is-an-extensi.png \\"Azure Optimization Engine - GitHub\\")\\n 4. **Download** and **extract** the ZIP file to a location you can easily navigate to in PowerShell (_I have extracted it to C:\\\\\\\\temp\\\\\\\\AzureOptimizationEngine-master\\\\\\\\AzureOptimizationEngine-master)_\\n 5. Open PowerShell _(or Windows Terminal)_\\n 6. Because the scripts were downloaded from the internet, we will need to **Unblock** these so that we can run them, open PowerShell and run the **script** below _(changing your path to the path that the files were extracted)_\\n\\n        Get-ChildItem -r \'C:\\\\temp\\\\AzureOptimizationEngine-master\\\\AzureOptimizationEngine-master\' | Unblock-File\\n 7. Now that the script and associated files have been unblocked **change** the **directory** to the **location** of the Deploy-AzureOptimizationEngine.ps1 **file**.\\n 8. Run: **.\\\\\\\\Deploy-AzureOptimizationEngine.ps1**\\n 9. ![Windows Terminal -\\\\\\\\Deploy-AzureOptimizationEngine.ps1](/uploads/2022-03-12-09_48_40-plex.png \\"Windows Terminal -\\\\Deploy-AzureOptimizationEngine.ps1\\")\\n10. A browser window will then popup, **authenticate to Azure** _(connect to the Azure tenant that has access to the Azure subscription you wish to set up Azure Optimization Engine on)_.\\n11. Once authentication, you will need to **confirm** the Azure **subscription** to which you want to deploy Azure Optimization Engine.\\n12. ![Azure Optimization Engine - Select Subscription](/uploads/aoe-selectazsubscription.png \\"Azure Optimization Engine - Select Subscription\\")\\n13. Once your subscription is selected, it\'s time to **choose** a **naming prefix** for your resources _(if you choose Enter, you can manually name each resource); in_ my case, my prefix will be: _aoegeek._ Because Azure Optimization Engine will be creating resources that are globally available, make sure you select a prefix that suits your organisation/use-case as you may run into issues with the name already being used.\\n14. ![Azure Optimization Engine - Select Region](/uploads/aoe-selectazprefix.png \\"Azure Optimization Engine - Select Region\\")\\n15. If you have an **existing Log Analytics** workspace that your Virtual Machines and resources are connected to, you can specify \'Y\' here to select your existing resource; I am creating this from fresh so that I will choose **\'N.**\'\\n16. ![Azure Log Analytics](/uploads/aoe-selectazloganalyticworkspace.png \\"Azure Log Analytics\\")\\n17. The Azure Optimization **Engine will now check that the names and resources are available** to be deployed to your subscriptions and resources _(nothing is deployed during this stage - if there is an error, you can fix the issue and go back)_.\\n18. Once validation has passed, **select** the **region** that Azure Optimization will be deployed to; I will deploy to australiaeast, so I choose 1.\\n19. Azure Optimization Engine now **requires** the **SQL Admin** username; for the SQL server and database it will create, I will go with: sqladmin\\n20. ![Azure Optimization Engine - Region](/uploads/aoe-selectlocationsql.png \\"Azure Optimization Engine - Region\\")\\n21. Now enter the **password** for the **sqladmin** account and press Enter\\n22. Verify that everything is correct, then press **Y** to deploy Azure Optimization Engine!\\n23. ![Windows Terminal - Deploy Azure Optimization Engine](/uploads/deploy-azureoptimizationengine.gif \\"Windows Terminal - Deploy Azure Optimization Engine\\")\\n24. Deployment could take 10-25 minutes... _(mine took 22 minutes and 51 seconds)_\\n25. While leaving the PowerShell window open, log into the Azure Portal; you should now have a new Resource Group, and your resources will start getting created... you can click on Deployments _(under Settings navigation bar)_ in the Resource Group to review the deployment status.\\n26. ![Azure Portal - Deployments](/uploads/deploycheck-azureoptimizationengine.gif \\"Azure Portal - Deployments\\")\\n27. If you notice a failure, in the Deployment tab for: \'PolicyDeployment\' you can ignore this, as it may have failed if the SQL Server hasn\'t been provisioned yet; once it has been provisioned, you can navigate back to this failed deployment and click \'Redeploy\', to deploy a SQL Security Alert policy.\\n\\n_Note: The Azure SQL database will have the Public IP from the location the script was deployed from, allowed on the Azure SQL database; you may need to adjust this depending on your requirements._\\n\\n#### Configure\\n\\n##### Onboard Azure VMs to Log Analytics using Azure Policy and PowerShell\\n\\nNow that Azure Optimization has been installed, let\'s onboard our current and future Azure Virtual Machines to Azure Optimization Engine, using Azure Policy. This is required if you want to get Azure Advisor Virtual Machine right-size recommendations augmented with guest OS metrics. If you don\'t collect metrics from the Virtual Machines, you will still have a fully functional Optimisation Engine, with many recommendations, but the Advisor Virtual Machine right-size ones will be served as is.\\n\\n1. Open **PowerShell** and **login** to **Azure** using: Connect-AzAccount\\n2. **Connect to** your Azure **subscription** that contains the Virtual Machines you want to onboard to Log Analytics\\n3. Type:\\n\\n       # Register the resource provider if it\'s not already registered\\n       Register-AzResourceProvider -ProviderNamespace \'Microsoft.PolicyInsights\'\\n4. The PowerShell script below will:\\n   * Copies the built-in Azure Policy definition of [Deploy - Configure Log Analytics extension to be enabled on Windows virtual machines](https://www.azadvertizer.net/azpolicyadvertizer/0868462e-646c-4fe3-9ced-a733534b6a2c.html?desc=compareJson&left=https%3A%2F%2Fwww.azadvertizer.net%2Fazpolicyadvertizerjson%2F0868462e-646c-4fe3-9ced-a733534b6a2c_2.0.0.json&right=https%3A%2F%2Fwww.azadvertizer.net%2Fazpolicyadvertizerjson%2F0868462e-646c-4fe3-9ced-a733534b6a2c_2.0.1.json \\" Azure Policy definition Deploy - Configure Log Analytics extension to be enabled on Windows virtual machines \\")\\n   * Create a User-Managed Identity\\n   * Assign: Log Analytics contributor rights to a subscription scope\\n   * Create a policy assignment, and assign it to the subscription\\n5. Just update the variables to match your setup\\n\\n       #requires -Version 1.0\\n       # Variables\\n       #Enter your subscription name\\n       $subscriptionName = \'luke.geek.nz\'\\n       #Enter the name of yuour \\n       $policyDisplayName = \'Deploy - Log Analytics\' #Cant Exceed 24 characters\\n       $location = \'australiaeast\'\\n       $resourceGroup = \'aoegeek-rg\'\\n       $UsrIdentityName = \'AOE_ManagedIdentityUsr\'\\n       $param = @{\\n         logAnalytics = \'aoegeek-la\'\\n       }\\n       # Get a reference to the subscription that will be the scope of the assignment\\n       $sub = Get-AzSubscription -SubscriptionName $subscriptionName\\n       $subid = $sub.Id\\n       #Creates User Managed identity \\n       $AzManagedIdentity = New-AzUserAssignedIdentity -ResourceGroupName $resourceGroup -Name $UsrIdentityName\\n       #Adds Contributor rights to User Managed identity to Subscription\\n       #Waits 10 seconds to allow for Azure AD to replicate and recognise Managed identity has been created.\\n       Start-Sleep -Seconds \'10\'\\n       #Assigns role assignement to managed identity\\n        New-AzRoleAssignment -Objectid $AzManagedIdentity.PrincipalId -scope (\'/subscriptions/\' + $subid ) -RoleDefinitionName \'Log Analytics Contributor\'\\n       # Get a reference to the built-in policy definition that will be assigned\\n       $definition = Get-AzPolicyDefinition | Where-Object -FilterScript {\\n         $_.Properties.DisplayName -eq \'Deploy - Configure Log Analytics extension to be enabled on Windows virtual machines\' \\n       }\\n       # Create the policy assignment with the built-in definition against your subscription\\n       New-AzPolicyAssignment -Name $policyDisplayName -DisplayName $policyDisplayName -Scope (\'/subscriptions/\' + $subid ) -PolicyDefinition $definition -IdentityType \'UserAssigned\'  -IdentityId $AzManagedIdentity.id -location $location -PolicyParameterObject $param\\n       #Creates R3mediation task, to deploy the extension to the VM\\n       $policyAssignmentID = Get-AzPolicyAssignment -Name $policyDisplayName | Select-Object -Property PolicyAssignmentId \\n       Start-AzPolicyRemediation -Name \'Deploy - LA Agent\' -PolicyAssignmentId $policyAssignmentID.PolicyAssignmentId -ResourceDiscoveryMode ReEvaluateCompliance\\n\\n_Note: The default \'Deploy - Configure Log Analytics extension to be enabled on Windows virtual machines\' policy doesn\'t currently support Gen 2 or Windows Server 2022 Virtual Machines; if you have these, then you can copy the Azure Policy definition and then make your own with the new imageSKUs, although this policy may be replaced by the: Configure Windows virtual machines to run Azure Monitor Agent policy. Although I haven\'t tested it yet, the same script above can be modified to suit._\\n\\n##### Onboard Azure VMs to Log Analytics using the Azure Portal\\n\\nIf you do not want to onboard VMs with Policy, you can do it manually via the Azure Portal.\\n\\n1. Open **Azure Portal**\\n2. Navigate to [**Log Analytic Workspaces**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.OperationalInsights%2Fworkspaces \\"Log Analytic Workspaces\\")\\n3. **Click on** the Log Analytic **workspace** that was provisioned for Azure Optimization Engine\\n4. Navigate to **Virtual Machines** _(under Workspace Data Sources)_\\n5. Click on the Virtual Machine you want to link up to the Log Analytics workspace, and click **Connect -** this will trigger the Log Analytic extension and agent o be installed. Repeat for any further Virtual Machines.\\n6. ![Log Analytics - Connect VM](/uploads/aoe-2019vmgen1_connectla.png \\"Log Analytics - Connect VM\\")\\n\\n##### Setup Log Analytic Performance Counters\\n\\nNow that we have Virtual Machines reporting to our Log Analytic instance, it\'s time to make sure we are collecting as much data as we need to give suitable recommendations, luckily a script has already been included in the Azure Optimisation repository called \'_Setup-LogAnalyticsWorkspaces.ps1_\' to configure the performance counters.\\n\\n1. Open **PowerShell** _(or Windows Terminal)_\\n2. **Change** the **directory** to the **location** of the Setup-LogAnalyticsWorkspaces.ps1, in the root folder of the repository extracted earlier\\n3. Run the following PowerShell commands to download the required PowerShell Modules:\\n\\n       Install-Module -Name Az.ResourceGraph\\n       Install-Module -Name Az.OperationalInsights\\n4. Then run: .**\\\\\\\\Setup-LogAnalyticsWorkspaces.ps1**\\n5. The script will then go through all Log Analytic workspaces that you have access to and check for performance counters.\\n6. ![Windows PowerShell - \\\\\\\\Setup-LogAnalyticsWorkspaces.ps1](/uploads/deploycheck-loganalytics.gif \\"Windows PowerShell - \\\\Setup-LogAnalyticsWorkspaces.ps1\\")\\n7. If they are missing from the Log Analytics workspace, then you can run:\\n\\n       ./Setup-LogAnalyticsWorkspaces.ps1 -AutoFix\\n\\nor\\n\\n     #Fix specific workspaces configuration, using a custom counter collection frequency\\n    ./Setup-LogAnalyticsWorkspaces.ps1 -AutoFix -WorkspaceIds \\"d69e840a-2890-4451-b63c-bcfc5580b90f\\",\\"961550b2-2c4a-481a-9559-ddf53de4b455\\" -IntervalSeconds 30\\n\\n##### Setup Azure AD-based recommendations by granting permissions to Managed Identity.\\n\\nAzure Optimization Engine, has the ability to do recommendations based on Microsoft Entra ID roles and permissions, but in order to do that, the System Assigned Identity of the Azure Optimization Engine account needs to be given \'Global Reader\' rights. As part of the deployment, you may have gotten the following error:\\n\\n_Cannot bind argument to parameter \'DirectoryRoleId\' because it is an empty string._\\n\\n_Could not grant role. If you want Azure AD-based recommendations, please grant the Global Reader role manually to the aoegeek-auto managed identity or, for previous versions of AOE, to the Run As Account principal._\\n\\nWe are going to grant the Azure Automation account \'Global Reader\' rights manually in the Azure Portal.\\n\\n 1. Open **Azure Portal**\\n 2. Navigate to [**Automation Accounts**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Automation%2FAutomationAccounts \\"Automation Accounts\\")\\n 3. **Open** your Azure Optimisation Engine **automation account**\\n 4. Navigate down the navigation bar to the **Account Settings** section and select: **Identity**\\n 5. ![Azure Automation - Identity](/uploads/aoe-managedidentityazautomate.png \\"Azure Automation - Identity\\")\\n 6. **Copy** the **object ID**\\n 7. Now navigate to [**Microsoft Entra ID**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview \\"Microsoft Entra ID\\")\\n 8. Click on **Roles and Administrators**\\n 9. Search for: **Global Reader**\\n10. Select Global Reader and select **+ Add assignments**\\n11. Paste in the object ID earlier, and click **Ok** to grant Global Reader rights to the Azure Automation identity.\\n\\n##### Azure Automation - Runbooks & Automation\\n\\nThe wind that gives Azure Optimization Engine its lift is Azure Automation and Runbooks, at the time I deployed this - I had x1 Azure Automation account and 33 runbooks!\\n\\nLooking at the runbooks deployed, you can get a sense of what Azure Optimization Engine is doing...\\n\\n| NAME | TYPE |\\n| --- | --- |\\n| aoegeek-auto | Automation Account |\\n| Export-AADObjectsToBlobStorage (aoegeek-auto/Export-AADObjectsToBlobStorage) | Runbook |\\n| Export-AdvisorRecommendationsToBlobStorage (aoegeek-auto/Export-AdvisorRecommendationsToBlobStorage) | Runbook |\\n| Export-ARGAppGatewayPropertiesToBlobStorage (aoegeek-auto/Export-ARGAppGatewayPropertiesToBlobStorage) | Runbook |\\n| Export-ARGAvailabilitySetPropertiesToBlobStorage (aoegeek-auto/Export-ARGAvailabilitySetPropertiesToBlobStorage) | Runbook |\\n| Export-ARGLoadBalancerPropertiesToBlobStorage (aoegeek-auto/Export-ARGLoadBalancerPropertiesToBlobStorage) | Runbook |\\n| Export-ARGManagedDisksPropertiesToBlobStorage (aoegeek-auto/Export-ARGManagedDisksPropertiesToBlobStorage) | Runbook |\\n| Export-ARGNICPropertiesToBlobStorage (aoegeek-auto/Export-ARGNICPropertiesToBlobStorage) | Runbook |\\n| Export-ARGNSGPropertiesToBlobStorage (aoegeek-auto/Export-ARGNSGPropertiesToBlobStorage) | Runbook |\\n| Export-ARGPublicIpPropertiesToBlobStorage (aoegeek-auto/Export-ARGPublicIpPropertiesToBlobStorage) | Runbook |\\n| Export-ARGResourceContainersPropertiesToBlobStorage (aoegeek-auto/Export-ARGResourceContainersPropertiesToBlobStorage) | Runbook |\\n| Export-ARGUnmanagedDisksPropertiesToBlobStorage (aoegeek-auto/Export-ARGUnmanagedDisksPropertiesToBlobStorage) | Runbook |\\n| Export-ARGVirtualMachinesPropertiesToBlobStorage (aoegeek-auto/Export-ARGVirtualMachinesPropertiesToBlobStorage) | Runbook |\\n| Export-ARGVMSSPropertiesToBlobStorage (aoegeek-auto/Export-ARGVMSSPropertiesToBlobStorage) | Runbook |\\n| Export-ARGVNetPropertiesToBlobStorage (aoegeek-auto/Export-ARGVNetPropertiesToBlobStorage) | Runbook |\\n| Export-AzMonitorMetricsToBlobStorage (aoegeek-auto/Export-AzMonitorMetricsToBlobStorage) | Runbook |\\n| Export-ConsumptionToBlobStorage (aoegeek-auto/Export-ConsumptionToBlobStorage) | Runbook |\\n| Export-RBACAssignmentsToBlobStorage (aoegeek-auto/Export-RBACAssignmentsToBlobStorage) | Runbook |\\n| Ingest-OptimizationCSVExportsToLogAnalytics (aoegeek-auto/Ingest-OptimizationCSVExportsToLogAnalytics) | Runbook |\\n| Ingest-RecommendationsToSQLServer (aoegeek-auto/Ingest-RecommendationsToSQLServer) | Runbook |\\n| Recommend-AADExpiringCredentialsToBlobStorage (aoegeek-auto/Recommend-AADExpiringCredentialsToBlobStorage) | Runbook |\\n| Recommend-AdvisorAsIsToBlobStorage (aoegeek-auto/Recommend-AdvisorAsIsToBlobStorage) | Runbook |\\n| Recommend-AdvisorCostAugmentedToBlobStorage (aoegeek-auto/Recommend-AdvisorCostAugmentedToBlobStorage) | Runbook |\\n| Recommend-ARMOptimizationsToBlobStorage (aoegeek-auto/Recommend-ARMOptimizationsToBlobStorage) | Runbook |\\n| Recommend-LongDeallocatedVmsToBlobStorage (aoegeek-auto/Recommend-LongDeallocatedVmsToBlobStorage) | Runbook |\\n| Recommend-UnattachedDisksToBlobStorage (aoegeek-auto/Recommend-UnattachedDisksToBlobStorage) | Runbook |\\n| Recommend-UnusedAppGWsToBlobStorage (aoegeek-auto/Recommend-UnusedAppGWsToBlobStorage) | Runbook |\\n| Recommend-UnusedLoadBalancersToBlobStorage (aoegeek-auto/Recommend-UnusedLoadBalancersToBlobStorage) | Runbook |\\n| Recommend-VMsHighAvailabilityToBlobStorage (aoegeek-auto/Recommend-VMsHighAvailabilityToBlobStorage) | Runbook |\\n| Recommend-VMSSOptimizationsToBlobStorage (aoegeek-auto/Recommend-VMSSOptimizationsToBlobStorage) | Runbook |\\n| Recommend-VNetOptimizationsToBlobStorage (aoegeek-auto/Recommend-VNetOptimizationsToBlobStorage) | Runbook |\\n| Remediate-AdvisorRightSizeFiltered (aoegeek-auto/Remediate-AdvisorRightSizeFiltered) | Runbook |\\n| Remediate-LongDeallocatedVMsFiltered (aoegeek-auto/Remediate-LongDeallocatedVMsFiltered) | Runbook |\\n| Remediate-UnattachedDisksFiltered (aoegeek-auto/Remediate-UnattachedDisksFiltered) | Runbook |\\n\\nA lot of the runbooks, such as the Log Analytics workspace ID, link up to Azure Automation variables, such as this period in Days to look back for Advisor recommendations, by default, this is \'7\' but you can change this variable to suit your organisation\'s needs.\\n\\n![Azure Automation - Runbooks & Automation](/uploads/aoe-variables.png \\"Azure Automation - Runbooks & Automation\\")\\n\\n##### Azure Automation - Schedules\\n\\nAlong with containing the variables and configurations used by the Runbooks, it also contains the schedules for the ingest of data into the storage account and SQL databases, most of these are Daily, but schedules such as ingesting from the Azure Advisor are weekly, by default these times are in UTC.\\n\\n![Azure Automation - Schedules](/uploads/aoe-schedules.png \\"Azure Automation - Schedules\\")\\n\\nWhen making changes to these schedules _(or moving the Runbooks to be run from a Hybrid worker)_, it is recommended to use the Reset-AutomationSchedules.ps1 script. These times need to be in UTC.\\n\\n![Terminal - Reset-AutomationSchedules.ps1](/uploads/update-automationschedules.gif \\"Terminal - Reset-AutomationSchedules.ps1\\")\\n\\n##### Azure Automation - Credentials\\n\\nWhen we set up the Azure SQL database earlier, as part of the Azure Optimisation setup, we configured the SQL Admin account and password, these credentials are stored and used by the Runbooks in the Azure Automation credential pane.\\n\\n![Azure Automation - Credentials](/uploads/aoe-credentials.png \\"Azure Automation - Credentials\\")\\n\\n### View Recommendations\\n\\nIt\'s worth noting, that because Azure Optimization Engine stores its data into Log Analytics and SQL, you can use languages such as KQL directly on the Log Analytics workspace to pull out any information you might need and develop integration into other toolsets.\\n\\n##### Workbooks\\n\\nThere are x3 Azure Log Analytics workbooks included in the Azure Optimization Engine, these are as follows:\\n\\n| NAME | TYPE |\\n| --- | --- |\\n| Resources Inventory | Azure Workbook |\\n| Identities and Roles | Azure Workbook |\\n| Costs Growing | Azure Workbook |\\n\\nThey can be easily accessed in the Azure Portal.\\n\\n1. Log in to the **Azure Portal**\\n2. Navigate to [**Log Analytics Workspace**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.OperationalInsights%2Fworkspaces \\"Log Analytics Workspace\\")\\n3. Click on the Log Analytics workspace you set up for Azure Optimization Engine earlier and click on **Workbooks** _(under General)_.\\n4. Click on: **Workbooks** filter at the top to display the 3 Azure Optimization Engine\\n5. ![Log Analtics - Workbooks](/uploads/aoe-displayworkbooks.png \\"Log Analtics - Workbooks\\")\\n6. After a few days of collecting data, you should now be able to see data like below.\\n\\n###### Resource Inventory - General\\n\\n![Resource Inventory - General](/uploads/aoe-workbookresourceinventory.png \\"Resource Inventory - General\\")\\n\\n###### Resource Inventory - Virtual Machines\\n\\n![Resource Inventory - Virtual Machines](/uploads/aoe-workbookresourceinventory_virtualmachines.png \\"Resource Inventory - Virtual Machines\\")\\n\\n###### Resource Inventory - Virtual Machine ScaleSets\\n\\n![Resource Inventory - Virtual Machine ScaleSets](/uploads/aoe-workbookresourceinventory_virtualmachinescaletset.png \\"Resource Inventory - Virtual Machine ScaleSets\\")\\n\\n###### Resource Inventory - Virtual Machine ScaleSets Disks\\n\\n![Resource Inventory - Virtual Machine ScaleSets Disks](/uploads/aoe-workbookresourceinventory_virtualmachinescaletsetdisks.png \\"Resource Inventory - Virtual Machine ScaleSets Disks\\")\\n\\n###### Resource Inventory - Virtual Networks\\n\\n![Resource Inventory - Virtual Networks](/uploads/aoe-workbookresourceinventory_virtualnetworks.png \\"Resource Inventory - Virtual Networks\\")\\n\\n###### Identities and Roles - Overview\\n\\n![Identities and Roles - Overview](/uploads/aoe-workbookaad_overview.png \\"Identities and Roles - Overview\\")\\n\\n##### Power BI\\n\\nThe true power of the Azure Optimisation engine, is the data stored in the SQL database, using PowerBI you can pull the data into dashboards and make it more meaningful, and the recommendations given from PowerBI and SQL.\\n\\nThe Optimisation Engine already has a starter PowerBI file, which pulls data from the database.\\n\\n###### Install PowerBI Desktop\\n\\n1. Open Microsoft Store and search for: [**Power BI Desktop**](https://aka.ms/pbidesktopstore \\" Microsoft Power BI Desktop\\")\\n2. Click **Get**\\n3. ![Power BI Desktop](/uploads/microsoft-store-powerbidesktop.png \\"Power BI Desktop\\")\\n4. Once Downloaded, click **Open**\\n\\n###### Obtain Azure SQL Information\\n\\nIn order to connect PowerBI to the Azure SQL database, we need to know the URL of the database and make sure our IP has been opened on the Azure SQL Firewall.\\n\\n 1. Open **Azure Portal**\\n 2. Navigate to [**SQL Servers**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Sql%2Fservers \\"Azure Portal - SQL servers\\")\\n 3. Click on the SQL server created earlier, under the Security heading click on **Firewall and Virtual Networks**\\n 4. Under: Client IP address, make sure your public IP is added and click **Save**\\n 5. ![Azure SQL - Virtual Network](/uploads/aoe-sql-server-firewall.png \\"Azure SQL - Virtual Network\\")\\n 6. Now that we have verified/added our client IP, we need to get the SQL **database** _(not server)_ URL\\n 7. Click on **Overview**\\n 8. **Click** on the _aoeoptimization_ **database** _(under Available resources, down the bottom)_\\n 9. Click on **Copy to Clipboard** for the **server Name**/URL\\n10. ![Azure SQL - Database URL](/uploads/aoe-sql-database-name.png \\"Azure SQL - Database URL\\")\\n\\n###### Open PowerBI Desktop File\\n\\nNow that we have PowerBI Desktop installed, it\'s time to open: AzureOptimizationEngine.pbix. This PowerBI file is located in the Views folder of the Azure Optimization Engine repository.\\n\\n 1. **Open**: **AzureOptimizationEngine.pbix** in PowerBI Desktop\\n 2. On the Home page ribbon, click on **Transform Data**\\n 3. Click **Data source settings**\\n 4. Click **Change Source**\\n 5. **Change** the default **SQL server** of aoedevgithub-sql.database.windows.net to your SQL database, copied earlier.\\n 6. Click **Ok**\\n 7. Click Ok and press **Apply Changes**\\n 8. It will prompt for credentials, click on **Database**\\n 9. Enter in your SQLAdmin details entered as part of the Azure Optimization Engine setup\\n10. Click **Connect**\\n\\nAfter PowerBI updates its database and queries, your PowerBI report should now be populated with data like below.\\n\\n###### PowerBI - Overview\\n\\n![PowerBI - Overview](/uploads/aoe-powerbi_overview.png \\"PowerBI - Overview\\")\\n\\n###### PowerBI - Cost\\n\\n![PowerBI - Cost](/uploads/aoe-powerbi_cost.png \\"PowerBI - Cost\\")\\n\\n###### PowerBI - High Availability\\n\\n![PowerBI - High Availability](/uploads/aoe-powerbi_ha.png \\"PowerBI - High Availability\\")\\n\\n###### PowerBI - Security\\n\\n![PowerBI - Security](/uploads/aoe-powerbi_security.png \\"PowerBI - Security\\")\\n\\n###### PowerBI - Operational Excellence\\n\\n![PowerBI - Operational Excellence](/uploads/aoe-powerbi_operationalexcellencepng.png \\"PowerBI - Operational Excellence\\")\\n\\n**Congratulations! You have now successfully stood up and configured Azure Optimization Engine!**\\n\\n#### Additional Recommended Reading\\n\\n* _\\"Augmenting Azure Advisor Cost Recommendations for Automated Continuous Optimization\\"_ blog post series:\\n  1. [Part 1 - Solution Overview](https://techcommunity.microsoft.com/t5/core-infrastructure-and-security/augmenting-azure-advisor-cost-recommendations-for-automated/ba-p/1339298?WT.mc_id=AZ-MVP-5004796 \\"Augmenting Azure Advisor Cost Recommendations for Automated Continuous Optimization \u2013 Part 1\\")\\n  2. [Part 2 - Collecting Data](https://techcommunity.microsoft.com/t5/core-infrastructure-and-security/augmenting-azure-advisor-cost-recommendations-for-automated/ba-p/1457687?WT.mc_id=AZ-MVP-5004796 \\"Augmenting Azure Advisor Cost Recommendations for Automated Continuous Optimization \u2013 Part 2\\")\\n  3. [Part 3 - Generating & Viewing Recommendations](https://techcommunity.microsoft.com/t5/core-infrastructure-and-security/augmenting-azure-advisor-cost-recommendations-for-automated/ba-p/1544796?WT.mc_id=AZ-MVP-5004796 \\"Augmenting Azure Advisor Cost Recommendations for Automated Continuous Optimization \u2013 Part 3\\")\\n  4. [Part 4 - Automating Continous Optimisation with the Azure Optimization Engine](https://techcommunity.microsoft.com/t5/core-infrastructure-and-security/automating-continuous-optimization-with-the-azure-optimization/ba-p/1851317?WT.mc_id=AZ-MVP-5004796 \\"Automating Continuous Optimization with the Azure Optimization Engine\\")\\n* [Azure Optimization Engine Github - Usage instructions](https://github.com/helderpinto/AzureOptimizationEngine#usage-instructions \\"Azure Optimization Engine\\")"},{"id":"azure/setup-azure-cloud-shell","metadata":{"permalink":"/azure/setup-azure-cloud-shell","source":"@site/blog/2022-03-07-setup-azure-cloud-shell.md","title":"Setup Azure Cloud Shell","description":"The Azure Cloud Shell allows connectivity to Microsoft Azure using an authenticated, browser-based shell experience that\u2019s hosted in the cloud and accessible from virtually anywhere as long as you have access to your favourite browser!","date":"2022-03-07T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.705,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-03-07 00:00:00 +1300","title":"Setup Azure Cloud Shell","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"uploads/cloudshell.png"},"slug":"azure/setup-azure-cloud-shell"},"unlisted":false,"prevItem":{"title":"Azure Optimization Engine","permalink":"/2022/03/16/azure-optimisation-engine"},"nextItem":{"title":"Native RDP Client & Azure Bastion","permalink":"/2022/02/16/native-rdp-client-azure-bastion"}},"content":"The Azure Cloud Shell allows connectivity to Microsoft Azure using an authenticated, browser-based shell experience that\u2019s hosted in the cloud and accessible from virtually anywhere as long as you have access to your favourite browser!\\n\\nAzure Cloud Shell is assigned per unique user account and automatically authenticated with each session.\\n\\nGet a modern command-line experience from multiple access points, including the [Azure portal](https://portal.azure.com/), [shell.azure.com](https://shell.azure.com/), [Azure mobile app](https://azure.microsoft.com/en-us/get-started/azure-portal/mobile-app/?WT.mc_id=AZ-MVP-5004796), Azure docs_(e.g._[_Azure CLI_](https://learn.microsoft.com/en-us/cli/azure/?view=azure-cli-latest&WT.mc_id=AZ-MVP-5004796)_,_ [_Azure PowerShell_](https://learn.microsoft.com/en-us/powershell/azure/get-started-azureps?view=azps-7.5.0&WT.mc_id=AZ-MVP-5004796)_)_, and [VS Code Azure Account extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.azure-account).\\n\\nBoth Bash and PowerShell experiences are available.\\n\\n> Microsoft routinely maintains and updates Cloud Shell, which comes equipped with commonly used CLI tools including Linux shell interpreters, PowerShell modules, Azure tools, text editors, source control, build tools, container tools, database tools, and more. Cloud Shell also includes language support for several popular programming languages such as Node.js, .NET, and Python.\\n\\nAlong with native tools such as Azure PowerShell, it also contains Terraform, allowing you to implement and test functionality such as Infrastructure as Code, without needing to touch your local machine and is always up-to-date, a full list of tools can be found \'[here](https://learn.microsoft.com/en-us/azure/cloud-shell/features?WT.mc_id=AZ-MVP-5004796 \\"Features & tools for Azure Cloud Shell\\")\'.\\n\\nJust some noticeable things to be aware of regarding the Azure Cloud Shell:\\n\\n* Cloud Shell runs on a temporary host provided on a per-session, per-user basis\\n* Cloud Shell times out after 20 minutes without interactive activity\\n* Cloud Shell requires an Azure file share to be mounted\\n* Cloud Shell uses the same Azure file share for both Bash and PowerShell\\n* Cloud Shell is assigned one machine per user account\\n* Cloud Shell persists $HOME using a 5-GB image held in your file share\\n* Permissions are set as a regular Linux user in Bash\\n\\nThe Azure Cloud Shell is very easy to set up and get going, but in this article, I will show you the additional configuration options you have available, such as selecting your own storage account, region and resource group to conform to any naming policies and preferences you may have.\\n\\nBy default, CloudShell creates a new Resource Group, Storage account, Fileshare in the Southeast Asia region.\\n\\n### To set up Azure Cloud Shell\\n\\n1. Navigate to the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure - Portal\\")\\n2. Click on the little **Terminal/PowerShell icon** in the upper navigation bar\\n3. ![Azure Portal - Cloud Shell](/uploads/cloudshell_azureportal_icon.png \\"Azure Portal - Cloud Shell\\")\\n4. You should get notified, \\"You have no storage mounted\\" click on **Show advanced settings**\\n5. ![Azure Portal - Cloud Shell](/uploads/nostgmounted_azureportal.png)\\n6. Here you can easily **create your CloudShell storage account with your own preferences**:\\n\\n* The subscription\\n* Region\\n* Resource Group _(new or existing)_\\n* Storage account _(new or existing)_\\n* Fileshare _(new or existing)_\\n\\n1. ![Azure Portal - Cloud Shell Storage Account](/uploads/stgconfigured_azureportal.png)\\n2. Click on **Create Storage** when you are ready to start the verification _(which happens after you click Create storage, don\'t worry as long as you have the window open you can make any additional changes)_ and deployment.\\n3. ![Azure Portal - Cloud Shell](/uploads/cloudshell.png)\\n\\nUsing this method is handy when you have more than one person administering the subscription, each person can have its own file share, which can then be backed up using Azure Backup and easily removed later when not needed."},{"id":"/2022/02/16/native-rdp-client-azure-bastion","metadata":{"permalink":"/2022/02/16/native-rdp-client-azure-bastion","source":"@site/blog/2022-02-16-native-rdp-client-azure-bastion.md","title":"Native RDP Client & Azure Bastion","description":"In early February 2022, Azure Bastion Preview support for the native Windows SSH and RDP\\") client came out, and this meant that we no longer have to rely on the Azure Portal and the limitations of a web browser - the support also includes File transfer through the clipboard by copying and pasted into the RDP session!","date":"2022-02-16T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.82,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Native RDP Client & Azure Bastion","authors":["Luke"],"tags":["Azure"],"date":"2022-02-16 00:00:00 +1300","toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Setup Azure Cloud Shell","permalink":"/azure/setup-azure-cloud-shell"},"nextItem":{"title":"AzureFeeds","permalink":"/2022/02/11/azurefeeds"}},"content":"In early February 2022, Azure Bastion Preview support for the [native Windows SSH and RDP](https://learn.microsoft.com/en-us/azure/bastion/connect-native-client-windows?WT.mc_id=AZ-MVP-5004796 \\"Connect to a VM using the native client (Preview)\\") client came out, and this meant that we no longer have to rely on the Azure Portal and the limitations of a web browser - the support also includes File transfer through the clipboard by copying and pasted into the RDP session!\\n\\n> Azure Bastion is a fully managed service that provides more secure and seamless Remote Desktop Protocol (RDP) and Secure Shell Protocol (SSH) access to virtual machines (VMs) without any exposure through public IP addresses. Provision the service directly in your local or peered virtual network to get support for all the VMs within it.\\n\\nLet\u2019s test the native RDP client through a secure connection using Azure Bastion!\\n\\n### Prerequisites\\n\\n* This configuration requires the [Standard](https://learn.microsoft.com/en-us/azure/bastion/configuration-settings?WT.mc_id=AZ-MVP-5004796 \\"Azure Bastion documentation\\") tier for Azure Bastion.\\n* A Virtual Machine(s) to connect\\n* Latest [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?WT.mc_id=AZ-MVP-5004796 \\"Azure CLI\\")\\n* Reader role on the Virtual Machine\\n* Read role on the Network Interface Card of the Virtual Machine.\\n* Reader role on the Azure Bastion resource\\n* Virtual Machine Administrator _(or User)_ login role _using_ [_Microsoft Entra ID_](https://learn.microsoft.com/en-us/azure/active-directory/devices/howto-vm-sign-in-azure-ad-windows?WT.mc_id=AZ-MVP-5004796 \\"Login to Windows virtual machine in Azure using Microsoft Entra ID authentication\\")_authentication_.\\n\\n### Create Azure Bastion\\n\\nIf you have a Virtual Machine but haven\'t set up Azure Bastion, run through the below to set it up:\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on **Create a resource**\\n 3. Search for: **Bastion** ![Azure - Bastion](/uploads/bastionmarketplace.png \\"Azure - Bastion\\")\\n 4. Click **Create**\\n 5. This is a Networking resource to place it in the same Resource Group as my Virtual Network.\\n 6. Please type in a **Name** for the **Bastion** instance; I will call mine: Bastion\\n 7. **Select** the **Region** that **matches** the Virtual **Network** region\\n 8. Select **Standard** Tier\\n 9. Select the **Virtual Network**\\n10. It now warns you about creating an: AzureBastionSubnet with a prefix of at least /27, so we need to create one; click on **Manage Subnet Configuration**.\\n11. Click **+ Subnet**\\n12. For the Name type in: **AzureBastionSubnet**\\n13. For the **Subnet** address range: **10.0.1.0/27**, _If you get an error that indicates the address is overlapping with another subnet, it may be because the Address space is only a /24; click Cancel and click on Address Space in the Virtual Network and change the /24 to/16 to increase the address range._\\n14. Click **Save** to create the subnet ![Azure - Bastion](/uploads/az_subnet.png \\"Azure - Bastion\\")\\n15. Up the Top, click **Create a Bastion**. Your subnet should be selected automatically to go back to the Bastion setup.\\n16. You do need a **Public IP** for Bastion, so **confirm** the **name** is appropriate, then click **Next: Tags.**\\n17. ![Azure Bastion](/uploads/2022-02-16-10_44_32-create-a-bastion-microsoft-azure-mozilla-firefox-private-browsing.png)\\n18. Add in appropriate tags, then click **Next: Advanced**\\n19. Check the box next to **Native client support (Preview)**\\n20. ![Azure Bastion](/uploads/2022-02-16-10_46_19-create-a-bastion-microsoft-azure-mozilla-firefox-private-browsing.png)\\n21. Click **Next: Review + Create**\\n22. Click on **Create** to create your Bastion instance!\\n\\n**Note: Bastion may take 10-20 minutes to provision.**\\n\\n### Check Bastion SKU\\n\\nIf you already have an Azure Bastion instance, let\'s check the SKU and, if needed, change it to Standard. Just a note:\\n\\n_Downgrading from a Standard SKU to a Basic SKU is not supported. To downgrade, you must delete and recreate Azure Bastion._\\n\\n1. Log in to the **Azure Portal**\\n2. Navigate to your **Bastion** resource\\n3. Click on: **Configuration**\\n4. Change Tier to **Standard**\\n5. Check: **Native client support (Preview)**\\n6. Click **Apply**\\n7. ![Azure Bastion](/uploads/2022-02-16-10_58_47-bastion-microsoft-azure-mozilla-firefox-private-browsing.png)\\n\\n### Connect to VM using Native RDP Support\\n\\n1. Open command prompt or Terminal\\n2. Type: **az login**\\n3. Login to your Azure subscription\\n4. We need the resource ID of the VM we need to connect to, type in: **az VM show --resource-group \'appserver-rg\' --name \'APP-P01\' --show-details**\\n5. _Change the resource group and VM name above to match your VM_\\n6. Copy the id of the Virtual Machine you want to connect to\\n7. Because I am running the Azure CLI from a PowerShell terminal, I am going to use the following variables:\\n\\n       $BastionName = \'Bastion\'\\n       $BastionRG = \'network-rg\'\\n       $VMResourceID= \'/subscriptions/000000-0000-0000-0000000/resourceGroups/appserver-rg/providers/Microsoft.Compute/virtualMachines/APP-P01\'\\n       az network bastion rdp --name $BastionName --resource-group $BastionRG --target-resource-id $VMResourceID\\n8. Run the command, your Remote Desktop window should open up, and the tunnel has been established; if you close the Azure CLI window, your RDP session will be dropped!\\n9. ![Azure Bastion](/uploads/azurebastiontst.gif)\\n\\nAs you could most likely tell, there are no options to enable drive passthrough, etc. You would usually find when connecting to Remote Desktop, but the copying of files/text, etc., does work!"},{"id":"/2022/02/11/azurefeeds","metadata":{"permalink":"/2022/02/11/azurefeeds","source":"@site/blog/2022-02-11-azurefeeds.md","title":"AzureFeeds","description":"Over the past few months, I have been busy working on a new project...","date":"2022-02-11T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.355,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-02-11 00:00:00 +1300","title":"AzureFeeds","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/azure-feeds-keep-up-to-date-with-the-ever-changing-and-evolving-microsoft-azur.png"}},"unlisted":false,"prevItem":{"title":"Native RDP Client & Azure Bastion","permalink":"/2022/02/16/native-rdp-client-azure-bastion"},"nextItem":{"title":"Datto Remote Management Azure VM Application Deployment","permalink":"/azure/azure-vm-application-deployment"}},"content":"Over the past few months, I have been busy working on a new project...\\n\\nA news aggregator for Azure news and updates, I tested some desire for this using AzureFeeds, on [LinkedIn ](https://www.linkedin.com/in/azure-feeds-709457212/recent-activity/ \\"Azure Feeds - Linkedin\\") as a platform _(which will continue)..._\\n\\nI didn\'t want my LinkedIn connections, to be spammed by Azure updates every day, when some connections were connected to me for other non-Azure related services, so I created a separate account that interested people could subscribe to AzureFeeds.\\n\\nI was lucky enough to acquire both [https://azureupdates.com](https://azureupdates.com \\"https://azureupdates.com\\") and [https://azurefeeds.com/](https://azurefeeds.com/ \\"https://azurefeeds.com/\\") and wanted to do something a bit more substantial than just forwarding to the relevant Microsoft pages.\\n\\nInspired by [PlanetPowerShell](https://www.planetpowershell.com/ \\"Planet PowerShell\\"), I wanted an Azure specific service, so I created the [AzureFeeds](https://azurefeeds.com/ \\"Azure Feeds\\") website.\\n\\nOriginally starting using official Microsoft services, due to the massive amount of updates and changes ongoing, I didn\'t want to make the feeds too busy, however with [feedback](https://twitter.com/lukemurraynz/status/1491139604879388673 \\"Twitter - Community or nah?\\") from the community through a Twitter poll, I then added an author section and added community-based content to supplement official updates, mimicking the functionality of PlanetPowerShell being content community-driven a lot more.\\n\\nYou can subscribe to a single feed by clicking \'Download Feed\' using your favourite RSS reader (even [Outlook](https://support.microsoft.com/en-us/office/what-are-rss-feeds-e8aaebc3-a0a7-40cd-9e10-88f9c1e74b97 \\" What are RSS feeds?\\")) and get the content delivered straight to you from official and community updates! Allowing you to keep up with Azure Updates and Azure community-driven updates easily, or just browse the webpage and filter by Author or Category for content that you may be after.\\n\\n**Check out..** [**https://azurefeeds.com/**](https://azurefeeds.com/ \\"https://azurefeeds.com/\\")\\n\\n[![Azure Feeds](/uploads/azure-feeds-keep-up-to-date-with-the-ever-changing-and-evolving-microsoft-azur.png \\"Azure Feeds\\")](https://azurefeeds.com/ \\"https://azurefeeds.com/\\")"},{"id":"azure/azure-vm-application-deployment","metadata":{"permalink":"/azure/azure-vm-application-deployment","source":"@site/blog/2022-02-05-azure-vm-application-deployment.md","title":"Datto Remote Management Azure VM Application Deployment","description":"The Azure Compute Gallery (superseded the Shared Image Gallery) offers more than just Azure Image management and replication, and you can deploy Applications to your Virtual Machines.","date":"2022-02-05T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":13.165,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-02-05 00:00:00 +1300","title":"Datto Remote Management Azure VM Application Deployment","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/azure-vm-application-deployment"},"unlisted":false,"prevItem":{"title":"AzureFeeds","permalink":"/2022/02/11/azurefeeds"},"nextItem":{"title":"Just in Time access to Azure Virtual Machines","permalink":"/2022/01/29/just-in-time-access-to-azure-virtual-machines"}},"content":"The Azure Compute Gallery _(superseded the Shared Image Gallery)_ offers more than just Azure Image management and replication, and you can deploy Applications to your Virtual Machines.\\n\\n### Overview\\n\\n> An Azure Compute Gallery helps you build structure and organization around your Azure resources, like images and [applications](https://learn.microsoft.com/en-us/azure/virtual-machines/vm-applications?WT.mc_id=AZ-MVP-5004796). An Azure Compute Gallery provides:\\n>\\n> * Global replication.\\n> * Versioning and grouping of resources for easier management.\\n> * Highly available resources with Zone Redundant Storage _(ZRS)_ accounts in regions that support Availability Zones. ZRS offers better resilience against zonal failures.\\n> * Premium storage support _(Premium_LRS)_.\\n> * Sharing across subscriptions, and even between Active Directory _(AD)_ tenants, using Azure RBAC.\\n> * Scaling your deployments with resource replicas in each region.\\n\\nWith images, Azure VM applications that support both Linux and Windows operating systems get these benefits.\\n\\n> While you can create an image of a VM with apps pre-installed, you would need to update your image each time you have application changes. Separating your application installation from your VM images means there\u2019s no need to publish a new image for every line of code change.\\n>\\n> Application packages provide benefits over other deployment and packaging methods:\\n>\\n> * Grouping and versioning of your packages\\n> * VM applications can be globally replicated to be closer to your infrastructure, so you don\u2019t need to use AzCopy or other storage copy mechanisms to copy the bits across Azure regions.\\n> * Sharing with other users through Azure Role Based Access Control (RBAC)\\n> * Support for virtual machines, and both flexible and uniform scale sets\\n> * If you have Network Security Group (NSG) rules applied on your VM or scale set, downloading the packages from an internet repository might not be possible. And with storage accounts, downloading packages onto locked-down VMs would require setting up private links.\\n> * VM applications can be used with the [DeployIfNotExists](https://learn.microsoft.com/en-us/azure/governance/policy/concepts/effects?WT.mc_id=AZ-MVP-5004796) policy.\\n\\nAzure VM Application packages _(stored in an Azure Storage account)_ uses multiple resources, as below:\\n\\n| Resource | Description |\\n| --- | --- |\\n| Azure compute gallery | A gallery is a repository for managing and sharing application packages. Users can share the gallery resource and all the child resources will be shared automatically. The gallery name must be unique per subscription. For example, you may have one gallery to store all your OS images and another gallery to store all your VM applications. |\\n| VM application | This is the\xa0definition of your VM application. This is a\xa0logical\xa0resource that stores the common metadata for all the versions under it. For example, you may have an application definition for Apache Tomcat and have multiple versions within it. |\\n| VM Application version | This is the deployable resource. You can globally replicate your VM application versions to target regions closer to your VM infrastructure. The VM Application Version must be replicated to a region before it may be deployed on a VM in that region. |\\n\\nThere is no extra charge for using VM Application Packages, but you will be charged for the following resources:\\n\\n* Storage costs of storing each package and any replicas.\\n* Network egress charges for replication of the first image version from the source region to the replicated regions. Subsequent replicas are handled within the region, so there are no additional charges.\\n\\nBefore we deploy our first VM application, there are a few things we need to be aware of:\\n\\n* VM Application requires an Azure Compute Gallery\\n* VM Application requires an Azure storage account to store your applications\\n* The VM Application gets downloaded to the VM using the name of the VM application _(not the actual name and Extension of your file in the storage account)_\\n* Currently, in order to retry a failed installation, you need to remove the application from the profile and add it back\\n* No more than five applications per Virtual Machine deployed at a time\\n* The maximum size of the application is 1 GB\\n* You can\'t have multiple versions of the same application installed on a Virtual Machine, and a newer version will supersede an older version either via an upgrade command or complete reinstall.\\n\\nIn this article, we are going to deploy the Datto Remote Management & Monitoring Agent to a Windows Server 2022 Virtual Machine; this agent is a simple executable that installs on a virtual machine and allows remote access and management of a virtual machine, without requiring any other form of connectivity _(Azure Bastion, RDP via Public IP, Site to Site VPN etc.)_ for an MSP _(Managed Service Provider)_ using the Datto toolset, the same concept can be applied to any application _(theoretically you can also use this to run PowerShell installs or chocolatey installs)_. \\n\\n> It\'s worth noting the VM Applications are currently in Public Preview, there is a good chance there will be changes in the way these operate and are configured when it becomes Generally Available.\\n\\n### Setup Azure VM Application Deployment\\n\\n#### Prerequisites\\n\\nIn order to use VM Applications, we need:\\n\\n* A storage account\\n* Azure Compute gallery\\n* VM application definition and version _(in my example: the Datto RMM agent)_\\n\\nFollowing the guide, we will run through the creation of everything from scratch; I am, however, assuming you already have the executable or application package and know the instructions to install/uninstall it - as each application is different. The Microsoft[VM Applications docs](https://learn.microsoft.com/en-us/azure/virtual-machines/vm-applications?WT.mc_id=AZ-MVP-5004796#exe-installer \\"VM Applications overview\\") give a few good examples for getting started with various applications.\\n\\n#### Setup Storage Account\\n\\nThe Storage account is where your application will be placed; it uses blobs; depending on the importance of your application deployments, you may want to go for geo-replication etc., but in this example, I will be going with a locally redundant, StorageV2 general-purpose account.\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Click on **+ Create a Resource**\\n 3. Search for: **Storage account**, and **select** it\\n 4. Click [**Create**](https://portal.azure.com/#create/Microsoft.StorageAccount-ARM \\"Create a storage account\\")\\n 5. **Select** your **subscription**\\n 6. **Select** a **Resource Group** for your storage account, **or create** a new **one**\\n 7. **Enter** your storage account **name** _(this needs to be globally unique)_\\n 8. **Select** your **region** that your application will be in; although the application can be replicated to other regions, it\'s better to select your primary region here.\\n 9. **Select** the **performance** and **redundancy** to match your requirements and click **Next: Advanced**\\n10. ![Azure Portal - Create Storage Account](/uploads/create-a-storage-account-microsoft-azure.png)\\n11. You can **leave** most **settings** here as **default**, the application executable will need to be able to be accessed directly; make sure the **Minimum TLS** is at least **1.2**.\\n12. You don\'t need hierarchical namespace etc.; unselect \'_Allow cross-tenant replication\'_ unless this is a feature you use.\\n13. ![Azure Portal - Create Storage Account](/uploads/create-a-storage-account-advanced-microsoft-azure.png)\\n14. Click **Review + Create** to skip to the last blade; most defaults are fine, but if you want to adjust the blob retainment and soft delete settings, go to the Data Protection tab, set them, then review your Configuration and select **Create**.\\n15. Go back to your storage account and click **Configuration**\\n16. Make sure: Allow storage account key access is: **Enabled**; if it is not, select Enabled and click **Save**.\\n\\n#### Setup Azure Compute Gallery\\n\\nNow that we have the Storage account to store your application binaries, we now need an Azure Compute Gallery _(previously the Shared Image Gallery)_ to keep your application definition and version metadata.\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Click on **+ Create a Resource**\\n 3. Search for: **Azure Compute Gallery** and **select** it\\n 4. Click [**Create**](https://portal.azure.com/#create/microsoft.sharedImageGallery \\"Create Azure compute gallery\\")\\n 5. **Select** your **subscription** and **resource group** _(in this case, I am going to use the same resource group as the Storage account I created earlier)_\\n 6. Type in a **name**, and **select** your **region**\\n 7. Although not mandatory, use the opportunity to fill in a description for the purpose of the Compute Gallery for future reference\\n 8. ![Azure Portal - Create Storage Account](/uploads/create-azure-compute-gallery-microsoft-azure.png)\\n 9. Select **Review + Create**\\n10. Verify everything is correct and click on: **Create**\\n\\n#### Create Application Definition\\n\\nVM application definitions are created within a gallery and carry information about the application and requirements for using it internally. This includes the operating system type for the VM application versions contained within the application definition. The name of your Application definition defines the name of the file that will be downloaded to your virtual machines.\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Navigate to \'**All Resources\'**\\n 3. **Find and click on** your **Azure Compute Gallery** you created earlier\\n 4. On the overview pane, select **+ Add**\\n 5. Click on +**VM application definition**\\n 6. Your subscription and resource group should be automatically selected to the location of the Compute Gallery, type in the **name of** your **applicatio.n**\\n 7. Select your **region**\\n 8. Select the **OS type** - in my case, and I select **Windows**\\n 9. ![Azure Portal - Create Application Definition](/uploads/create-a-vm-application-definition-microsoft-azure.png)\\n10. Click **Next: Publishing Options**\\n11. The following fields are not mandatory, but I recommend filling in areas to help report on and manage your applications.\\n    * Description\\n    * End of life date\\n    * Eula link\\n    * Privacy URI\\n    * Release notes URI\\n12. ![Azure Portal - Create Metadata](/uploads/create-a-vm-application-definition-metadata-microsoft-azure.png)\\n13. Click **Review + create**\\n14. Verify your Configuration and select **Create**\\n\\n#### Create Application version\\n\\nNow that we have the application definition setup, it\'s time to set up the version and upload our binary file.+\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Navigate to \'**All Resources\'**\\n 3. **Find and click on** your **Azure Compute Gallery** you created earlier\\n 4. Click on **Definitions**_(besides the Get Started link)_\\n 5. **Select** your Application **definition**\\n 6. Click on: **+Add** \\n 7. **Enter** in your **version number**, and this will increment and grow as you adjust and troubleshoot your application; I recommend starting with 0.0.1 then working your way up, with 1.0.0 being potentially your final/production-ready releast.\\n 8. Select your **Region**\\n 9. Now we need to select our source application package _(you can enter in your blob URL if you know it)_; we haven\'t uploaded it to our storage account yet, so we will select **Browse**\\n10. **Select** your **Storage account**\\n11. Press **+ Container**\\n12. **Enter** in the **name** of your **container** _( it has to be in lowercase)_, such as the application name _(to keep things separate, consider a container per application)_\\n13. Press **Upload**\\n14. **Browse** to your **file** and select it\\n15. Expand **Advanced**\\n16. Make sure that Blob type is: **Blob**\\n17. ![Azure Portal - Azure Blob](/uploads/upload-blob-microsoft-azure.png)\\n18. Click **Upload**\\n19. **Select** your newly uploaded file and click **Select**\\n20. _Note: You can only upload one file as part of your package, you can upload a ZIP file and have your Install script extract it_\\n21. The **Install script** is the command to install to your application, by default windows applications are set to install cmd. This already knows the directory your files are in because the file will be uploaded as the application name (i.e. DattoRMM), it needs to be renamed to include .exe and then ran, I will switch to PowerShell for the Install script, so will enter:\\n\\n        powershell.exe -command \\"Rename-Item \'.\\\\DattoRMM\' -NewName \'DattoRMM.exe\'; Start-Process \'.\\\\DattoRMM.exe\'\\"\\n22. If you have a script to uninstall the application, enter it _(in my case, I am just going to put a \'.\' to skip this, as I don\'t currently have an uninstall script developed)_\\n23. The rest of the Configuration isn\'t mandatory; the Update script is used by Azure when a new version of an application is created; by default, the Azure VM extension will treat an upgrade like a completely new install and run the install steps unless an update script is defined.\\n24. ![Azure Portal - Application Version](/uploads/create-a-vm-application-version-microsoft-azure.png)\\n25. Click **Next: Replication**\\n26. Like Azure Compute Images, you can replicate your Azure VM applications across multiple regions _(depending on where your workloads are)_, such as Australia East to West Europe, and store it then Zone Redundant or Local storage. In my example, I am going to leave mine as one replica in Australia East on locally-redundant storage and click **Review + create**\\n27. Verify everything looks ok and click **Create** to create your application version! This may take a few minutes to create, depending on your configuration and replication.\\n\\n### Deploy Azure VM Application\\n\\n#### Deploy Azure VM Application to Virtual Machines using the Azure Portal\\n\\nNow that your Azure VM Application has been created, it is now time to deploy to a Virtual Machine. I have a Windows Server 2022 Datacenter Azure Gen 2 VM running as a Standard_B2ms as my test machine, and because I am going to use the Datto RMM agent to connect to the machine, I don\'t need any RDP ports open etc.\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Microsoft Azure Portal\\")\\n 2. Navigate to **Virtual Machines**\\n 3. Click on your Virtual Machine\\n 4. Under Settings, click **Extensions + Applications**\\n 5. Click **VM Applications**\\n 6. Click **+ Add application**\\n 7. **Select** your **application** _(note you can select a particular version, by default, it is the latest)_\\n 8. Click **Ok**\\n 9. ![Azure Portal - VM-P01](/uploads/vm-p01-microsoft-azure.png)\\n10. You can select your Install Order _(i.e. if you had multiple applications, you can select which one installs 1st, 2nd, third and so on)_; I will select No Reference and click Save to start the deployment.\\n11. If you click Extensions, you should see that a: VMAppExtension has started to be installed; click on Refresh to update the status and click on the Extension to a more detailed status message, hopefully you see \\":Operational Install is SUCCESS\\"\\n12. My Virtual Machine has now had the Datto Remote Management agent installed successfully and has appeared in the portal for me to connect to!\\n13. ![Azure - Datto RMM](/uploads/vm-p01-datto-rmm.png)\\n\\n#### Deploy Azure VM Application to Multiple Virtual Machines using PowerShell\\n\\nI\'ve created the PowerShell script below to deploy an application to multiple Virtual Machines at once, it can easily be adjusted for a PowerShell Runbook that runs periodically to install software on machines it may be missing. As usual, please make sure you test and run any PowerShell scripts first in a demo environment.\\n\\n    $allvms = Get-AzVM\\n    $applicationname = \'DattoRMM\'\\n    $galleryname = \'AzComputeGallery\'\\n    $galleryrg = \'vmapps-prod-rg\'\\n    $appversion = \'0.0.1\'\\n    \\n      \\n    try\\n    {\\n      ForEach ($vm in $allvms)\\n    \\n      {\\n        $AzVM = Get-AzVM -ResourceGroupName $vm.ResourceGroupName -Name $vm.Name\\n        $appversion = Get-AzGalleryApplicationVersion `\\n        -GalleryApplicationName $applicationname `\\n        -GalleryName $galleryname `\\n        -Name $appversion `\\n        -ResourceGroupName $galleryrg\\n        $packageid = $appversion.Id\\n        $app = New-AzVmGalleryApplication -PackageReferenceId $packageid\\n        Add-AzVmGalleryApplication -VM $AzVM -GalleryApplication $app\\n        Update-AzVM -ResourceGroupName $vm.ResourceGroupName -VM $AzVM -ErrorAction Stop\\n      }\\n    }\\n    \\n    catch [Microsoft.Azure.Commands.Compute.Common.ComputeCloudException]\\n    {\\n      #Most likely failed due to duplicate package ID/identical version\\n      [Management.Automation.ErrorRecord]$e = $_\\n    \\n      $info = [PSCustomObject]@{\\n        Exception = $e.Exception.Message\\n        Reason    = $e.CategoryInfo.Reason\\n        Target    = $e.CategoryInfo.TargetName\\n      }\\n    \\n      $info\\n    }\\n\\n### Troubleshooting VM Application\\n\\nIf you have problems installing a package, just a reminder that the VM Application, uploads your file based on the name of the Application, to the server and needs to be renamed with a valid extension as part of the install script.\\n\\n#### Package Location\\n\\nThe package/extension location is here: \\n\\n* C:\\\\\\\\Packages\\\\\\\\Plugins\\\\\\\\Microsoft.CPlat.Core.VMApplicationManagerWindows\\\\\\\\{VERSION#}\\\\\\\\\\n\\nYou will find your Application binary under Downloads.\\n\\n#### Logs\\n\\n**For the extension status logs, navigate to:** \\n\\n* C:\\\\\\\\Packages\\\\\\\\Plugins\\\\\\\\Microsoft.CPlat.Core.VMApplicationManagerWindows\\\\\\\\{VERSION#}\\\\\\\\Status\\n\\nYou should see files such as:\\n\\n* 0.status\\n\\nYou can right-click these and open them in Notepad, and you should have the timestamp and the last status message, this should be identical to what you see in the Azure Portal.\\n\\n**For the application install logs, navigate to:** \\n\\n* C:\\\\\\\\Packages\\\\\\\\Plugins\\\\\\\\Microsoft.CPlat.Core.VMApplicationManagerWindows\\\\\\\\{VERSION#}\\\\\\\\Downloads\\\\\\\\{APPNAME}\\\\\\\\{APPVERSION}\\\\\\\\\\n\\nYou may see files such as:\\n\\n* stderr\\n* stdout\\n\\nYou can right-click these and open them in Notepad, any errors will be noted in these.\\n\\n#### Troubleshooting during preview\\n\\n* [Troubleshooting during preview](https://learn.microsoft.com/en-us/azure/virtual-machines/vm-applications?WT.mc_id=AZ-MVP-5004796#troubleshooting-during-preview \\"Troubleshooting during preview\\")"},{"id":"/2022/01/29/just-in-time-access-to-azure-virtual-machines","metadata":{"permalink":"/2022/01/29/just-in-time-access-to-azure-virtual-machines","source":"@site/blog/2022-01-29-just-in-time-access-to-azure-virtual-machines.md","title":"Just in Time access to Azure Virtual Machines","description":"Microsoft Defender for Cloud offers advanced security features, combining functions such as adaptive application controls (application whitelisting), networking hardening (machine learning that learns the traffic passing through your network security group, which helps you create more restricted rules) and advanced antivirus and threat protection; however, a hidden gem of this suite is: Just in Time VM Access.","date":"2022-01-28T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.875,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-28 00:00:00 +1300","title":"Just in Time access to Azure Virtual Machines","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Datto Remote Management Azure VM Application Deployment","permalink":"/azure/azure-vm-application-deployment"},"nextItem":{"title":"Application Security Groups in Microsoft Azure","permalink":"/azure/application-security-groups-in-microsoft-azure"}},"content":"Microsoft Defender for Cloud offers advanced security features, combining functions such as adaptive application controls _(application whitelisting)_, networking hardening _(machine learning that learns the traffic passing through your network security group, which helps you create more restricted rules)_ and advanced antivirus and threat protection; however, a hidden gem of this suite is: Just in Time VM Access.\\n\\nFor a [fee](https://azure.microsoft.com/en-us/pricing/details/defender-for-cloud/?WT.mc_id=AZ-MVP-5004796 \\" Microsoft Defender for Cloud pricing\\") for Microsoft Defender for Cloud, you can use Just In Time or JIT to lock down inbound traffic to your Azure Virtual Machines when you only need it.\\n\\nThis reduces exposure to attacks while providing easy access when you need to connect to a VM.\\n\\nToday, we will use the Azure Portal and configure Just-in-Time access for RDP for a Windows virtual machine running in Microsoft Azure that has a public IP.\\n\\nThis article assumes you have the authority and permissions (_at least the Security Administrator role)_ to configure and pay for Defender for Cloud.\\n\\n### Configure Defender for Cloud\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Navigate to the Virtual Machine you would like to enable Defender for Cloud on and enable Just in Time Access for\\n 3. Click on **Configuration**\\n 4. Click on: **Upgrade your Security Center subscription** to enable just-in-time access.\\n 5. ![Azure Portal - Configuration](/uploads/azureportal-configurationinitialjit.png)\\n 6. **Select** the **Subscription** that holds your VM and select **Upgrade**\\n 7. If you want to make use of the rest of the Cloud Defender offerings (and you should), then make sure you install the Log Analytics agent; however, you don\'t need it for Just In Time Access.\\n 8. **Navigate** back to your **Virtual Machine**\\n 9. Click on **Configuration**\\n10. Now you should see: **Enable for Just-in-Time VM access**; select this to **enable** Just in Time.\\n\\nNow, if you go to the Network Security Group attached to the network interface of the VM, you should see a Deny Rule for 3389 that\'s been created with a priority lower than the allowed rules, forcing the block.\\n\\n![Azure Portal](/uploads/azureportal-justintime_nsgblock.png)\\n\\n### Configure Just In Time\\n\\nNow that we have enabled Defender for Cloud, it\'s time to configure what ports (can be used for more than just RDP and SSH access) and the length of time that access is allowed. \\n\\n 1. Click on **Microsoft Defender for Cloud**\\n 2. Select **Workload Protections**\\n 3. Select **Just-In-Time VM access**\\n 4. ![Azure Portal - Defender for Cloud](/uploads/azureportal-defenderforcloud.png)\\n 5. **Select** the **VM** you have configured.\\n 6. Click the **ellipsis** on the right-hand side\\n 7. Select **Edit**\\n 8. Azure has automatically added the port for RDP (3389) and sets a maximum 3 hour request time (that I am allowed to connect to the VM); I can also restrict Just In TIme access from specific IP addresses, and IP ranges to avoid anyone being able to RDP from their home IP.\\n 9. ![Azure Portal - Defender for Cloud](/uploads/azureportal-configurationports.png)\\n10. **Make any adjustments** you feel suits your environment. I am going to leave the configuration as is.\\n\\n### Request Access for Just In Time Access to RDP\\n\\nYou can [programmatically](https://learn.microsoft.com/en-us/azure/defender-for-cloud/just-in-time-access-usage?tabs=jit-config-asc%2Cjit-request-powershell&WT.mc_id=AZ-MVP-5004796 \\"Secure your management ports with just-in-time access\\")_request JiT access to Azure VM through PowerShell and the REST API _(see Additional Resources below for a PowerShell script to get you started)_, but we are going to use the Azure Portal.\\n\\n 1. Open the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. **Navigate** to the **Virtual Machine** you would like to enable Defender for Cloud on and enable Just in Time Access for\\n 3. Click **Connect**\\n 4. You should see an information alert at the top of the blade \\"This VM has a just-in-time access policy. Select \\"Request access\\" before connecting.\\"\\n 5. ![Azure Portal - Just In Time](/uploads/azureportal-requestaccess.png)\\n 6. Select **Source IP (My IP)**\\n 7. Click **Request access**\\n 8. You should now have access to RDP to the machine!\\n 9. If you look at the Network Security Group, you should be able to see a new \'Allow\' rule has been created with a priority lower than the block rule.\\n10. ![Azure Portal - Just In Time](/uploads/azureportal-justintime_nsgallow.png)\\n11. After 3 hours, the allowed rules will be removed automatically.\\n\\nHopefully, this helps keep your environment secure; if you implement this, make sure you read about the custom roles. To prevent the people from just adding in a rule for their public IP in the address manually, it may be better to create a custom role _(see the script in Additional Resources below to create this role)_.\\n\\n### Additional Resources\\n\\n* [Secure your management ports with just-in-time access](https://learn.microsoft.com/en-us/azure/defender-for-cloud/just-in-time-access-usage?tabs=jit-config-asc%2Cjit-request-asc&WT.mc_id=AZ-MVP-5004796 \\"Secure your management ports with just-in-time access\\")\\n* [Understanding just-in-time (JIT) VM access](https://learn.microsoft.com/en-us/azure/defender-for-cloud/just-in-time-access-overview?WT.mc_id=AZ-MVP-5004796 \\"Understanding just-in-time (JIT) VM access\\")\\n* [Just in Time Access Policy Script & Custom Role Script](https://github.com/Azure/Microsoft-Defender-for-Cloud/tree/main/Powershell%20scripts/JIT%20Scripts \\"JIT Scripts\\")"},{"id":"azure/application-security-groups-in-microsoft-azure","metadata":{"permalink":"/azure/application-security-groups-in-microsoft-azure","source":"@site/blog/2022-01-21-application-security-groups-in-microsoft-azure.md","title":"Application Security Groups in Microsoft Azure","description":"Azure Application Security Groups (ASG) allow you to define what workloads (Virtual Machines) you are running in Azure has access to what resource - without being tied by managing complex IP address rules inside a Network Security Group.","date":"2022-01-21T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.845,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Application Security Groups in Microsoft Azure","authors":["Luke"],"tags":["Azure"],"date":"2022-01-21 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/highleveldiagram_asg.png"},"slug":"azure/application-security-groups-in-microsoft-azure"},"unlisted":false,"prevItem":{"title":"Just in Time access to Azure Virtual Machines","permalink":"/2022/01/29/just-in-time-access-to-azure-virtual-machines"},"nextItem":{"title":"AWESOME-Azure-Architecture List","permalink":"/2022/01/16/awesome-azure-architecture-list"}},"content":"Azure Application Security Groups (ASG) allow you to define what workloads _(Virtual Machines)_ you are running in Azure has access to what resource - without being tied by managing complex IP address rules inside a [Network Security Group](https://learn.microsoft.com/en-us/azure/virtual-network/network-security-groups-overview?WT.mc_id=AZ-MVP-5004796 \\"Network security groups\\").\\n\\n> Application security groups enable you to configure network security as a natural extension of an application\'s structure, allowing you to group virtual machines and define network security policies based on those groups. You can reuse your security policy at scale without manual maintenance of explicit IP addresses.\\n\\nThese Azure Application Security groups allow you to define your workloads, for example, Azure Virtual Desktop Session Hosts as a \'group\' and what they may have access to in your Azure Virtual Network without opening up the service to everything inside of that VNET or creating overly complex rules that could make it hard to troubleshoot.\\n\\nThere are a few things to be mindful of:\\n\\n* Azure Application Security Groups are Virtual Network-specific, so they can work to allow resources across subnets, but not in separate Virtual Networks, even if they have peered.\\n* As with most Azure resources, there are Subscription level[limits](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?toc=%2Fazure%2Fvirtual-network%2Ftoc.json&WT.mc_id=AZ-MVP-5004796#azure-resource-manager-virtual-networking-limits \\"Networking limits - Azure Resource Manager\\"); you cannot have more than 3,000 Azure Application Security groups in a single subscription and region.\\n* The rules that specify an application security group as the source or destination are only applied to the network interfaces that are members of the application security group; this does not affect anything not in this group, even though your Network Security Group is based on the subnet.\\n* You can assign more than one Application Security group to a resource\\n\\nIn my example, I have a single virtual network, with 2 subnets _(one subnet, has an Azure Virtual Desktop session host and the other one has a webserver running IIS)_, using Azure Application Security Groups, we will restrict IIS access to the webserver from the Azure Virtual Session hosts only - so IIS won\'t be accessible from any other machine in the Virtual Network.\\n\\n![High Level  - Diagram ASG](/uploads/highleveldiagram_asg.png \\"High Level  - Diagram ASG\\")\\n\\n#### Create Application Security Group\\n\\nLet\'s get started by creating an Application Security Group.\\n\\n1. Open the **Azure Portal**\\n2. Click on [**+ Create a resource**](https://portal.azure.com/#create/hub \\"Azure Portal - Create a resoruce\\")\\n3. Search for: **Application security group** and select it\\n4. Click **Create**\\n5. **Select** the **subscription** that the Application Security group will be created in\\n6. **Select** the **Resource Group** _(in my example, I am selecting AVD)_\\n7. ![Create an Application Security Group](/uploads/create-applicationsecuritygroup.png \\"Create an Application Security Group\\")\\n8. Click **Review + create**\\n9. Click **Create**\\n\\n#### Assign Application Security Group\\n\\nNow that the Application Security group has been created it\'s time to assign it this our Azure Virtual Desktop session hosts.\\n\\n1. Open the **Azure Portal**\\n2. **Navigate** to your **Azure Virtual Desktop** session host _(or other workloads you are going to use)_\\n3. Select **Networking**\\n4. Select **Application security groups**\\n5. Click **Configure the application security groups**\\n6. **Select** the Application Security **group** created earlier\\n7. ![Assign Application Security Group](/uploads/assign-applicationsecuritygroup.png \\"Assign Application Security Group\\")\\n8. Click **Save**\\n\\n#### Assign Block Rule Security Group\\n\\nNow it\'s time to assign a block rule to our web server, for port 80 as by default it is allowed through the \'AllowVnetInBound\' default rule.\\n\\n1. Navigate to the **Network Security Group** that holds your web server _(I am going to make the change on a Network Security group that is tied to the Network Interface of the web server, but the same principle applies if it was applied to a Network Security Group on the subnet - you just need to add the destination IP of the webserver)_\\n2. Click on **Inbound security rules**\\n3. Click **+ Add**\\n4. Add a **Deny** port **rule** for port **80** for all source\\n5. ![Assign Block Network Security Group Rule](/uploads/create-blocknsg80rule.png \\"Assign Block Network Security Group Rule\\")\\n6. Click **Save**\\n\\nAfter a few minutes, traffic from any workloads on the virtual network will now be blocked from accessing the web server on port 80.\\n\\n![Port 80 blocked](/uploads/avd-testport80_deny.png \\"Port 80 blocked\\")\\n\\n#### Create Allow rule using Application Security Group\\n\\nJust a note around the priorities of Network Security Group rules: \\n\\n> Rules are processed in priority order _(using a number between 100 and 4096)_, with lower numbers processed before higher numbers, because lower numbers have higher priority. Once traffic matches a rule, processing stops. \\n>\\n> As a result, any rules that exist with lower priorities (higher numbers) that have the same attributes as rules with higher priorities are not processed.\\n\\n1. Navigate to the **Network Security Group** that holds your web server _(I am going to make the change on a Network Security group that is tied to the Network Interface of the web server, but the same principle applies if it was applied to a Network Security Group on the subnet - you just need to add the destination IP of the webserver)_\\n2. Click on **Inbound security rules**\\n3. Click **+ Add**\\n4. For **Source**, select the **Application Security Group** \\n5. Select **HTTP** as the service\\n6. Select **Allow** as the action\\n7. Set the **Priority** to be **lower** than the block rule, ie 4095\\n8. Click **Save**\\n9. ![Create Network Security Rule Allow](/uploads/avd-testport80_allow.png \\"Create Network Security Rule Allow\\")\\n\\nAfter a few minutes, traffic from any workloads on the virtual network will now be allowed for any workloads from your Azure Virtual Desktop farm only _(assigned to the Application Security group)_.\\n\\n![AVD - Test Port 80 Allow](/uploads/avd-testport80_edgeallow.png)\\n\\nIf I attempted to access the webserver from my application server, it fails:\\n\\n![AVD - Test Port 80 Deny](/uploads/avd-testport80_deny.png)\\n\\nHopefully, this helps you avoid overly complex security rules that are reliant on knowing and managing the IP of your workloads and help secure your networks."},{"id":"/2022/01/16/awesome-azure-architecture-list","metadata":{"permalink":"/2022/01/16/awesome-azure-architecture-list","source":"@site/blog/2022-01-16-awesome-azure-architecture-list.md","title":"AWESOME-Azure-Architecture List","description":"A very brief Blog article today, I have created an AWESOME-Azure-Architecture list, this list is hosted in Github and located here:","date":"2022-01-16T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.42,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-16 00:00:00 +1300","title":"AWESOME-Azure-Architecture List","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/Awesome_Azure_Architecture.png"}},"unlisted":false,"prevItem":{"title":"Application Security Groups in Microsoft Azure","permalink":"/azure/application-security-groups-in-microsoft-azure"},"nextItem":{"title":"Create Azure Point to Site VPN using Microsoft Entra ID authentication","permalink":"/azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication"}},"content":"A very brief Blog article today, I have created an AWESOME-Azure-Architecture list, this list is hosted in Github and located here:\\n\\n* [AWESOME-Azure-Architecture](https://github.com/lukemurraynz/awesome-azure-architecture/blob/main/README.md \\"AWESOME-Azure-Architecture\\")\\n\\nThis list is a curated list of AWESOME blogs, videos, tutorials, code, tools & scripts, related to the design and implementation of solutions in Microsoft Azure.\\n\\nThis list contains anything that can help with your **Microsoft Azure architecture** and quickly get you up and running when designing, planning, and implementing services that empower organisations around the planet to achieve more."},{"id":"azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication","metadata":{"permalink":"/azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication","source":"@site/blog/2022-01-11-create-azure-point-to-site-vpn-using-azure-active-directory-authentication.md","title":"Create Azure Point to Site VPN using Microsoft Entra ID authentication","description":"You may be working remotely or only have a few devices needing access to your resources in Azure; a solution that can be deployed is a point to site connection straight into your Microsoft Azure network.","date":"2022-01-12T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.23,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-12 00:00:00 +1300","title":"Create Azure Point to Site VPN using Microsoft Entra ID authentication","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"uploads/hl_azurep2s.png"},"slug":"azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication"},"unlisted":false,"prevItem":{"title":"AWESOME-Azure-Architecture List","permalink":"/2022/01/16/awesome-azure-architecture-list"},"nextItem":{"title":"My Website Setup","permalink":"/2022/01/08/my-website"}},"content":"You may be working remotely or only have a few devices needing access to your resources in Azure; a solution that can be deployed is a point to site connection straight into your Microsoft Azure network.\\n\\nThis functionality allows your computer to connect privately to resources over a secure tunnel using your internet connection, using an Azure Virtual Network gateway, you can seamlessly connect to resources without the need of opening up your resources to the internet or having to whitelist your _(or third party vendor)_ IP address, which may change daily.\\n\\nYou know only your specified users access your Azure resources using Microsoft Entra ID.\\n\\nYou can have a site to site and point to site VPN running on the same Gateway today. We will set up a Point to Site VPN using Windows 11.\\n\\n![Azure Point to Site](/uploads/hl_azurep2s.png)\\n\\nDepending on the SKU of your Virtual Network Gateway, depends on the number of concurrent connections and throughput you are allowed; because we are using Microsoft Entra ID and the OpenVPN protocol, I will be selecting Generation 1, VpnGw1, supporting a max of 250 connections _(you can double the number of throughput and connections if you are running in Active/Active and have a second gateway, or select a higher SKU)_.\\n\\n> Azure AD authentication is supported for OpenVPN\xae protocol connections only and requires the Azure VPN client.\\n\\nA note about Gateway SKUs _(apart from Basic)_ you can resize in the same generation _(i.e. Generation 1 VpnGw1 to VpnGw3, but you can\'t go from Generation 1 VpnGw1 to Generation 2 VpnGw5, in order to upgrade, you have to delete and recreate the Gateway, just keep this in mind when deciding on the SKU of your resources_).\\n\\nYou can read more about the Virtual Network Gateways and VPN SKUs at the official Microsoft documentation [here](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways?WT.mc_id=AZ-MVP-5004796); your Gateway SKU may differ depending on your requirements.\\n\\n### Create Azure Point to Site VPN using Microsoft Entra ID authentication\\n\\n#### Prerequisites\\n\\n* An Azure subscription _(that you have at least contributor rights to and the ability to create Users and Groups)_\\n* An endpoint device running Windows 10 or 11 that you can install the [Azure VPN client](https://www.microsoft.com/en-us/p/azure-vpn-client-preview/9np355qt2sqb?rtc=2&activetab=pivot:overviewtab \\" Azure VPN Client\\") onto\\n\\n#### Create Virtual Network\\n\\nFirst things first, let\'s create a Virtual Network.\\n\\n 1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Click on **+ Create a resource**\\n 3. Search for: **Virtual Network** and click on it\\n 4. Click **Create**\\n 5. **Select** or create your **Resource Group** that you want your network resource to sit in _(I recommend Virtual Network and the gateway resources sit in its own Resource Group away from other resources so that they can be protected by resource locks, RBAC and they are usually classified as a shared resource)_.\\n 6. ![Azure Virtual Network](/uploads/azureportal-createvirtualnetwork.png \\"Azure Virtual Network\\")\\n 7. Click **Next: IP Addresses**\\n 8. Now we need to **define** the **Address space** and subnets; I will leave the Address space as 10.0.0.0/16 but remove the Default subnet _(select the checkbox next to the Subnet and select Delete)_\\n 9. Click +**Add Subnet**, and add a new subnet with the name of GatewaySubnet with an IP range of: 10.0.1.0/27 _(this Subnet will be used by our Virtual Network Gateway, and the name needs to be exactly GatewaySubnet)_.\\n10. Now I will add a subnet named: app servers, for the Virtual Machines I will need to connect to will be placed.\\n11. ![Azure Virtual Network](/uploads/azureportal-createvirtualnetworksubnets.png \\"Azure Virtual Network\\")\\n12. Click **Next: Security**\\n13. Leave everything _(BastionHost, DDoS Protection Standard, Firewall)_ as Disabled.\\n14. Click **Next: Tags**\\n15. Enter in any tags and click **Review + Create**\\n16. Review your configuration and click **Create**\\n\\n#### Create Virtual Network Gateway\\n\\nNow that we have the foundation of our setup - an Azure Virtual Network, it is time to provision the Gateway itself; just a note before we continue, the Gateway can take 30-60 minutes to provision.\\n\\n 1. Log in to the [Azure Portal](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Click on **+ Create a resource**\\n 3. Type in and search for: **Virtual Network Gateway**\\n 4. Click **Create**\\n 5. Type in the **name** of your Azure Virtual Network **Gateway**\\n 6. Select the **region** _(it must be the same region as your virtual network)_\\n 7. Select the Gateway Type: **VPN**\\n 8. The VPN type is: **Route-based**\\n 9. **Select** the **SKU**, in this example - I will be going with VpnGw1\\n10. **Select** the **Generation** of the Virtual Network Gateway; I am going with: Generation 1\\n11. **Select** the **Virtual Network** that you created earlier, and it will automatically find and assign the Gateway to the Subnet named: GatewaySubnet\\n12. **Select** Standard **public IP** address **SKU**\\n13. **Select Public IP** address and select: **Create new**\\n14. **Type** in your public IP **name**\\n15. **Leave** \'Enable active-active mode\' and \'Configure BGP\' as **Disabled**.\\n16. Click **Review + Create**\\n17. ![Azure Virtual Network Gateway](/uploads/azureportal-createvnetfgw.png \\"Azure Virtual Network Gateway\\")\\n18. **Verify configuration** is correct and clicks **Create**\\n19. It can take up to 30-60 minutes for the Virtual Network Gateway to be created.\\n\\n#### Setup Microsoft Entra ID authentication on the Virtual Network Gateway\\n\\nNow that the Virtual Network has been created, we can now set up Microsoft Entra ID authentication.\\n\\n##### Collect Microsoft Entra ID Tenant ID\\n\\nFirst, we need to collect the Azure AD Tenancy ID\\n\\n1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n2. Click on **Microsoft Entra ID**\\n3. In the Overview pane, **copy** the **Tenant ID** and save this for the next step.\\n\\n##### Grant Azure VPN Client permisisons\\n\\nNow we need to grant the Azure VPN application permissions.\\n\\n 1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Open a new window and type in and press Enter:\\n\\n        https://login.microsoftonline.com/common/oauth2/authorize?client_id=41b23e61-6c1e-4545-b367-cd054e0ed4b4&response_type=code&redirect_uri=https://portal.azure.com&nonce=1234&prompt=admin_consent\\n 3. If you get an error about external identity, then replace /**common**/ with your tenant ID.\\n 4. ![Azure VPN Permissions](/uploads/azureportal_azurevpnpermissions.png \\"Azure VPN Permissions\\")\\n 5. Click **Accept**\\n 6. Navigate back to **Microsoft Entra ID**\\n 7. Select **Enterprise Applications**\\n 8. Select **Azure VPN**\\n 9. **Copy** the **Application ID** of the Azure VPN enterprise application _(you will need both Application ID and tenant ID for the next steps)_\\n10. ![Azure VPN](/uploads/azurevpn_enterpriseappvpn.png \\"Azure VPN\\")\\n\\n##### Configure Point to Site Connection\\n\\nNow its time to configure the Virtual Network Gateway\\n\\n 1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. **Navigate** to the Virtual Network **Gateway** you created earlier\\n 3. Click on **Point-to-site configuration**\\n 4. Click **Configure now**\\n 5. Enter in your **address pool** _(this is the address pool of the VPN clients, make sure this doesn\'t overlap with any other IP range you use, I will go with: 172.0.0.0/16)_\\n 6. Make sure the Tunnel type is: **OpenVPN (SSL)**\\n 7. Select **Microsoft Entra ID** for the **Authentication** type\\n 8. For **Tenant, ID enter** in: https://login.microsoftonline.com/**TENANTID**/ and enter in your own Tenant ID.\\n 9. For the Audience (this is the users and groups that are assigned to the Enterprise Azure VPN application), put in the Application ID of the Azure VPN.\\n10. For the Issuer, enter in: https://sts.windows.net/**TENANTID**/\\n11. ![Azure Virtual Network Gateway](/uploads/azure-point-to-site-configuration.png \\"Azure Virtual Network Gateway\\")\\n12. Click **Save**\\n13. It may take 1-5 minutes to save the configuration\\n\\n##### Install and connect using the Azure VPN client\\n\\nNow that the Point to Site VPN has been configured it\'s time to connect!\\n\\n 1. Click on **Download VPN client** _(if it is greyed out, then navigate to the Overview pane, then back to the Point-to-site configuration)_.\\n 2. Extract the zip file, you will need these files\\n 3. **Download** the [Azure VPN Client](https://go.microsoft.com/fwlink/?linkid=2117554) to your computer.\\n 4. ![Azure VPN Client](/uploads/windowsstore-azurevpn.png \\"Azure VPN Client\\")\\n 5. Once, downloaded click **Open.**\\n 6. Click the **+** sign (lower left)\\n 7. Click **Import**\\n 8. **Navigate** to the: **azurevpnconfig.xml** file that you downloaded earlier and click **Open**\\n 9. You can change the Connection Name to something more user friendly _(you can also edit the file directly for when you look at pushing out this to multiple users, but make sure you have a backup of the file)_\\n10. Click **Save**\\n11. ![Azure VPN Connection](/uploads/azurevpnclient-beforeconnection.png \\"Azure VPN Connection\\")\\n12. Click **Connect**\\n13. Enter in your Microsoft Entra ID credentials _(you may be prompted for MFA, depending on the rules - you can use Azure VPN application under conditional access)_\\n14. ![Azure VPN Connection](/uploads/azurevpnclient-afterconnection.png \\"Azure VPN Connection\\")\\n15. **You should now be connected to the Azure network through a point to site VPN!**\\n16. If I run \'ipconfig /all\' on my device, I can see a PPP adapter that is connected and on the VPN address range created earlier: 172.0.0.2\\n17. ![Azure Point to Site Connections](/uploads/azurevpn-ipconfig.png \\"Azure Point to Site Connections\\")\\n18. If I navigate back to the Point-to-site connection in the Azure Portal, I can see, my connection has been allocated:\\n19. ![Azure Point to Site Connections](/uploads/azurevpn-p2sconnections.png \\"Azure Point to Site Connections\\")\\n20. I can now use Remote Desktop to connect to a Virtual Machine, running in my AppServers Subnet, which I am running without the need of a Public IP or bastion/jump host:\\n21. ![Azure Point to Site VPN](/uploads/azurevpn-rdp.png \\"Azure Point to Site VPN\\")\\n\\nNote: I don\'t have a DNS service running in Azure, but the Azure VPN agent will take DNS from the Virtual Network if you have this configured to point towards a DNS server (Active Directory, or other DNS forwarder (pointing towards Azure DNS IP: 168.63.129.16) such as Azure Firewall DNS proxy; you can set Custom DNS servers by modifying your DNS configuration, or add entries into the host file of the computers.\\n\\nYou can set your Custom DNS settings (remember to add the DNS suffix if needed) and configure the VPN to automatically connect by following the details on the [OpenVPN Azure AD](https://learn.microsoft.com/en-us/azure/vpn-gateway/openvpn-azure-ad-client#faq \\"Microsoft Entra ID authentication: Configure a VPN client for P2S OpenVPN protocol connections\\") Client page.\\n\\nUsing Intune, you can also push this configuration to your Windows 10 and 11 clients\\n\\n* [Create custom Intune profiles to deploy VPN client profiles](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-profile-intune?WT.mc_id=AZ-MVP-5004796)"},{"id":"/2022/01/08/my-website","metadata":{"permalink":"/2022/01/08/my-website","source":"@site/blog/2022-01-08-my-website.md","title":"My Website Setup","description":"Pretty simple article today regarding \'My website setup\'.","date":"2022-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.79,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-08 00:00:00 +1300","title":"My Website Setup","authors":["Luke"],"tags":["Misc"],"toc":false,"header":{"teaser":"images/cover.jpg"}},"unlisted":false,"prevItem":{"title":"Create Azure Point to Site VPN using Microsoft Entra ID authentication","permalink":"/azure/create-azure-point-to-site-vpn-using-azure-active-directory-authentication"},"nextItem":{"title":"Azure Public DNS as Code","permalink":"/azure/azure-public-dns-as-code"}},"content":"Pretty simple article today regarding \'My website setup\'.\\n\\nI\'ve had a few people ask what CMS _(Content Management System)_ my website runs on - and no it\'s not running on an Azure App Service!  \\n  \\nI am using:  \\n\\n* Github Pages (running Jekyll and Ruby on Rails)  \\n* Cloudflare as my DNS CDN (which also allows me to set HTTPS) and cache the website across the planet  \\n  \\nBecause the pages are in a git repository, I have version control across my pages, can roll back or make any changes easily and allow others to submit pull requests for changes, or issues natively.  \\n  \\nThe pages are created using Markdown, I usually have a OneNote page with an idea or blurb, then Forestry to do the initial post, and then manually edit the files and verify the syntax is correct, add tables into the page and fix any issues that may have been caused (Forestry doesn\'t support markdown tables and can make some content look a bit weird and unstructured, but its usually an easy fix editing the markdown manually).  \\n  \\nHaving it on Github pages, helped me learn a lot more about using git and source control, versioning methodologies.  \\n  \\nThen for comments, I use Disqus and for analytics, Google Analytics and Bing Webmaster Tools.  \\n  \\n* [Markdown](https://www.markdownguide.org/getting-started/ \\"https://www.markdownguide.org/getting-started/\\")\\n* [https://pages.github.com/](https://pages.github.com/ \\"https://pages.github.com/\\")\\n* [What CMS is this site using?](https://whatcms.org/?s=luke.geek.nz \\"https://whatcms.org/?s=luke.geek.nz\\")\\n* [https://forestry.io/](https://forestry.io/ \\"https://forestry.io/\\")\\n* [https://disqus.com/](https://disqus.com/ \\"https://disqus.com/\\")  \\n  \\nAll in all - I just have to pay for the domain, everything else is free and because it\'s stateless, caching content is a lot easier and I don\'t have to worry about keeping a CMS up to date/patched or a database tuned!  \\n  \\nIf you\'re wondering why it\'s not running on an Azure App Service? I wanted something cheap, could further challenge and learn from, at the end of the day I wanted a stateless website (static websites in Storage account, wasn\'t available when I set this up) and I wanted to reserve my limited Azure credits to be able to actually learn and play more. I have no regrets in putting it in Github Pages and depending on your requirements - recommend you try it out!"},{"id":"azure/azure-public-dns-as-code","metadata":{"permalink":"/azure/azure-public-dns-as-code","source":"@site/blog/2022-01-07-azure-public-dns-as-code.md","title":"Azure Public DNS as Code","description":"The Microsoft Azure ecosystem offers a lot of capabilities that empower individuals and businesses; one of those capabilities that are often overlooked is DNS(Domain Name System).","date":"2022-01-07T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":12.63,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-07 00:00:00 +1300","title":"Azure Public DNS as Code","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/azure-public-dns-as-code"},"unlisted":false,"prevItem":{"title":"My Website Setup","permalink":"/2022/01/08/my-website"},"nextItem":{"title":"Controlled Chaos in Azure using Chaos Studio","permalink":"/azure/controlled-chaos-in-azure-using-chaos-studio"}},"content":"The Microsoft Azure ecosystem offers a lot of capabilities that empower individuals and businesses; one of those capabilities that are often overlooked is [DNS](https://en.wikipedia.org/wiki/Domain_Name_System)_(Domain Name System)_.\\n\\n> _Azure DNS allows you to host your DNS domain in Azure, so you can manage your DNS records using the same credentials, billing, and support contract as your other Azure services. Zones can be either public or private, where Private DNS Zones (in Managed Preview) are only visible to VMs that are in your virtual network._\\n>\\n> _You can configure Azure DNS to resolve hostnames in your public domain. For example, if you purchased the contoso.xyz domain name from a domain name registrar, you can configure Azure DNS to host the contoso.xyz domain and resolve `www.contoso.xyz` to the IP address of your web server or web app._\\n\\nIn this article, we are going to focus on [Azure Public DNS](https://learn.microsoft.com/en-us/azure/dns/dns-overview?WT.mc_id=AZ-MVP-5004796).\\n\\nI had my external DNS under source control using Terraform and the Cloudflare provider a few years ago. I wanted to see if I use source control and continuous integration to do the same thing using Azure DNS and Azure Bicep.\\n\\nMy theory was I could make a change to a file and then commit it and have the Azure DNS records created or modified automatically, allowing changes to DNS to be gated, approved, scheduled and audited, allowing changes and rollback a lot easier \u2013 without having to give people access to be able to create DNS records with no auditability, turns out you can!\\n\\nUsing an Azure DevOps pipeline and repository and Azure Bicep, we will deploy an Azure Public DNS zone to a resource group automatically on a successful commit and any records.\\n\\n![Azure Bicep - Pipeline High Level](/uploads/azurebicep_dns_hld.png \\"Azure Bicep - Pipeline High Level\\")\\n\\n### Create Azure Public DNS as Code\\n\\n#### Prerequisites\\n\\n* An [Azure DevOps](https://azure.microsoft.com/en-us/pricing/details/devops/azure-devops-services/?WT.mc_id=AZ-MVP-5004796) account and permissions to create a service endpoint\\n* An Azure subscription that you have at least contributor rights to\\n* A git repository _(I am going to use the repository in Azure DevOps, but you could use a nested repository from GitHub)_\\n* The latest [Azure PowerShell](https://learn.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.5.0&WT.mc_id=AZ-MVP-5004796) modules and [Azure Bicep/Azure CLI](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install?WT.mc_id=AZ-MVP-5004796) for local editing\\n* A domain name and rights to change the nameservers to point towards Azure DNS\\n\\nIn this article, I will be using an Azure subscription. I have access to an Azure DevOps _(free)_ subscription and a custom domain I joined named \'badasscloud.com\'.\\n\\nI will assume that you have nothing set up but feel free to skip the sections that aren\'t relevant.\\n\\nThat that we have the prerequisites sorted let\'s set it up...\\n\\n#### Create Azure DevOps Repository\\n\\n 1. [**Sign in to Azure DevOps**](https://go.microsoft.com/fwlink/?LinkId=2014676&githubsi=true&clcid=0x409&WebUserId=e3e298aac5104b0e8e949b3b5bbeb314)\\n 2. Select **+ New Project**\\n 3. Give your **project** a **name** _(i.e., I am going with: DNSAsCode)_\\n 4. ![Azure DevOps - Create New Project](/uploads/azuredevops-creatednsproject.png \\"Azure DevOps - Create New Project\\")\\n 5. Click **Create** _(your project will now be created)_\\n 6. Click on **Repos**\\n 7. Click on **Files**\\n 8. Find the \'**Initialize Main branch with a README or gitignore**\' section and click **Initialize.**\\n 9. ![Azure DevOps - Create New Project](/uploads/azuredevops-initializerepo.png \\"Azure DevOps - Create New Project\\")\\n10. You should now have an empty git repository!\\n\\n    #### Create Azure DevOps Service Connection\\n\\n    For Azure DevOps to connect to Microsoft Azure, we need to set up a service principal; you can create the service connection in Azure DevOps. However, it usually generates a service principal with a name that could be unrecognizable in the future in Azure, and I prefer to develop them according to naming convention and something that I can look at and instantly recognize its use-case. To do that, we will create it using Azure CLI.\\n     1. Open **PowerShell**\\n     2. **Run** the following **commands** to connect to Azure and **create** your **Service Principal** with Contributor **access** to **Azure**:\\n\\n            #Connects to Microsoft Azure\\n            az.cmd login\\n            #Set SPN name\\n            $AppRegName = \'SPN.AzureSubscription.Contributor\'\\n            #Creates SPN and sets SPN as Contributor to the subscription\\n            $spn = az.cmd ad sp create-for-rbac --name $AppRegName --role \'contributor\'\\n            #Exports Password, Tenant & App ID for better readability - required for Azure DevOps setup\\n            $spn | ConvertFrom-Json | Select-Object -Property password, tenant, appId\\n            az.cmd account show --query id --output tsv\\n            az.cmd account show --query name --output tsv\\n     3. Make sure you **record** the **password**, **application ID** and the **subscription ID/name**; you will need this for the next step - you won\'t be able to view it anywhere else; if you lose it, you can rerun the sp create command to generate a new password. Now that we have the SPN, we need to add the details into Azure DevOps.\\n     4. [**Sign in to Azure DevOps**](https://go.microsoft.com/fwlink/?LinkId=2014676&githubsi=true&clcid=0x409&WebUserId=e3e298aac5104b0e8e949b3b5bbeb314)\\n     5. Navigate to the DNS As Code **project** you created earlier\\n     6. Click on **Project Setting**s _(bottom right-hand side of the window)_\\n     7. Click on **Service connections**\\n     8. Click on: **Create a service connection**\\n     9. Select **Azure Resource Manager**\\n    10. Click **Next**\\n    11. Click on: **Service Principal (Manual**) and click Next\\n    12. **Enter** in the following **details** that we exported earlier from the creation of the service principal:\\n        * Subscription ID\\n        * Subscription Name\\n        * Service Principal ID _(the appId)_\\n        * Service principal key _(password)_\\n        * Tenant ID\\n    13. Click **Verify** to verify that Azure DevOps can connect to Azure; you should hopefully see a Verification succeeded.\\n    14. Give the **Service** connection a **name** _(this is the display name that is visual in Azure DevOps)_\\n    15. **Add** a **description** _(i.e. created by, created on, created for)_\\n    16. Click on **Verify and save**\\n    17. You now have a new Service connection!\\n    18. ![Azure DevOps - Service Connection](/uploads/new_azure_serviceconnectioncreated.png \\"Azure DevOps - Service Connection\\")\\n\\n_Note: The password for the service principal is valid for one year, so when they expire, you can come into the Azure DevOps service connection and update it here._\\n\\n#### Add Azure Bicep to Repository\\n\\nNow that Azure DevOps has the delegated rights to create resources in Microsoft Azure, we need to add the Azure Bicep for Azure DNS Zone.\\n\\nI have created the below Azure Bicep file named: Deploy-PublicDNS.bicep\\n\\n**Don\'t edit the file yet. You can add your DNS records later - after we add some variables into the Azure Pipeline.**\\n\\nThis file will:\\n\\n* Create a new public Azure DNS zone, if it doesn\'t exist\\n* Add/Remove and modify any records\\n\\nI have added CNAME, A Record and TXT Records as a base.\\n\\n```bicep title=\\"Deploy-PublicDNS.bicep\\"\\n///Variables - Edit, these variables can be set in the script or implemented as part of Azure DevOps variables.\\n//Set the Domain Name Zone:\\nparam PrimaryDNSZone string = \'\'\\n//Deploys to the location of your resource group, that is specified during the deployment.\\nvar location = \'Global\'\\n//Variable array for your A records. Add, remove and amend as needed, any new record needs to be included in {}.\\nvar arecords = [\\n  {\\n    name: \'@\'\\n    ipv4Address: \'8.8.8.8\'\\n  }\\n  {\\n    name: \'webmail\'\\n    ipv4Address: \'8.8.8.8\'\\n  }\\n]\\n//Variable array for your CNAME records. Add, remove and amend as needed, any new record needs to be included in {}.\\nvar cnamerecords = [\\n  {\\n    name: \'blog\'\\n    value: \'luke.geek.nz\'\\n  }\\n]\\n\\n// \\n\\nvar txtrecords = [\\n  {\\n    name: \'@\'\\n    value: \'v=spf1 include:spf.protection.outlook.com -all\'\\n  }\\n    \\n  ]\\n\\n///Deploys your infrastructure below.\\n\\n//Deploys your DNS Zone.\\n\\nresource DNSZone \'Microsoft.Network/dnsZones@2018-05-01\' = {\\n  name: toLower(PrimaryDNSZone)\\n  location: location\\n  properties: {\\n    zoneType: \'Public\'\\n  }\\n}\\n\\n//Deploys your A records that are listed in the arecord variable table above.\\n\\nresource DNSARecords \'Microsoft.Network/dnsZones/A@2018-05-01\' = [for arecord in arecords: {\\n  name: toLower(arecord.name)\\n  parent: DNSZone\\n  properties: {\\n    TTL: 3600\\n    ARecords: [\\n      {\\n        ipv4Address: arecord.ipv4Address\\n      }\\n      \\n    ]\\n  targetResource: {}\\n  }\\n}]\\n\\n//Deploys your CNAME records that are listed in the cnamerecord variable table above.\\n\\nresource CNAMErecords \'Microsoft.Network/dnsZones/CNAME@2018-05-01\' = [for cnamerecord in cnamerecords: {\\n  name: toLower(cnamerecord.name)\\n  parent: DNSZone\\n\\n  properties: {\\n    \'TTL\': 3600\\n    CNAMERecord: {\\n      \\n      cname: cnamerecord.value\\n      \\n    }\\n  targetResource: {}\\n  }\\n}]\\n\\nresource TXTrecords \'Microsoft.Network/dnsZones/TXT@2018-05-01\' = [for txtrecord in txtrecords: {\\nname: toLower(txtrecord.name)\\nparent: DNSZone\\n\\nproperties: {\\n   \'TTL\': 3600\\n   TXTRecords: [\\n      {\\n value: [\\n        txtrecord.value               \\n       ]\\n      }\\n      \\n    ]\\n}\\n     \\n \\n}]\\n\\n\\noutput cnamerecords string = CNAMErecords[0].properties.CNAMERecord.cname\\noutput arecords string = arecords[0].ipv4Address\\n```\\n\\nTo add the Azure Bicep file into Azure DevOps, you can commit it into the git repository; see a previous post on \'[Git using Github Desktop on Windows for SysAdmins](https://luke.geek.nz/windows/git-using-github-desktop-on-windows-for-sysadmins/ \\"Git using Github Desktop on Windows for SysAdmins \\")\' to help get started. However, at this stage, I will create it manually in the portal.\\n\\n 1. [**Sign in to Azure DevOps**](https://go.microsoft.com/fwlink/?LinkId=2014676&githubsi=true&clcid=0x409&WebUserId=e3e298aac5104b0e8e949b3b5bbeb314)\\n 2. Navigate to the DNS As Code **project** you created earlier\\n 3. Click on **Repos**\\n 4. Click on **Files**\\n 5. Click on the **Ellipsis** on the right-hand side\\n 6. Click **New**\\n 7. Click **File**\\n 8. ![Azure DevOps - New File](/uploads/azuredevops-createfile.png \\"Azure DevOps - New File\\")\\n 9. Type in the name of your file _(including the bicep extension)_, i.e. **Deploy-PublicDNS.bicep**\\n10. Click **Create**\\n11. **Copy** the **contents** of the Azure **Bicep** file supplied above and **paste** them into the Contents of **Deploy-PublicDNS.bicep in Azure DevOps**\\n12. ![Azure DevOps - Azure Bicep](/uploads/azuredevops-deploypublicdnsinitialcommit.png \\"Azure DevOps - Azure Bicep\\")\\n13. Click **Commit**\\n14. Click **Commit** again\\n15. While we are here, let\'s **delete** the **README.md** file (as it will cause issues with the pipeline later on), click on the README.md file.\\n16. Click on the Ellipsis on the right-hand side\\n17. Click **Delete**\\n18. Click **Commit**\\n19. You should now only have your: Deploy-PublicDNS.bicep in the repository.\\n\\n#### Create Azure DevOps Pipeline\\n\\nNow that we have the initial Azure Bicep file, it\'s time to create our pipeline that will do the heavy lifting. I have created the base pipeline that you can download, and we will import it into Azure DevOps.\\n\\n```yml title=\\"azure-pipelines.yml\\"\\n# Variable \'location\' was defined in the Variables tab\\n# Variable \'PrimaryDNSZone\' was defined in the Variables tab\\n# Variable \'ResourceGroupName\' was defined in the Variables tab\\n# Variable \'SPN\' is defined in the Variables tab\\ntrigger:\\n  branches:\\n    include:\\n    - refs/heads/main\\njobs:\\n- job: Job_1\\n  displayName: Agent job 1\\n  pool:\\n    vmImage: ubuntu-latest\\n  steps:\\n  - checkout: self\\n  - task: AzureCLI@2\\n    displayName: \'Azure CLI \'\\n    inputs:\\n      connectedServiceNameARM: $(SPN)\\n      scriptType: pscore\\n      scriptLocation: inlineScript\\n      inlineScript: >2-\\n         az group create --name $(ResourceGroupName) --location $(location)\\n                        az deployment group create  `\\n                        --template-file $(Build.SourcesDirectory)\\\\Deploy-PublicDNS.bicep `\\n                        --resource-group $(ResourceGroupName) `\\n                        --parameters PrimaryDNSZone=$(PrimaryDNSZone)\\n      powerShellErrorActionPreference: continue\\n\\n```\\nThis pipeline will run through the following steps:\\n\\n* Spin up an Azure-hosted agent running Ubuntu _(it already has the Azure CLI and PowerShell setup)_\\n* Create the Azure resource group to place your DNS zone into _(if it doesn\'t already exist)_\\n* Finally, do the actual Azure Bicep deployment and create your Primary DNS zone resource, and, if necessary, modify any resources.\\n\\nCopy the contents of the YAML pipeline above, and let\'s import it to Azure DevOps.\\n\\n 1. [**Sign in to Azure DevOps**](https://go.microsoft.com/fwlink/?LinkId=2014676&githubsi=true&clcid=0x409&WebUserId=e3e298aac5104b0e8e949b3b5bbeb314)\\n 2. Navigate to the DNS As Code **project** you created earlier\\n 3. Click on **Pipelines**\\n 4. Click on the **Create Pipeline**\\n 5. Select **Azure Repos Git (YAML)**\\n 6. **Select** your DNSAsCode **repository**\\n 7. Select **Starter pipeline**\\n 8. **Overwrite** the contents of the starter **pipeline** with the YAML file supplied\\n 9. ![Azure DevOps - YAML](/uploads/azuredevops-newpipeline.png \\"Azure DevOps - YAML\\")\\n10. Click on the arrow next to Save and Run and select **Save**\\n11. Select **Commit directly to the main branch**\\n12. Click **Save**\\n13. You may get an error about the trigger. You can ignore it - we will need to set the variables and trigger now.\\n14. Click on **Pipelines**, select your newly created pipeline\\n15. Select **Edit**\\n16. Click **Variables**\\n17. Click on **New Variable**\\n18. We need to **add** four **variables**. To make the deployment more environment-specific, add the following variables into Azure DevOps _(these variables will be accessible by this pipeline only)._\\n\\n| Variable | Note |\\n| --- | --- |\\n| location | Location where you want to deploy the Resource into \u2013 i.e. \u2018Australia East\u2019 |\\n| PrimaryDNSZone | The name of your domain you want the public zone to be, i.e. badasscloud.com |\\n| ResourceGroupName | The name of the Resource Group that the DNS Zone resource will be deployed into, i.e. DNS-PRD-RG |\\n| SPN | The name of the Service Connection, that we created earlier to connect Azure DevOps to Azure, i.e., SPN.AzureDNSCode |\\n\\n 1. ![Azure DevOps Variables](/uploads/azuredevops-variables.png \\"Azure DevOps Variables\\")\\n 2. Click **Save**\\n\\n    #### Test & final approval of Azure DevOps Pipeline\\n\\n    Now that the Azure Pipeline has been created and variables set, it\'s time to test, warning **this will run an actual deployment to your Azure subscription**!\\n\\n    We will deploy a once-off to grant the pipeline access to the service principal created earlier and verify that it works.\\n 3. [**Sign in to Azure DevOps**](https://go.microsoft.com/fwlink/?LinkId=2014676&githubsi=true&clcid=0x409&WebUserId=e3e298aac5104b0e8e949b3b5bbeb314)\\n 4. Navigate to the DNS As Code **project** you created earlier\\n 5. Click on **Pipelines**\\n 6. Click on your **Pipeline**\\n 7. Select **Run pipeline**\\n 8. Click **Run**\\n 9. **Click** on A**gent job 1**\\n10. You will see a message: **This pipeline needs permission to access a resource before this run can continue**\\n11. Click **View**\\n12. ![Azure DevOps - badasscloud.com DNS deployment](/uploads/azuredevops-spn-approval.png \\"Azure DevOps - badasscloud.com DNS deployment\\")\\n13. Click **Permit**\\n14. Click **Permit** again, to authorise your SPN access to your pipeline for all future runs\\n15. Your **pipeline** will be added to the **queue** and once an agent becomes available will start to **run**.\\n\\nAs seen below, there were no resources before my deployment and the Azure Pipeline agent kicked off and created the resources in the Azure portal.\\n\\n_Note: You can expand the Agent Job to see the steps of the job, I hid it as it revealed subscription ID information etc during the deployment._\\n\\n![Azure DevOps - badasscloud.com DNS deployment](/uploads/azure-devopsdeployment-azurebicep.gif \\"Azure DevOps - badasscloud.com DNS deployment\\")\\n\\nRemember to update your nameserver records for your domain to point towards the nameserver entries in the Azure DNS zone resource, to use Azure DNS!\\n\\n#### Edit the Bicep file\\n\\nNow that you have successfully deployed your Azure Bicep file, you can go into the Azure Bicep and update the A, CNAME records to match your own environment - any new change to this repository will automatically trigger Continous Integration and deployment, you can override this behaviour by editing the Pipeline, clicking Edit Trigger and unselect \'Enable; continuous integration\\n\\nEach variable _(var object) (cnames, arecords)_ is enclosed in brackets, this array allows you to add multiple records, for example, if I wanted to add another name record, it would look like this:\\n\\n    //Variable array for your CNAME records. Add, remove and amend as needed, any new record needs to be included in {}.\\n    var cnamerecords = [\\n     {\\n     name: \'blog\'\\n     value: \'luke.geek.nz\'\\n     }\\n      {\\n     name: \'fancierblog\'\\n     value: \'azure.com\'\\n     }\\n    ]\\n\\nSimply add another object under the first, as long as it is included in the brackets, then upon deployment Azure Bicep will parse the variable array and for each record, create/modify the DNS records, you only ever need to edit the content in the variable without touching the actual resource deployment.\\n\\nAs records are added and removed over time, you will develop a commit history and with the power of Azure DevOps, can implement scheduling changes at certain times and approval!\\n\\nHopefully, this article helps you achieve Infrastructure as Code for your Azure DNS resource, the same concept can be applied for other resources using Azure Bicep as well."},{"id":"azure/controlled-chaos-in-azure-using-chaos-studio","metadata":{"permalink":"/azure/controlled-chaos-in-azure-using-chaos-studio","source":"@site/blog/2022-01-06-controlled-chaos-in-azure-using-chaos-studio.md","title":"Controlled Chaos in Azure using Chaos Studio","description":"Chaos engineering has been around for a while; Netflix runs their own famous Chaos Monkey, supposedly running 24/7, taking down their resources and pushing them to the limit continuously; it almost sounds counter-intuitive \u2013 but it\'s not.","date":"2022-01-06T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":12.645,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-06 00:00:00 +1300","title":"Controlled Chaos in Azure using Chaos Studio","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/chaosengineering-banner.png"},"slug":"azure/controlled-chaos-in-azure-using-chaos-studio"},"unlisted":false,"prevItem":{"title":"Azure Public DNS as Code","permalink":"/azure/azure-public-dns-as-code"},"nextItem":{"title":"Microsoft Entra ID Application Proxy Implementation","permalink":"/azure/azure-active-directory-application-proxy-implementation"}},"content":"Chaos engineering has been around for a while; Netflix runs their own famous [Chaos Monkey](https://netflix.github.io/chaosmonkey/), supposedly running 24/7, taking down their resources and pushing them to the limit continuously; it almost sounds counter-intuitive \u2013 but it\'s not.\\n\\n> Chaos engineering is defined as \u201cthe discipline of experimenting on a system in order to build confidence in the system\u2019s capability to withstand turbulent conditions in production\u201d (Principles of Chaos Engineering, [http://principlesofchaos.org/](http://principlesofchaos.org/ \\"http://principlesofchaos.org/\\")). In other words, it\u2019s a software testing method focusing on finding evidence of problems before they are experienced by users.\\n>\\n> Chaos engineering is a methodology that helps developers attain consistent reliability by hardening services against failures in production. Another way to think about chaos engineering is that it\'s about embracing the inherent chaos in complex systems and, through experimentation, growing confidence in your solution\'s ability to handle it.\\n>\\n> A common way to introduce chaos is to deliberately inject faults that cause system components to fail. The goal is to observe, monitor, respond to, and improve your system\'s reliability under adverse circumstances. For example, taking dependencies offline (stopping API apps, shutting down VMs, etc.), restricting access (enabling firewall rules, changing connection strings, etc.), or forcing failover (database level, Front Door, etc.), is a good way to validate that the application is able to handle faults gracefully.\\n\\nIntroducing controlled Chaos tools such as Chaos Monkey and now \u2013 [Azure Chaos Studio](https://azure.microsoft.com/en-us/services/chaos-studio/?WT.mc_id=AZ-MVP-5004796) allows you to put pressure and, in some cases, take down your services to teach you how your services will react under strain and identity areas of improvement as resiliency and scalability to improve your systems.\\n\\n![Chaos](/images/chaosengineering-banner.png \\"Chaos\\")\\n\\nAzure Chaos Studio _(currently in Preview and only supported in several_ [_regions_](https://azure.microsoft.com/en-us/global-infrastructure/services/?products=chaos-studio&WT.mc_id=AZ-MVP-5004796)_now)_ is an enabler for \'controlled Chaos\' in the Microsoft Azure ecosystem. Using that same tool that Microsoft uses to test and improve their services \u2013 you can as well!\\n\\nChaos Studio works by creating Experiments _(i.e., Faults/Capabilities)_ that run against Targets _(your resources, whether they are agent or service-based)_.\\n\\nThere are two types of methods you can use to target your resources:\\n\\n* Service-direct\\n* Agent-based\\n\\nService-direct is tied into the Azure fabric and puts pressure on your resources from outside them _(i.e., supported on most resources that don\'t need agent-based, PaaS resources, such as Network Security Groups)._ For example, a service-direct capability may be to add or remove a security rule from your network security group for faulty findings.\\n\\nAgent-based relies on an agent installed; these are targeted at resources such as Virtual Machine and Virtual Machine scale sets; agent-based targets use a user-assigned managed identity to manage an agent on your virtual machines and wreak havoc by running capabilities such as stopping services and putting memory and disk pressure on your workloads.\\n\\nJust a word of warning, before you proceed to allow Chaos to reign in your environment, make sure it is done out of hours or, better yet \u2013 against development or test resources, also make sure that any resources that support autoscaling are disabled \u2013 or you might suddenly find ten more instances of that resource you were running _(unless of course you\'re testing that autoscaling is working)_! \ud83d\ude0a\\n\\nIn my test setup, I have the following already pre-created that I will be running my experiments against:\\n\\n* Virtual Machine Scale set _(running Windows with two instances)_\\n* Single Virtual Machine _(running Windows)_ to test shutdown against\\n\\nThe currently supported resource types of Azure Chaos Studio can be found \'[here](https://learn.microsoft.com/en-us/azure/chaos-studio/chaos-studio-fault-providers?WT.mc_id=AZ-MVP-5004796)\'.\\n\\n### Setup Azure Chaos Studio\\n\\n#### Create Managed Identity\\n\\nBecause we will use Agent-based capabilities to generate our Faults, I needed to create a Managed Identity to give Chaos Studio the ability to wreak havoc on my resources!\\n\\n 1. In the **Azure Portal,** search for [**Managed Identities**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.ManagedIdentity%2FuserAssignedIdentities)\\n 2. Click on **Create**\\n 3. **Select** the **subscrSubscription**ng the resources that you want to test against\\n 4. **Select** your **Resource Group** to place the managed identity in (_I suggest creating a new Resource Group, as your Chaos experiments may have a different lifecycle than your resources, but it\'s just a preference, I will be placing mine in the Chaos Studio resource group so I can quickly delete it later)_.\\n 5. **Select** the **RegionRegion**ur resources\\n 6. Type in a **name** (_this will be the identity that you will see in logs running these experiments, so make sure its something you can identify with)_\\n 7. ![Azure Portal - Create User Management Identity](/uploads/azure_userassignedmanageidentity.png \\"Azure Portal - Create User Management Identity\\")\\n 8. Click **Next: Tags**\\n 9. Make sure you enter appropriate tags to make sure that the resource can be identified and tracked, and click **Review + Create**\\n10. ![Azure Portal Tags](/uploads/azuretags_chaos.png \\"Azure Portal Tags\\")\\n11. Verify that everything looks good and click **Create** to create your User Assigned Managed identity.\\n\\n#### Create Application Insights\\n\\nNow, it\'s time to create an Application Insights resource. Applications Insights is for the logs of the experiments to go into, so you can see the faults and their behaviours.\\n\\n 1. In the **Azure Portal**, search for [**Application Insights**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/microsoft.insights%2Fcomponents)\\n 2. Click on **Create**\\n 3. **Select** the **Subscription** the resources that you want to test against\\n 4. **Select** your **Resource Group** to place the Application Insights resource into (_I suggest creating a new Resource Group, as your Chaos experiments may have a different lifecycle than your resources, but it\'s just a preference, I will be placing mine in the Chaos Studio resource group so I can easily delete it later)_.\\n 5. **Select** the **Region** the resources are in\\n 6. Type in a **name**\\n 7. **Select** your **Log Analytics workspace** you want to link Application Insights to _(if you don\'t have a Log Analytics workspace, you can create one \'_[_here_](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.OperationalInsights%2Fworkspaces)_\')_.\\n 8. ![Azure Portal - Application Insights](/uploads/azure_applicationinsights.png \\"Azure Portal - Application Insightsv\\")\\n 9. Click **Tags**\\n10. Make sure you **enter appropriate tags** to make sure that the resource can be identified and tracked, and click **Review + Create**\\n11. Verify that everything looks good and click **Create** to create your Application Insights.\\n\\n#### Setup Chaos Studio Targets\\n\\nIt is now time to add the resources targets to Chaos Studio\\n\\n 1. In the **Azure Portal**, search for [**Chaos Studio**](https://portal.azure.com/#blade/Microsoft_Azure_Chaos/ChaosStudioMenuBlade/overview)\\n 2. On the left band side Blade, select **Targets**\\n 3. ![Azure Chaos Studio](/uploads/azure_chaosstudio_targets.png \\"Azure Chaos Studio\\")\\n 4. As you can see, I have a Virtual Machine Scale Set and a front-end Network Security Group.\\n 5. **Select** the **checkbox** next to Name to **select all** the Resources\\n 6. Select **Enable Targets**\\n 7. ![Azure Chaos Studio](/uploads/azure_chaosstudio_targets2.png \\"Azure Chaos Studio\\")\\n 8. Select **Enable service-direct targets (All resources)**\\n 9. Enabling the service-direct targets will then add the capabilities supported by Service-direct targets into Chaos Studio for you to use.\\n10. Once completed, I will **select** the scale set and click **Enable Target**\\n11. Then finally, **Enable agent-based targets (VM, VMSS)**\\n12. This is where you link the user-managed identity, and Application Insights created earlier\\n13. **Select** your **Subscription**\\n14. Select your **managed identity**\\n15. Select Enabled for Application Insights and select your Application Insights account. The instrumentation key should be selected manually.\\n16. ![Azure Chaos Studio - Enable targets](/uploads/azure_chaosstudio_enableagenttargets.png \\"Azure Chaos Studio - Enable targets\\")\\n17. _If your instrumentation key isn\'t filled in, you can find it on the Overview pane of the Application Insights resource._\\n18. Click **Review + Enable**\\n19. **Review** the **resources** you want to enable Chaos Studio to target and select **Enable**\\n20. Finally, you should now be back at the Targets pane make sure you select Manage actions and make sure that all actions are ticked and click **Save**\\n21. ![Azure Chaos Studio Capabilities](/uploads/azurechaosactions.png \\"Azure Chaos Studio Capabilities\\")\\n\\n### Configure and run Azure Chaos Studio\\n\\n#### Action exclusions\\n\\nThere may be actions that you don\'t want to be run against specific resources; an example might be you don\'t want anyone to kill any processes on a Virtual Machine.\\n\\n1. In the Target pane of Chaos Studio, select **Actions** next to the resource\\n2. **Unselect** the **capability** you don\'t want to run on that resource\\n3. Select **Save**\\n4. ![Azure Chaos Studio Actions](/uploads/azure_chaosstudio_manageactions.png \\"Azure Chaos Studio - Enable targets\\")\\n\\n#### Configure Experiments\\n\\nAn experiment is a collection of capabilities to create faults, put pressure on your resources, and cause Chaos that will run against your target resources. These experiments are saved so you can run them multiple times and edit them later, although currently, you cannot reassign the same experiments to other resources.\\n\\nNote: If you name an Experiment the same as another experiment, it will replace the older Experiment with your new one and retain the previous history.\\n\\n 1. In the **Azure Portal,** search for [**Chaos Studio**](https://portal.azure.com/#blade/Microsoft_Azure_Chaos/ChaosStudioMenuBlade/overview).\\n 2. On the left band side Blade, select **Experiments**\\n 3. Click **+ Create**\\n 4. **Select** your **Subscription**\\n 5. **Select** your **Resource Group** to save the Experiment into\\n 6. **Type** in a **name** for your **Experiment** that makes sense; in this case, we will put some Memory pressure on the VM scale set.\\n 7. **Select** your **Region**\\n 8. Click **Next: Experiment Designer**\\n 9. **Using** Experiment **Designer**, you can **design** your **Faults**; you can have multiple capabilities hit a resource with expected delays, _i.e., you can have Memory pressure on a VM for 10 minutes, then CPU pressure, then shutdown._\\n10. We are going to select **Add Action**\\n11. Then **Add Fault**\\n12. I am going to select **Physical Memory** pressure\\n13. Leave the duration to **10 minutes**\\n14. Because this will go against my VM scale set, I will add in the instances I want to target _(if you aren\'t targeting a VM Scale set, you can leave this blank, you can find the instance ID by going to your VM Scale set click on Instances, click on the VM instance you want to target and you should see the Instance ID in the Overview pane)_\\n15. ![Azure Chaos Studio - Add fault](/uploads/azure_chaosstudio_createexperimentaddfault.png \\"Azure Chaos Studio - Add fault\\")\\n16. Select **Next: Target resources**\\n17. **Select** your **resources** _(you will notice as this is an Agent-based capability, only agent supported resources are listed)_\\n18. Select **Add**\\n19. I am then going to **Add delay for 5 Minutes**\\n20. Then add an **abrupt VM shutdown** for 10 minutes _(Chaos Studio will automatically restart the VM after the 10-minute duration)_.\\n21. ![Azure Chaos Studio create experiment](/uploads/azure_chaosstudio_createexperimentaddfault2.png \\"Azure Chaos Studio create experiment\\")\\n22. As you can see with the Branches _(items that will run in parallel)_ and actions, you can have multiple faults running at once in parallel by using branches or one after the other sequentially.\\n23. Now that we are ready with our faulty, we are going to click **Review + Create**\\n24. Click **Create**\\n\\n_Note: I had an API error; after some investigation, I found it was having problems with the \'?\' in my experiment name, so I removed it and continued to create the Experiment._\\n\\n#### Assign permissions for the Experiments\\n\\nNow that the Experiment has been created, we need to give rights to the Managed User account created earlier _(and/or the System managed identity that was created when the Experiment was created for service-direct experiments)_.\\n\\nI will assign permissions to the Resource Group that the VM Scale set exists in, but you might be better off applying the rights to the individual resource for more granular control. You can see suggested roles to give resources: [Supported resource types and role assignments for the Chaos Studio](https://learn.microsoft.com/en-us/azure/chaos-studio/chaos-studio-fault-providers?WT.mc_id=AZ-MVP-5004796) Microsoft page.\\n\\n 1. In the **Azure Portal**, click on the **Resource Group** containing the resources you want to run the Experiment against\\n 2. Select **Access control (IAM)**\\n 3. Click **+ Add**\\n 4. Click **Add Role Assignment**\\n 5. Click **Reader**\\n 6. Click **Next**\\n 7. Select **Assign access to Managed identity**\\n 8. Click on **+ Select Members**\\n 9. **Select** the **User** assigned **management identity**\\n10. Click **Review** and **assign**.\\n11. Because the shutdown is a service-direct, **go back and give the experiment system managed identity Virtual Machine Contributor rights**, so it has access to shutdown the VM.\\n\\n#### Run Experiments\\n\\nNow that the Experiment has been created, it should appear as a resource in the resource group you selected earlier; if you open it, you can see the Experiment\'s History, Start, and Edit buttons.\\n\\n1. Click **Start**\\n2. ![Azure Chaos studio - Run experiment](/uploads/azure_chaosstudio_whatmemory.png \\"Azure Chaos studio - Run experiment\\")\\n3. Click **Ok** to **start** the **Experiment** _(and place it into the queue)_\\n4. **Click** on **Details** to see the **experiment progress** _(and any errors)_, and if it fails one part, it may move to the next step depending on the fault.\\n5. ![Azure Chaos studio - Run experiment](/uploads/azure_chaosstudio_whatmemoryrun.png \\"Azure Chaos studio - Run experiment\\")\\n6. **Azure Chaos studio should now run rampant and do best \u2013 cause Chaos**!\\n\\nThis service is still currently in Preview. If you have any issues, take a look at the: [Troubleshoot issues with Azure Chaos Studio](https://learn.microsoft.com/en-us/azure/chaos-studio/troubleshooting).\\n\\n### Monitor and Auditing of Azure Chaos Studio\\n\\nNow that Azure Chaos Studio is in use by your organization, you may want to know what auditing is available, along with reporting to Application Insights.\\n\\n#### Azure Activity Log\\n\\nWhen an Azure Chaos Studio experiment has touched a resource, there will be an audit trail in the Azure activity log of that resource; here, you can see that \'WhatMemory\', which is the Name of my Chaos Experiment, has successfully powered off and on my VM.\\n\\n![Azure Activity Log - Azure Chaos Studio](/uploads/azure_chaosstudio_activitylog.png \\"Azure Activity Log - Azure Chaos Studio\\")\\n\\n#### Azure Alerts\\n\\nIt is easy to set up alerts when a Chaos experiment kicks off; to create an Azure, do the following.\\n\\n 1. In the **Azure Portal**, click on **Azure Monito**r\\n 2. Click on **Alerts**\\n 3. Click **+ Create**\\n 4. Select **Alert Rule**\\n 5. Click **Create resource**\\n 6. **Filter** your resource **type** to **Chaos Experiments**\\n 7. **Filter** your **alert** to **Subscription** and click **Done**\\n 8. Click **Add Condition**\\n 9. Select: **Starts a Chaos Experiment**\\n10. Make sure that: *_Event initiated by is set to (All services and users)_\\n11. Click **Done**\\n12. Click **Add Action Group**\\n13. If you have one, **assign** an **action group** _(these are who and how the alerts will get to you)_. If you don\'t have one, click: **+ Create an action group**.\\n14. **Specify** a resource **group** to hold your action groups _(usually a monitor or management resource group)_\\n15. Type the **Action Group name**\\n16. Type the **Action group Display name**\\n17. Click **Next: Notifications**\\n18. Select **Notification Type**\\n19. Select **email**\\n20. Select **Email**\\n21. **Type** in your **email** address to be notified\\n22. Click **ok**\\n23. **Type** in the **Name** of the **mail** to be a reference in the future _(i.e. Help Desk)_\\n24. Click **Review + Create**\\n25. Click **Create** to create your Action group\\n26. Type in your **rule name** _(i.e. Alert \u2013 Chaos Experiment \u2013 Started)_\\n27. Type in a **description**\\n28. **Specify** the **resource group** to place the alert in _(again, usually a monitor or management resource group)_\\n29. Check **Enable alert rule on creation**\\n30. Click **Create alert rule**\\n\\n_Note: Activity Log alerts are hidden types; they are not shown in the resource group by default, but if you check the: Show hidden types box, they will appear._\\n\\n![Azure Activity Log - Azure Chaos Studio](/uploads/azure_chaosstudio_alert.png \\"Azure Activity Log - Azure Chaos Studio\\")"},{"id":"azure/azure-active-directory-application-proxy-implementation","metadata":{"permalink":"/azure/azure-active-directory-application-proxy-implementation","source":"@site/blog/2022-01-03-azure-active-directory-application-proxy-implementation.md","title":"Microsoft Entra ID Application Proxy Implementation","description":"Are you running internal web-based applications that you want to give access to users working remotely securely without the need for a VPN or firewall? Do you want to enforce or use Azure Conditional Access policies to protect and manage access?","date":"2022-01-02T11:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":10.98,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2022-01-02T11:00:00.000Z","title":"Microsoft Entra ID Application Proxy Implementation","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"uploads/aadproxynetworkdiagram.png"},"slug":"azure/azure-active-directory-application-proxy-implementation"},"unlisted":false,"prevItem":{"title":"Controlled Chaos in Azure using Chaos Studio","permalink":"/azure/controlled-chaos-in-azure-using-chaos-studio"},"nextItem":{"title":"Git using Github Desktop on Windows for SysAdmins","permalink":"/2021/12/30/git-using-github-desktop-on-windows-for-sysadmins"}},"content":"Are you running internal web-based applications that you want to give access to users working remotely securely without the need for a VPN or firewall? Do you want to enforce or use Azure Conditional Access policies to protect and manage access?\\n\\nLet me introduce the Microsoft Microsoft Entra ID Application Proxy...\\n\\n> Application Proxy is a feature of Azure AD that enables users to access on-premises web applications from a remote client. Application Proxy includes both the Application Proxy service which runs in the cloud, and the Application Proxy connector, which runs on an on-premises server. Azure AD, the Application Proxy service, and the Application Proxy connector work together to securely pass the user sign-on token from Azure AD to the web application. Application Proxy also supports single sign-on.\\n>\\n> Application Proxy is recommended for giving remote users access to internal resources. Application Proxy replaces the need for a VPN or reverse proxy.\\n\\n## Overview\\n\\nThe [Microsoft Entra ID Application Proxy](https://learn.microsoft.com/en-us/azure/active-directory/app-proxy/what-is-application-proxy?WT.mc_id=AZ-MVP-5004796)has been around for a few years, but appears to be a hidden gem; the Application Proxy allows users_(by using Microsoft Entra ID and an Application Proxy Connector(s))_ to connect to internally hosted web applications, by the connector relaying the traffic.\\n\\n![Azure Application Proxy - Network Diagram](/uploads/aadproxynetworkdiagram.png \\"Azure Application Proxy - Network Diagram\\")\\n\\nApplication Proxy supports the following types of applications:\\n\\n* Web applications\\n* Web APIs that you want to expose to rich applications on different devices\\n* Applications hosted behind a Remote Desktop Gateway\\n* Rich client apps that are integrated with the Microsoft Authentication Library (MSAL)\\n\\nAzure Application Proxy can often be overlooked to solve your business requirements without the need to implement costly third-party firewalls _(it also doesn\'t have to be an on-premises workload, for example, if the web application is running on a VM in Azure, it will also work)_.\\n\\nThe Azure Application proxy connector is a lightweight agent installed on a Windows Server machine that is logically close to the backend service that you want to deliver through the proxy. \\n\\nThe Connector gives access to and relays the information to the Application proxy service in Microsoft Azure via HTTP/HTTPS as long as it has access to the following:\\n\\n| URL | Port | How it\'s used |\\n| --- | --- | --- |\\n| *.msappproxy.net  *.servicebus.windows.net | 443/HTTPS | Communication between the connector and the Application Proxy cloud service |\\n| crl3.digicert.com  crl4.digicert.com  ocsp.digicert.com  crl.microsoft.com  oneocsp.microsoft.com  ocsp.msocsp.com | 80/HTTP | The connector uses these URLs to verify certificates. |\\n| login.windows.net  secure.aadcdn.microsoftonline-p.com  *.microsoftonline.com  *.microsoftonline-p.com  *.msauth.net  *.msauthimages.net  *.msecnd.net  *.msftauth.net  *.msftauthimages.net  *.phonefactor.net  enterpriseregistration.windows.net  management.azure.com  policykeyservice.dc.ad.msft.net  ctldl.windowsupdate.com  www.microsoft.com/pkiops | 443/HTTPS | The connector uses these URLs during the registration process. |\\n| ctldl.windowsupdate.com | 80/HTTP | The connector uses this URL during the registration process. |\\n\\n## Setup Azure Application Proxy\\n\\nI will set up an Azure Application Proxy to grant access to my Synology NAS _(Network Attached Storage)_ device web page in this guide. \\n\\nAlthough I am using my local NAS web administration page, it can be any webpage _(Unifi Controller, hosted on Apache, IIS etc.)_ accessible from the connector.\\n\\n* I have a Windows Server 2022 Domain Controller.\\n* Synology NAS _(not domain joined, but accessible on the network via a DNS record from the domain)_\\n* Microsoft 365 Developer subscription with appropriate licenses\\n\\n### Pre-requisites for Azure Application Proxy setup\\n\\nThe following resources and rights will be needed to set up Azure Application Proxy:\\n\\n* An Microsoft Entra ID tenant\\n* A minimum of Application Administrator rights is required to set up the Application and user and group assignments.\\n* A server running Windows Server 2012 R2 or above to install the Application Proxy connector on (and the permissions to install)\\n* If you are using a third-party domain _(you will need a public SSL certificate)_ and, of course, the ability to edit external DNS records, the domain will need to be added to Microsoft Entra ID as a custom domain in order to be used.\\n* Microsoft Entra ID Premium P1 license or M365 Business Premium/E3 license for each user using Microsoft Entra ID Application Proxy.\\n\\n![Microsoft Entra ID Application Proxy Licensing](/uploads/aadproxylicensing.png \\"Microsoft Entra ID Application Proxy Licensing\\")\\n\\n_(Note: Normal [Azure AD service limits](https://learn.microsoft.com/en-us/azure/active-directory/enterprise-users/directory-service-limits-restrictions?WT.mc_id=AZ-MVP-5004796) and restrictions apply)_.\\n\\nI will be configuring the Azure Application Proxy on a domain controller running Windows Server 2022.\\n\\n### Disable IE Enhanced Security Configuration\\n\\nThe Azure Application Proxy connector requires you to log in to Microsoft Azure, and I will be installing this on a Windows Server 2022 domain controller; if this Enhanced Security Configuration is enabled _(as it should be),_ you will have problems authenticating to Microsoft Azure, so the easiest thing is to turn it off temporarily.\\n\\n1. Open **Server Manager**\\n2. Click on **Local Server**\\n3. Click on: **IE Enhanced Security Configuration**\\n4. Select Off for: **Administrators**\\n5. **Close** Microsoft **Edge** _(if you have it opened)_\\n6. ![Disable IE Enhanced Security Configuration](/uploads/disable_ie_enhancedconfiguration.png \\"Disable IE Enhanced Security Configuration\\")\\n\\n### Install Azure Application Proxy Connector\\n\\n 1. Login to **Azure Portal** _(on the server that you want to install the Connector on)_\\n 2. Navigate to: [**Microsoft Entra ID**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview)\\n 3. Select **Application Proxy**\\n 4. ![Azure Portal - Application Proxy](/uploads/azureportal-applicationproxy.png \\"Azure Portal - Application Proxy\\")\\n 5. Click on: **Download connector service**.\\n 6. Accept the system requirements and click **Accept Terms & Download**\\n 7. A file named: \'AADApplicationProxyConnectorInstaller.exe\' should have been downloaded. **Run** it.\\n 8. Select: **I agree to the license terms and conditions** and select **Install**\\n 9. ![Microsoft Microsoft Entra ID Application Proxy Connector Installation](/uploads/microsoftazureapplicationproxyconnector.png \\"Microsoft Microsoft Entra ID Application Proxy Connector Installation\\")\\n10. Wait for the Microsoft Microsoft Entra ID Application to display and **log in** with an Microsoft Entra ID account with Application Administrator rights.\\n11. The Microsoft Microsoft Entra ID Application **Connector will now** be **registered** in your Microsoft Entra ID tenancy.\\n12. ![Microsoft Microsoft Entra ID Application Proxy Connector Installation](/uploads/microsoftazureapplicationproxyconnectorinstalled.png \\"Microsoft Microsoft Entra ID Application Proxy Connector Installation\\")\\n13. Click **Close**\\n14. Now **re-enable IE enhanced security configuration**.\\n\\nYou should now see two new services appear in services as Automatic (Delayed Start):\\n\\n* WAPCsvc - Microsoft AAD Application Proxy Connector\\n* WAPCUpdaterSvc - Microsoft AAD Application Proxy Connector Updater\\n\\nAnd the following processes running:\\n\\n* ApplicationProxyConnectorService\\n* ApplicationProxyConnectorUpdateService\\n\\n![ApplicationProxyConnectorService](/uploads/azureaadapplicationservices.png \\"ApplicationProxyConnectorService\\")\\n\\nIf you are running Server Core, Microsoft Microsoft Entra ID Application Proxy can be installed via [PowerShell](https://learn.microsoft.com/en-us/azure/active-directory/app-proxy/application-proxy-register-connector-powershell?WT.mc_id=AZ-MVP-5004796).\\n\\nThe Azure Application Proxy Connector agent gets [updated](https://learn.microsoft.com/en-us/azure/active-directory/app-proxy/application-proxy-faq?WT.mc_id=AZ-MVP-5004796#why-is-my-connector-still-using-an-older-version-and-not-auto-upgraded-to-latest-version-) automatically when a new major version is released by Microsoft.\\n\\n### Configure Connector Group\\n\\nNow that you have created the Connector, the Application Proxy has put our Connector in a group that has defaulted to Asia; because you can have more than one Application Proxy Connector for redundancy and different applications, we will create a new Connector Group that is set to use the Australia region if Asia works for you \u2013 feel free to skip this step.\\n\\n 1. Login to **Azure Portal** _(on any PC/server)_\\n 2. Navigate to: [**Microsoft Entra ID**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview)\\n 3. Select **Application Proxy**\\n 4. You should now see: Default and your Region\\n 5. If you **expand** the Default **Group**, will you see your Connector:\\n 6. ![Azure AD Application Proxy Connector Groups](/uploads/azureportal-applicationproxyconnector.png \\"Azure AD Application Proxy Connector Groups\\")\\n 7. Click on **+ New Connector Group**\\n 8. Give it a **name** _(i.e., On-premises_)\\n 9. **Select** the **Connector** you had earlier and **select the** region closest to you _(currently, the following regions can be chosen: Asia, Australia, Europe, North America)_\\n10. ![Azure AD Application Proxy - New Connector Group](/uploads/azureportal-applicationproxynewconnectorgroup.png \\"Azure AD Application Proxy - New Connector Group\\")\\n11. **Click + Create**\\n12. Clicking create will create your new On-premises connector group and add the Connector to the group.\\n\\n### Configure your Azure Application Proxy Application\\n\\nNow that you have your Connector setup, its time to set up your application\\n\\n1. Login to **Azure Portal** _(on any PC/server)_\\n2. Navigate to: [**Microsoft Entra ID**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview)\\n3. Select **Application Proxy**\\n4. Click on: **+ Configure an app**\\n5. **Fill** in the **details** that match your **application**:\\n\\n* **Name**: This is the application that users will see _(i.e. I am going with Pizza, which is the name of my NAS)_\\n* **Internal URL**: This is the internal URL used to access your application inside the network _(in my example, it is:_ [_http://pizza.corp.contoso.com/_](http://pizza.corp.contoso.com/)_)_\\n* **External Url**: This is the external URL that will be created so that users can access the application form; _I will go with Pizza._ Note this URL down.\\n* **Pre-Authentication**: You don\'t have to authenticate with Azure AD, you can use passthrough, but it is not something I would recommend without delving into requirements, testing _\u2013 I am going to select: Microsoft Entra ID._\\n* **Connector Group**: Select the connector group you created earlier or that your Connector is signed to.\\n* **Leave** all **Additional Settings as default** \u2013 they can be changed later if you need to.\\n  1. ![Azure Application Proxy](/uploads/azureportal-applicationproxynewapplication.png)\\n  2. **Verify** that **everything** is filled out **correctly** and, click **+ Add**\\n  3. Azure **Application Proxy has now created a new Enterprise Application for you**; based on the name mentioned earlier, if you navigate to the external URL mentioned earlier, you should get a prompt similar to below:\\n  4. ![Azure AD Login Error](/uploads/azureportal-pizzaloginerror.png \\"Azure AD Login Error\\")\\n  5. It is now time to assign the permissions for users to access the Application via Microsoft Entra ID!\\n\\n### Assign rights to your Azure Application Proxy Application\\n\\n 1. Login to **Azure Portal** _(on any PC/server)_\\n 2. Navigate to: [**Microsoft Entra ID**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview)\\n 3. Select [**Enterprise Applications**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/StartboardApplicationsMenuBlade/AllApps/menuId/)\\n 4. **Find** the **application** that was **created** earlier by the Azure Application Proxy service.\\n 5. ![Microsoft Entra ID, Enterprise Application](/uploads/azureportal-enterpriseapplicationspane.png \\"Microsoft Entra ID, Enterprise Application\\")\\n 6. Click on the **Application**\\n 7. Click on: **Users and Groups**\\n 8. Click **Add Assignment**\\n 9. **Add** a **user** or **group** _(preferred)_ you want to have access to this application.\\n10. Click **Assigned**\\n11. ![Azure AD Enterprise Applications - User & Group Assignment](/uploads/azureportal-enterpriseapplicationsuserandgrpassignment.png \\"Azure AD Enterprise Applications - User & Group Assignment\\")\\n12. Click on **Application Proxy**\\n13. Here you can see and edit the information you created earlier when you created the application, **copy** the **External URL**\\n14. **Open** Microsoft **Edge** (_or another browser of your choice)_\\n15. **Paste** in the External **URL**\\n16. **Log in** with the Microsoft Entra ID **account** that was **assigned** to the Enterprise **application.**\\n17. You should now have access to your on-premises web application from anywhere in the world, and because you are using Microsoft Entra ID, your conditional access policies and restrictions will be in effect:\\n18. ![Synology Login](/uploads/microsoftazureapplicationproxyloginscreen.png \\"Synology Login\\")\\n\\n_Note: Because the Synology web interface was running on port: 5000, I had to go back and add the port to the internal URL, as the Application Proxy was attempting to route to the incorrect port._\\n_Note: You may also notice that Microsoft has supplied an *.msappproxy.net certificate, even if your backend service doesn\'t have one.._\\n\\n### Setup Password-based Single-Sign on\\n\\nAzure Application Proxy supports various [single](https://learn.microsoft.com/en-us/azure/active-directory/app-proxy/application-proxy-config-sso-how-to?WT.mc_id=AZ-MVP-5004796) sign-on methods, including Kerberos SPN integration. \\n\\nHowever, my Synology NAS uses standalone accounts, so I will set Password-based single sign-on, allowing the MyApps extension to store my credentials _(if you want single-sign-on using the password-based sign in, then every user will need to have this extension configured)_.\\n\\n 1. Download and install the [**MyApps Secure Sign-in extension**](https://microsoftedge.microsoft.com/addons/detail/my-apps-secure-signin-ex/gaaceiggkkiffbfdpmfapegoiohkiipl#:\\\\~:text=My%20Apps%20Secure%20Sign-in%20Extension.%20This%20extension%20is,to%20cloud%20applications%20within%20your%20organization%20or%20school.)\\n 2. **Log** in using your Microsoft account to the MyApps **extension**\\n 3. ![Azure App Proxy](/uploads/myappsextensionlogo.png)\\n 4. **Login** to **Azure Portal** (_on any PC/server)_\\n 5. Navigate to: [**Microsoft Entra ID**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview)\\n 6. Select [**Enterprise Applications**](https://portal.azure.com/#blade/Microsoft_AAD_IAM/StartboardApplicationsMenuBlade/AllApps/menuId/)\\n 7. **Find** the **application** that was created earlier by the Azure Application Proxy service.\\n 8. Click on **Single sign-on**\\n 9. Select **Password-based**\\n10. ![Azure Portal - Single Signon](/uploads/azureportal-appproxysso.png \\"Azure Portal - Single Signon\\")\\n11. **Type** in the **URL** of the **authentication webpage** and click **Save**\\n12. ![Azure App Proxy](/uploads/azureportal-appproxyssourl.png)\\n13. The Azure AD Application Proxy didn\'t find my sign-in login and password fields, so I have to manually configure them, select: **Configure Pizza Password Single Sign-on Settings**.\\n14. Select: **Manually detect sign-in fields**\\n15. Select **Capture sign-in fields**\\n16. ![Azure Application Proxy - Configure Sign-on](/uploads/azureportal-configuresignin.png \\"Azure Application Proxy - Configure Sign-on\\")\\n17. Your MS Edge Extension should show **Capture Field**:\\n18. ![Azure Application Configure Extension](/uploads/azureportal-configuresigninextension.png \\"Azure Application Configure Extension\\")\\n19. **Enter** in your **username**\\n20. Press **Enter**\\n21. **Enter** in your **password**\\n22. **Select** the MS Apps **extension** and select **Save**\\n23. Navigate back to the **Azure Portal**\\n24. Select \'**I was able to sign in.\'**\\n25. If successful, **Azure AD should now have mapped the fields**:\\n26. ![Azure Portal - Signin Fields](/uploads/azureportal-configuresigninextensionfields.png \\"Azure Portal - Signin Fields\\")\\n27. Click **Save**\\n28. Next time you log in to the application, the **My Apps Secure Sign-in Extension will have cached the credentials.** It should automatically log you into the application, meaning you should only log in once with your Azure AD credentials.\\n\\n### Access your Azure Application Proxy published application\\n\\n1. You can now go to [**My Apps (microsoft.com)**](https://myapps.microsoft.com/), and you will **see** your **application**.\\n2. ![M365 Waffle](/uploads/myapps.png)\\n3. Your application will also **appear** in the **Microsoft 365 Waffle** _(it may take up to an hour to appear)_:\\n4. ![M365 Waffle](/uploads/m365waffle_pizza.png)\\n\\nI recommend you go into the Enterprise Application and upload a better image/logo so your users can quickly tell it apart."},{"id":"/2021/12/30/git-using-github-desktop-on-windows-for-sysadmins","metadata":{"permalink":"/2021/12/30/git-using-github-desktop-on-windows-for-sysadmins","source":"@site/blog/2021-12-30-git-using-github-desktop-on-windows-for-sysadmins.md","title":"Git using Github Desktop on Windows for SysAdmins","description":"Git (Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development, allowing versioning, source control and enablement of continuous Integration and deployment)has been around for years(development and the first release began in 2005 by Linus Torvolds).","date":"2021-12-30T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":8.585,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Git using Github Desktop on Windows for SysAdmins","authors":["Luke"],"tags":["Windows"],"date":"2021-12-30 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/githubdesktop-overview.png"}},"unlisted":false,"prevItem":{"title":"Microsoft Entra ID Application Proxy Implementation","permalink":"/azure/azure-active-directory-application-proxy-implementation"},"nextItem":{"title":"Azure Bicep and Insert Resource","permalink":"/azure/azure-bicep-and-insert-resource"}},"content":"[Git](https://en.wikipedia.org/wiki/Git) (_Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development, allowing versioning, source control and enablement of continuous Integration and deployment)_has been around for years_(development and the first release began in 2005 by Linus Torvolds)_.\\n\\nAlthough primary driven and consumed by software developers \u2013 it is now a staple of everyday life for an IT professional of many disciplines _(i.e. Operations, Delivery),_ even if a git repository is used to store your PowerShell scripts _(hint \u2013 it should!)_.\\n\\nYou don\'t have to know every single git command line syntax to use Git.\\n\\nTools such as Visual Studio Code allows you to utilize git source control efficiently, and of course, you can use Git directly from the command line; however, sometimes you want an easy way to leverage Git through a point and click interface, there a lot of tools out there to give you easy access to Git, but today I will concentrate on Github Desktop.\\n\\nIf you are looking at something a bit more powerful _(especially if you are wanting to do submodules)_, then I suggest [Atlassian Sourcetree](https://www.atlassian.com/software/sourcetree \\"Atlassian Sourcetree\\").\\n\\nIntroducing Github Desktop... _\\"Focus on what matters instead of fighting with Git. Whether you\'re new to Git or a seasoned user, GitHub Desktop simplifies your development workflow.\\"_\\n\\n![Github Desktop - Overview](/uploads/githubdesktop-overview.png \\"Github Desktop - Overview\\")\\n\\nGithub Desktop gives you a clean, light and easy to use tool to work with git repositories that is constantly [kept up to date](https://github.com/desktop/desktop \\"Github Desktop - Github\\") and improved upon!\\n\\nAlthough Github Desktop is published by Github \u2013 this doesn\'t mean you cannot use a git repository hosted by another provider, such as Azure DevOps.\\n\\nThis article assumes that you have a Git repository initialized already; you can create free repositories from [Azure DevOps](https://azure.microsoft.com/en-us/services/devops/?nav=min&WT.mc_id=AZ-MVP-5004796 \\"Azure DevOps\\") or [Github](https://github.com/ \\"GitHub\\"). Microsoft owns Azure DevOps and Github; personally, I have moved from Azure DevOps to Github for my git repositories but utilize AzureDevOps pipelines.\\n\\n![Git High level workflow](/images/posts/HLGit_Workflow.png \\"Git High level workflow\\")\\n\\n### Install Github Desktop\\n\\nInstallation of Github Desktop is pretty simple, but assuming you have rights to install the software:\\n\\n1. In your web browser, navigate to [Github Desktop](https://desktop.github.com/) homepage and click on: **Download**\\n2. ![Github Desktop - Download](/uploads/githubdesktop-download.png \\"Github Desktop - Download\\")\\n3. Once it\'s downloaded, you should have a file such as GitHubDesktopSetup-x64.exe _(it should only take a few seconds, the file is about 109 MB at the time this article was written),_ then **run** it to **install**.\\n4. ![Github Desktop - Installing](/uploads/githubdesktop-installing.png \\"Github Desktop - Installing\\")\\n\\nCongratulations, you have now installed Github Desktop!\\n\\n### Add your Azure DevOps repository\\n\\nIf you have an Azure DevOps git repository, then follow the steps below \u2013 if you have chosen to go: Github, then feel free to skip this section for the next.\\n\\n 1. Sign in to [**Azure DevOps**](https://azure.microsoft.com/en-us/services/devops/?nav=min&WT.mc_id=AZ-MVP-5004796 \\"Azure DevOps\\")\\n 2. **Navigate** to the **project** you want to add to Github Desktop\\n 3. Click on **Repos**, **Files**\\n 4. ![Azure DevOps - Repo](/uploads/azuredevops-repos.png \\"Azure DevOps - Repo\\")\\n 5. In the address bar, you will see your **URL**, and it should look like this: [https://dev.azure.com/%username%/_git/%projectname%](https://dev.azure.com/%username%/_git/%projectname% \\"https://dev.azure.com/%username%/_git/%projectname%\\")\\n 6. **Copy** the **URL** and **open Github Desktop**\\n 7. Click on **File** and **Clone a repository**\\n 8. Click on **URL**\\n 9. ![Github Desktop - Clone a Repository](/uploads/githubdesktop-clonearepo.png \\"Github Desktop - Clone a Repository\\")\\n10. **Paste** in the **repository URL** you copied earlier.\\n11. **Select** the Local **path** of where you want the Git **repository to be saved** locally on your device\\n12. Now we need to **generate** git **credentials** to clone your repository, navigate back to Azure DevOps.\\n13. ![Azure DevOps - Clone](/uploads/azuredevops-clonerepo.png \\"Azure DevOps - Clone\\")\\n14. Click on **Generate** Git **Credentials**\\n15. Azure DevOps will now generate the username and password that will be used by Github Desktop to authenticate with your git repository.\\n16. **Navigate** back to **Github Desktop**\\n17. Click **Clone**\\n18. **Enter** in the **username** and **password** that you received from the git credentials, generated by Azure DevOps and click **Clone**.\\n19. Github Desktop should now **clone** your **repository locally**.\\n\\nCongratulations, you have set up an Azure DevOps git repository using Github Desktop.\\n\\n### Add your Github repository.\\n\\nIf you have an Azure DevOps git repository, follow the steps above \u2013 otherwise, follow these steps to add your Github repository into Github Desktop.\\n\\n1. Open **Github Desktop**\\n2. Click **File**\\n3. Click **Clone repository\u2026.**\\n4. ![Github Desktop - Clone repository](/uploads/githubdesktop-cloneareposnap.png \\"Github Desktop - Clone repository\\")\\n5. On the **Github.com tab**, **enter** your Github **credentials**\\n6. **Select** the Local **path** of where you want the Git **repository** to be **saved** locally on your device\\n7. Click **Clone**\\n\\nCongratulations, you have now set up a Github git repository using Github Desktop.\\n\\n### Using Github Desktop\\n\\nNow that you have a git repository cloned locally, it\'s time to use it.\\n\\n#### Initial Commit\\n\\nOnce you have a file created and saved into the folder of your git repository, i.e. a PowerShell script, you will want to commit it to the git repository.\\n\\n 1. Open **Github Desktop**\\n 2. Click on: **Current repository** to make sure your repository is selected.\\n 3. ![Github Desktop - Initial Commit](/uploads/githubdesktop-initialhelloworld.png \\"Github Desktop - Initial Commit\\")\\n 4. In my example, I have created a new file called: HelloWorld.ps1 in my PowerShell repository.\\n 5. What you can see in the screenshot below is the various components that make up the Github Desktop; you can see the changed file _(i.e. the new file)_, the contents of the file and what will be added, the commit title and the all-important commit description.\\n 6. ![Github Desktop - Overview](/uploads/githubdesktop-productoverview.png \\"Github Desktop - Overview\\")\\n 7. You can **change** the **title** to something more appropriate if you want, but with your commit **description**, this is what you will use for versioning and seeing what changes you made in the future from a quick glance \u2013 make sure it\'s an appropriate description and click **Commit** to master.\\n 8. Committing it to master does not push it to its \'Origin\'. I.e. the actual remote git repository _(stored in Github or Azure DevOps)_ will commit to the local git repository. This allows you to work on code locally without requiring every change to be uploaded to a local repository. In order to commit to the Origin and remote repository, click on: **Push Origin**.\\n 9. ![Github Desktop - Header](/uploads/githubdesktop-header.png \\"Github Desktop - Header\\")\\n10. Once it has been committed, you should be able to see the file on the origin git repository, and you can Push multiple local git changes at once.\\n11. If you click on: **History** should now **see your commit** with your file and description _(as you can see, I was using an old PowerShell repository that I had merged into other repositories since then but thought it was worth using it for this article)._\\n12. ![Github Desktop - Initial commit](/uploads/githubdesktop-initialhelloworldcommit.png \\"Github Desktop - Initial commit\\")\\n\\nCongratulations, you now committed your first file into Git! It wasn\'t that difficult!\\n\\n#### Restore file from the previous version\\n\\nOne of the benefits of using Git is version control and restoring a file if something stops working, or someone had an \'Oops!\' moment! With Github Desktop, restoring a previous version is straightforward.\\n\\n1. Open **Github Desktop**\\n2. Click on: **Current repository** to make sure your repository is selected\\n3. Click on **History** _(you may need to click Fetch Origin if files have been updated remotely)_\\n4. As you can see, someone _(i.e. Luke Murray)_ has **made a** **change** to my\' HelloWorld. ps1\'\' file, to be: \\"I like Unicorn\\" and changed the background and foreground colour to be both Yellow\'.\\n5. I can **right-click** that **file** and select **Revert changes** in the commit using Github Desktop.\\n6. ![Github Desktop - Revert changes](/uploads/githubdesktop-revertchanges.png \\"Github Desktop - Revert changes\\")\\n7. You will now have a new entry in the History that will revert the commit, and you can quickly **push it** back to **Origin** again.\\n\\nCongratulations, you have successfully reverted a commit to a previous version using Github Desktop.\\n\\n#### Working with branches\\n\\nA significant function of Git is the ability to create and use branches. Branches allow you to work on features without touching the main or master branch _(where you can have your production or thoroughly tested resources, for example)_.\\n\\n 1. Open **Github Desktop**\\n 2. Click on: **Current repository** to make sure your repository is selected\\n 3. To **create** a branch, click on the **Current branch** and select **New branch** and give it a name, _i.e. Dev_\\n 4. **Make** a **change** to the **file** like you typically would and **save**\\n 5. Github Desktop has automatically added your changes, and you can **commit** them to the **dev branch** without touching master.\\n 6. ![Github Desktop - Branch commit](/uploads/githubdesktop-branchcommit.png \\"Github Desktop - Branch commit\\")\\n 7. If you navigate to the master branch, you can see that the file has remained untouched. All the control and versioning is done by Git!\\n 8. When you are **ready** to **merge** the dev branch into master, click the **current branch**.\\n 9. Select: **Choose a branch** to **merge** into master\\n10. **Select** your **branch**, _i.e. Dev_\\n11. ![Github Desktop - Merge branch](/uploads/githubdesktop-branchmerge.png \\"Github Desktop - Merge branch\\")\\n12. Click on **create a merge commit**.\\n13. You should see a message in Github notifying that the merge was successful, and you can **push** your **changes** to the **origin** repository.\\n14. Github Desktop should redirect you to the master branch, and you can **now see** your **changes**:\\n15. ![Github Desktop ](/uploads/githubdesktop-branchcommited.png)\\n16. You can go back to using Dev to develop additional features, testing etc. and repeat the same process.\\n\\nUsing a master branch allows others to get production-ready scripts or code, or avoid automation around continuous deployment to production resources, while you may be still working on functionality that you don\'t quite want to be released yet.\\n\\nHopefully, this article gives you an excellent base to start your git journey! \\n\\nThere is a lot more functionality built into Github Desktop, especially around branching, but for day to day use, the above should give you all you need! \\n\\nIt is also worth reading this article on the .[gitignore](https://www.atlassian.com/git/tutorials/saving-changes/gitignore \\" .gitignore \\") file, to make sure your git repositories don\'t end up bloated by unwanted files and you are only committing the files you need to be."},{"id":"azure/azure-bicep-and-insert-resource","metadata":{"permalink":"/azure/azure-bicep-and-insert-resource","source":"@site/blog/2021-12-29-azure-bicep-and-insert-resource.md","title":"Azure Bicep and Insert Resource","description":"Azure Bicep is a Domain Specific Language (DSL) for deploying Azure resources declaratively. Azure Bicep is a transparent abstraction over ARM and ARM templates, which means anything that can be done in an ARM Template can be done in Bicep.","date":"2021-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.365,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-12-29 00:00:00 +1300","title":"Azure Bicep and Insert Resource","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/images/AzureBicepBanner.jpg"},"slug":"azure/azure-bicep-and-insert-resource"},"unlisted":false,"prevItem":{"title":"Git using Github Desktop on Windows for SysAdmins","permalink":"/2021/12/30/git-using-github-desktop-on-windows-for-sysadmins"},"nextItem":{"title":"Day in the Life of a Technical Lead","permalink":"/2021/12/29/day-in-the-life-of-a-tech-lead"}},"content":"Azure Bicep is a Domain Specific Language (DSL) for deploying Azure resources declaratively. Azure Bicep is a transparent abstraction over ARM and ARM templates, which means anything that can be done in an ARM Template can be done in Bicep.\\n\\nAzure Bicep has recently _(December 2021)_ been updated to: v0.4.1124, along with various other hotfixes and enhancements; this version supports \'Insert Resource\' functionality.\\n\\nInsert Resource simplifies ARM to Bicep conversion without exporting entire ARM templates, then compiles them to Bicep when you are only after export for a single resource.\\n\\nTo use Insert Resource, you will need to have:\\n\\n* Bicep version greater than v0.4.1124\\n* Azure CLI\\n* Visual Studio Code with the Bicep extension\\n\\nYou can easily install both or upgrade following the Microsoft documentation on the: [Install Bicep tools](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install?WT.mc_id=AZ-MVP-5004796) page.\\nYou can also review the Bicep changes and latest release notes on Github here: [Azure Bicep releases](https://github.com/Azure/bicep/releases)\\n\\n#### Import Resources into Bicep using Azure CLI and Bicep\\n\\n 1. Open a new file in **Visual Studio Code**\\n 2. Set the Language mode to **Bicep**\\n 3. ![Visual Studio Code - Bicep](/uploads/bicep.png \\"Visual Studio Code - Bicep\\")\\n 4. Now we need to **login** to **Azure**; in Visual Studio code, click **View** and **Terminal.**\\n 5. In the terminal, type in: **az login**\\n 6. Login to Azure using the credentials that have read access to the Resource you want to export.\\n 7. Once you are logged in, type in: **az resource list**\\n 8. In the JSON output in the terminal, **copy** the **resource ID** _(inside the double quotes from the id value)_\\n 9. Now we need to **open** the **Command Palette**, press: CTRL+Shift+P on your keyboard _(or click on View, Command Palette)_\\n10. **Start typing** in **Bicep**; if you have the latest version, **you should see: Bicep: Insert Resource.**, select this\\n11. ![Azure Bicep - Insert Resource](/uploads/bicepinsertresource.png \\"Azure Bicep - Insert Resource\\")\\n12. Enter in the resource ID you copied earlier.\\n13. ![Azure Bicep - Insert Resource](/uploads/bicepinsertresourceenterresourceid.png)\\n14. Azure Bicep should have connected and exported your Resource straight into Bicep! As below, it had imported a Log Analytics workspace in my subscription straight into Bicep.\\n15. ![Azure Bicep - Insert Resource](/uploads/bicepinsertedresource.png)\\n\\n#### To find the resource ID using the Azure Portal.\\n\\nYou can use the Azure CLI to find the Resource ID, but you can also use the Azure Portal by navigating to it below:\\n\\n1. Log in to the **Azure Portal**\\n2. **Navigate** to the **Resource** you want to export to Bicep\\n3. On the **Overview** pane, click on **JSON view**\\n\\nI had problems connecting to export an App Service and App Service plan, so for some resources with multiple dependencies, you may be better off exporting the ARM template from the resources/resource groups and decompiling that way, but the Insert Resource functionality is a very quick way to bring your resources into Bicep!"},{"id":"/2021/12/29/day-in-the-life-of-a-tech-lead","metadata":{"permalink":"/2021/12/29/day-in-the-life-of-a-tech-lead","source":"@site/blog/2021-12-29-day-in-the-life-of-a-tech-lead.md","title":"Day in the Life of a Technical Lead","description":"Being a \'Technical Lead\' or \'Tech Lead\' means different things to some people and organizations; based on definitions found online, a Technical Lead is:","date":"2021-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Service Management","permalink":"/tags/service-management"}],"readingTime":3.01,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Day in the Life of a Technical Lead","authors":["Luke"],"tags":["Misc","Service Management"],"toc":false,"date":"2021-12-29 00:00:00 +1300","header":{"teaser":"/uploads/techlead.png"}},"unlisted":false,"prevItem":{"title":"Azure Bicep and Insert Resource","permalink":"/azure/azure-bicep-and-insert-resource"},"nextItem":{"title":"Azure Storage Account SFTP errors","permalink":"/azure/azure-storage-account-sftp-errors"}},"content":"Being a \'Technical Lead\' or \'Tech Lead\' means different things to some people and organizations; based on definitions found online, a Technical Lead is:\\n\\n> \\"A technical lead is a professional who oversees a team of technical personnel at a software or technology company. They often lead software development or software engineering teams and troubleshoot technical issues that involve software development, engineering tasks and product releases.\\"\\n\\nAlthough I agree with this, I would flesh out a bit more around architectural governance _(or technical assurance, which is what the problem this role or function is for)_ across it - it also doesn\'t need to be software development heavy; it can sit in the operational and delivery spaces as well _(waterfall or agile)_ and is more than a specific role, but a frame of mind.\\n\\n![Tech Lead - Venn diagram](/uploads/techlead.png \\"Tech Lead - Venn diagram\\")\\n\\nAt a very high level, this is what a day in the life of a technical lead means to me:\\n\\n#### Day in the Life of a Tech Lead\\n\\n* Work alongside: Technical Product Owners, Chapter Members, Architecture, Business stakeholders and Service Partners to develop/roadmap/architect and improve technology.\\n* Manage delivery and operational risks and dependencies, and remove impediments to the achievement of the team objectives\\n* Test and develop roadmaps for preview Cloud capabilities for immediate or future value\\n* Act as a Subject Matter Expert _(or Consultant)_ to assist in Design Decisions, Monitoring, Cost and Capacity Requirements\\n* Develop Governance processes for onboarding services into BAU, enabling Technology Infrastructure and Operations staff to use technology in a consistent and secure manner\\n* Work alongside Security and Developers to enable cross-team visibility and collaboration\\n* Champion improvements in People/Processes and ways of working\\n* Work alongside Chapter Members and Chapter Lead to develop Training/Skill programs for Technical areas\\n* Develop and promote an \'everything as code\', \'everything is automated\' mindset\\n* Problem/Incident Management _(i.e. Continous improvement)_\\n\\n#### A Technical lead mindset may look like below\\n\\n* Automate what\'s trivial, boring, mundane, and belittling\\n* Build what you can\'t buy. Buy what you can\'t live without\\n* Make your work visible. Shift your value to performance.\\n* Work is never completed. Establish feedback loops.\\n* Target high impact problems.\\n* Get out of the way of the work, think outside of the box, don\'t limit others.\\n* Try, Learn, Adapt, Try again\\n* Agile is about speed to adapt, not velocity\\n* Log what\'s useful, monitor what matters, alert on what\'s actionable\\n* Empower others while making sure that everything is auditable, standardised.\\n* We live in a VUCA (Volatile, Uncertain, Complexity, Ambiguity) world, you will never see perfect.\\n\\n\\nThe views above are my own, but shout out to [Teal Unicorn](https://tealunicorn.com/ \\"Teal Unicorn\\") for independent consulting on Ways of Working, Continuous improvement; I attended a few of their workshops on Ways of Working, Ways of Managing and Ways of Consulting, and it helped me take a step back and look at what this kind of mindset may look like, or should be and current blockers.\\n\\nOverall, I have noticed that Information Technology roles are now blending disciplines that once required specific job roles _(ie Business Analyst, Service Delivery Manager, Developer, Architect),_ although pure technical roles still exist with Cloud technologies, different skillsets are required to get the most value out of technology stacks, as technology becomes more consumable.  You may also be interesting in reading my thoughts on: [The Cloud Frame of Mind](https://luke.geek.nz/the-cloud-frame-of-mind \\"The Cloud Frame of Mind\\")\\n\\nHopefully this has helped or at least encouraged looking at problems differently, or areas of improvements for any readers out there!"},{"id":"azure/azure-storage-account-sftp-errors","metadata":{"permalink":"/azure/azure-storage-account-sftp-errors","source":"@site/blog/2021-12-27-azure-storage-account-sftp-errors.md","title":"Azure Storage Account SFTP errors","description":"As part of standing up and using an Azure Storage account as an SFTP service, I ran into a few issues. This blog post is merely intended to show my findings in case others run into similar issues.","date":"2021-12-27T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.19,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Storage Account SFTP errors","authors":["Luke"],"tags":["Azure"],"date":"2021-12-27 00:00:00 +1300","toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/azure-storage-account-sftp-errors"},"unlisted":false,"prevItem":{"title":"Day in the Life of a Technical Lead","permalink":"/2021/12/29/day-in-the-life-of-a-tech-lead"},"nextItem":{"title":"SFTP in Microsoft Azure using Azure Blob Storage","permalink":"/azure/sftp-in-microsoft-azure-using-azure-blob-storage"}},"content":"As part of standing up and using an Azure Storage account as an SFTP service, I ran into a few issues. This blog post is merely intended to show my findings in case others run into similar issues.\\n\\n#### PTY allocation request failed on channel 0\\n\\nEven though you appear to have connected successfully, you may see the following errors:\\n\\n* PTY allocation request failed on channel 0\\n* shell request failed on channel 0\\n\\nYou may laugh, but the solution for this was very simple, switch from SSH to **SFTP**!\\n\\nIf you were like me, I just flicked to SSH as a habit.\\n\\n#### Home Directory is not accessible\\n\\nMake sure that the Home directory _(Folder)_ is created in your container, SFTP won\'t create this for you.\\n\\nAlso make sure that the Home directory for the user, references Container/Folder, like the below:\\n\\n![Azure Portal - Enable SFTP](/uploads/AzurePortal_SFTPLocalUsercreate.png \\"Azure Portal - Enable SFTP\\")\\n\\n#### Wrong username, authentication failed\\n\\nWhen attempting to connect to SFTP using a tool such as WinSCP, I got: \\n\\n* Using username \\"lukeftpuser\\".\\n* Authentication failed.\\n\\nThe username is actually comprised of:\\n\\nSTORAGEACCOUNTNAME+FTPNAME, ie: sftpstorageacc1337.lukeftpuser\\n\\n![WinSCP Connection Azure SFTP](/uploads/sftp_winscptest.png \\"WinSCP Connection Azure SFTP\\")\\n\\n#### Unable to find Azure Storage SSH Keys\\n\\nThis is not an error, but Azure Keyvault, does not currently support SSH keypairs, so once they are created by Azure, they are stored in a Microsoft.Compute.sshPublicKeys resource found here: [SSH Keys](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Compute%2FsshPublicKeys \\"SSH Keys\\")"},{"id":"azure/sftp-in-microsoft-azure-using-azure-blob-storage","metadata":{"permalink":"/azure/sftp-in-microsoft-azure-using-azure-blob-storage","source":"@site/blog/2021-12-27-sftp-in-microsoft-azure-using-azure-blob-storage.md","title":"SFTP in Microsoft Azure using Azure Blob Storage","description":"SSH File Transfer Protocol (SFTP) support is now supported in Preview for Azure Blob Storage accounts with hierarchical namespace enabled.","date":"2021-12-27T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":10.585,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-12-27 00:00:00 +1300","title":"SFTP in Microsoft Azure using Azure Blob Storage","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/sftp-in-microsoft-azure-using-azure-blob-storage"},"unlisted":false,"prevItem":{"title":"Azure Storage Account SFTP errors","permalink":"/azure/azure-storage-account-sftp-errors"},"nextItem":{"title":"Whitelisting your Public IP with Azure Bicep and PowerShell","permalink":"/azure/whitelisting-your-public-ip-with-azure-bicep"}},"content":"SSH File Transfer Protocol _(SFTP)_ support is now supported in Preview for Azure Blob Storage accounts with hierarchical namespace enabled.\\n\\nAlthough tools such as Storage Explorer, Data Factory, AzCopy allows a copy to and from Azure storage accounts, sometimes your applications need more traditional integration, so SFTP is a welcome addition to the Microsoft Azure ecosystem, which in some cases removes the need for additional Virtual Machine(s).\\n\\nThis support enables standard SFTP connectivity to an Azure Storage account. As an Azure PaaS _(Platform as a Service)_ resource, it offers additional flexibility, reduces operational overhead, and increases redundancy and scalability.\\n\\nWe will run through the initial setup of the Azure Storage account using the Azure Portal.\\n\\nSFTP using an Azure Storage account does not support shared access signature (SAS) or Microsoft Entra ID (Azure AD) authentication for connecting SFTP clients. Instead, SFTP clients must use a password or a Secure Shell _(SSH)_ private/public keypair.\\n\\nBefore we head into the implementation, just a bit of housekeeping, this is currently still in Preview at the time this post was written; the functionality MAY change by the time it becomes GA (_Generally Available)_.\\n\\n> During the public preview, the use of SFTP does not incur any additional charges. However, the standard transaction, storage, and networking prices for the underlying Azure Data Lake Store Gen2 account still apply. SFTP might incur additional charges when the feature becomes generally available. As of the time of the preview SFTP support is only avaliable in certain [regions](https://learn.microsoft.com/en-us/azure/storage/blobs/secure-file-transfer-protocol-support?WT.mc_id=AZ-MVP-5004796#regional-availability \\"SSH File Transfer Protocol (SFTP) support for Azure Blob Storage (preview)\\").\\n\\nYou can connect to the SFTP storage account by using local _(to the SFTP storage account)_ SSH public-private keypair or Password _(or both)_. You can also set up individual HOME directories _(because of the hierarchical namespace, these are folders not containers)_ for each user _(maximum 1000 local user accounts_).\\n\\n![SFTP Azure Storage Account - High Level Diagram](/images/posts/SFTP_Azure_HLDiagram.png \\"SFTP Azure Storage Account - High Level Diagram\\")\\n\\n### Creating an Azure Storage account for SFTP\\n\\nThis article assumes you have an Azure subscription and rights to create a new Storage account resource, however if you have an already existing storage account the following pre-requisites are required:\\n\\n* A standard general-purpose v2 or premium block blob storage account. You can also enable SFTP as you create the account.\\n* The account redundancy option of the storage account is set to either locally-redundant storage (LRS) or zone-redundant storage (ZRS); GRS is not supported.\\n* The hierarchical namespace feature of the account must be enabled for existing storage accounts. To enable the hierarchical namespace feature, see [Upgrade Azure Blob Storage with Azure Data Lake Storage Gen2 capabilities](https://learn.microsoft.com/en-us/azure/storage/blobs/upgrade-to-data-lake-storage-gen2-how-to?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796).\\n* If you\'re connecting from an on-premises network, make sure that your client allows outgoing communication through port 22. The SFTP uses that port.\\n\\n#### Fill out the SFTP Public Preview Interest Form\\n\\nBecause the SFTP functionality is currently in Private Preview, Microsoft has asked that anyone interested in the SFTP Preview fill out a Microsoft Forms:\\n\\n* [SFTP Public Preview Interest Form](https://forms.office.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRxE4mlJMX2FKhD4ROGugH69URjBGVVdNSVRRWVUxSFA3WkM5OVk4STJFQS4u \\"SFTP Public Preview Interest Form\\")\\n\\nThis MAY be required before proceeding to the following steps; initially, I believe this was required - but there appears to have been a few people who I know have registered the feature without the form - either way, the SFTP Public Preview Interest form, is a good opportunity to supply your use-case information to Microsoft directly, to help improve the nature of the service going forward.\\n\\n#### Registering the Feature\\n\\nTo create an Azure Storage account that supports SFTP - we need to enable the Preview Feature.\\n\\n1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n2. Navigate to: **Subscriptions**\\n3. **Select** the **Subscription** that you want to enable **SFTP** preview for\\n4. Click on: **Preview features**\\n5. Search for: **SFTP**\\n6. Click on: **SFTP support for Azure Blob Storage** and click **Register** - _this may take from minutes to a few days to be registered, as each preview request may need to be manually approved by Microsoft personnel based on the Public Preview Interest form - my feature registration occurred quite quickly, so there is a chance that they either have automated the approvals or I was just lucky._\\n\\n   _As you can see in the screenshot below, I had already registered mine:_\\n7. ![Azure Portal SFTP Preview Feature](/uploads/azureportal_sftppreview.png \\"Azure Portal SFTP Preview Feature\\")\\n8. You can continue to hit refresh until it changes from: Registering to Registered.\\n9. While we are here, let\'s check that the Microsoft.Storage resource provider is registered _(it should already be enabled, but it is a good opportunity to check before attempting to create a resource and get a surprise_), by clicking on Resource providers in the left-hand side menu and search for: Storage, if it is set to NotRegistered - click on Microsoft.Storage and click Register.\\n\\nTo register the SFTP feature using PowerShell, you can run the following cmdlet:\\n\\n    Register-AzProviderFeature -FeatureName \\"AllowSFTP\\" -ProviderNamespace \\"Microsoft.Storage\\"\\n\\n#### Create the Azure Storage Account\\n\\nNow that the Preview feature has been registered, we can now create a new Storage account.\\n\\n 1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. Click on **+Create a resource**\\n 3. Type in: **Storage account** and click on the Microsoft Storage account resource and click Create\\n 4. ![Azure Portal - Storage account](/uploads/azureportal_createresourcestorageaccount.png \\"Azure Portal - Storage account\\")\\n 5. Select your **Subscription** you enabled the SFTP feature in earlier\\n 6. Select your **Resource Group** _(or create a new resource group)_ to place your storage account into.\\n 7. Select your storage account name _(_[_this needs to be globally unique and a maximum of 24 characters_](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-name-rules?WT.mc_id=AZ-MVP-5004796#microsoftstorage \\"Naming rules and restrictions for Azure resources\\")_), in my example; I am going with: sftpstorageacc1337_\\n 8. **Select** your **Region**; remember that only specific regions currently have SFTP support at the time of this article _.\\n 9. **Select** your **performance tier**; premium is supported but remember to select Blob, select Standard.\\n10. **Select** your **Redundancy**; remember that GRS-R, GRS isn\'t supported at this time; I will select Zone-redundant storage (ZRS) so that my storage account is replicated between the three availability zones, but you can also select LRS _(Locally Redundant Storage)._\\n11. ![Azure Portal - Create v2 Storage Account](/uploads/azureportal_createstorageaccount.png \\"Azure Portal - Create v2 Storage Account\\")\\n12. Click **Next: Advanced**\\n13. Leave the Security options as-is and check: **Enable hierarchical namespace** under the Data Lake Storage Gen2 subheading.\\n14. Click **Enable SFTP**\\n15. ![Azure Portal - Enable SFTP](/uploads/azureportal_createstorageaccountenablesftp.png \\"Azure Portal - Enable SFTP\\")\\n16. Click: **Next: Networking**\\n17. SFTP supports Private Endpoints (_as a blob storage sub-resource)_, but in this case, I will be keeping Connectivity as a **Public endpoint (all networks)**\\n18. ![Azure Portal - Enable SFTP](/uploads/azureportal_createstorageaccountnetwork.png \\"Azure Portal - Enable SFTP\\")\\n19. Click **Next: Data Protection**\\n20. Here you can enable [soft-delete](https://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-blob-overview \\"Soft delete for blobs\\") for your blobs and containers, so if a file is deleted, it is retained for seven days until it\'s permanently deleted; I am going to leave mine set as the default of 7 days and click: **Next: Tags.**\\n21. Add in any applicable Tags, i.e. who created it, when you created it, what you created it for and click **Review + Create**\\n22. Review your configuration, make sure that Enable SFTP is enabled with Hierarchical namespace and click **Create**.\\n\\nIn case you are interested in Infrastructure as Code, here is an Azure Bicep file I created to create a storage account ready for SFTP here that can be deployed to a Resource Group, ready for the next steps:\\n\\n```bicep title=\\"storageaccount.bicep\\"\\nparam storageaccprefix string = \'\'\\nvar location = resourceGroup().location\\n\\nresource storageacc \'Microsoft.Storage/storageAccounts@2021-06-01\' = {\\n  name: \'${storageaccprefix}${uniqueString(resourceGroup().id)}\'\\n  location: location\\n  sku: {\\n    name: \'Standard_ZRS\'\\n  }\\n  kind: \'StorageV2\'\\n  properties: {\\n    defaultToOAuthAuthentication: false\\n    allowCrossTenantReplication: false\\n    minimumTlsVersion: \'TLS1_2\'\\n    allowBlobPublicAccess: true\\n    allowSharedKeyAccess: true\\n    isHnsEnabled: true\\n    supportsHttpsTrafficOnly: true\\n    encryption: {\\n      services: {\\n  \\n        blob: {\\n          keyType: \'Account\'\\n          enabled: true\\n        }\\n      }\\n      keySource: \'Microsoft.Storage\'\\n    }\\n    accessTier: \'Hot\'\\n  }\\n}\\n```\\n\\n#### Setup SFTP\\n\\nNow that you have a compatible Azure storage account, it is time to enable SFTP!\\n\\n 1. Log in to the [**Azure Portal**](https://portal.azure.com/#home \\"Azure Portal\\")\\n 2. **Navigate** to the **Storage account** you have created for SFTP and click on it\\n 3. On the Storage account blade, under **Settings**, you will see: **SFTP**\\n 4. ![Azure Portal - Enable SFTP](/uploads/azureportal_storageaccountstpblade.png \\"Azure Portal - Enable SFTP\\")\\n 5. Click on **SFTP** and click **+ Add local user.**\\n 6. Type in the username of the user you would like to use _(remember you can have up to 1000 local users, but there is no integration into Azure AD, Active Directory or other authentication services currently_), in my example I will use: lukeftpuser\\n 7. You can use either _(and both)_ SSH keys or passwords, in this article - I am simply going to use a password so I select: **SSH Password**.\\n 8. Click **Next**\\n 9. Our storage account is empty, we now need to create a top-level container, so I will sect **Create new** and set the name to: ftp\\n10. I will leave the Public access level to **Private (no anonymous access)**\\n11. Click **Ok**\\n12. Now that the ftp container has been created, we need to **set** the **permissions**, I am simply going to give the permissions of Read, Create, Delete, List and Write. _It\'s worth noting, that if you only need to read or list contents, then that is the only permissions you need, these permissions are for the Container, not the folder, so you may find your users may have permissions to other folders in the same Container if not managed appropriately._\\n13. Now we set the **Home directory**. This is the directory that the user will be automatically mapped to, this is optional but if you don\'t have a Home directory filled in for the user, they will need to connect to the appropriate folders when connecting to SFTP manually. The home directory needs to be relative, ie: ftp/files _(the container name and the files folder, located in the ftp container)._\\n14. ![Azure Portal - Enable SFTP](/uploads/AzurePortal_SFTPLocalUsercreate.png \\"Azure Portal - Enable SFTP\\")\\n15. Because we specified Password earlier, **Azure** has automatically **created** a new **password** for that account, although you can generate new passwords - you are unable to specify what the Password is, make sure you **copy** this and **store** it in a password **vault** of some kind, the length of the password that was generated for me was: 89 characters.\\n16. ![Azure Portal - Enable SFTP](/uploads/azureportal_sftp_localusercreatepassword.png \\"Azure Portal - Enable SFTP\\")\\n17. You should see the connection string of the user, along with the Authentication method and container permissions.\\n18. ![Azure Storage Account SFTP - Local User Created](/uploads/azureportal_sftp_localusercreated.png \\"Azure Storage Account SFTP - Local User Created\\")\\n\\n### Test Connectivity via SFTP to an Azure Storage Account\\n\\nI will test Connectivity to the SFTP Azure Storage account using Windows 11, although the same concepts apply across various operating systems _(Linux, OSX, etc.)_.\\n\\n#### Test using SFTP from Windows using command prompt\\n\\n1. Make sure you have a copy of the Connection String and user password from the SFTP user account created earlier.\\n2. Open **Command Prompt**\\n3. Type in **sftp** CONNECTIONSTRING, example below and press Enter:\\n   * _sftp sftpstorageacc1337.lukeftpuser@sftpstorageacc1337.blob.core.windows.net_\\n4. If you get a prompt to verify the authenticity of the host matches (i.e. the name/URL of the storage account matches) and type in: Yes, to add the storage account to your known host\'s list\\n5. Press **Enter** and paste in the copy of the Password that was generated for you earlier.\\n6. You should be **connected to the Azure Storage account via SFTP**!\\n7. As you can see below, I am in the Files folder, which is my users home folder, and there is a file named: Test in it.\\n8. ![SFTP Windows](/uploads/sftp_windowstest.png \\"SFTP Windows\\")\\n\\n   > Once you have connected to SFTP using the Windows command line you can type in: **?**\\n   >\\n   > That will give you a list of all the available commands to run, ie upload files etc \\n\\n#### Test using WinSCP\\n\\n1. Make sure you have a copy of the Connection String and user password from the SFTP user account created earlier.\\n2. If you haven\'t already, download WinSCP and install it\\n3. You should be greeted by the **Login page** _(but if you aren\'t, click on Session, New Session)_\\n4. For the hostname, type in the **URL** for the **storage account** _(after the @ in the connection string)_\\n5. For the **username,** type in everything before the @\\n6. Type in your **Password**\\n7. **Verify** that the port is 22 and file protocol is **SFTP** and click **Login**\\n8. ![Azure SFTP - WinSCP](/uploads/sftp_winscptest.png \\"Azure SFTP - WinSCP\\")\\n9. ![Azure SFTP - WinSCP](/uploads/sftp_winscptest2.png \\"Azure SFTP - WinSCP\\")\\n\\nCongratulations! You have now created and tested Connectivity to the Azure Storage SFTP service!"},{"id":"azure/whitelisting-your-public-ip-with-azure-bicep","metadata":{"permalink":"/azure/whitelisting-your-public-ip-with-azure-bicep","source":"@site/blog/2021-12-14-whitelisting-your-public-ip-with-azure-bicep.md","title":"Whitelisting your Public IP with Azure Bicep and PowerShell","description":"Allowing and restricting Azure resources by being accessible by specific Public IP (Internet Protocol) addresses has been around for years; most Azure resources support it, a Storage account is no different.","date":"2021-12-14T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.18,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Whitelisting your Public IP with Azure Bicep and PowerShell","authors":["Luke"],"tags":["Azure"],"date":"2021-12-14 00:00:00 +1300","toc":false,"header":{"teaser":"images/powershell-blog-feature-banner.png"},"slug":"azure/whitelisting-your-public-ip-with-azure-bicep"},"unlisted":false,"prevItem":{"title":"SFTP in Microsoft Azure using Azure Blob Storage","permalink":"/azure/sftp-in-microsoft-azure-using-azure-blob-storage"},"nextItem":{"title":"Capturing Virtual Machine images and Snapshots in Azure using WVDAdmin","permalink":"/azure/capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin"}},"content":"Allowing and restricting Azure resources by being accessible by specific Public IP _(Internet Protocol)_ addresses has been around for years; most Azure resources support it, a Storage account is no different.\\n\\nIn this article, I will be using PowerShell to obtain my current public IP, then parse that variable into my Azure Bicep deployment to create a storage account, with the firewall rule allowing ONLY my public IP address.\\n\\nI will assume that you have both [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install?WT.mc_id=AZ-MVP-5004796#windows \\"Azure Bicep - Install\\") and [PowerShell Azure](https://learn.microsoft.com/en-us/powershell/azure/install-az-ps?WT.mc_id=AZ-MVP-5004796 \\"PowerShell - Azure\\") modules installed and the know-how to connect to Microsoft Azure.\\n\\nUtilising PowerShell to create dynamic variables in your deployment can open the doors to more flexible deployments, such as including the name of the person deploying the infrastructure into the tags of the resource - or in this case, adding a whitelisted IP automatically to your Azure resource to be secure by default.\\n\\nI will be using PowerShell [splatting](https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_splatting?view=powershell-7.2&WT.mc_id=AZ-MVP-5004796 \\"Splatting\\") as it\'s easier to edit and display. You can easily take the scripts here to make them your own.\\n\\nAzure Bicep deployments *(like ARM)* have the following command: \'TemplateParameterObject\'. \'TemplateParameterObject\' allows Azure Bicep to accept parameters from PowerShell directly, which can be pretty powerful when used with a self-service portal or pipeline.\\n\\nNow we are ready to create the Azure Storage account...\\n\\nI will first make an Azure Resource Group using PowerShell for my storage account first, then use the New-AzResourceGroupDeployment cmdlet to deploy my storage account from my bicep file.\\n\\n    #Connects to Azure\\n    Connect-AzAccount\\n    #Grabs the Public IP of the currently connected PC and adds it into a variable.\\n    $publicip = (Invoke-WebRequest -uri \\"http://ifconfig.me/ip\\").Content\\n    #Resource Group Name\\n    $resourcegrpname = \'storage_rg\'\\n    #Creates a resource group for the storage account\\n    New-AzResourceGroup -Name $resourcegrpname -Location \\"AustraliaEast\\"\\n    # Parameters splat, for Azure Bicep\\n    # Parameter options for the Azure Bicep Template, this is where your Azure Bicep parameters go\\n    $paramObject = @{\\n      \'storageaccprefix\' = \'stg\'\\n      \'whitelistpublicip\'  = $publicip\\n    }\\n    # Parameters for the New-AzResourceGroupDeployment cmdlet goes into.\\n    $parameters = @{\\n      \'Name\'                  = \'StorageAccountDeployBase\'\\n      \'ResourceGroupName\'     = $resourcegrpname \\n      \'TemplateFile\'          = \'c:\\\\temp\\\\storageaccount.bicep\'\\n      \'TemplateParameterObject\'    = $paramObject\\n      \'Verbose\'               = $true\\n    }\\n    #Deploys the Azure Bicep template\\n    New-AzResourceGroupDeployment @parameters\\n\\n![Azure Bicep - Parameter](/uploads/storageaccount_publicip.png \\"Azure Bicep - Parameter\\")\\n\\nAs you can see above, I am grabbing my current IP Address from the ifconfig website and storing it in a variable *(as a string object)*, then referencing it in the paramObject - which will be passed through to the TemplateParameterObject command as Parameters strings for Azure Bicep, my IP address _(I am running this from an Azure VM)_ is then passed through, to Azure Bicep.\\n\\nMy Azure Bicep is below:\\n\\n    param storageaccprefix string = \'\'\\n    param whitelistpublicip string = \'\'\\n    var location = resourceGroup().location\\n    \\n    resource storageaccount \'Microsoft.Storage/storageAccounts@2021-06-01\' = {\\n      name: \'${storageaccprefix}${uniqueString(resourceGroup().id)}\'\\n      location: location\\n      sku: {\\n        name: \'Standard_ZRS\'\\n      }\\n      kind: \'StorageV2\'\\n      properties: {\\n        defaultToOAuthAuthentication: false\\n        allowCrossTenantReplication: false\\n        minimumTlsVersion: \'TLS1_2\'\\n        allowBlobPublicAccess: true\\n        allowSharedKeyAccess: true\\n        isHnsEnabled: true\\n        networkAcls: {\\n          resourceAccessRules: []\\n          bypass: \'AzureServices\'\\n          virtualNetworkRules: []\\n          ipRules: [\\n            {\\n              value: whitelistpublicip\\n              action: \'Allow\'\\n            }\\n          ]\\n          defaultAction: \'Deny\'\\n        }\\n        supportsHttpsTrafficOnly: true\\n        encryption: {\\n          services: {\\n      \\n            blob: {\\n              keyType: \'Account\'\\n              enabled: true\\n            }\\n          }\\n          keySource: \'Microsoft.Storage\'\\n        }\\n        accessTier: \'Hot\'\\n      }\\n    }\\n\\nIn Azure Bicep - I am accepting the whitelistpublicip variable from PowerShell and have passed that along to the virtualNetworkRules object as an Allow, while the defaultAction is \'Deny\'.\\n\\nIf I navigate to the Azure Portal, I can see my newly created storage account; under the Networking blade, I can see that the Firewall has been enabled and my Public IP has been added successfully:\\n\\n![Azure Storage Account - Network](/uploads/storageaccount_firewall.png \\"Azure Storage Account - Network\\")\\n\\nHopefully, this helps you be more secure from deployment time and gives you a good framework to work on; in the future, the same process can be used to create inbound RDP rules for Virtual Machines, as an example."},{"id":"azure/capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin","metadata":{"permalink":"/azure/capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin","source":"@site/blog/2021-12-13-capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin.md","title":"Capturing Virtual Machine images and Snapshots in Azure using WVDAdmin","description":"WVDAdmin - is a native administration GUI (graphical user interface) for Azure Virtual Desktop (AVD). WVDAdmin is a free custom-built tool designed to make managing and standing up Azure Virtual Desktop infrastructure easy. Not only can you use it to roll out your Azure Virtual Desktop infrastructure and manage existing workspaces and host pools - you can use it to create Virtual Machine images that can be used for Virtual Scale Sets, but Base also builds or Azure Virtual Desktop session hosts! In addition, WVDAdmin automates creating and using snapshots and virtual machine images in a simple point and click interface - that just works!","date":"2021-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.305,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Capturing Virtual Machine images and Snapshots in Azure using WVDAdmin","authors":["Luke"],"tags":["Azure"],"date":"2021-12-13 00:00:00 +1300","toc":true,"header":{"teaser":"uploads/snapshot-restore.png"},"slug":"azure/capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin"},"unlisted":false,"prevItem":{"title":"Whitelisting your Public IP with Azure Bicep and PowerShell","permalink":"/azure/whitelisting-your-public-ip-with-azure-bicep"},"nextItem":{"title":"Cloud Adoption Framework for Azure - Tools and Templates","permalink":"/2021/12/11/cloud-adoption-framework-for-azure"}},"content":"[WVDAdmin](https://blog.itprocloud.de/Windows-Virtual-Desktop-Admin/ \\"WVDAdmin\\") - is a native administration GUI (graphical user interface) for Azure Virtual Desktop (AVD). WVDAdmin is a free custom-built tool designed to make managing and standing up Azure Virtual Desktop infrastructure easy. Not only can you use it to roll out your Azure Virtual Desktop infrastructure and manage existing workspaces and host pools - you can use it to create Virtual Machine images that can be used for Virtual Scale Sets, but Base also builds or Azure Virtual Desktop session hosts! In addition, WVDAdmin automates creating and using snapshots and virtual machine images in a simple point and click interface - that just works!\\n\\n### Prerequisites\\n\\n* Azure subscription\\n* Resource Group\\n* Virtual Machine _(to be used as your master image)_\\n* Of course - WVDAdmin\\n\\nYou can download WVDAdmin from the following page:  [Azure Windows Virtual Desktop administration with WVDAdmin](https://blog.itprocloud.de/Windows-Virtual-Desktop-Admin/ \\" Azure Windows Virtual Desktop administration with WVDAdmin\\").\\n\\nAlso, make sure you have set up a [service principal](https://blog.itprocloud.de/Windows-Virtual-Desktop-Admin/#service-principal-functional-account \\"Service principal (functional account) Link\\") with the appropriate rights to the Resource Groups that holds your Virtual Machine.\\n\\nBefore proceeding ahead, **_make sure you have a virtual machine backup!_**\\n\\n### Capturing a Snapshot\\n\\nAlthough, possible to do using the Azure Portal, quickly taking an OS disk snapshot and then reverting the change can be a bit tedious, especially if you want to make a backup quickly of the operating system disk before patching or application upgrade, Snapshots are a lot quicker to take and work well for immediate and temporary recovery, especially when you want to quickly try something out - without having to wait for an Azure Backup. Please note this tool does not snapshot any data drives present.\\n\\n#### Capture a Snapshot\\n\\n 1. Open **WVDAdmin**\\n 2. On the \\"Welcome\\" tab, **enter** in your Azure **Tenant id**\\n 3. **Enter** in your **Service principal (application) ID** and **key**\\n 4. Click on **Reload all** - to connect to Azure\\n 5. **Expand Azure**\\n 6. **Expand Virtual Machines**\\n 7. **Expand** your **Resource group**; in my example; it is: SERVERS-RG\\n 8. **Right-click** your **server**; in my example, it is: Server2019\\n 9. Select **SnapShot-Create**\\n10. ![WVDAdmin - Create Snapshot](/uploads/snapshot-create.png \\"WVDAdmin - Create Snapshot\\")\\n11. WVDAdmin will then prompt you to verify that you want to create your Snapshot.\\n12. ![WVD - Verify Snapshot](/uploads/snapshot-create_verify.png \\"WVD - Verify Snapshot\\")\\n13. Confirm the server is correct and click **Ok**\\n14. Depending on the size of your disk, this process may only take a few seconds; the virtual Machine may experience a slight performance hit. Still, I did not lose RDP connectivity during the snapshot process in my testing.\\n15. Review the logs to make sure that the Snapshot has been created successfully:\\n16. ![Snapshot](/uploads/snapshot-create_verify_log.png)\\n17. You should now see the Snapshot in the Azure Portal, in the same Resource Group as the server.\\n18. ![Azure Portal - Snapshot](/uploads/snapshot-azureportal.png \\"Azure Portal - Snapshot\\")\\n\\n#### Restore a Snapshot\\n\\nBefore you proceed, just a warning that restoring the Snapshot will discard any changes made after the Snapshot. The virtual machine will also be deallocated, so it will stop any connections to it.\\n\\n 1. Open **WVDAdmin**\\n 2. On the \\"Welcome\\" tab, **enter** in your Azure **Tenant id**\\n 3. **Enter** in your **Service principal (application) ID** and **key**\\n 4. Click on **Reload all** - to connect to Azure\\n 5. **Expand Azure**\\n 6. **Expand Virtual Machines**\\n 7. **Expand** your **Resource group**; in my example; it is: SERVERS-RG\\n 8. **Right-click** your **server**; in my example, it is: Server2019\\n 9. Select **SnapShot-Restore**\\n10. ![Azure Disk Snapshot](/uploads/snapshot-restore.png \\"Azure Disk Snapshot\\")\\n11. **Select** the **Snapshot** you would like to **restore** to, and when you are ready, click **Ok**. This will force the Virtual Machine to be shut down and deallocated and the Snapshot to be restored.\\n12. ![Azure Disk Snapshot](/uploads/snapshot-restore_verify.png \\"Azure Disk Snapshot\\")\\n13. You may also start the VM from WVDAdmin, by right-clicking on the Virtual Server after the Snapshot restores and click: Start.\\n14. ![Azure Disk Snapshot](/uploads/wvdadmin-startvm.png \\"Azure Disk Snapshot\\")\\n15. Verify that your Virtual Machine is back up and running and remove any unneeded snapshots and disks from the Azure Portal, to reduce additional costs. If you intend to keep any around, make sure you add appropriate Tags and a review date so you know what and why they existed in the first place.\\n\\nA few things to note:\\n\\n* WVDAdmin gave me errors, stating that the \\"Recovering snapshot was not successful\\", however, this occurred after the Swapping disk process when the old disk was attempting to be deleted. The recovery did, in fact, reoccur; I then successfully deleted the disks in the Azure Portal manually.\\n* I also had the: _\\"Virtual machine agent status is not ready.\\"_ error occur. After the Virtual Machine had enough time to start the Azure agent, this self-resolved.\\n\\n### Capturing a Virtual Machine Image\\n\\nVirtual Machine images work well for Azure Virtual Desktop and Virtual Machines scale sets, where you want consistency between your various virtual machines. The same process I will run through works with Windows 10/11 along with Windows Server 2022 and below _(and I would also imagine Linux workloads)_. \\n\\nI will be using the Windows Server 2019 Virtual Machine I had created before, however with various applications that I want to be standard across new builds; in my demo I used chocolatey to install:\\n\\n* Adobe Reader\\n* Microsoft Visual C++ runtimes\\n* 7Zip\\n* VLC\\n\\nThen added a custom user policy to set the wallpaper. WVDAdmin will automatically generalise _(sysprep)_ the Machine for you by creating a \'Temp\' machine without touching your original Virtual Machine!\\n\\n#### Capture a Virtual Machine Image\\n\\n 1. Open **WVDAdmin**\\n 2. On the \\"Welcome\\" tab, **enter** in your Azure **Tenant id**\\n 3. **Enter** in your **Service principal (application) ID** and **key**\\n 4. Click on **Reload all** - to connect to Azure\\n 5. **Expand Azure**\\n 6. **Expand Virtual Machines**\\n 7. **Expand** your **Resource group**; in my example, it is: SERVERS-RG\\n 8. **Right-click** your **server**; in my example, it is: Server2019\\n 9. Select **Create a template image**\\n10. ![WVDAdmin - Create a template image](/uploads/vmimage-create.png \\"WVDAdmin - Create a template image\\")\\n11. WVDAdmin will then display the: **Capture Image tab.**\\n12. Type in an appropriate image name (make sure you understand it, add specific versioning etc.)\\n13. **Verify** that your Template **VM** is **correct**\\n14. Select your **Target Resource Group** for your **Image**\\n15. If you have a custom PowerShell script, you may add additional customisations. Add the script path here (_make sure it\'s publically accessible by Azure, i.e. Azure storage account, Github repository etc.)_.\\n16. Before proceeding to the next step, **your VM will be deallocated**\\n17. When you are ready, select **Capture**\\n18. WVDAdmin will then deallocate your VM and run through the following process:\\n19. Deallocate VM -> Create a snapshot of VM ->Create a temporary VM from the snapshot -> Generalise the VM -> deallocate temporary VM -> create the image -> delete temp VM resources\\n20. ![WVDAdmin - Capture Image](/uploads/vmimage-log.png \\"WVDAdmin - Capture Image\\")\\n21. You should now see your Image in your Azure Portal.\\n22. ![Azure - Custom Image](/uploads/inkedvmimage-azureportal.jpg \\"Azure - Custom Image\\")\\n23. You can now create additional Virtual Machines from your Custom image using the Azure Portal.\\n24. WVDAdmin can also copy your Custom Image into a Shared Image Gallery, or you can use it to create an Azure Virtual Desktop session host!\\n25. ![WVDAdmin - New Session Host](/uploads/wvdadmin_newsessionhost.png \\"WVDAdmin - New Session Host\\")\\n\\nHopefully, this article has been of some use - even if you don\'t use Azure Virtual Desktop - WVDAdmin is a great tool to help with day-to-day Azure Virtual Machine operations."},{"id":"/2021/12/11/cloud-adoption-framework-for-azure","metadata":{"permalink":"/2021/12/11/cloud-adoption-framework-for-azure","source":"@site/blog/2021-12-11-cloud-adoption-framework-for-azure.md","title":"Cloud Adoption Framework for Azure - Tools and Templates","description":"To help with your Microsoft Cloud Adoption and Azure migration, you need a few things to be successful:","date":"2021-12-11T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.32,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-12-11 00:00:00 +1300","title":"Cloud Adoption Framework for Azure - Tools and Templates","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/posts/Microsoft_CloudAdoptionFramework_Azure.png"}},"unlisted":false,"prevItem":{"title":"Capturing Virtual Machine images and Snapshots in Azure using WVDAdmin","permalink":"/azure/capturing-virtual-machine-images-and-snapshots-in-azure-using-wvdadmin"},"nextItem":{"title":"Azure NAT Gateway - Implementation and Testing","permalink":"/2021/12/03/azure-nat-gateway"}},"content":"To help with your Microsoft Cloud Adoption and Azure migration, you need a few things to be successful:\\n\\n1. Define your **strategy**, what are your expected outcomes? Where do you start, what skills do you have or need?\\n2. **Plan**, this may include organisational alignment to get moving to the Cloud\\n3. **Ready**, this is where you look at your governance, Landing Zones /Blueprints\\n4. **Adopt**, this is where you actually migrate your workloads into the cloud, existing apps and new\\n\\n![Cloud Adoption Framework for Azure](/images/posts/Microsoft_CloudAdoptionFramework_Azure.png \\"Cloud Adoption Framework for Azure\\")\\n\\nHere are some useful tools, templates, and assessments provided by Microsoft to help on your journey:\\n\\n*Note: It is not as if you can\'t get these resources elsewhere, I purely just wanted a list format for easy reference.*\\n\\n**Define strategy**\\n\\n* [Cloud journey tracker](https://aka.ms/adopt/journeytracker?WT.mc_id=AZ-MVP-5004796)\\n  Navigate to the most relevant adoption content efficiently, and detect early adoption blockers\\n* [Business outcome template](https://archcenter.blob.core.windows.net/cdn/business-outcome-template.xlsx)\\n\\n**Plan**\\n\\n* [Cloud adoption plan generator](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/plan/template?WT.mc_id=AZ-MVP-5004796)\\n* [Azure DevOps demo generator](https://azuredevopsdemogenerator.azurewebsites.net/?name=CloudAdoptionPlan)\\n  Leverage Azure DevOps to log and track your cloud adoption plan\\n\\n**Ready**\\n\\n* [Azure setup guide](https://aka.ms/adopt/setupguide?WT.mc_id=AZ-MVP-5004796)\\n  Step-by-step guidance to help admins plan, set up, and secure Azure for your organization\\n* [Readiness checklist](https://raw.githubusercontent.com/microsoft/CloudAdoptionFramework/master/ready/readiness-checklist.docx)\\n* [Naming and tagging tracking template](https://raw.githubusercontent.com/microsoft/CloudAdoptionFramework/master/ready/naming-and-tagging-conventions-tracking-template.xlsx)\\n* [Azure naming tool](https://github.com/mspnp/AzureNamingTool)\\n* [Landing zone blueprints](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/landing-zone/?WT.mc_id=AZ-MVP-5004796)\\n  Host your workloads, pre-provisioned through code. Including foundational capabilities using a defined set of cloud services and best practices.\\n\\n**Adopt**\\n\\n* [Strategic migration assessment and readiness tool (SMART)](https://aka.ms/smarttool?WT.mc_id=AZ-MVP-5004796)\\n  Prepare for a scale migration\\n* [Azure migration guide](https://aka.ms/adopt/migration/guide?WT.mc_id=AZ-MVP-5004796)\\n  Step-by-step guidance to help assess your current environment, prepare for migration, and make the shift to Azure\\n* [Azure migration execution guide](https://github.com/Azure/migration/tree/main)\\n  Step-by-step guidance to help assess your current environment, prepare for migration, and make the shift to Azure\\n* [Azure innovation guide](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/innovate/innovation-guide/?WT.mc_id=AZ-MVP-5004796)\\n  Step-by-step guidance to help build innovative solutions leveraging Azure platform capabilities\\n\\n**Govern**\\n\\n* [Governance process template](https://archcenter.blob.core.windows.net/cdn/fusion/governance/Governance%20Discipline%20Template.docx)\\n* [Cost Management process template](https://archcenter.blob.core.windows.net/cdn/fusion/governance/Cost%20Management%20Discipline%20Template.docx)\\n* [Deployment acceleration process template](https://archcenter.blob.core.windows.net/cdn/fusion/governance/Deployment%20Acceleration%20Discipline%20Template.docx)\\n* [Identity process template](https://archcenter.blob.core.windows.net/cdn/fusion/governance/Identity%20Baseline%20Discipline%20Template.docx)\\n* [Resource consistency process template](https://archcenter.blob.core.windows.net/cdn/fusion/governance/Resource%20Consistency%20Discipline%20Template.docx)\\n* [Security baseline process template](https://archcenter.blob.core.windows.net/cdn/fusion/governance/Security%20Baseline%20Discipline%20Template.docx)\\n\\n**Manage**\\n\\n* [Azure Well-Architecture review](https://aka.ms/adopt/architecturereview?WT.mc_id=AZ-MVP-5004796)\\n  Examine your workloads through the lenses of resiliency, cost, DevOps practices, security, and scalability.\\n* [Best practices source code](https://github.com/microsoft/CloudAdoptionFramework/tree/master/manage/Automation-Best-Practices)\\n* [Operations management workbook](https://raw.githubusercontent.com/microsoft/CloudAdoptionFramework/master/manage/opsmanagementworkbook.xlsx)\\n\\n> The [Microsoft Cloud Adoption Framework](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/?WT.mc_id=AZ-MVP-5004796 \\"Microsoft Cloud Adoption Framework for Azure\\") page has everything listed above and more! If you are serious about Cloud Adoption, then reading through the official documentation not only gives you better context to the resources linked to this page but gives you more ways to think about potential opportunities to help with your Cloud adoption!\\n\\n> Most of these tools can be found directly in the public Cloud Adoption Framework GitHub repository: [microsoft/CloudAdoptionFramework](https://github.com/microsoft/CloudAdoptionFramework/tree/master#microsoft-cloud-adoption-framework-for-azure) so keep an eye on that!\\n\\n> Make sure you follow the [Cloud Adoption Framework - Whats new page, to keep up with the current best practices](https://learn.microsoft.com/azure/cloud-adoption-framework/get-started/whats-new?WT.mc_id=AZ-MVP-5004796)"},{"id":"/2021/12/03/azure-nat-gateway","metadata":{"permalink":"/2021/12/03/azure-nat-gateway","source":"@site/blog/2021-12-03-azure-nat-gateway.md","title":"Azure NAT Gateway - Implementation and Testing","description":"With most Cloud resources being accessible over the internet, each publically accessible resource has its own public IP address, this makes it a lot more challenging to administer the security and access rules to access third party services.","date":"2021-12-03T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":7.275,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-12-03 00:00:00 +1300","title":"Azure NAT Gateway - Implementation and Testing","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Cloud Adoption Framework for Azure - Tools and Templates","permalink":"/2021/12/11/cloud-adoption-framework-for-azure"},"nextItem":{"title":"Benefits to using the Microsoft Azure Cloud to host your Infrastructure","permalink":"/2021/10/28/benefits-to-using-the-microsoft-azure-cloud-to-host-your-infrastructure"}},"content":"With most Cloud resources being accessible over the internet, each publically accessible resource has its own public IP address, this makes it a lot more challenging to administer the security and access rules to access third party services.\\n\\n_Think along the lines of - you or your organisation might use software-as-a-service CRM product. That product is only accessible from your organisations IP for compliance/security reasons, you might access the CRM product from various Azure Virtual Desktop hosts, each with its public IP or a random Microsoft Azure datacenter IP, or you want to control Multifactor authentication/conditional access policies for users using Azure services_.\\n\\nThe administration of this, particularly in scenarios where other people or teams can create and manage resources, can be complex, sure; you can use Standard Load Balancers, which would help, but you have to manage and pay for it, which is sometimes overkill.\\n\\nTunnelling outbound traffic through to a specific IP address or IP addresses to _\'known controllable IP addresses_\' for Azure resources _(both IaaS and PaaS)_ which sit in the same Virtual Network is where the Azure NAT Gateway comes in, allowing you to easily allow and control what IPs your traffic is coming from. NAT Gateway replaces the default Internet destination in the virtual network\u2019s routing table for the subnets identified\\n\\n\\"The Azure NAT gateway is a fully managed, highly resilient service built into the Azure fabric, which can be associated with one or more subnets in the same Virtual Network, that ensures that all outbound Internet-facing traffic will be routed through the gateway. As a result, the NAT gateway gives you a predictable public IP for outbound Internet-facing traffic. It also significantly increases the available [SNAT ports](https://learn.microsoft.com/en-us/azure/app-service/troubleshoot-intermittent-outbound-connection-errors?WT.mc_id=AZ-MVP-5004796) in scenarios where you have a high number of concurrent connections to the same public address/port combination.\\"\\n\\n### My Testing\\n\\nNow lets get testing the Azure NAT Gateway! To test the gateway, I created:\\n\\n* Virtual Network\\n* NAT Gateway\\n* IP Public Address prefix\\n* 1 Windows VM (Windows Server 2019) with Public IP\\n* 1 Linux (Ubuntu 18.04) VM with Public IP\\n* 1 Windows VM (Windows Server 2019) as a backend pool for an Azure Load Balancer\\n* Virtual Machine Scale Set with four instances (each with Windows Server 2019)\\n\\n_Note: Each VM has RDP opened to allow inbound traffic from my network using the Public IP and a NAT rule allowing RDP traffic on the Load Balancer. There is no point-to-site or site-to-site VPN; RDP connections are directly over the internet to Australia East, from New Zealand._\\n\\n![NAT Gateway - Test](/uploads/natgw_test.png \\"NAT Gateway - Test\\")\\n\\nOnce the Azure resources were created, I then connected to each machine using RDP/SSH on their Public IP address and tested:\\n\\n#### Linux Machine with Public IP for RDP\\n\\n* Inbound Public IP: 20.53.92.19\\n* Outbound IP: 20.53.73.184\\n\\n![Linux Azure NAT Gateway](/uploads/linux_ubuntu_nat_test.png \\"Linux Azure NAT Gateway\\")\\n\\nAs you can see, I connected to the Linux VM\'s public IP via SSH and did a curl to: [https://ifconfig.me/](https://ifconfig.me/ \\"https://ifconfig.me/\\") to grab my public IP. The public IP of my Linux box was my NAT Gateway Public IP prefix!\\n\\n#### Windows Machine with Public IP for RDP\\n\\n* Inbound Public IP: 20.70.228.211\\n* Outbound IP: 20.53.73.184\\n\\n![Windows Azure NAT Gateway](/uploads/window_nat_test.png \\"Windows Azure NAT Gateway\\")\\n\\nUsing RDP to the public IP of the Windows Server, I navigated to: [https://www.whatismyip.com/](https://www.whatismyip.com/ \\"https://www.whatismyip.com/\\"). As you can see, the Public IP of my outbound IP address was my NAT Gateway Public IP prefix!\\n\\n#### Windows Machine behind an Azure Load Balancer\\n\\n* Inbound Public IP: 20.211.100.67\\n* Outbound IP: 20.53.73.185\\n\\n![Windows Machine behind Azure Load Balancer NAT Gateway](/uploads/windows_nat_test_loadbalancer.png \\"Windows Machine behind Azure Load Balancer NAT Gateway\\")\\n\\nThis was the last of the 3 test machines; I stood up. Using RDP to the public IP of the Azure Load BalancerI navigated to: [https://www.whatismyip.com/](https://www.whatismyip.com/ \\"https://www.whatismyip.com/\\"). As you can see, the Public IP of my outbound IP address was my NAT Gateway Public IP prefix; however, this was \'20.53.73.18**5**\', which was the second IP address available in my /31 IP address prefix.\\n\\n#### Windows Machine behind a VM Scale Set\\n\\nAlthough not in the diagram, I decided to add a VM Scale Set of 4 Virtual Machines into my testing _(to save on cost, they are just Standard_B2ms machines but more than enough for my testing)_.\\n\\n![Azure NAT Gateway - VM Scale Set](/uploads/vmss_nat_test.png \\"Azure NAT Gateway - VM Scale Set\\")\\n\\nAs you can see from the mess that is my screenshot above, all machines had completely different inbound Public IP addresses. Still, the outbound public IP addresses came from the NAT Gateway as expected.\\n\\n#### Findings and Observations\\n\\n* The outbound public IP did seem to change between the workloads; if I refreshed \'_whatismyip_\' and \'_ifconfig_\', the public IP changed between 184 and 185. However, no loss of connectivity occurred to the Virtual Machines. This was linked to the \'4-minute idle timeout\' configured on the NAT Gateway; I saw no reason to change the default timeout value; if I were that worried about the same IP address - I would have chosen with a Public IP vs a Public IP prefix on the NAT Gateway.\\n* Any Public IP used on the same subnet as a NAT Gateway needs to be Standard.\\n* If I had both a Public IP address and a Public IP prefix on my NAT gateway, the Prefix seemed to take precedence.\\n* You cannot use a Public IP Prefix that is in use by the NAT Gateway for any other workload, _i.e. any inbound Public IPs. It would be best if you had another Public IP prefix resource._\\n* A single [NAT gateway resource](https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/nat-gateway-resource?WT.mc_id=AZ-MVP-5004796) supports from 64,000 up to 1 million concurrent flows. Each IP address provides 64,000 SNAT ports to the available inventory. Therefore, you can use up to 16 IP addresses per NAT gateway resource. The SNAT mechanism is described [here](https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/nat-gateway-resource?WT.mc_id=AZ-MVP-5004796#source-network-address-translation) in more detail.\\n\\n### Create a NAT Gateway\\n\\nTo create my NAT Gateway, I used the ARM Quickstart template, located here: [https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/quickstart-create-nat-gateway-template](https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/quickstart-create-nat-gateway-template?WT.mc_id=AZ-MVP-5004796 \\"https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/quickstart-create-nat-gateway-template?WT.mc_id=AZ-MVP-5004796\\").\\n\\nThen I created the additional Virtual Machines and Load Balancers and added them to the same VNET created as part of the NAT Gateway.\\n\\n#### **To create a NAT Gateway using the Azure Portal**\\n\\n 1. Log in to the **Azure Portal** and navigate to **Create a resource**, **NAT Gateway** (this link will get you there: [Create-NATGateway](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2Fazure-quickstart-templates%2Fmaster%2Fquickstarts%2Fmicrosoft.network%2Fnat-gateway-1-vm%2Fazuredeploy.json \\"Create network address translation (NAT) gateway\\")).\\n 2. Select your **Subscription**\\n 3. **Enter** your NAT **Gateway name**\\n 4. Enter your **Region**\\n 5. Enter your availability zone\\n 6. Set your idle timeout _(I suggest leaving this at 4 minutes, you can change it later if it presents issues)_\\n 7. ![Create Azure NAT Gateway](/uploads/create_natgateway1.png \\"Create Azure NAT Gateway\\")\\n 8. Click **Next: Outbound IP**\\n 9. We are just going to **create** a new **Public IP address** (_it has to be Standard and Static, the Azure Portal automatically selects this for you - although you can create your Public IP prefix here for scalability, you don\'t need it both)_.\\n10. ![Create Azure NAT Gateway](/uploads/create_natgateway2.png \\"Create Azure NAT Gateway\\")\\n11. Click **Next: Subnet**\\n12. Create or **link** your existing **Virtual Network** and **subnets** and click **Next: Tags**\\n13. **Enter** in any **tags** that may be relevant _(Creator, Created on, Created for, Support Team etc.)_\\n14. Click **Next: Review  + Create**\\n15. Verify everything looks ok then click **Create**\\n\\n**Congratulations, you have now created your NAT Gateway!**\\n\\n#### **To create a NAT Gateway using Azure Bicep**\\n\\nJust a quick Bicep snippet I created to create the NAT Gateway resource only:\\n\\n```bicep title=\\"Create-NATGateway.bicep\\"\\n\\n//Target Scope is: Resource Group\\n\\ntargetScope = \'resourceGroup\'\\n\\n//Set Variables and Parameters\\n\\n@allowed([\\n  \'Prod\'\\n  \'Dev\'\\n])\\nparam environment string = \'Prod\'\\nparam location string = resourceGroup().location\\n\\nparam dateTime string = utcNow(\'d\')\\nparam resourceTags object = {\\n  Application: \'Azure NAT Gateway/Azure Network Management\'\\n  CostCenter: \'Operational\'\\n  CreationDate: dateTime\\n  Environment: environment\\n}\\n\\n//// Resource Creation\\n\\n/// Create - NAT Gateway\\n\\nresource NATGW \'Microsoft.Network/natGateways@2021-03-01\' = {\\n  name: \'aznatgw\'\\n  tags: resourceTags\\n\\n  location: location\\n  sku: {\\n    name: \'Standard\'\\n  }\\n\\n  properties: {\\n    idleTimeoutInMinutes: 4\\n  }\\n}\\n```\\n\\nIt can be deployed by opening PowerShell _(after_ [_Bicep is installed_](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install?WT.mc_id=AZ-MVP-5004796#windows \\"Install Bicep tools\\")_using the PowerShell method)_and logging into your Azure and running the following_(replace RGNAME with the name of the Resource Group you will be deploying it to)_:\\n\\n_When you are actually ready to deploy, remove the -Whatif at the end. Then you can go into the resource and add the Public IP/prefix. PowerShell will prompt you for the name of the NAT Gateway and be created in the same location as the Resource Group by default._\\n\\n    New-AzResourceGroupDeployment -Name NatGwDeployment -ResourceGroupName RGNAME -TemplateFile .\\\\Create_NATGateway.bicep -whatif\\n\\n### Additional Resources\\n\\n* [What is Virtual Network NAT?](https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/nat-overview?WT.mc_id=AZ-MVP-5004796)\\n* [Design Virtual Networks that use NAT gateway resources](https://learn.microsoft.com/en-us/azure/virtual-network/nat-gateway/nat-gateway-resource?WT.mc_id=AZ-MVP-5004796)\\n* [NAT Gateway Pricing](https://azure.microsoft.com/en-us/pricing/details/virtual-network/?WT.mc_id=AZ-MVP-5004796#pricing \\"Azure NAT Gateway Pricing\\")"},{"id":"/2021/10/28/benefits-to-using-the-microsoft-azure-cloud-to-host-your-infrastructure","metadata":{"permalink":"/2021/10/28/benefits-to-using-the-microsoft-azure-cloud-to-host-your-infrastructure","source":"@site/blog/2021-10-28-benefits-to-using-the-microsoft-azure-cloud-to-host-your-infrastructure.md","title":"Benefits to using the Microsoft Azure Cloud to host your Infrastructure","description":"Cloud computing offers many benefits, from your traditional on-premises infrastructure, ecosystems such as Microsoft Azure, have an underlying fabric built for today\'s \'software as a service\' or \'software defined\' world.","date":"2021-10-27T11:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.505,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-10-27T11:00:00.000Z","title":"Benefits to using the Microsoft Azure Cloud to host your Infrastructure","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Azure NAT Gateway - Implementation and Testing","permalink":"/2021/12/03/azure-nat-gateway"},"nextItem":{"title":"Always on VPN - Error 809 The network connection between your computer and the VPN server could not be established ","permalink":"/2021/08/25/always-on-vpn-error-809-the-network-connection-between-your-computer-and-the-vpn-server-could-not-be-established"}},"content":"Cloud computing offers many benefits, from your traditional on-premises infrastructure, ecosystems such as Microsoft Azure, have an underlying fabric built for today\'s \'software as a service\' or \'software defined\' world.\\n\\nThe shift of technologies from managing on-premises Exchange environments for mail to consuming Microsoft 365 services has allowed more time for the IT and businesses to adopt, consume and improve their technology and continuously improve - to get the most use of it and remain competitive in this challenging world.\\n\\nBelow is a high-level list of what I consider some of the benefits of using the Microsoft Azure ecosystem:\\n\\n* Each Azure datacentre \'region\' has 3 Availability Zones, each zone acts as a separate datacentre, giving redundant power and networking services, quickly allowing you to separate your services across different fault domains and zones, providing better resiliency, while also giving you the ability to keep them logically and physically close together.\\n* Geo-redundant replication of backups for Virtual Machines, PaaS/File Shares, and ability to do cross-region restore functionality (i.e., Australia/Australia East).\\n* A multitude of hosts (supporting both AMD and Intel workloads), which are continually patched and maintained, and tuned for virtualisation performance, stability and security, no longer do we need to spend hours patching, maintaining, licensing on-premises hypervisors, ever so increasing as these systems get targeted for vulnerabilities and architecting how many physical hosts, we may need to support a system.\\n* Consistent, up-to-date hardware, no need to worry about lead times for new hardware, purchasing new hardware every three years and procurement and implementation costs of hardware, allowing you to spend the time improving on the business and tuning your services _(scaling up and down, trying new technologies, turning off devices etc.)_\\n* For those that like to hoard every file that ever existed, the Azure platform allows scale _(in and out to suit your file sizes)_ along with cost-saving opportunities and tweaks with Automation and migrating files between cool/hot tiers.\\n* No need to pay datacentre hosting costs\\n* No need to worry about redundant switching\\n* With multiple hosts, there is no risk around air conditioning leaks, hardware failure; you don\'t need to worry about some of these unfortunate events occurring.\\n* No need to pay electricity costs to host your workloads.\\n* Reduced IT labour costs and time to implement and maintain systems\\n* OnDemand resources available can stand up separate networks unattached to your production network for testing or other devices easily without working out through VLANs or complex switching and firewalls.\\n* Azure Network have standard DDOS protection enabled by default\\n* Backups are secure by default; they are offline and managed by Microsoft, so if a ransomware attack occurs, won\'t be able to touch your backups.\\n* Constant Security recommendations, improvements built into the platform.\\n* Azure Files is geo-redundant and across multiple storage arrays, encrypted at rest.\\n* Windows/SQL licensing is all covered as part of the costings, so need to worry about not adhering to MS licensing, Azure helps simplify what can sometimes be confusing and complex licensing.\\n* Extended security updates for out-of-date Server OS such as Windows Server 2008 R2, Windows Server 2021 R2 without having to pay for extended update support.\\n* Ability to leverage modern and remote desktop and application technologies such as Windows 365 and Azure Virtual Desktop, by accessing services hosted in Azure.\\n* Having your workloads in Azure gives you a step towards, removing the need for traditional domain controllers and migrating to Microsoft Entra ID joined devices.\\n* Azure AutoManage functionality is built in to automatically patch Linux (and Windows of course!), without having to manage separate patching technologies for cross-platform infrastructure.\\n* Azure has huge support for Automation, via PowerShell, CLI and API, allowing you to standardize, maintain, tear down and create infrastructure and services, monitoring, self-users on an as needed basis.\\n* Azure datacentres are sustainable and run off renewable energy where they can, Microsoft has commitments to be fully renewable.\\n* No need for NAS or Local Backups, the backups are all built into Azure.\\n* Compliant datacentre across various global security standards - [https://learn.microsoft.com/en-us/compliance/assurance/assurance-datacenter-security](https://learn.microsoft.com/en-us/compliance/assurance/assurance-datacenter-security?WT.mc_id=AZ-MVP-5004796 \\"https://learn.microsoft.com/en-us/compliance/assurance/assurance-datacenter-security?WT.mc_id=AZ-MVP-5004796\\")\\n* Ability to migrate or expand your resources from Australia to \u2018NZ North\u2019 or other new or existing data centres! Azure is global and gives you the ability to scale your infrastructure to a global market easily or bring your resources closer to home if a data centre becomes available.\\n* We all know that despite the best of intentions, we rarely ever test, develop, and improve disaster recovery scenarios, sometimes this is because of the complexity of the applications and backup infrastructure. Azure Site Recovery, Geo-Redundant backup, Load Balancers and automation helps make this a lot easier.\\n* Ability to better utilise Cloud security tools _(ie such as the Azure Security Center)_, across Cloud and on-premises workloads consistently using Azure Arc and Azure policies.\\n* And finally - more visibility into the true cost and value of your IT infrastructure, the total cost of your IT Infrastructure is hidden behind electricity costs, outages and incidents that would not have impacted cloud resources, slow time to deployment or market, outdated and insecure technologies and most likely services you are running which you don\'t need to run!\\n\\n\\\\#ProTip - Resources such as the [Azure Total Cost of Ownership (TCO)](https://azure.microsoft.com/en-us/pricing/tco/calculator/?WT.mc_id=AZ-MVP-5004796 \\"Total Cost of Ownership (TCO) Calculator\\") can help you calculate the true cost of your workloads."},{"id":"/2021/08/25/always-on-vpn-error-809-the-network-connection-between-your-computer-and-the-vpn-server-could-not-be-established","metadata":{"permalink":"/2021/08/25/always-on-vpn-error-809-the-network-connection-between-your-computer-and-the-vpn-server-could-not-be-established","source":"@site/blog/2021-08-25-always-on-vpn-error-809-the-network-connection-between-your-computer-and-the-vpn-server-could-not-be-established.md","title":"Always on VPN - Error 809 The network connection between your computer and the VPN server could not be established ","description":"I ran into a weird issue, troubleshooting an \'Always On VPN\' installation running off Windows Server 2019, the clients were getting the following error in the Application event log:","date":"2021-08-25T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":2.57,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-08-25 00:00:00 +1200","title":"Always on VPN - Error 809 The network connection between your computer and the VPN server could not be established ","authors":["Luke"],"tags":["Windows"],"toc":true,"header":{"teaser":"/uploads/windows-server.jpg"}},"unlisted":false,"prevItem":{"title":"Benefits to using the Microsoft Azure Cloud to host your Infrastructure","permalink":"/2021/10/28/benefits-to-using-the-microsoft-azure-cloud-to-host-your-infrastructure"},"nextItem":{"title":"Update-AdmPwdAdSchema - The requested attribute does not exist","permalink":"/2021/08/21/update-admpwdadschema-the-requested-attribute-does-not-exist"}},"content":"I ran into a weird issue, troubleshooting an \'Always On VPN\' installation running off Windows Server 2019, the clients were getting the following error in the Application event log:\\n\\n> Error 809 The network connection between your computer and the VPN server could not be established\\n\\nIn my case, the issue wasn\'t due to [IKEv2 Fragmentation](https://directaccess.richardhicks.com/2019/02/14/troubleshooting-always-on-vpn-error-code-809/ \\"Troubleshooting Always On VPN Error Code 809\\") or anything to do with [NAT](https://directaccess.richardhicks.com/2020/04/13/always-on-vpn-ikev2-load-balancing-and-nat/ \\"Always On VPN IKEv2 Load Balancing and NAT\\") to allow the origin IP to flow to the Always-on VPN server. It was due to the ports being limited to: \'2\'. I found an old post regarding Windows Server 2008 R2: \\n\\n* [The maximum number of WAN Miniport (IKEv2) ports changes from 128 to two after you install Windows Server 2008 R2 SP1](https://support.microsoft.com/en-us/topic/the-maximum-number-of-wan-miniport-ikev2-ports-changes-from-128-to-two-after-you-install-windows-server-2008-r2-sp1-15aeb929-abe9-ece0-5d71-d2223d6a94d0 \\" The maximum number of WAN Miniport (IKEv2) ports changes from 128 to two after you install Windows Server 2008 R2 SP1\\")\\n\\n> \\"If more than two clients try to connect to the server at the same time, the Routing and Remote Access service rejects the IKEv2 connection requests. Additionally, the following message is logged in the Rastapi.log file:\\"\\n\\nThis matched my issue; I had never seen more than 2 connections at once.\\n\\n### Increase Ports\\n\\n 1. Open **Routing and Remote Access**\\n 2. **Click** on your Routing and **Remote Access server**\\n 3. **Right**-**click** on **Ports**\\n 4. Click on: **WAN Miniport (IKEv2)**\\n 5. Click **Configure**\\n 6. **Ensure** that: To **enable remote access**, select Remote access **connections** (inbound only) is **checked.**\\n 7. **Change Maximum ports** from 2 _(as an example)_ to a number that matches how many connections you want - I went with **128**\\n 8. Click **Ok**\\n 9. Click **Apply**\\n10. **Restart** the Routing and Remote Access **server. You** should now see more ports listed \'as inactive\' until a new session comes along and uses it.\\n\\n![Routing and Remote Access](/uploads/wan_miniport_ikev2.png \\"Routing and Remote Access\\")\\n\\n![Routing and Remote Access](/uploads/wan_miniport_ports.png \\"Routing and Remote Access\\")\\n\\n### Enable TLS 1.1\\n\\nAlthough this wasn\'t my initial fix, I had a Microsoft Support call opened regarding this issue; after analysing the logs, they recommended enabling TLS 1.1 _(which was disabled by default on a Windows Server 2019 server)_. I would only do this as a last resort - if required.\\n\\nRun the PowerShell script below (as Administrator) to Enable; you can always rerun the Disable script to remove the changes.\\n\\n#### Enable TLS 1.1\\n\\n    function enable-tls-1.1\\n    {\\n        New-Item \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Server\' -Force\\n        New-Item \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Client\' -Force\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Server\' -name \'Enabled\' -value \'1\' \u2013PropertyType \'DWORD\'\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Server\' -name \'DisabledByDefault\' -value \'0\' \u2013PropertyType \'DWORD\'\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Client\' -name \'Enabled\' -value \'1\' \u2013PropertyType \'DWORD\'\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Client\' -name \'DisabledByDefault\' -value \'0\' \u2013PropertyType \'DWORD\'\\n        Write-Host \'Enabling TLSv1.1\'\\n    }\\n    enable-tls-1.1\\n\\n#### Disable TLS 1.1\\n\\n    function disable-tls-1.1\\n    {\\n        New-Item \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Server\' -Force\\n        New-Item \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Client\' -Force\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Server\' -name \'Enabled\' -value \'0\' \u2013PropertyType \'DWORD\'\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Server\' -name \'DisabledByDefault\' -value \'1\' \u2013PropertyType \'DWORD\'\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Client\' -name \'Enabled\' -value \'0\' \u2013PropertyType \'DWORD\'\\n        New-ItemProperty -Path \'HKLM:\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\SecurityProviders\\\\SCHANNEL\\\\Protocols\\\\TLS 1.1\\\\Client\' -name \'DisabledByDefault\' -value \'1\' \u2013PropertyType \'DWORD\'\\n        Write-Host \'Disabling TLSv1.1\'\\n    }\\n    disable-tls-1.1"},{"id":"/2021/08/21/update-admpwdadschema-the-requested-attribute-does-not-exist","metadata":{"permalink":"/2021/08/21/update-admpwdadschema-the-requested-attribute-does-not-exist","source":"@site/blog/2021-08-21-update-admpwdadschema-the-requested-attribute-does-not-exist.md","title":"Update-AdmPwdAdSchema - The requested attribute does not exist","description":"Are you attempting to update the Active Directory Schema for LAPS (Local Administrator Password Solution) and keep getting the error below?","date":"2021-08-21T00:00:00.000Z","tags":[{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":0.455,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-08-21 00:00:00 +1200","title":"Update-AdmPwdAdSchema - The requested attribute does not exist","authors":["Luke"],"tags":["PowerShell"],"toc":false,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Always on VPN - Error 809 The network connection between your computer and the VPN server could not be established ","permalink":"/2021/08/25/always-on-vpn-error-809-the-network-connection-between-your-computer-and-the-vpn-server-could-not-be-established"},"nextItem":{"title":"AVD-Collect - Azure Virtual Desktop Diagnostics and Logging","permalink":"/powershell/avd-collect"}},"content":"Are you attempting to update the Active Directory Schema for LAPS (Local Administrator Password Solution) and keep getting the error below?\\n\\n**Update-AdmPwdAdSchema: The requested attribute does not exist**\\n\\nHere are few things you can check:\\n\\n* Make sure you are a Schema Admin\\n* Run PowerShell as Administrator\\n* Run the PowerShell to update the schema directly from the Schema Master\\n\\nYou can use the snippet below to check which Domain Controller the Schema Master role is running from:\\n\\n    Get-ADDomainController -Filter * | Select-Object Name, Domain, Forest, OperationMasterRoles | Where-Object {$_.OperationMasterRoles}"},{"id":"powershell/avd-collect","metadata":{"permalink":"/powershell/avd-collect","source":"@site/blog/2021-08-08-avd-collect.md","title":"AVD-Collect - Azure Virtual Desktop Diagnostics and Logging","description":"AVD-Collect is a handy PowerShell script created by Microsoft Customer Support Services to assist with troubleshooting and resolving issues with Azure Virtual Desktop (and Windows 365), by capturing Logs for analysis (which could then be passed to Microsoft or allow you to delve deeper) and running basic Diagnostics against some common known issues.","date":"2021-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":6.015,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"AVD-Collect - Azure Virtual Desktop Diagnostics and Logging","authors":["Luke"],"tags":["PowerShell"],"date":"2021-08-08 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/avd-collect-diagreport.png"},"slug":"powershell/avd-collect"},"unlisted":false,"prevItem":{"title":"Update-AdmPwdAdSchema - The requested attribute does not exist","permalink":"/2021/08/21/update-admpwdadschema-the-requested-attribute-does-not-exist"},"nextItem":{"title":"Implement WebJEA for self-service Start/Stop of Azure Virtual Machines","permalink":"/2021/07/18/implement-webjea-for-self-service-azure-resource-creation-with-powershell"}},"content":"AVD-Collect is a handy PowerShell script created by Microsoft Customer Support Services to assist with troubleshooting and resolving issues with Azure Virtual Desktop (and Windows 365), by capturing Logs for analysis _(which could then be passed to Microsoft or allow you to delve deeper)_ and running basic Diagnostics against some common known issues.\\n\\nYou can download this script from: [https://aka.ms/avd-collect](https://aka.ms/avd-collect \\"https://aka.ms/avd-collect?WT.mc_id=AZ-MVP-5004796\\")\\n\\n> There is no publically avaliable github repository for it currently, Microsoft will retain the latest version of the script at this link.\\n>\\n> This script was NOT created by me and comes \'As/Is\', this article is merely intended to share the script to assit others in their AVD troubleshooting.\\n>\\n> This script is intended to help support Microsoft Customer Support with assisting customers, but was made publically accessible to assist with MS Support cases and Azure Virtual Desktop diagnostics. No data is automatically uploaded to Microsoft.\\n>\\n> Please be aware that the script may change and include new functionality not part of this article, please review the Changelog and Readme of the script directly.\\n\\nA lot of the information below is contained in the script readme _(including a list of the extensive diagnostics and log locations)_ and changelog; however, I am supplying this article for reference and to help share this nifty tool.\\n\\n### Script pre-requisites\\n\\n1. The script must be run with elevated permissions to collect all required data.\\n2. All collected data will be archived into a .zip file located in the same folder as the script itself.\\n3. As needed, run the script on AVD host VMs and/or Windows-based devices from where you connect to the AVD hosts.\\n4. When launched, the script will present the Microsoft Diagnostic Tools End User License Agreement (EULA). You need to accept the EULA before you can continue using the script.\\n5. If the script does not start, complaining about execution restrictions, then in an elevated PowerShell console run:\\n\\n       \\tSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Force -Scope Process\\n\\n_Acceptance of the EULA will be stored in the registry under HKCU\\\\\\\\Software\\\\\\\\Microsoft\\\\\\\\CESDiagnosticTools, and you will not be prompted again to accept it as long as the registry key is in place._ _You can also use the \\"-AcceptEula\\" command line parameter to accept the EULA silently._ _This is a per-user setting, so each user running the script will accept the EULA once._\\n\\n### Script scenarios\\n\\n#### Core - suitable for troubleshooting issues that do not involve Profiles or Teams or MSIX App Attach\\n\\n* Collects core troubleshooting data without including Profiles/FSLogix/OneDrive or Teams or MSIXAA related data\\n* Runs Diagnostics.\\n\\n#### Core + Profiles - suitable for troubleshooting Profiles issues\\n\\n* Collects all Core data\\n* Collects Profiles/FSLogix/OneDrive related information, as available\\n* Runs Diagnostics. \\n\\n#### Core + Teams - suitable for troubleshooting Teams issues\\n\\n* Collects all Core data\\n* Collects Teams related information, as available\\n* Runs Diagnostics.\\n\\n#### Core + MSIX App Attach - suitable for troubleshooting MSIX App Attach issues\\n\\n* Collects all Core data\\n* Collects MSIX App Attach related information, as available\\n* Runs Diagnostics.\\n\\n#### Core + MSRA - suitable for troubleshooting Remote Assistance issues\\n\\n* Collects all Core data\\n* Collects Remote Assistance related information, as available\\n* Runs Diagnostics.\\n\\n#### Extended (all) - suitable for troubleshooting most issues, including Profiles/FSLogix/OneDrive, Teams and MSIX App Attach\\n\\n* Collects all Core data\\n* Collects Profiles/FSLogix/OneDrive related information, as available\\n* Collects Microsoft Teams related information, as available\\n* Collects MSIX App Attach related information, as available\\n* Runs Diagnostics.\\n\\n#### DiagOnly\\n\\n* Skips all Core/Extended data collection and runs Diagnostics only _(regardless of any other parameters that have been specified)_.\\n\\n**The default scenario is \\"Core\\".\u200b\u200b\u200b\u200b\u200b\u200b\u200b**\\n\\n#### Available command line parameters (to preselect the desired scenario)\\n\\n* Core - Collects Core data + Runs Diagnostics\\n* Extended - Collects all Core data + Extended _(Profiles/FSLogix/OneDrive, Teams, MSIX App Attach)_ data + Runs Diagnostics\\n* Profiles - Collects all Core data + Profiles/FSLogix/OneDrive data + Runs Diagnostics\\n* Teams - Collects all Core data + Teams data + Runs Diagnostics\\n* MSIXAA - Collects all Core data + MSIX App Attach data + Runs Diagnostics\\n* MSRA - Collects all Core data + Remote Assistance data + Runs Diagnostics\\n* DiagOnly - The script will skip all data collection and will only run the diagnostics part _(even if other parameters have been included)_.\\n* AcceptEula - Silently accepts the Microsoft Diagnostic Tools End User License Agreement.\\n\\n### Usage example with parameters\\n\\nTo collect only Core data (excluding Profiles/FSLogix/OneDrive, Teams, MSIX App Attach):\\n\\n    \\t.\\\\AVD-Collect.ps1 -Core\\n\\nTo collect Core + Extended data (incl. Profiles/FSLogix/OneDrive, Teams, MSIX App Attach):\\n\\n    \\t.\\\\AVD-Collect.ps1 -Extended\\n\\nTo collect Core + Profiles + MSIX App Attach data\\n\\n    \\t.\\\\AVD-Collect.ps1 -Profiles -MSIXAA\\n\\nTo collect Core + Profiles data\\n\\n    \\t.\\\\AVD-Collect.ps1 -Profiles\\n\\n\u200b\u200b\u200b\u200b\u200b\u200b\u200bIf you are missing any of the data that the script should normally collect, check the content of the \\"__AVD-Collect-Log.txt\\" and \\"__AVD-Collect-Errors.txt\\" files for more information. Some data may not be present during data collection and thus not picked up by the script.\\n\\n### Execute the script\\n\\n 1. **Download** the AVD-Collect **script** to the session host you need to collect the logs from, if you haven\'t already.\\n 2. **Extract** the **script** to a folder _(i.e. C:\\\\\\\\Users\\\\\\\\%username&\\\\\\\\Downloads\\\\\\\\AVD-Collect)_\\n 3. Right-click on: AVD-Collect.ps1, select Properties\\n 4. Because this file has been downloaded from the Internet, it may be in a protected/block status - select **Unblock** and click **Apply**\\n 5. Open Windows Powershell as Administrator\\n 6. Now we need to **change** the **directory** for where the script is located; in my example, the command I use is:\\n\\n        cd \'C:\\\\Users\\\\Luke\\\\Downloads\\\\AVD-Collect\'\\n 7. By default, the script will run as \'Core\', and I want to include everything, profiles, Teams etc., so **run** Extended:  \\n\\n        .\\\\AVD-Collect.ps1 -Extended -AcceptEula\\n 8. **Read** the notice from the Microsoft Customer Support centre and press \'**Y**\' if you **accept** to move onto the next steps.\\n 9. The **script** will now **run**:\\n10. ![AVD- Script Running](/uploads/avd-collect_running.png \\"AVD- Script Running\\")\\n11. You will start to see new folders get created in the directory that the script is running from with the extracted log files. The script will take a few minutes to complete as it extracts the logs and then zips them.\\n12. Once the script has ran, there will now be a **ZIP file** of all the Logs collected by the script. In my example, the **logs** consisted of:\\n\\n* Certificates\\n* Recent Event Log\\n* FSLogix logs\\n* Networking\\n* Registry Keys\\n* Teams information\\n* System information\\n* Networking and Firewall information\\n\\n13. ![AVD-Collect Logs](/uploads/avd-collect-postrun.png \\"AVD-Collect Logs\\")\\n14. If needed, you can now **send** or upload the **ZIP** file to Microsoft **support**. If you are troubleshooting yourself, you can navigate to the folders to look at the specific logs you want, all in one place!\\n15. To **look** at **Diagnostic** information, open the: **AVD-Diag.html** file.\\n16. You can now see a list of common issues, what the script is looking for, and whether the host has passed or failed these scripts _(this can be very useful for Azure Virtual Desktop hosts, to make sure all the standard configuration is done or being applied, including making sure that the session host has access to all the external resources it needs_):\\n17. ![AVD-Collect Diagnostics](/uploads/avd-collect-diagreport.png \\"AVD-Collect Diagnostics\\")"},{"id":"/2021/07/18/implement-webjea-for-self-service-azure-resource-creation-with-powershell","metadata":{"permalink":"/2021/07/18/implement-webjea-for-self-service-azure-resource-creation-with-powershell","source":"@site/blog/2021-07-18-implement-webjea-for-self-service-azure-resource-creation-with-powershell.md","title":"Implement WebJEA for self-service Start/Stop of Azure Virtual Machines","description":"WebJEA allows you to build web forms for any PowerShell script dynamically. WebJEA automatically parses the script at page load for description, parameters and validation, then dynamically builds a form to take input and display formatted output!","date":"2021-07-18T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":14.24,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Implement WebJEA for self-service Start/Stop of Azure Virtual Machines","authors":["Luke"],"tags":["Azure"],"date":"2021-07-18 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/webjea_startstopazvm.png"}},"unlisted":false,"prevItem":{"title":"AVD-Collect - Azure Virtual Desktop Diagnostics and Logging","permalink":"/powershell/avd-collect"},"nextItem":{"title":"Well-Architected Framework Azure infrastructure review with PSRule for Azure","permalink":"/2021/07/13/validate-that-your-azure-resources-are-deployed-per-the-well-architected-framework"}},"content":"WebJEA allows you to build web forms for any PowerShell script dynamically. WebJEA automatically **parses the script at page load for description, parameters and validation**, **then dynamically builds a form to take input and display formatted output!**\\n\\nThe main goals for WebJEA:\\n\\n* Reduce delegation of privileged access to users\\n* Quickly automate on-demand tasks and grant access to less-privileged users\\n* Leverage your existing knowledge in PowerShell to build web forms and automate on-demand processes\\n* Encourage proper script creation by parsing and honouring advanced function parameters and comments\\n\\nBecause WebJEA is simply a Self-Service Portal for PowerShell scripts, anything you can script with PowerShell you can run through the Portal! Opening a lot of opportunities for automation without having to learn third party automation toolsets! Anyone who knows PowerShell can use it! Each script can be locked down to specific users and AD groups!\\n\\nYou can read more about WebJEA directly on the GitHub page: [https://github.com/markdomansky/WebJEA](https://github.com/markdomansky/WebJEA \\"https://github.com/markdomansky/WebJEA\\").\\n\\nThis guide will concentrate on setting up WebJEA for self-service Azure VM management. However, WebJEA can be used to enable much more than what this blog article covers, from things such as new user onboarding, to resource creation.\\n\\n![WebJEA - Start/Stop](/uploads/webjea_startstopazvm.png)\\n\\nWe will use a Windows Server 2019, running in Microsoft Azure, to run WebJEA from.\\n\\n### Prerequisites\\n\\n* Domain Joined server running Windows 2016+ Core/Full with PowerShell 5.1\\n* The server must have permission to go out over the internet to Azure and download PowerShell modules.\\n* CPU/RAM Requirements will depend significantly on your usage, start low _(2-vCPU/4GB RAM)_ and grow as needed.\\n\\nI\'ve created a Standard_B2ms _(2vCPU, 8GB RAM)_ virtual machine, called: WEBJEA-P01 in an Azure Resource Group called: webjea_prod\\n\\nThis server is running: Windows Server 2019 Datacenter and is part of my Active Directory domain; I\'ve also created a service account called: webjea_services.\\n\\n### Setup WebJEA\\n\\nOnce we have a Windows Server, now it\'s time to set up WebJEA!\\n\\n#### Setup Self-Signed Certificate\\n\\nIf you already have a certificate you can use, skip this step. In the case of this guide, we are going to use a self-signed certificate.\\n\\n**Log** into the WebJEA **Windows server** using your service account _(in my case, it is: luke\\\\\\\\webjea_services)_.\\n\\nOpen **PowerShell** ISE as Administrator, and after replacing the DNS name to suit your own environment, **run the following to create the Root CA and Self-Signed certificate**:\\n\\nNow that the Root CA is created and trusted, we want to create the actual self-signed certificate:\\n\\n    #Create RootCA\\n    $rootCA = New-SelfSignedCertificate -Subject \\"CN=MyRootCA\\"  `\\n    -KeyExportPolicy Exportable  `\\n    -KeyUsage CertSign,CRLSign,DigitalSignature  `\\n    -KeyLength 2048  `\\n    -KeyUsageProperty All  `\\n    -KeyAlgorithm \'RSA\'  `\\n    -HashAlgorithm \'SHA256\'  `\\n    -Provider \\"Microsoft Enhanced RSA and AES Cryptographic Provider\\"  `\\n    -NotAfter (Get-Date).AddYears(10)\\n    \\n    #Create Self-Signed Certificate\\n    $cert = New-SelfSignedCertificate -Subject \\"CN=WEBJEA-P01.luke.geek.nz\\"  `\\n    -Signer $rootCA  `\\n    -KeyLength 2048  `\\n    -KeyExportPolicy Exportable  `\\n    -DnsName WEBJEA-P01.luke.geek.nz, WEBJEA, WEBJEA-P01  `\\n    -KeyAlgorithm \'RSA\'  `\\n    -HashAlgorithm \'SHA256\'  `\\n    -Provider \\"Microsoft Enhanced RSA and AES Cryptographic Provider\\"  `\\n    -NotAfter (Get-Date).AddYears(10)\\n    $certhumbprint = $cert.Thumbprint\\n    \\n    #Add Root CA to Trusted Root Authorities\\n    New-Item -ItemType Directory \'c:\\\\WebJea\\\\certs\' -Force\\n    Export-Certificate -Cert $rootCA -FilePath \\"C:\\\\WebJEA\\\\certs\\\\rootCA.crt\\" -Force\\n    Import-Certificate -CertStoreLocation \'Cert:\\\\LocalMachine\\\\Root\' -FilePath \\"C:\\\\WebJEA\\\\certs\\\\rootCA.crt\\"\\n    \\n    Write-Host -ForegroundColor Green -Object \\"Copy this: $certhumbprint - The Thumbprint is needed for the DSCDeploy.ps1 script\\"\\n\\n**Copy** the **Thumbprint** _(if you do this manually, make sure it is the Thumbprint of the certificate, not the Trusted Root CA certificate)_; we will need that later.\\n\\n#### Setup a Group Managed Service Account\\n\\nThis is the **account** we will use to **run WebJEA under**; it **can** be a normal **Active Directory** user **account** if you feel more comfortable or want to assign permissions to.\\n\\nI am using a normal AD (Active Directory) service account in this guide because I am using Microsoft Entra ID Domain Services as my Domain Controller, and GMSA is not currently supported. I have also seen some scripts require the ability to create and read user-specific files. However, it\'s always good to follow best practices where possible.\\n\\n_Note: Group Managed Services accounts automatically renew and update the passwords for the accounts; they allow for additional security. You can read more about them here:_ [_Group Managed Service Accounts Overview_](https://learn.microsoft.com/en-us/windows-server/security/group-managed-service-accounts/group-managed-service-accounts-overview?WT.mc_id=AZ-MVP-5004796 \\"Group Managed Service Accounts Overview\\")_._\\n\\n    #Create A group MSA account\\n    Add-kdsrootkey -effectivetime ((get-date).addhours(-10))\\n    New-ADServiceAccount -name webjeagmsa1 -dnshostname (get-addomaincontroller).hostname -principalsallowedtoretrievemanagedpassword WEBJEA-P01.luke.geek.nz\\n    \\n    #Create AD Group\\n    New-ADGroup -Name \\"WebJEAAdmins\\" -SamAccountName WebJEAAdmins -GroupCategory Security -GroupScope Global -DisplayName \\"WebJEA - Admins\\" -Description \\"Members of this group are WebJEA Admins\\"\\n    \\n    Install-adserviceaccount webjeagmsa1\\n    Add-ADGroupmember -identity \\"luke.geek.nz\\\\WebJEAAdmins\\" -members (get-adserviceaccount webjeagmsa1).distinguishedname\\n\\n**Add** the **WebJEAAdmins group** to the Administrators group of your WebJEA server.\\n\\n#### Install WebJEA\\n\\n**Download** the **latest** [release package](https://github.com/markdomansky/WebJEA/releases \\"WebJEA - Releases\\") _(zip file)_ onto the **WebJEA** Windows server\\n\\nExtract it, and you should have 2 files and 2 folders:\\n\\n* Site\\\\\\\\\\n* StarterFiles\\\\\\\\\\n* DSCConfig.inc.ps1\\n* DSCDeploy.ps1\\n\\n  Open PowerShell ISE as Administrator and open DSCDeploy.ps1\\n\\n**WebJEA uses PowerShell DSC _(Desired State Configuration)_ to set up a lot of the setup**.\\n\\nDSC will do the following for us:\\n\\n* Install IIS\\n* Create the App Pool and set the identity\\n* Create and migrate the Site files to the IIS website folder\\n* Configure SSL (if we were using it)\\n* Update the WebJEA config files to point towards the script and log locations\\n\\nEven though most of the work will be automated for us by Desired State Configuration, we have to do some configurations to work in our environment.\\n\\nI am not using a Group Managed Service Account. Instead, I will use a normal AD account as a service account _(i.e. webjea_services)_, but if you use a GMSA, you need to put the username in the AppPoolUserName; no credentials are needed _(make sure the GMSA has access to the server)_.\\n\\n**Change** the **following variables** to suit **your setup**; in my case, I have moved WebJEA resources to their own folder, so it\'s not sitting directly on the OS drive, but until its own Folder.\\n\\n| Variable | Note |\\n| --- | --- |\\n| NodeName | This is a DSC variable, leave this. |\\n| WebAppPoolName | WebApp Pool Name, it may be best to leave this as: WebJEA, however you   can change this. |\\n| AppPoolUserName | Add in your GMSA or Domain Service account username |\\n| AppPoolPassword | If using a Domain Account, add the password in here, if GSMA leave bank |\\n| WebJEAIISURI | This is the IIS URL, ie server/WebJEA. You can change this if you want. |\\n| WebJEAIISFolder | IIS folder location, this can be changed if you wanted to move IIS to   another drive or location. |\\n| WebJEASourceFolder | The source folder, this is the source folder for the WebJEA files when   they are first downloaded and extracted (ie Downloads directory) |\\n| WebJEAScriptsFolder | This is where the scripts folder will be placed (ie WebJEA installed) |\\n| WebJEAConfigPath | This is where the config file will be placed (ie WebJEA installed - it   needs to be the same location as the Scripts folder) |\\n| WebJEALogPath | WebJEA log path |\\n| WebJEA_Nlog_LogFile | WebJEA system log location |\\n| WebJEA_Nlog_UsageFile | WebJEA usage log location |\\n\\n![WebJEA - DSC](/uploads/webjea_dsc.png)\\n\\nOne thing to note is that the DSCDeploy.ps1 is calling _(dot sourcing)_ the DSCConfig deploy script; by default, it is looking for it in the same folder as the DSCDeploy.ps1 folder.\\n\\nIf you just opened up PowerShell ISE, you may notice that you are actually in C:\\\\\\\\Windows\\\\\\\\System32, so it won\'t be able to find the script to run; you can either change the script to point directly to the file location, or you can change the directory you are into to match the files, in my case in the Script pane I run the following:\\n\\n    cd \'C:\\\\Users\\\\webjea_services\\\\Downloads\\\\webjea-1.1.157.7589\'\\n\\nNow **run** the **script** and **wait**.\\n\\nIf you get an error saying that the script is not digitally signed, run the following in the script pane:\\n\\n    Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\\n\\nThis is because the PowerShell execution policy hasn\'t been set; depending on the scripts you are running, you may have to update the execution policy for the entire system, but for now, we will set it to Bypass for this process only, now re-run the script again, you should see DSC kick-off and start your configuration and setup of IIS and the WebJEA site.\\n\\n![WebJEA - DSC](/uploads/webjea_startingdsc.png)\\n\\nYou should also see the files/folders starting to be created!\\n\\n_Note: If you need to make a configuration change, please change it in the DSCDeploy.ps1, DSC will ensure that the configuration is applied as per your configuration and rerun the script, i.e. if you need to replace the certificate from a self-signed certificate to a managed PKI certificate._\\n\\nOnce DSC has been completed, your **server should now be running IIS and the WebJEA site**\\n\\nTo add the IIS Management Tool, this is not required but will help you manage IIS, run the following PowerShell cmdlet:\\n\\n    Enable-WindowsOptionalFeature -Online -FeatureName IIS-ManagementConsole\\n\\nOpen an Internet Browser and navigate to _(your equivalent of)_: [https://webjea-p01.luke.geek.nz/WebJEA](https://webjea-p01.luke.geek.nz/WebJEA \\"https://webjea-p01.luke.geek.nz/WebJEA\\").\\n\\nIf you need assistance finding the Website path, open the Internet Information (IIS) Manager, installed and uncollapse Sites, Default WebSite, right-click WebJEA, Manage Application and select Browse.\\n\\n![WebJEA - IIS](/uploads/webjea_iis_authentication.png)\\n\\nIf successful, you should get a username and password prompt:\\n\\n![WebJEA - IIS](/uploads/webjea_authentication.png)\\n\\nThat\'s normal - it means you haven\'t been given access and now need to configure it.\\n\\n### Configure WebJEA\\n\\nNow that WebJEA has been set up, it is time to configure it; the first thing we need to do is create a Group for WebJEA admins _(see all scripts)_.\\n\\n**Create an Active Directory group for:**\\n\\n* **WebJEA-Admins**\\n* WebJEA-Users\\n\\n**Add** your **account** to the: WebJEA-**Admins group**.\\n\\n**Navigate** to your **WebJEA scripts folder**; in my case, I set it up under c:\\\\\\\\WebJEA\\\\\\\\Scripts:\\n\\n![WebJEA - Scripts](/uploads/webjea_scripts.png)\\n\\nBefore we go any further, **take** a **Backup** of the config.json file, rename it to \\"config.bak\\".\\n\\n_I recommend using Visual Studio Code to edit the config.json to help avoid any syntax issues._\\n\\nNow right click **config.json** and open it to **edit**\\n\\nThis file is the glue that holds WebJEA together.\\n\\nWe are going to make a few **edits**:\\n\\n* Feel free to update the **Title** to match your company or Teams\\n* **Add in** the WebJEA-**Admins** group earlier _(include the Domain Name)_ into the permitted group\'s session - this controls access for ALL scripts.\\n\\n_Note the: \\\\\\\\\\\\\\\\ for each path that is required. If you get a syntax error when attempting to load the WebJEA webpage, this is most likely missing._\\n\\n![WebJEA - Demo](/uploads/webjea_democonfig.png)\\n\\n**Save the config file** and **relaunch** the **WebJEA** webpage. It should now load without prompting for a username and password.\\n\\n**Set** the PowerShell **execution policy** on the machine to Unrestricted so that you can run any PowerShell scripts on it:\\n\\n    Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope LocalMachine\\n\\n![WebJEA - Demo](/uploads/webjea_initialoverview.png)\\n\\nIf you get an: AuthorizationManager check failed error, it is because the PowerShell scripts are still in a blocked state from being downloaded from the internet, run the following command to unblock them, then refresh the WebJEA webpage:\\n\\n    Get-ChildItem -Path \'C:\\\\WebJEA\\\\scripts\\\\\' -Recurse | Unblock-File\\n\\n**You now have a base WebJEA install**! By default, WebJEA comes with 2 PowerShell files:\\n\\n* overview.ps1\\n* validate.ps1\\n\\nYou may have noticed these in the config.json file; WebJEA has actually run the overview.ps1 file as soon as the page loads, so you can have scripts run before running another one, which is handy when you need to know the current state of something before taking action.\\n\\nThe validate.ps1 script is an excellent resource to check out the parameter types used to generate the forms.\\n\\n### Setup Azure Virtual Machine Start/Stop\\n\\nNow that we have a working WebJEA install, it\'s time to set up the Azure VM Start/Stop script for this demo.\\n\\nOn the WebJEA server, we need to **install** the **Azure PowerShell modules**, run the following in Powershell as Administrator:\\n\\n    Install-Module Az -Scope AllUsers\\n\\n#### Create Service Principal\\n\\nOnce the Az PowerShell modules are installed, we need to **set** a **Service Principal** for the PowerShell script to connect to Azure to manage our Virtual Machines.\\n\\nRun the following PowerShell cmdlet to connect to Azure:\\n\\n    Connect-AzAccount\\n\\nNow that we are connected to Azure, we now need to create the SPN, run the following:\\n\\n    $sp = New-AzADServicePrincipal -DisplayName WebJEA-AzureResourceCreator -Role Contributor\\n    $BSTR = [System.Runtime.InteropServices.Marshal]::SecureStringToBSTR($sp.Secret)\\n    $UnsecureSecret = [System.Runtime.InteropServices.Marshal]::PtrToStringAuto($BSTR)\\n\\nNow you have created an SPN called: WebJEA-AzureResourceCreator. We now need to grab the Tenant ID, run the following:\\n\\n    Get-AzContext | Select-Object Tenant\\n\\nNow that we have the SPN and Tenant ID, it\'s time to test connectivity.\\n\\n    # Login using service principal \\n    $TenantId = \'TENANTIDHERE\' \\n    $ApplicationId = \'APPLICATIONIDHERE\'  \\n    $Secret = ConvertTo-SecureString -String \'SECRETSTRINGHERE\' -AsPlainText -Force \\n    $Credential = [System.Management.Automation.PSCredential]::New($ApplicationId, $Secret) \\n    Connect-AzAccount -ServicePrincipal -Credential $Credential -TenantId $TenantId\\n\\nCopy the TenantID into the TenantID section\\n\\nType:\\n\\n    $sp.ApplicationID\\n\\nTo retrieve the ApplicationID created from the SPN in the previous step and add it into the ApplicationID part.\\n\\nType in:\\n\\n    $UnsecureSecret\\n\\nTo retrieve the Secret, created in the SPN and add it to the String.\\n\\nNow run the snippet, and you should be successfully connected to Azure.\\n\\n#### Create Get-VM script\\n\\nOne of the features of WebJEA is the ability to run scripts on page load. So, we will get the current Power State of our Azure VMs, in the **WebJEA scripts directory** to create a new PS1 file called: **Get-VM.ps1**.\\n\\nAdd the following script to it:\\n\\n    # Login using service principal \\n    $TenantId = \'TENANTIDHERE\' \\n    $ApplicationId = \'APPLICATIONIDHERE\'  \\n    $Secret = ConvertTo-SecureString -String \'SECRETSTRINGHERE\' -AsPlainText -Force \\n    $Credential = [System.Management.Automation.PSCredential]::New($ApplicationId, $Secret) \\n    Connect-AzAccount -ServicePrincipal -Credential $Credential -TenantId $TenantId\\n    Get-AzVM -Status | Select-Object Name, PowerState, ResourceGroupName\\n\\n**Save** the file.\\n\\n#### Create Set-VM script\\n\\nNow, it\'s time to create the Script to Start/Stop the Virtual Machine. In the **WebJEA scripts directory**, create a new PS1 file called: **Set-VM.ps1**\\n\\nAdd the following script to it:\\n\\n    #Variables\\n    [CmdletBinding(SupportsShouldProcess=$True,ConfirmImpact=\'Low\')]\\n    param\\n    (\\n    [Parameter(Position=1, mandatory=$true,\\n    HelpMessage=\'What is the name of the Azure Virtual Machine?\')]\\n    $VMName,\\n    [Parameter(Position=2, mandatory=$true,\\n    HelpMessage=\'What is the name of the Azure Resource Group that the Virtual Machine is in?\')]\\n    $RGName,\\n    [Parameter(Position=3, mandatory=$true,\\n    HelpMessage=\'What action do you want to do?\')]\\n    [VALIDATESET(\'Start\',\'Stop\')] \\n    $VMAction\\n    )\\n    # Login using service principal \\n    $TenantId = \'TENANTIDHERE\' \\n    $ApplicationId = \'APPLICATIONIDHERE\'  \\n    $Secret = ConvertTo-SecureString -String \'SECRETSTRINGHERE\' -AsPlainText -Force \\n    $Credential = [System.Management.Automation.PSCredential]::New($ApplicationId, $Secret) \\n    Connect-AzAccount -ServicePrincipal -Credential $Credential -TenantId $TenantId\\n    Get-AzVM -Status | Select-Object Name, PowerState, ResourceGroupName\\n    if ($VMAction -eq \\"Start\\")\\n    {\\n         Start-AzVM -Name $VMName -ResourceGroupName $RGName -Confirm:$false  -Force\\n        return\\n    }\\n    elseif ($VMAction -eq \\"Stop\\")\\n    {\\n           Stop-AzVM -Name $VMName -ResourceGroupName $RGName -Confirm:$false  -Force\\n    }\\n\\n**Save** the file.\\n\\n#### Set VM in WebJEA Config\\n\\nNow that the scripts have been created, it\'s time to add them to WebJEA to use.\\n\\nNavigate to your scripts file and make a backup of the config.json file, then edit: config.json\\n\\nOn the line beneath the \\"onloadscript\\": \\"overview.ps1\\" file, add:\\n\\n},\\n\\nThen add in:\\n\\n    {\\n    \\"id\\": \\"StartStopAzVM\\",\\n    \\"displayname\\": \\"StartStop-AzVM\\",\\n    \\"synopsis\\": \\"Starts or Stops Azure Based VMs\\",\\n    \\"permittedgroups\\": [\\".\\\\\\\\Administrators\\", \\"luke.geek.nz\\\\\\\\WebJEAAdmins\\"],\\n    \\"script\\": \\"Set-VM.ps1\\",\\n    \\"onloadscript\\": \\"Get-VM.ps1\\"\\n     }\\n\\nSo your config.json should look similar to:\\n\\n```json title=\\"config.json\\"\\n\\n{\\n    \\"Title\\": \\"Luke Web Automation\\",\\n    \\"defaultcommandid\\": \\"overview\\",\\n    \\"basepath\\": \\"C:\\\\\\\\WebJEA\\\\\\\\scripts\\",\\n    \\"LogParameters\\": true,\\n    \\"permittedgroups\\": [\\".\\\\\\\\Administrators\\", \\"luke.geek.nz\\\\\\\\WebJEAAdmins\\"],\\n    \\"commands\\": [\\n{\\n        \\"id\\": \\"overview\\",\\n        \\"displayname\\": \\"Overview\\",\\n        \\"synopsis\\": \\"Congratulations, WebJEA is now working!  We\'ve pre-loaded a demo script that will help you verify everything is working.  <br/><i>Tip: You can use the synopsis property of default command to display any text you want.  Including html.</i>\\",\\n        \\"permittedgroups\\": [\\".\\\\\\\\Administrators\\"],\\n        \\"script\\": \\"validate.ps1\\",\\n        \\"onloadscript\\": \\"overview.ps1\\"\\n    },\\n{\\n        \\"id\\": \\"StartStopAzVM\\",\\n        \\"displayname\\": \\"StartStop-AzVM\\",\\n        \\"synopsis\\": \\"Starts or Stops Azure Based VMs\\",\\n        \\"permittedgroups\\": [\\".\\\\\\\\Administrators\\", \\"luke.geek.nz\\\\\\\\WebJEAAdmins\\"],\\n        \\"script\\": \\"Set-VM.ps1\\",\\n        \\"onloadscript\\": \\"Get-VM.ps1\\"\\n    }\\n\\n]\\n}\\n\\n```\\n\\n### Test Azure Virtual Machine Start/Stop\\n\\nNow that the scripts have been created **open** the **WebJEA** webpage.\\n\\nClick on the **StartStop-AzVM** page _(it may take a few seconds to load, as it is running the Get-VM script_). You should be greeted by a window similar to below:\\n\\n![WebJEA - Demo](/uploads/webjea_startstopazvm.png)\\n\\n**Congratulations, you have now set up WebJEA and can Start/Stop any Azure Virtual Machines using self-service!**\\n\\n### Additional Notes\\n\\n* There is room for improvement around error checking, doing more with the scripts, such as sending an email when triggered, etc., to remind the server to be powered off.\\n* Because most of the configuration is JSON/PowerShell files, you could have the entire scripts folder in a git repository to make changes, roll back and keep version history.\\n* Remove any hard coding of any secrets to connect to Azure (as an example) from the scripts and implement a password management tool with API access or even the Windows Credential Manager. You want a system where you can easily update the passwords of accounts, limit access and prevent anything from being stored in plain text.\\n* Using the permitted group\'s section of the config.json file, you can restrict the ability for certain groups to run scripts this way, and you can set granular control on who can do what.\\n* If you use a normal Active Directory user account as the service account - then for added security, make sure that the WebJEA server is the only device that - that account can be logged in as and only has the permissions assigned that it needs to, look at implementing PIM (Privilaged Access Management) for some tasks so it only has access at the time that it needs it."},{"id":"/2021/07/13/validate-that-your-azure-resources-are-deployed-per-the-well-architected-framework","metadata":{"permalink":"/2021/07/13/validate-that-your-azure-resources-are-deployed-per-the-well-architected-framework","source":"@site/blog/2021-07-13-validate-that-your-azure-resources-are-deployed-per-the-well-architected-framework.md","title":"Well-Architected Framework Azure infrastructure review with PSRule for Azure","description":"Imagine if you could validate that your Azure Resources are deployed per the Well-Architected Framework (WAF).. just imagine!","date":"2021-07-13T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.445,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Well-Architected Framework Azure infrastructure review with PSRule for Azure","authors":["Luke"],"tags":["Azure"],"date":"2021-07-13 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/windowsterminal_data_psrules-azure.png"}},"unlisted":false,"prevItem":{"title":"Implement WebJEA for self-service Start/Stop of Azure Virtual Machines","permalink":"/2021/07/18/implement-webjea-for-self-service-azure-resource-creation-with-powershell"},"nextItem":{"title":"Configure Azure Virtual Desktop Monitoring Insights","permalink":"/2021/07/10/configure-azure-virtual-desktop-monitoring-insights"}},"content":"Imagine if you could validate that your Azure Resources are deployed per the Well-Architected Framework (WAF).. just **imagine**!\\n\\n**Of a way of validating your services are secure and deployed following the Azure Architecture framework, both before and after the resources have been created!**\\n\\nImagine no longer! There is a PowerShell module designed specifically for that purpose: **PSRule for Azure**.\\n\\n![PSRule - Azure](/uploads/windowsterminal_data_psrules-azure.png \\"PSRule - Azure\\")\\n\\nPSRule is a suite of rules to validate resources and infrastructure as code _(IaC)_ using PSRule, and the Azure component uses the base PSRule module.\\n\\nFeatures of [PSRule for Azure](https://azure.github.io/PSRule.Rules.Azure/ \\"PSRule for Azure\\")\\n include:\\n\\n* Leverage over 200 pre-built rules across five WAF pillars:\\n  * Cost Optimization\\n  * Operational Excellence\\n  * Performance Efficiency\\n  * Reliability\\n  * Security\\n* Validate resources and infrastructure code pre or post-deployment using Azure DevOps or Github!\\n* It runs on macOS, Linux, and Windows.\\n\\nWith over 200 inbuilt rules _(and you can add your own),_ there is a lot of resource types covered, such as _(but not limited to)_:\\n\\n* Azure App Service\\n* Azure Key vault\\n* Azure Virtual Machine\\n* Azure Storage\\n* Azure Network\\n* Azure Public IP\\n\\nAzure PSRules has been in development since 2019 and is under constant updates and fixes.\\n\\nPSRule for Azure provides two methods for analyzing Azure resources:\\n\\n* Pre-flight - Before resources are deployed from Azure Resource Manager templates.\\n* In-flight - After resources are deployed to an Azure subscription.\\n\\nPre-flight validation is used to scan ARM (Azure Resource Manager) templates before services are deployed and allow for quality gaps and better information in pull requests to improve and implement your infrastructure as code components.\\n\\nThe in-flight method can also be used in Azure DevOps for validation of Terraform resource deployments etc. Still, in this demo, I will run you through installing the Module and doing an export and scan from your PowerShell console!\\n\\nWe are going to install the PSRule.Azure _(based on the Well-Architected Framework & Cloud Adoption Framework)_.\\n\\nI recommend keeping the Modules _(and as such the in-built rules)_ up-to-date and do scans at least every quarter or after a major deployment or project to help verify your resources are set up according to some best-practice rules. This does not replace Security Center and Azure Advisor; this is intended to be a supplement.\\n\\n### Install PSRule.Azure\\n\\n1. Open **PowerShell** console and **run** the following **commands**:\\n\\n       #The main Module and base rules to validate Azure resources..\\n       Install-Module PSRule.Rules.Azure -Scope CurrentUser\\n\\n![Install-Module PSRule](/uploads/windowsterminal_install_psrules-azure.png \\"Install-Module PSRule\\")\\n2. Press \'**Y**\' to **accept PSGallery** as a trusted repository; just a note, you can prevent the confirmation prompt when installing Modules from the PSGallery, by classifying it as a \'Trusted Repository\' by running the following. Just be wary that won\'t get rechallenged:\\n\\n       Set-PSRepository -Name \'PSGallery\' -InstallationPolicy Trusted\\n3. You should now have the following modules installed:\\n\\n* PSRule\\n* PSRule.Rules.Azure\\n\\n### Extract Azure Subscription PSRule JSON files\\n\\nNow that PSRule has been installed, it\'s time to log in to Azure and extract information regarding your Azure resources for analysis; these extracted files are JSON files containing information, such as your resource names, subscription ID, etc. resource groups in plain text.\\n\\nAs you can see from the screenshot below, we can target specific Subscriptions, Tenancies _(yes, as long as the account you have access to has access to the subscription, you can export those as well)_, Resource Groups and Tags.\\n\\n![Export-AzRuleData](/uploads/powershellise_exportazruledata.png \\"Export-AzRuleData\\")\\n\\nBecause I want to get the most data available across all resources, I will target everything with the \'_-All_\' parameter.\\n\\n1. First, we need to connect to the Azure subscription and then **connect** to the **Azure** subscription we have access to or are targeting by running the following:\\n\\n       Connect-AzAccount\\n       \\n       Get-AzSubscription  | ogv -PassThru | Set-AzContext\\n2. Now that you have connected its time to export the Azure resource information, **run** the following **PowerShell** cmdlet, and **point** it towards an **empty folder**:\\n\\n       Export-AzRuleData -OutputPath c:\\\\temp\\\\AzRuleData -All\\n3. If the **folder doesn\'t** **exist**, _don\'t worry_ - the Export command will **create** it **for you**. Depending on how many resources and subscriptions you are extracting, this may take a few minutes.\\n\\nYou should **see** the **JSON files** appearing if you open one of these. In addition, you should be able to see information about the resources it has extracted.\\n\\n### Run PSRule across your JSON files\\n\\nNow that you have extracted the JSON files of your Azure resources, it\'s now time to analyse them following Microsoft Cloud Adoption and Well Architectured framework and the rules builtin into PSRule.Azure!\\n\\nYou don\'t need to be connected to Azure; for this analysis, have the PSRule modules installed and access the JSON files.\\n\\nPSRule.Azure has a few [baselines](https://azure.github.io/PSRule.Rules.Azure/en/baselines/Azure.All/ \\" PSRule for Azure - All Baselines\\"); these baselines contain the rules used to analyse your resources and range from Preview to newly released rules; again, we will target ALL rules, as we are after all recommendations.\\n\\n1. In **PowerShell, run** the following:\\n\\n       Assert-PSRule -Module \'PSRule.Rules.Azure\' -InputPath \'C:\\\\temp\\\\AzRuleDataExport\\\\*.json\' -Baseline \'Azure.All\'\\n2. This will trigger **PSRules** to **scan** your **extracted JSON** files with the **ALL** rules, and you will get **output** like below:\\n3. ![Invoke-PSRules](/uploads/windowsterminal_data_psrules-azure.png \\"Invoke-PSRules\\")\\n4. Although it is good being able to see a high level, I prefer to look at it all at once in Excel, so run the following to **export** the rules to a **CSV**:\\n\\n       Invoke-PSRule -Module \'PSRule.Rules.Azure\' -InputPath \'C:\\\\temp\\\\AzRuleDataExport\\\\*.json\' -Baseline \'Azure.All\' | Export-csv C:\\\\temp\\\\AzRuleDataExport\\\\Exported_Data.csv\\n5. You should now have a CSV file to review and look for common issues, concerns and work on improving your Azure infrastructure setup!\\n\\n![PS Rules Azure - Export CSV](/uploads/export_azruledata_excel.png \\"PS Rules Azure - Export CSV\\")\\n\\n_Note: The export contains the Subscription/Resource Names, so you can definitely see what resources can improve upon; however, I removed it from my screenshot._\\n\\n**Congratulations**! You now have more visibility and, hopefully, some useful recommendations for improving your Azure services!\\n\\n_If you want to get a good understanding of the type of data rules, check out my extracted CSV \'_[_here_](http://luke.geek.nz/uploads/files/Exported_Data_PSRuleAzure.csv)_\'._\\n\\n### Additional Resources\\n\\n* If you found PSRules.Azure interesting; how about getting any Failed rules? How about getting any failed rules pushed to Azure Monitor?\\n\\n[PSRule to Azure Monitor](https://github.com/microsoft/PSRule.Monitor \\"PSRule to Azure Monitor\\")\\n\\n* If you are interested in the CI (Continous Integration) options, check out the links below:\\n\\n[Azure DevOps Pipeline & Github Actions](https://microsoft.github.io/PSRule/ \\"PSRule\\")\\n\\n* Extend the PSRules to include Cloud Adoption Framework as well?\\n\\n[PSRule for Cloud Adoption Framework](https://github.com/microsoft/PSRule.Rules.CAF \\"PSRule for Cloud Adoption Framework\\")\\n\\n* And finally, creating Custom Rules for your organisation, including Tagging, Naming conventions etc.?\\n\\n[PSRule.Azure Custom Rules](https://azure.github.io/PSRule.Rules.Azure/customization/ \\"Organization specific rules\\")"},{"id":"/2021/07/10/configure-azure-virtual-desktop-monitoring-insights","metadata":{"permalink":"/2021/07/10/configure-azure-virtual-desktop-monitoring-insights","source":"@site/blog/2021-07-10-configure-azure-virtual-desktop-monitoring-insights.md","title":"Configure Azure Virtual Desktop Monitoring Insights","description":"Microsoft has now added a built-in Monitoring workbook for Azure Virtual Desktop performance monitoring; this monitoring includes dashboards related (but not limited to):","date":"2021-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.06,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-07-10 00:00:00 +1200","title":"Configure Azure Virtual Desktop Monitoring Insights","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/azportal_azurevirtualdesktopinsights.png"}},"unlisted":false,"prevItem":{"title":"Well-Architected Framework Azure infrastructure review with PSRule for Azure","permalink":"/2021/07/13/validate-that-your-azure-resources-are-deployed-per-the-well-architected-framework"},"nextItem":{"title":"How to create a Free Azure Log Analytics Workspace using PowerShell","permalink":"/2021/07/10/how-to-create-a-free-log-analytics-workspace"}},"content":"Microsoft has now added a built-in Monitoring workbook for Azure Virtual Desktop performance monitoring; this monitoring includes dashboards related _(but not limited to)_:\\n\\n* Session host diagnostics\\n* Connection performance\\n* Host performance\\n* User login events\\n* Remote Desktop client versions\\n\\nTo configure, we first need to create a Log Analytics workspace that both the Host Pool and Workspace will connect to.\\n\\n### Create a Log Analytics workspace\\n\\nYou can use a Log Analytics workspace if it already exists; if not, we will have to create one.\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on + **Create a Resource**\\n 3. Search for: **Log Analytics workspace**\\n 4. Click **Create**\\n 5. ![Azure Portal - Log Analytics Marketplace](/uploads/azportal_loganalyticsworkspace.png \\"Azure Portal - Log Analytics Marketplace\\")\\n 6. Here we can **select** the **Resource Group**, **Name** and **Location** of the Log Analytics workspace we will create.\\n 7. I am going to **create** a new **Resource Group** called: aad_mgmt\\n 8. I click Create New and **enter** in the **name** of the **Resource Group**\\n 9. Under Instance Details, make sure you select a name that adheres to your naming governance.\\n10. _Note: the name of your Log Analytics workspace is now scoped to the Resource Group, so you can have Log Analytics workspaces with the same name, as long as they are indifferent resource groups._\\n11. ![Azure Portal - Create Log Analytics](/uploads/azportal_createloganalyticsworkspace.png \\"Azure Portal - Create Log Analytics\\")\\n12. Click on: **Next: Pricing Tier**\\n13. **Select** the applicable **pricing tier**, I only have Pay-as-you-go (Per GB 2018), so I will select that.\\n14. _Note: You can view the Pricing for Log Analytics on the_ [_Pricing Calculator_](https://azure.microsoft.com/en-us/pricing/details/monitor/?WT.mc_id=AZ-MVP-5004796, \\"Azure Pricing Calculator - Azure Monitor\\")_:  look at the Pay-As-You rates._\\n15. ![Azure Portal - Create Log Analytics](/uploads/azportal_createloganalyticsworkspacepricing.png \\"Azure Portal - Create Log Analytics\\")\\n16. Click **Next: Tags**\\n17. **Enter** in any **applicable tags**, such as Creator, Who it may get billed to, Project ID etc. that\u2019s relevant and select **Review + Create**\\n18. **Review** the **configuration** and click **Create** to create your Log Analytics workspace! _(It should take less than a minute._)\\n\\n### Configure Azure Virtual Desktop Insights\\n\\n 1. Log in to the **Azure Portal**\\n 2. **Search** for: **Azure Virtual Desktop**\\n 3. Click on [**Insights**](https://portal.azure.com/#blade/Microsoft_Azure_WVD/WvdManagerMenuBlade/insights \\"Azure Virtual Desktop - Insights\\")\\n 4. A Workbook blade should now greet you\\n 5. This is where we will **configure** the Azure Virtual Desktop **Insights**. You can see on the lower right-hand side that we will be deploying Azure Monitor for \'Windows Virtual Desktop v1.0.4\' _(however, this will be managed by Microsoft, but it is handy to know the version in case of support later on)._\\n 6. ![Azure Virtual Desktop - Insights](/uploads/azportal_azurevirtualdesktop_insights.png \\"Azure Virtual Desktop - Insights\\")\\n 7. Click on **Open Configuration Workbook**\\n 8. Here, **select** the Log Analytics **workspace** you created earlier _(or want to use)_\\n 9. Select **Configure host pool**\\n10. ![Azure Virtual Desktop - Insights](/uploads/azportal_azurevirtualdesktopcheckconfiguration.png \\"Azure Virtual Desktop - Insights\\")\\n11. Click on **Deploy** _(make sure all your Session Hosta are started so that Azure can deploy and configure the Log Analytics agent on the Virtual Machines)_\\n12. You can select View Template and Parameters if you want to confirm the host pool and workspace configured.\\n13. ![Azure Virtual Desktop - Insights](/uploads/azportal_azurevirtualdesktophostpooldeploy.png \\"Azure Virtual Desktop - Insights\\")\\n14. While the Diagnostic host pool settings are being configured, click on **Configure workspace.**\\n15. Click on: **Deploy**\\n16. Once the Workspace and Host Pool deployments are done, click on **Refresh.**\\n17. ![Azure Virtual Desktop - Insights](/uploads/azportal_azurevirtualdesktopcheckconfigrefresh.png \\"Azure Virtual Desktop - Insights\\")\\n18. **Confirm** that **Enabled** is: True\\n19. ![Azure Virtual Desktop](/uploads/azportal_azurevirtualdesktopcheckconfig.png)\\n20. The journey is not over yet; now that the Host Pool and Workspace have been configured, we need to **add** the **Session Hosts** and configure the **performance counters** to the same workspace!\\n21. Click on: **Session host data settings.**\\n22. **Select** your Log Analytics **workspace**\\n23. Select **Add hosts to workspace**\\n24. ![Azure Virtual Desktop](/uploads/azportal_sessionhostdatasettings.png)\\n25. Confirm the Deployment and click **Deploy**\\n26. **Wait until** the **deployment** has **succeeded,** or you may get API errors, then select:\\n27. Navigate down and click **Configure performance counters**\\n28. ![Azure Virtual Desktop](/uploads/azportal_performancecounterssettings.png)\\n29. Click on **Apply** config.\\n30. **Wait until** the **deployment** has **succeeded,** or you may get API errors, then select:\\n31. Navigate down and click on **Configure events**\\n32. ![Azure Virtual Desktop](/uploads/azportal_eventlogssettings.png)\\n33. Click on **Deploy**\\n\\n    Now click on: **Refresh**, and you should see \'No missing performance counters\', \'No missing events found\'.\\n34. ![Azure Virtual Desktop](/uploads/azportal_performancecountersreview.png)\\n35. **You have now configured Azure Virtual Desktop Insights!**\\n\\n    _It may take a few minutes to an hour to populate and collect the data for some of the events and counters._\\n36. ![Azure Virtual Desktop](/uploads/azportal_azurevirtualdesktopinsights.png)\\n\\n    _On the plus side, all the data is also in Log Analytics so that it can be queried, and you can set up Alert rules against it and get more visibility into your Azure Virtual Desktop environment and use._\\n\\n    ![Azure Virtual Desktop](/uploads/azportal_azurevirtualdesktoploganalytiicsquery.png)"},{"id":"/2021/07/10/how-to-create-a-free-log-analytics-workspace","metadata":{"permalink":"/2021/07/10/how-to-create-a-free-log-analytics-workspace","source":"@site/blog/2021-07-10-how-to-create-a-free-log-analytics-workspace.md","title":"How to create a Free Azure Log Analytics Workspace using PowerShell","description":"When you create a Log Analytics workspace using the Azure Portal, you only get the Pricing or \'Pay-as-you-go\' tiers to select.","date":"2021-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.705,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to create a Free Azure Log Analytics Workspace using PowerShell","authors":["Luke"],"tags":["Azure"],"date":"2021-07-10 00:00:00 +1300","toc":false,"header":{"teaser":"/uploads/log_analytics_free.png"}},"unlisted":false,"prevItem":{"title":"Configure Azure Virtual Desktop Monitoring Insights","permalink":"/2021/07/10/configure-azure-virtual-desktop-monitoring-insights"},"nextItem":{"title":"How to set a Log Analytics Daily Data Cap","permalink":"/2021/07/10/how-to-set-a-log-analytics-data-cap"}},"content":"When you create a Log Analytics workspace using the Azure Portal, you only get the Pricing or \'Pay-as-you-go\' tiers to select.\\n\\nYou used to create a \'Free\' tier using the Azure Portal; however, since 2018; they removed it with a change in plans and it became a legacy offering.\\n\\nHowever, using PowerShell, you can still create a Log Analytics Free SKU!\\n\\n> The Free pricing tier is a [legacy pricing tier](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/cost-logs?WT.mc_id=AZ-MVP-5004796#legacy-pricing-tiers) that is available for trying Azure Log Analytics. It has a data cap of 500 MB/day and only 7 days of data retention, so it is intended only for testing and is not to be used for production deployments.\\n\\nYou can change a Free Tier Log Analytics workspace to a Pay-as-you-go or commitment tier later.\\n\\nYou cannot change a Log Analytics workspace created on a higher tier back to Free, even using PowerShell, due to [adjustments](https://azure.microsoft.com/en-us/blog/introducing-a-new-way-to-purchase-azure-monitoring-services/?WT.mc_id=AZ-MVP-5004796 \\"Introducing a new way to purchase Azure monitoring services\\") in 2018 around the Log Analytics billing and plans.\\n\\n![Azure Log Analytics - Free](/uploads/log_analytics_free.png)\\n\\n## Create a \'Free Tier\' Log Analytics using PowerShell\\n\\nChange the script\'s variables below to suit your environment, connect to Azure and run the script to create your Log Analytics workspace.\\n\\nNote: I tested this script on an MSDN subscription, which I\'ve had for a few years and a recent one created a few months back _(2021)_, but there may be limitations on other subscription types that I haven\'t tested _- see blurb below the script_.\\n\\n    #Connect to Azure\\n    Connect-AzAccount\\n    \\n    #Set Variables\\n    $ResourceGroup = \'aad_mgmt\'\\n    $Location = \'australiaeast\'\\n    $LogAnalyticsName = \'la-free\'\\n    $SKU = \'Free\'\\n    \\n    #Creates Log Analytics Workspace\\n    New-AzOperationalInsightsWorkspace -Location $Location -Name $LogAnalyticsName -Sku $SKU -ResourceGroupName $ResourceGroup\\n\\n> If you get an error: Error Message: Pricing tier doesn\'t match the subscription\'s billing model. Read [http://aka.ms/PricingTierWarning](http://aka.ms/PricingTierWarning \\"http://aka.ms/PricingTierWarning?WT.mc_id=AZ-MVP-5004796#moving-to-the-new-pricing-model\\") for more details, unfortunately it means that your Subscription is under a different Billing model, and may have been created recently are you are unable to use the \'Free\' tier, instead you may have to create it using \'standard\' instead.\\n\\n![Azure Log Analytics - Free](/uploads/log_analytics_free_tier.png)"},{"id":"/2021/07/10/how-to-set-a-log-analytics-data-cap","metadata":{"permalink":"/2021/07/10/how-to-set-a-log-analytics-data-cap","source":"@site/blog/2021-07-10-how-to-set-a-log-analytics-data-cap.md","title":"How to set a Log Analytics Daily Data Cap","description":"This is just an additional configuration that may help with sizing and pricing Log Analytics, you can set a \'Daily cap\' for the amount of Data you ingest _per day_, to help restrict cost.","date":"2021-07-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.935,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to set a Log Analytics Daily Data Cap","authors":["Luke"],"tags":["Azure"],"date":"2021-07-10 00:00:00 +1300","toc":false,"header":{"teaser":"/uploads/azportal_loganalyticscap.png"}},"unlisted":false,"prevItem":{"title":"How to create a Free Azure Log Analytics Workspace using PowerShell","permalink":"/2021/07/10/how-to-create-a-free-log-analytics-workspace"},"nextItem":{"title":"Create Custom Roles for Microsoft Azure","permalink":"/2021/07/06/create-custom-roles-for-microsoft-azure"}},"content":"This is just an additional configuration that may help with sizing and pricing Log Analytics, you can set a \'Daily cap\' for the amount of Data you ingest **_per day_**, to help restrict cost.\\n\\nThe downside of this is if you reach the cap, you will no longer collect any data, until the following day, meaning you may miss key events or issues.\\n\\nThis is something that I would recommend ONLY to do if you run into any financial constraints, giving you more time time to work through, of course, situation depending.\\n\\nThis is a pretty quick \'How To\' so let\'s get straight into it:\\n\\n1. Log in to the **Azure Portal**\\n2. Search for your Log Analytics Workspace\\n3. Select **Usage and estimated costs**\\n4. Click on **Daily Cap**\\n5. **Set** your **cap** in **GB** _(I put 0.166 as my thinking was 5GB per free each month, so 166MB a day, should cap my Log Analytics workspace, although useful for this demo/lab, it\'s not a number I would recommend for Production)_\\n6. Click **Ok**\\n\\n![Log Analytics - Set Daily Cap](/uploads/azportal_loganalyticscap.png \\"Log Analytics - Set Daily Cap\\")"},{"id":"/2021/07/06/create-custom-roles-for-microsoft-azure","metadata":{"permalink":"/2021/07/06/create-custom-roles-for-microsoft-azure","source":"@site/blog/2021-07-06-create-custom-roles-for-microsoft-azure.md","title":"Create Custom Roles for Microsoft Azure","description":"Microsoft Azure uses Role\'s to define who can access what - Role-Based Access Control (RBAC).","date":"2021-07-06T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":11.28,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Create Custom Roles for Microsoft Azure","authors":["Luke"],"tags":["Azure"],"date":"2021-07-06 00:00:00 +1300","toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"How to set a Log Analytics Daily Data Cap","permalink":"/2021/07/10/how-to-set-a-log-analytics-data-cap"},"nextItem":{"title":"Add a shortcut to the Azure Virtual Desktop Web Client to the Microsoft 365 waffle","permalink":"/2021/07/05/add-shortcut-to-azure-virtual-desktop-in-the-microsoft-365-waffle"}},"content":"Microsoft Azure uses Role\'s to define who can access what - Role-Based Access Control (_RBAC)_.\\n\\nYou may be familiar with some of the more common ones, such as:\\n\\n* Owner\\n* Contributor\\n* Reader\\n\\nBehind the scenes, each role is a separate grouping of permissions that determine what level of permissions someone or something has in Azure; these permissions are usually in the form of:\\n\\n* Read\\n* Write\\n* Delete\\n* Action\\n\\nEach role can be assigned to a specific Resource, Subscription, Management Group or Resource Group through an \'Assignment\' _(you assign a role if you give someone Contributor rights to a Resource Group, for example)_.\\n\\nThese permissions can be manipulated and custom roles created.\\n\\n> Why would you use custom roles you ask? As usual - it depends!\\n\\nCustom Roles can give people or objects JUST the right amount of permissions to do what they need to do, nothing more and nothing less, an example of this is maybe you are onboarding a support partner, if they are will only be supporting your Logic Apps, WebApps and Backups, you may not want them to be able to log support cases for your Azure resources; instead of attempting to mash several roles together that may give more or fewer rights than you need, you can create a custom role that specifically gives them what they need, you can then increase or decrease the permissions as needed, however, if a built-in role already exists for what you want. There is no need to reinvent the wheel, so use it!\\n\\nI will run through a few things to help arm you understand and build your own Custom Roles, primarily using PowerShell.\\n\\n### Install the Azure PowerShell Modules\\n\\nAs a pre-requisite for the following, you need to install the Azure (Az) PowerShell Module. You can skip this section if you already have the PowerShell modules installed.\\n\\n1. Open **Windows PowerShell**\\n2. Type **in**:\\n\\n       Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\\n       Install-Module -Name Az -Scope CurrentUser -Repository PSGallery -Force\\n3. If you have issues **installing** the **Azure PowerShell module** - see the Microsoft documentation directly: Install the [Azure Az PowerShell module](https://learn.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.5.0&WT.mc_id=AZ-MVP-5004796 \\"Install the Azure Az PowerShell module\\").\\n4. Once you have the Azure PowerShell module installed, you can **connect to** your **Azure** subscription using the little snippet below:\\n\\n       #Prompts for Azure credentials \\n       Connect-AzAccount\\n       #Prompts Window allowing you to select which  Azure Subscription to connect to \\n       $subscriptionName = (Get-AzSubscription) | Out-GridView -Title \'Select Azure Subscription\' -PassThru | Set-AzContext -SubscriptionName $subscriptionName\\n\\n### Export Built-in Azure Roles\\n\\nOne of the best ways to learn about how an Azure Role is put together is to look at the currently existing roles.\\n\\n1. The following PowerShell command will **list** all **current** Azure **roles**:\\n\\n       Get-AzRoleDefinition\\n2. For a more human-readable view that **lists** the B**uilt-in Azure roles and their descriptions,** you can filter it by:\\n\\n       Get-AzRoleDefinition | Select-Object Name, Description\\n3. As you can see in the screenshot below, there are many various roles, from EventGrid Contributor to AgFood Platform Service and more! At the time of this article, there were 276 built-in roles.\\n4. ![Azure Builtin Roles](/uploads/az_roledefinitions.png \\"Azure Builtin Roles\\")\\n5. Now that we have successfully been able to pull a list of the existing roles, we will now **export** them as **JSON** files to take a proper look at them.\\n6. The PowerShell script below will create a few folders on your computer as a base to work from _(feel free to change the folders to suit your folder structure or access rights)_.\\n   * c:\\\\\\\\Temp\\n   * c:\\\\\\\\Temp\\\\\\\\AzureRoles\\n   * C:\\\\\\\\Temp\\\\\\\\AzureRoles\\\\\\\\BuiltinExports\\\\\\\\\\n   * C:\\\\\\\\Temp\\\\\\\\AzureRoles\\\\\\\\CustomRoles\\n7. Once the folders have been created, it will Get the Azure Role definitions and export them into JSON into the **BuiltinExports folder** to be reviewed.\\n\\n       New-Item -ItemType Directory -Path c:\\\\Temp -Force\\n       New-Item -ItemType Directory -Path c:\\\\Temp\\\\AzureRoles -Force\\n       New-Item -ItemType Directory -Path c:\\\\Temp\\\\AzureRoles\\\\BuiltInExports -Force\\n       New-Item -ItemType Directory -Path c:\\\\Temp\\\\AzureRoles\\\\CustomRoles -Force\\n       \\n       $a = Get-AzRoleDefinition\\n       \\n       Foreach ($role in $a)\\n       {\\n           $name = $role.Name\\n           Get-AzRoleDefinition -Name ($role).Name | ConvertTo-Json | Out-File c:\\\\Temp\\\\AzureRoles\\\\BuiltInExports\\\\$name.json\\n       }\\n8. Once completed, you should now **see** the **JSON files** below:\\n9. ![Azure Role - JSON files](/uploads/az_exportroles.png \\"Azure Role - JSON files\\")\\n\\n_Although you can use Notepad, I recommend using_ [_Visual Studio Code_](https://code.visualstudio.com/Download)_to read these files. This is because Visual Studio Code will help with the syntax as well._\\n\\n### Review Built-in Azure Roles\\n\\nIf you open one of the roles, I will open the Azure Digital Twins Data Owner role; however, it doesn\'t matter.\\n\\nYou should see the following fields:\\n\\n* Name\\n* Id\\n* IsCustom\\n* Description\\n* Actions\\n* NotActions\\n* DataActions\\n* NotDataActions\\n* AssignableScopes\\n\\nThese fields make up your Role.\\n\\n![Azure Role - JSON](/uploads/az_rolereview_azdigitaltwinsdataowner.png \\"Azure Role - JSON\\")\\n\\n* The **Name field** is pretty self-explanatory - this is the name of the Azure Role and what you see in the Azure Portal, under Access control (IAM).\\n* ![Azure Portal - Role](/uploads/az_rolereview_azdigitaltwinsdataowneriam.png \\"Azure Portal - Role\\")\\n* The same is true for the: **Description** **field**.\\n\\n  _These are essential fields as they should tell the users what resource or resources the role is for and what type of access is granted._\\n* The **IsCustom field** is used to determine if the Azure Role is a custom made policy or not; any user-created Role will be set to True, while any In-Built role will be False.\\n* The **Actions field** is used to determine what management operations can be performed. However, the Azure Digital Twins role doesn\'t have any (as it is mainly Data Action based) if we look at another Role such as the: Azure Kubernetes Service RBAC Admin role:\\n  * \\"\\"Microsoft.Authorization/*/read\\",\\n  * \\"Microsoft.Insights/alertRules/*\\",\\n  * \\"Microsoft.Resources/deployments/write\\",\\n\\n  You can see that it has the rights to Read the permissions, create and delete any Alert rules and update resources.\\n* The **NotActions field** is used to exclude anything from the Allowed actions\\n* The **DataActions field** allows you to determine what data operations can be performed. Usually, these are sub-resource tasks, where management or higher-level operations are performed in the Actions field, more specific resource actions are performed in the DataActions field.\\n\\n  The NotDataActions field is used to exclude anything from the Allowed actions in the DataActions\\n\\nTo help get a feel of the differences with the Actions, here is a list of Actions and DataActions for the Azure Kubernetes Service RBAC Admin role:\\n\\n* ![Azure Custom Role - JSON](/uploads/az_rolereview_azkuberservicerbacactions.png \\"Azure Custom Role - JSON\\")\\n* And finally, the **AssignableScopes** is used to specify where the role will be available for assignment, whether it can be assigned at a subscription or resource group or management group level. You will notice that most if not all built-in Azure Roles have an Assignable scope of \\"/\\" - this means that it can be assigned everywhere _(Subscriptions, Resource Groups, Management Groups etc.)._\\n\\n### Review Azure Provider Namespaces\\n\\nYou may have noticed that each Action has a provider. In the example of a Virtual Machine, the provider is Microsoft.Compute.\\n\\n1. To get a **list** of all **current Providers,** run the following command:\\n\\n       Get-AzProviderOperation | Select-Object ProviderNamespace -Unique\\n\\n   At the time of writing, there are 198 current Providers! So that\'s 198 providers or overall buckets of resources that has permissions over.\\n2. We can **drill** into a **provider** a bit **further** to check out current Operations:\\n\\n       Get-AzProviderOperation -Name Microsoft.Compute/*\\n3. This **displays** a **list** of **all providers** within the Microsoft.Compute namespace, such as (but definitely not limited to):\\n   1. Virtual machines\\n   2. Virtual Machine Scale Sets\\n   3. Locations\\n   4. Disks\\n   5. Cloud Services\\n4. If we wanted to drill into the Virtual Machines **providers** a bit more, we could **filter** it like:\\n\\n       Get-AzProviderOperation -Name Microsoft.Compute/virtualMachines/*\\n5. Here we can finally see the available actions, and for example, the following Action will allow you to Read the VM sizes available to a Virtual Machine:\\n\\n* Operation: Microsoft.Compute/virtualMachines/vmSizes/read\\n* operation name: Lists Available Virtual Machine Sizes\\n* ProviderNamespace: Microsoft Compute\\n* ResourceName: Virtual Machine Size\\n* Description: Lists available sizes the virtual machine can be updated to\\n* IsDataAction      : False\\n\\n1. You can use the PowerShell script below to export all the Providers and their Operations to a CSV for review:\\n\\n       $Providers = Get-AzProviderOperation\\n       \\n       $results = @()\\n       \\n       ForEach ($Provider in $Providers) {\\n       \\n       \\n       \\n           $results += [pscustomobject]@{\\n               \'Provider NameSpace\' = $Provider.ProviderNamespace\\n               Description          = $Provider.Description\\n               \'Operation Name\'     = $Provider.OperationName\\n               Operation            = $Provider.Operation\\n               ResourceName         = $Provider.ResourceName\\n       \\n                               \\n           }\\n       \\n       }\\n         \\n       $results | Export-csv c:\\\\temp\\\\AzureRBACPermissions.csv -NoTypeInformation\\n\\nUsing the namespace, providers and actions, you should now be able to see the power behind Role-based access control and how granular you can get.\\n\\n### Add a Custom Role using PowerShell\\n\\nNow that we understand how to navigate the Namespaces and Built-In Roles available in Microsoft Azure using PowerShell, now we will create one.\\n\\nI have created a base template to help you start.\\n\\nThis base template has the following fields that the majority of most custom roles will use:\\n\\n* Name\\n* IsCustom\\n* Description\\n* Actions\\n* AssignableScopes _(make sure you put in the <SubscriptionID> of your Azure subscription, you are assigning the role to.)_\\n\\n1. **Edit** these **fields** _(apart from IsCustom, which you should leave as True)_ as you need.\\n\\n```json title=\\"CustomRoleTemplate.json\\"\\n\\n{\\n    \\"properties\\": {\\n        \\"roleName\\": \\"Custom Role - Template\\",\\n        \\"IsCustom\\": true,\\n        \\"description\\": \\"This is a Template for creating Custom Roles.\\",\\n        \\"assignableScopes\\": [\\n            \\"/subscriptions/<SubscriptionID>\\"\\n        ],\\n        \\"permissions\\": [\\n            {\\n                \\"actions\\": [\\n                    \\"Microsoft.Support/register/action\\",\\n                    \\"Microsoft.Support/checkNameAvailability/action\\",\\n                    \\"Microsoft.Support/operationresults/read\\",\\n                    \\"Microsoft.Support/operationsstatus/read\\",\\n                    \\"Microsoft.Support/operations/read\\",\\n                    \\"Microsoft.Support/services/read\\",\\n                    \\"Microsoft.Support/services/problemClassifications/read\\",\\n                    \\"Microsoft.Support/supportTickets/read\\",\\n                    \\"Microsoft.Support/supportTickets/write\\",\\n                    \\"Microsoft.Resources/subscriptions/resourceGroups/read\\",\\n                    \\"Microsoft.Resources/subscriptions/resourcegroups/resources/read\\"\\n                ],\\n                \\"notActions\\": [],\\n                \\"dataActions\\": [],\\n                \\"notDataActions\\": []\\n            }\\n        ]\\n    }\\n}\\n\\n```\\n\\nThis Custom Role - Template allows you to read the name of all Resource Groups in a subscription and open a Microsoft Support case.\\n\\nIn my example, I am going to add a new role called:\\n\\n* LukeGeek-WebApp Deployment-RW\\n\\nThis role will allow users to Deploy and modify Azure WebApps, among other things!\\n\\n```json title=\\"LukeGeekWebDeployment-RW.json\\"\\n\\n{\\n    \\"properties\\": {\\n        \\"roleName\\": \\"Custom Role - Template\\",\\n        \\"description\\": \\"This is a Template for creating Custom Roles.\\",\\n         \\"IsCustom\\": true,\\n        \\"assignableScopes\\": [\\n            \\"/subscriptions/<SubscriptionID>\\"\\n        ],\\n        \\"permissions\\": [\\n            {\\n                \\"actions\\": [\\n                    \\"Microsoft.Support/register/action\\",\\n                    \\"Microsoft.Support/checkNameAvailability/action\\",\\n                    \\"Microsoft.Support/operationresults/read\\",\\n                    \\"Microsoft.Support/operationsstatus/read\\",\\n                    \\"Microsoft.Support/operations/read\\",\\n                    \\"Microsoft.Support/services/read\\",\\n                    \\"Microsoft.Support/services/problemClassifications/read\\",\\n                    \\"Microsoft.Support/supportTickets/read\\",\\n                    \\"Microsoft.Support/supportTickets/write\\",\\n                    \\"Microsoft.Resources/subscriptions/resourceGroups/read\\",\\n                    \\"Microsoft.Resources/subscriptions/resourcegroups/resources/read\\"\\n                ],\\n                \\"notActions\\": [],\\n                \\"dataActions\\": [],\\n                \\"notDataActions\\": []\\n            }\\n        ]\\n    }\\n}\\n\\n```\\n\\n1. To **add** the **Custom Role** to Azure, I will **run** the following **PowerShell** command:\\n\\n       New-AzRoleDefinition -InputFile \\"C:\\\\\\\\temp\\\\\\\\AzureRoles\\\\\\\\CustomRoles\\\\\\\\LukeGeek-WebApp Deployment-RW.json\\" -Verbose\\n\\n**Your new Custom Role has now been uploaded to Azure and can be selected for an assignment.**\\n\\n### Add a Custom Role using the Azure Portal\\n\\nNow that we have been through and investigated the Azure roles and their providers and actions, instead of using PowerShell to look through and create manually, you can use the Azure Portal!\\n\\n> _*Gasp!* Why didn\'t you tell me earlier about this, Luke?_\\n>\\n> _Well, fellow Azure administrator, I found it easier to look at PowerShell and JSON to explain how the Custom Roles were made, vs staring at the Azure Portal and to be honest, really just because! Like most things in IT there are multiple ways something can be done!_\\n\\n 1. Log in to the **Azure Portal**\\n 2. Navigate to your **Subscription**\\n 3. Click on **Access Control (IAM)** on the left-hand side blade\\n 4. Click on **Add**\\n 5. Click on **Add Custom Role**\\n 6. Type in the **Role Name**, for example, WebAdmin-RO\\n 7. Type in a clear **description** so that you can remember what this role is used for in a year!\\n 8. For Baseline permissions, select: **Start from Scratch**\\n 9. Click **Next**\\n10. Click **Add Permissions**\\n11. If you want, you can select: Download all permissions to review the providers and actions _(very similar to the Get-AzProviderOperation PowerShell command)_.![Azure Portal - Create Custom Role](/uploads/ad_role_addpermissions.png \\"Azure Portal - Create Custom Role\\")\\n12. As you should **see**, all the **Namespace providers** are listed with the Actions/Permissions that you can do.\\n13. In my example, I am going to search for **Microsoft Web Apps**\\n14. **Select** all \'**Read**\' operations _(remember to look at Data Actions as well, there may be resource level actions you might want to allow or exclude)_\\n15. Click **Add**\\n16. ![Azure Portal - Create Custom Role](/uploads/ad_role_webpermissions.png \\"Azure Portal - Create Custom Role\\")\\n17. Review the permissions and click **Next**\\n18. **Select** your assignable **scope** _(where the Role will be allowed so that you can assign it)_\\n19. Click **Next**\\n20. You can **review** and download the JSON for backup later _(this is handy if you are going to Automate the creation of roles in the future and want a base to start from)_\\n21. Click **Next**\\n22. Click **Create to create your Custom Role!**\\n23. ![Azure Portal - Create Custom Role](/uploads/ad_role_createcustomroleportal.png \\"Azure Portal - Create Custom Role\\")\\n\\n### Assign a Custom Role using the Azure Portal\\n\\nNow that you have created your Custom Role - it is time to assign it! So it is actually in use.\\n\\n1. Log in to the **Azure Portal**\\n2. Navigate to your **Subscription or Resource Group** you want to delegate this role to\\n3. Click on **Access Control (IAM)**\\n4. Click **Add**\\n5. Click on **Role Assignment**\\n6. Under the \'Role\' dropdown, **select** your Custom **Role.**\\n7. ![Azure Portal - Add Role Assignments](/uploads/ad_roleassignmentportal.png \\"Azure Portal - Add Role Assignments\\")\\n8. Now you can **select** the **Azure AD Group/User or Service Principal** you want to **assign** the role to and click **Save.**\\n9. Congratulations, **you have now assigned your Custom role!**\\n\\n### Assign a Custom Role using PowerShell\\n\\nYou can assign Custom Role\'s using PowerShell. To do this, you need a few things such as the Object ID, Assignable Scope IDs etc., instead of rehashing it, this Microsoft article does an excellent job of running through the process.\\n\\n* [Assign Azure roles using Azure PowerShell](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-powershell?WT.mc_id=AZ-MVP-5004796 \\"Assign Azure roles using Azure PowerShell\\")"},{"id":"/2021/07/05/add-shortcut-to-azure-virtual-desktop-in-the-microsoft-365-waffle","metadata":{"permalink":"/2021/07/05/add-shortcut-to-azure-virtual-desktop-in-the-microsoft-365-waffle","source":"@site/blog/2021-07-05-add-shortcut-to-azure-virtual-desktop-in-the-microsoft-365-waffle.md","title":"Add a shortcut to the Azure Virtual Desktop Web Client to the Microsoft 365 waffle","description":"If you are like me, you use the application launchers in the Microsoft 365 waffle daily, if not hourly! Then having it as a single pane of glass to access all your applications is a no-brainer!","date":"2021-07-05T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.605,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-07-05 00:00:00 +1200","title":"Add a shortcut to the Azure Virtual Desktop Web Client to the Microsoft 365 waffle","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"/uploads/m365_pin.png"}},"unlisted":false,"prevItem":{"title":"Create Custom Roles for Microsoft Azure","permalink":"/2021/07/06/create-custom-roles-for-microsoft-azure"},"nextItem":{"title":"Start VM on Connect for Azure Virtual Desktop","permalink":"/2021/07/03/start-vm-on-connect-for-azure-virtual-desktop"}},"content":"If you are like me, you use the application launchers in the Microsoft 365 waffle daily, if not hourly! Then having it as a single pane of glass to access all your applications is a no-brainer!\\n\\nThat includes access to the Azure Virtual Desktop Web client! In addition, Microsoft has given us the ability to add Custom App Launchers for applications that are accessible to a URL to the Launchers in the waffle!\\n\\n> Create custom tiles that will appear in the All apps section of the Office 365 app launcher for all of your users. Users can pin the custom tiles directly to their app launcher for quick access.\\n\\nYou can add much more than the Azure Virtual Desktop web client to help improve your user\'s experience, but this quick guide will focus on adding the Azure Virtual Desktop Web Client.\\n\\n![M365 Waffle](/uploads/m365_waffle_default.png \\"M365 Waffle\\")\\n\\n1. Open the [**Microsoft 365 Admin Panel**](https://admin.microsoft.com/#/homepage \\"M365 Admin\\")\\n2. Expand **Settings**\\n3. Click on **Org Settings**\\n4. Select **Organisation Profile**\\n5. Click on **Custom app launcher tiles**\\n\\n![M365 - Organisation Profile](/uploads/m365_customapplaunchertitle.png \\"M365 - Organisation Profile\\")\\n\\n 1. Click **+ Add a custom title.**\\n 2. Please type in the name of your Desktop; in my example, it is **Contoso Desktop.**\\n 3. For the **URL** of the website, type in: [**https://rdweb.wvd.microsoft.com/arm/webclient/index.html**](https://rdweb.wvd.microsoft.com/arm/webclient/index.html \\"https://rdweb.wvd.microsoft.com/arm/webclient/index.html\\")\\n 4. Type in a **URL** of the **icon** you want the App Launcher to have _(Make sure this is a location that you have access to and can manage (i.e. even sitting on your website or Azure Storage account as long as it\'s publically available))._\\n 5. **Add** a **description** _(such as Contoso Desktop, used for Line of Business Applications)_\\n 6. ![M365 - Custom App Launcher](/uploads/m365_customapplaunchertitle1.png \\"M365 - Custom App Launcher\\")\\n 7. Click **Save**\\n 8. ![M365 - Custom App Launcher](/uploads/m365_contosoapp.png \\"M365 - Custom App Launcher\\")\\n 9. Log out of your Admin account and **log** into an **account** with an Exchange license attached to it. It may take some time for the Custom App Launcher to display.\\n10. Once the **Custom App Launcher** has displayed, your users can **pin** it to the **launcher,** so it is always right on top.\\n11. **Click** on your Azure Virtual Desktop **launcher,** and you should be **redirected** to the Azure Virtual Desktop **Web client**!\\n12. ![M365 Waffle - App Launcher](/uploads/m365_pin.png \\"M365 Waffle - App Launcher\\")\\n\\nJust some notes on additional testing:\\n\\n* I attempted copying the Azure Virtual Desktop RDP file (C:\\\\\\\\Users\\\\\\\\%UserAccount%\\\\\\\\AppData\\\\\\\\Local\\\\\\\\rdclientwpf) to my website to access directly however received an error, even opening up the RDP file directly failed, to test the Remote Desktop client.\\n* I had some success opening that RDP up with the Remote Desktop application directly using \'Open With\' C:\\\\\\\\Users\\\\\\\\%UserAccount%\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Apps\\\\\\\\Remote Desktop\\\\\\\\msrdcw.exe, instead of the default Remote Desktop Connection client locally.\\n* This will add it for all M365 users, if you want to restrict it to Users/Groups, I would look at testing and creating an App Registration.\\n\\nAt this stage, having a launcher to the Web Client is the best bet vs a shortcut directly to the RDP file as you don\'t have to worry about users having the Remote Desktop agent installed when working remotely."},{"id":"/2021/07/03/start-vm-on-connect-for-azure-virtual-desktop","metadata":{"permalink":"/2021/07/03/start-vm-on-connect-for-azure-virtual-desktop","source":"@site/blog/2021-07-03-start-vm-on-connect-for-azure-virtual-desktop.md","title":"Start VM on Connect for Azure Virtual Desktop","description":"One of the models of Cloud governance and cost in Microsoft Azure is \'Pay As You Go\', ie. Pay for what you need when you need it.","date":"2021-07-02T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.525,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-07-02T12:00:00.000Z","title":"Start VM on Connect for Azure Virtual Desktop","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/avddesktopfull.png"}},"unlisted":false,"prevItem":{"title":"Add a shortcut to the Azure Virtual Desktop Web Client to the Microsoft 365 waffle","permalink":"/2021/07/05/add-shortcut-to-azure-virtual-desktop-in-the-microsoft-365-waffle"},"nextItem":{"title":"Azure Virtual Desktop Optimisations","permalink":"/azure/Azure-Virtual-Desktop-Optimisations"}},"content":"One of the models of Cloud governance and cost in Microsoft Azure is _\'Pay As You Go\', ie. Pay for what you need when you need it._\\n\\nThe Azure Resource Manager fabrics allow you to scale up and down resources when you need it, whether built-in to the Azure portal or through various other automation mechanisms.\\n\\nFor Azure Virtual Desktop, this means ensuring that session hosts _(Virtual Machines)_ are available for users to connect to consume their services when they need it the most, whether first thing in the morning or late hours of the evening.\\n\\nOne of the technologies that can help with this is: [Start VM on Connect](https://learn.microsoft.com/en-us/azure/virtual-desktop/start-virtual-machine-connect?WT.mc_id=AZ-MVP-5004796 \\"Start VM On Connect\\")_(Start VM on Connect allows users to start the virtual machine from a deallocated state)_.\\n\\n> **You no longer need to create a Custom Role for Start VM on Connect - a built-in role now exists named:** [**Desktop Virtualization Power On Contributor**](https://learn.microsoft.com/azure/virtual-desktop/start-virtual-machine-connect?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796#assign-the-desktop-virtualization-power-on-contributor-role-with-the-azure-portal) **- once that role is assigned to the Azure Virtual Desktop application, you can skip straight to Configure** \\n\\n* Imagine a 9 AM -> 5 PM Monday to Friday business; during the day, Azure Virtual Desktop is available, however anything out of these hours (through Scheduled Shutdowns or Azure Automation Runbooks etc.), the session hosts are shut down to reduce operational costs.\\n* A business user gets some urgent work on Saturday morning and then tries to connect to Azure Virtual Desktop resources to complete the work; because they were turned off outside of business hours, they can\'t connect and then have to ring IT support to get resources started (the alternative would be to leave Virtual Machines running, which may or may not be needed).\\n* Using \'Start Virtual Machine on Connect\', the moment that the user attempts to connect a Virtual Machine is started.\\n* Then it allows the users to log in and do their work without a call to IT, overall saving money, as the hosts are only started when they are first needed. The feature will also only turn on additional VMs (if available) when the first VM reaches the session limit.\\n\\nThis is a host-level setting, so setting \'Start VM on Connect\' will affect all session hosts in the host pool. Therefore, you cannot target specific Virtual Machines in a session host at this stage. This is now supported for both Personal and Pooled session hosts!\\n\\n> _As of 03/07/21 (NZ date format - DD/MM/YY): The Start VM on Connect feature is currently in public preview. This preview version is provided without a service level agreement, and it\'s not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see_ [_Supplemental Terms of Use for Microsoft Azure Previews_](https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/?WT.mc_id=AZ-MVP-5004796)_._\\n\\nFollow the guide below to implement; the Microsoft documentation is pretty good but hoping this might fill in a few gaps for people.\\n\\n### Create a Custom Role for \\"Windows Virtual Desktop\\"\\n\\nFor the \\"Windows Virtual Desktop\\" service principal (this should already exist, it is an inbuilt SPN created by the Azure infrastructure, it is currently called Windows Virtual Desktop but expect this name to be updated in the future) to have the ability to Start a Virtual Machine, we first need to give it rights. You could give it Contributor or Virtual Machine Contributor rights but want to go with the least privileged to create a custom role.\\n\\n 1. Log in to the **Azure Portal**\\n 2. Navigate to the **Subscription** _(you can only currently create custom roles at a subscription level)_ that your session hosts exist in\\n 3. Look for the **Subscription ID** _(**copy** this, we will need it later on, usually found on the Overview window of the Subscription)_\\n 4. **Download** the AVD-StartVMOnConnect **JSON** file **below** and **save** it to a location you can edit.\\n\\n```json title=\\"AVD-StartVMOnConnect.json\\"\\n{\\n    \\"properties\\": {\\n        \\"roleName\\": \\"AVD-StartVMOnConnect\\",\\n        \\"description\\": \\"Custom role, designed to allow \'Windows/Azure Virtual Desktop\' rights to Start session hosts.\\",\\n        \\"assignableScopes\\": [\\n            \\"/subscriptions/<SubscriptionID>\\"\\n        ],\\n        \\"permissions\\": [\\n            {\\n                \\"actions\\": [\\n                    \\"Microsoft.Compute/virtualMachines/start/action\\",\\n                    \\"Microsoft.Compute/virtualMachines/read\\"\\n                ],\\n                \\"notActions\\": [],\\n                \\"dataActions\\": [],\\n                \\"notDataActions\\": []\\n            }\\n        ]\\n    }\\n}\\n\\n```\\n\\n 6. **Open** up the **JSON** file _(this is the Custom Role we are creating, as you can see, we are only allowing the ability to Read a Virtual Machine and Start it)_\\n 7. **Replace** the: <SubscriptionID> with your **subscription ID**, created earlier and save the JSON file.\\n 8. ![AVD-StartVMOnConnect Custom Role](/uploads/customrolejson_subscriptionid.png \\"AVD-StartVMOnConnect Custom Role\\").\\n 9. Click on **Access Control (IAM)** on the left-hand side blade\\n10. Click **Add**\\n11. Click **Add Custom Role**\\n12. ![AVD-StartVMOnConnect Custom Role](/uploads/azureportal_iam_customrole.png \\"AVD-StartVMOnConnect Custom Role\\")\\n13. **Name** your Custom Role **Name** something meaningful, for example, _AVD-StartVMOnConnect._\\n14. **Add** a meaningful **Description**; for example, mine is:\\n\\n    _Created: 03/07/21_\\n\\n    _Created by: Luke Murray_\\n\\n    _Created for: Custom role, designed to allow \'Windows/Azure Virtual Desktop\' rights to Start session hosts._\\n15. For: Baseline permissions, select **Start from JSON**\\n\\n    **Select** the **JSON file** you downloaded and edited earlier\\n16. ![AVD-StartVMOnConnect Custom Role](/uploads/azureportal_iam_customrole_create.png \\"AVD-StartVMOnConnect Custom Role\\")\\n17. Click on **Next**\\n18. **Verify** the **permissions** are as below _(if they aren\'t, you may need the redownload or check the JSON file for syntax issues - I recommend downloading_ [_Visual Studio Code_](https://code.visualstudio.com/ \\"Visual Studio Code\\")_)_:\\n19. ![AVD-StartVMOnConnect Custom Role](/uploads/azureportal_iam_customrole_permissions.png \\"AVD-StartVMOnConnect Custom Role\\")\\n20. Click **Next**\\n21. We used the subscription property to **select** the **assignable scope** _(i.e. the scope is where this role will be available for you to assign access to)_, but now using the Azure Portal, we can select a specific Resource Group to limit the roles access, please be careful with doing this, especially if you are planning on expanding out your Azure Virtual Desktop infrastructure in the future as you may forget that this role may not be available in other resource groups. I am going to leave mine at the Subscription level and click **Next**\\n22. Here we can **verify** and **save** the changed JSON file _(if you want for future reference)_ and click Next to review your configuration.\\n23. Click Create to create your Custom Role!\\n24. ![AVD-StartVMOnConnect Custom Role](/uploads/azureportal_iam_customrole_reviewcreate.png \\"AVD-StartVMOnConnect Custom Role\\")\\n\\n### Assign your Custom Role\\n\\nNow that you have created your custom role for Azure Virtual Desktop, it is now time to assign it, and this is where you can assign and lock down the role; in my case, I only have one Resource Group where my session hosts sit in, so going to assign it a Resource Group level, but feel free to assign this at a subscription level.\\n\\n 1. Log in to the **Azure Portal**\\n 2. **Navigate** to the **Resource Group** _(or Subscription)_ that has your Azure Virtual Desktop session hosts\\n 3. Click on **Access Control (IAM)** in the left-hand side blade\\n 4. Click on **+ Add**\\n 5. Click on **Add role assignment**\\n 6. **Select** the **Role** you created earlier _(i_.e. _AVD-StartVMOnConnect)_\\n 7. **Specify** the \'**Windows Virtual Desktop**\' service principal and select **Save**\\n 8. ![AVD-StartVMOnConnect Custom Role](/uploads/azureportal_addroleassignment.png \\"AVD-StartVMOnConnect Custom Role\\")\\n 9. If you want, you can click on Role Assignments to verify your role has been assigned:\\n10. ![AVD-StartVMOnConnect Custom Role](/uploads/azureportal_assignedrolecheck.png \\"AVD-StartVMOnConnect Custom Role\\")\\n\\n### Configure Start VM on Connect\\n\\n 1. Log in to the **Azure Portal**\\n 2. Navigate to your **Host Pool**\\n 3. Click on **Properties**\\n 4. Select \'**Yes**\' to **Start VM on Connect**\\n 5. Click **Save**\\n 6. ![Azure Virtual Desktop - Start VM on Connect](/uploads/azureportal_startvmonconnect.png \\"Azure Virtual Desktop - Start VM on Connect\\")\\n 7. **Congratulations, you have now set up Azure Virtual Desktop - Start VM on Connect**; next time someone connects to a turned-off Azure Virtual Desktop session host, the Virtual Machines will now automatically start the users will get a prompt like below:\\n 8. ![Azure Virtual Desktop - Start VM on Connect](/uploads/avd_startvmconnectprogress1.png \\"Azure Virtual Desktop - Start VM on Connect\\")\\n 9. ![Azure Virtual Desktop - Start VM on Connect](/uploads/avd_startvmconnectprogress2.png \\"Azure Virtual Desktop - Start VM on Connect\\")\\n10. Before finally prompting for their login credentials!"},{"id":"azure/Azure-Virtual-Desktop-Optimisations","metadata":{"permalink":"/azure/Azure-Virtual-Desktop-Optimisations","source":"@site/blog/2021-07-01-Azure-Virtual-Desktop-Optimisations.md","title":"Azure Virtual Desktop Optimisations","description":"If you are running Azure Virtual Desktop, you want to get the most performance and stability out of them as possible, to reduce cost and increase user experience.","date":"2021-07-01T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":13.115,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-07-01 00:00:00 +1200","title":"Azure Virtual Desktop Optimisations","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/avddesktopfull.png"},"slug":"azure/Azure-Virtual-Desktop-Optimisations"},"unlisted":false,"prevItem":{"title":"Start VM on Connect for Azure Virtual Desktop","permalink":"/2021/07/03/start-vm-on-connect-for-azure-virtual-desktop"},"nextItem":{"title":"How to setup FSLogix profiles for Azure Virtual Desktop","permalink":"/azure/how-to-setup-fslogix-profiles-for-azure-virtual-desktop"}},"content":"If you are running Azure Virtual Desktop, you want to get the most performance and stability out of them as possible, to reduce cost and increase user experience.\\n\\nThese are a few recommended policies and optimisations to apply to your Azure Virtual Desktop setup. These are in no particular order; they are just recommendations.\\n\\n### Configure Timezone Redirection\\n\\nTimezone redirection will allow you to pass through the time from the local device to the Azure Virtual Desktop host. This is useful to keep the consistent time between the device you are connecting from and the session host, and by default, the timezone in Azure is UTC.\\n\\n1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n2. **Expand** your **domain** and **Group Policy Objects**.\\n3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Policies** > **Administrative Templates** > **Windows Components** > **Remote Desktop Services** > **Remote Desktop Session Host** > **Device** **and Resource Redirection**.\\n5. **Enable** the setting **Allow time zone redirection**.\\n6. **Close** the **Group Policy Management console**; as this is a Computer-based policy, it may take up to 90 minutes to take effect unless the session hosts are restarted to force it to pick up the policy sooner.\\n\\n### Configure Session Time Limit Policies\\n\\nYou can use this policy to specify the maximum amount of time that a disconnected session remains active on the server. By default, Remote Desktop Services allows users to disconnect from a Remote Desktop Services session without logging off and ending the session. Unfortunately, this means that sessions users sessions may remain open for an extended period of time, taking up usable resources.\\n\\nWhen configuring these, take into consideration a users normal work time, the time they have for lunch etc., the sweet spot to disconnect their session is not during their lunch break, but after they have finished for the day, usually 8-12 hours is recommended, but is dependant on how Azure Virtual Desktop is used.\\n\\n1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n2. **Expand** your **domain** and **Group Policy Objects**.\\n3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Policies** > **Administrative Templates** > **Windows Components** > **Remote Desktop Services** > **Remote Desktop Session Host** > **Session Time Limits**.\\n5. Configure the below settings per your organisation policies:\\n\\n* **Set time limit for active but idle Remote Desktop Services sessions**\\n* _This policy allows you to specify the maximum amount of time that an active Remote Desktop Services session can be idle (without user input) before it is automatically disconnected._\\n* **Set time limit for active Remote Desktop Services sessions**\\n* _This policy allows you to specify the maximum amount of time that a Remote Desktop Services session can be active before it is automatically disconnected._\\n* **Set time limit for disconnected sessions**\\n* _This policy allows you to configure a time limit for disconnected Terminal Services sessions._\\n* **End session when time limits are reached**\\n* _This policy allows you to specify whether to terminate a timed-out Terminal Services session instead of disconnecting it._\\n* **Set a time limit for log off of RemoteApp sessions**\\n* _This policy allows you to specify how long a user\'s RemoteApp session will remain in a disconnected state after closing all RemoteApp programs before the session is logged off from the RD Session Host server._\\n* **Close** the **Group Policy Management console**; as this is a Computer-based policy, it may take up to 90 minutes to take effect unless the session hosts are restarted to force it to pick up the policy sooner.\\n\\n_Reference: Taken from:_ [_https://kb.parallels.com/en/123638_](https://kb.parallels.com/en/123638 \\"https://kb.parallels.com/en/123638\\")\\n\\n### DeleteUserAppContainersOnLogoff\\n\\nBack in March 2019, there were issues with slow server performance caused by numerous Windows Firewall Rules getting created on user login. A patch was released; however, to enable this \'_fix_\', a registry key needs to be set. You could eventually run into host performance/hang issues if this key is not configured. See: [https://support.microsoft.com/en-us/help/4490481](https://support.microsoft.com/en-us/help/4490481 \\"https://support.microsoft.com/en-us/help/4490481\\")\\n\\n 1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n 2. **Expand** your **domain** and **Group Policy Objects**.\\n 3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n 4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Preferences**> **Windows Settings** > **Registry**.\\n 5. Right-click in the window and select **New**, **Registry Item**\\n 6. Select **Update** as the Action\\n 7. Make sure **HKEY_LOCAL_MACHINE** is set to **Hive**\\n 8. Enter in the following for the Key Path: **SYSTEM\\\\\\\\CurrentControlSet\\\\\\\\Services\\\\\\\\SharedAccess\\\\\\\\Parameters\\\\\\\\FirewallPolicy**\\n 9. For the Value name type: **DeleteUserAppContainersOnLogoff**\\n10. Change the Value type to **REG_DWORD**\\n11. Put: \'**1**\' to enable the option and click **Apply**\\n12. **Close** the **Group Policy Management console**. As this is a Computer-based policy, it may take up to 90 minutes to take effect unless the session hosts are restarted to force it to pick up the policy sooner.\\n\\n![Delete user Apps](/uploads/deleteuserapp.png)\\n\\n### Configure RDP Shortpath\\n\\nRDP Shortpath is a feature of Azure Virtual Desktop that establishes a direct UDP-based transport between Remote Desktop Client and Session host. RDP uses this transport to deliver Remote Desktop and RemoteApp while offering better reliability and consistent latency. RDP Shortpath establishes the direct connectivity between Remote Desktop client and Session Host. Direct connectivity reduces the dependency on the Azure Virtual Desktop gateways, improves the connection\'s reliability, and increases the bandwidth available for each user session. You can read more about it here: [Azure Virtual Desktop RDP Shortpath](https://learn.microsoft.com/en-us/azure/virtual-desktop/shortpath?WT.mc_id=AZ-MVP-5004796 \\"Azure Virtual Desktop RDP Shortpath\\").\\n\\n 1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n 2. **Expand** your **domain** and **Group Policy Objects**.\\n 3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n 4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Preferences**> **Windows Settings** > **Registry**.\\n 5. Right-click in the window and select **New**, **Registry Item**\\n 6. Select **Update** as the Action\\n 7. Make sure **HKEY_LOCAL_MACHINE** is set to **Hive**\\n 8. Enter in the following for the Key Path: **SYSTEM\\\\\\\\CurrentControlSet\\\\\\\\Control\\\\\\\\Terminal Server\\\\\\\\WinStations**\\n 9. For the Value name type: **fUseUdpPortRedirector**\\n10. Change the Value type to **REG_DWORD**\\n11. Put: \'**1**\' to enable the option and click **Apply**\\n12. Right-click in the window and select **New**, **Registry Item**\\n13. Select **Update** as the Action\\n14. Make sure **HKEY_LOCAL_MACHINE** is set to **Hive**\\n15. Enter in the following for the Key Path: **SYSTEM\\\\\\\\CurrentControlSet\\\\\\\\Control\\\\\\\\Terminal Server\\\\\\\\WinStations**\\n16. For the Value name type: **UdpPortNumber**\\n17. Change the Value type to **REG_DWORD**\\n18. Put: \'**3390**\' as the UDP report and click **Apply**\\n19. **Close** the **Group Policy Management console**. Restart the session hosts.\\n\\n### Virtual-Desktop-Optimization-Tool\\n\\nAutomatically apply a range of optimisations for pooled and personal Azure Desktop hosts, this is a good resource to add to your initial image creation builds. \\n\\n[Virtual-Desktop-Optimization-Tool](https://github.com/The-Virtual-Desktop-Team/Virtual-Desktop-Optimization-Tool \\"Virtual-Desktop-Optimization-Tool\\")\\n\\n### Implement Windows Defender FSLogix exclusions\\n\\nMake sure to configure antivirus exclusions for FSLogix Profiles.\\n\\nFor a list of exclusions, along with a PowerShell script to implement them, please refer to the following Microsoft documentation: [FSLogix for the enterprise](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/wvd/windows-virtual-desktop-fslogix?WT.mc_id=AZ-MVP-5004796 \\"FSLogix for the enterprise\\")\\n\\n### Implement FSLogix Profile Exclusions\\n\\nBy default, FSLogix will capture a lot of user profile data, including Teams Cache, Chrome cache and save it to the profile VHD/VHDX; this causes profile size bloat and can decrease the performance of your applications.\\n\\nIt is recommended to implement exclusions to reduce storing user profile data that you don\'t need.\\n\\n1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n2. **Expand** your **domain** and **Group Policy Objects**.\\n3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Policies** > **Administrative Templates** > **FSLogix** > **Profile Containers** > **Advanced**\\n5. **Enable** the setting **Provide RedirXML file to customize directions**.\\n6. Point the path to a **UNC path** that is **accessible** to all session hosts that **contains** are \'**redirections.xml**\' file. This just needs the folder; it will automatically pick up the redirections.xml file.\\n7. **Close** the **Group Policy Management console**. As this is a Computer-based policy, it may take up to 90 minutes to take effect unless the session hosts are restarted to force it to pick up the policy sooner.\\n\\nAn example redirections.xml can be found here:\\n\\n```xml title=\\"redirections.xml\\"\\n\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>\\n<FrxProfileFolderRedirection ExcludeCommonFolders=\\"0\\">\\n \\n<Excludes>\\n<Exclude>AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cache\\\\</Exclude>\\n<Exclude>AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Cached Theme Images\\\\</Exclude>\\n<Exclude>AppData\\\\Roaming\\\\Google\\\\Chrome\\\\UserData\\\\Default\\\\Code Cache\\\\js</Exclude>\\n<Exclude>AppData\\\\Local\\\\Google\\\\Chrome\\\\UserData\\\\Default\\\\Code Cache\\\\js</Exclude>\\n<Exclude>AppData\\\\Local\\\\Mozilla\\\\Firefox</Exclude>\\n<Exclude Copy=\\"0\\">AppData\\\\Local\\\\Microsoft\\\\Terminal Server Client</Exclude>\\n<Exclude Copy=\\"0\\">AppData\\\\Local\\\\Microsoft\\\\Edge SxS\\\\User Data\\\\Default\\\\Cache</Exclude>\\n<Exclude>AppData\\\\Roaming\\\\Adobe\\\\Flash Player\\\\AssetCache</Exclude>\\n<Exclude>AppData\\\\Roaming\\\\Adobe\\\\Flash Player\\\\NativeCache</Exclude>\\n<Exclude>AppData\\\\Roaming\\\\Microsoft\\\\Teams\\\\Cache</Exclude>\\n<Exclude>AppData\\\\Roaming\\\\Microsoft\\\\Teams\\\\Service Worker\\\\CacheStorage</Exclude>\\n<Exclude>Desktop</Exclude>\\n<Exclude>Documents</Exclude>\\n<Exclude>Downloads</Exclude>\\n<Exclude>Musics</Exclude>\\n<Exclude>Pictures</Exclude>\\n<Exclude>Videos</Exclude>\\n</Excludes>\\n \\n<Includes>\\n<Include Copy=\\"3\\">AppData\\\\LocalLow\\\\Sun\\\\Java\\\\Deployment\\\\security</Include>\\n<Include>AppData\\\\Roaming\\\\Google\\\\Chrome\\\\User Data\\\\Default\\\\Extensions</Include>\\n</Includes>\\n \\n</FrxProfileFolderRedirection>\\n\\n```\\n\\n_Note: Make sure you test and adjust this for your own environment. The Desktop/Documents have been excluded as the assumption is these are redirected or covered by OneDrive._\\n\\n### DeleteUserAppContainersOnLogoff\\n\\nBack in March 2019, there were issues with slow server performance caused by numerous Windows Firewall Rules getting created on user login. A patch was released; however, to enable this \'_fix_\', a registry key needs to be set. You could eventually run into host performance/hang issues if this key is not configured. See: [https://support.microsoft.com/en-us/help/4490481](https://support.microsoft.com/en-us/help/4490481 \\"https://support.microsoft.com/en-us/help/4490481\\")\\n\\n 1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n 2. **Expand** your **domain** and **Group Policy Objects**.\\n### Implement Storage Sense\\n\\nOn Windows 10, Storage sense is a built-in tool designed to free up space automatically. When it\'s enabled, the feature monitors your device. When it\'s running low on space, it deletes temporary files, empties the Recycle Bin, cleans up the Downloads folder, removes previous installation files, and more to make space to install new updates or store more important data. Storage Sense can also help dehydrate files that are available locally and do not need to be stored locally anymore, helping to reduce profile space and OneDrive processing.\\n\\n_Note: If you find that Storage Sense is missing, it is because it is mainly a client setting and may be missing from the Windows Server; you can copy the PolicyDefinitions folder from an Azure Virtual Desktop host to your domains Central Store, i.e. in my case \\\\\\\\\\\\\\\\luke.geek.nz\\\\\\\\SYSVOL\\\\\\\\luke.geek.nz\\\\\\\\Policies\\\\\\\\PolicyDefinitions. Or just look for StorageSense.admx and StorageSense.adml and copy it (the ADML goes in the language directory, i.e. en-US)._\\n\\n1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n2. **Expand** your **domain** and **Group Policy Objects**.\\n3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Policies** > **Administrative Templates** > **System** > **Storage Sense**.\\n5. **Enable** the setting **Allow Storage Sense**.\\n6. **Enable** the setting **Configure Storage Sense Cloud Content dehydration threshold**\\n7. Now we can provide the minimum number of days a cloud-backed file can remain unopened before Storage Sense dehydrates it back to Files on Demand, for example, **30** days since it was last accessed.\\n6. **Enable** the setting **Configure Storage Storage Downloads cleanup threshold**\\n7. **Type** in a minimum number of **days**, that files sit in the Downloads before before Storage sense will delete it.\\n8. **Close** the **Group Policy Management console**. As this is a Computer-based policy, it may take up to 90 minutes to take effect unless the session hosts are restarted to force it to pick up the policy sooner.\\n\\n![Storage Sense - Group Policy](/uploads/storagesense_gpo.png \\"Storage Sense - Group Policy\\")\\n\\n### Configure Microsoft Teams Optimisations\\n\\nYou can run Microsoft Teams in Azure Virtual Desktop. To do so, you need to install as a Machine installer and set the WVD environment variable.\\n\\n**Install as Machine:**\\n\\n    msiexec /i Teams_windows_x64 /l*v teams_install.log ALLUSER=1\\n\\n**Set IsWVDEnvironment key**:\\n\\n 1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n 2. **Expand** your **domain** and **Group Policy Objects**.\\n 3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n 4. In the Group Policy Management Editor, **navigate to Computer Configuration** > **Preferences**> **Windows Settings** > **Registry**.\\n 5. Right-click in the window and select **New**, **Registry Item**\\n 6. Select **Update** as the Action\\n 7. Make sure **HKEY_LOCAL_MACHINE** is set to **Hive**\\n 8. Enter in the following for the Key Path: **SOFTWARE\\\\\\\\Microsoft\\\\\\\\Teams**\\n 9. For the Value name type: **IsWVDEnvironment**\\n10. Change the Value type to **REG_DWORD**\\n11. Put: \'**1**\' to enable the option and click **Apply**\\n12. **Close** the **Group Policy Management console**. Restart the session hosts.\\n\\n**Install the Remote Desktop WebRTC Redirector**\\n\\n1. The Remote Desktop WebRTC Redirector onto the Sessions Hosts: [https://learn.microsoft.com/en-us/azure/virtual-desktop/teams-on-AVD#install-the-teams-websocket-service](https://learn.microsoft.com/en-us/azure/virtual-desktop/teams-on-AVD#install-the-teams-websocket-service \\"hhttps://learn.microsoft.com/en-us/azure/virtual-desktop/teams-on-AVD?WT.mc_id=AZ-MVP-5004796#install-the-teams-websocket-service\\")\\n\\n### Configure Auto Close Apps on Logoff\\n\\nWhen users may go to logoff, open applications may halt or prolong the logoff process and prompts for users to close applications, this can leave to sessions being left connected, if a user hits logoff or shutdown and walks away. To stop the prompt about open Applications we need to set a registry key - this is not an \'optimisation\' to be treated lightly, as it won\'t ask users to double check some of the apps they have open, as soon as they hit the logoff button - that it is, any open apps will be closed!\\n\\n 1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n 2. **Expand** your **domain** and **Group Policy Objects**.\\n 3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n 4. In the Group Policy Management Editor, **navigate to User Configuration** > **Preferences**> **Windows Settings** > **Registry**.\\n 5. Right-click in the window and select **New**, **Registry Item**\\n 6. Select **Update** as the Action\\n 7. Make sure **HKEY_CURRENT_USER** is set to **Hive**\\n 8. Enter in the following for the Key Path: **Control Panel\\\\\\\\Desktop**\\n 9. For the Value name type: **AutoEndTasks**\\n10. Change the Value type to **REG_SZ**\\n11. Put: \'**1**\' to enable the option and click **Apply**\\n12. **Close** the **Group Policy Management console**. Restart the session hosts.\\n\\nThis is a user-based policy, so will take effect on next logon.\\n\\n### Hide the Shutdown button\\n\\nThis is not so much of an optimization, but it is one of my favourite group policy configurations, something I implement in server base policies; it prevents that \\"Oops!\\" moment when someone clicks Shutdown on a server, especially with multi-session VDI machines, this just removes the shortcuts to shutdown and restart the server from the Start Menu.\\n\\n_Note: You can still restart and shut down the server from the Command Prompt with the \'shutdown\' command._\\n\\n1. On a server with the Group Policy Management Console is installed for managing your Azure Virtual Desktop farm, **open** the **Group Policy Management Console**.\\n2. **Expand** your **domain** and **Group Policy Objects**.\\n3. **Right**-**click** the **GPO** that you created for the group policy settings and select **Edit**.\\n4. In the Group Policy Management Editor, **navigate to User Configuration** > **Policies** > **Administrative Templates** > **Start Menu and Taskbar**\\n5. **Enable** the setting **Remove and prevent access to Shut Down, Restart, Sleep, and Hibernate commands.**\\n6. **Close** the **Group Policy Management console**; as this is a User-based policy, it should take effect on the next user login."},{"id":"azure/how-to-setup-fslogix-profiles-for-azure-virtual-desktop","metadata":{"permalink":"/azure/how-to-setup-fslogix-profiles-for-azure-virtual-desktop","source":"@site/blog/2021-06-29-how-to-setup-fslogix-profiles-for-azure-virtual-desktop.md","title":"How to setup FSLogix profiles for Azure Virtual Desktop","description":"If you have a few Azure Virtual Desktop machines, you need some way to keep user persistence\'s and application customisations, which would usually be stored in the user profile locally across multiple machines (or even the same machine if using Ephemeral OS), this is where FSLogix Profile Containers can assist.","date":"2021-06-29T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.4,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-06-29 00:00:00 +1200","title":"How to setup FSLogix profiles for Azure Virtual Desktop","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/how-to-setup-fslogix-profiles-for-azure-virtual-desktop"},"unlisted":false,"prevItem":{"title":"Azure Virtual Desktop Optimisations","permalink":"/azure/Azure-Virtual-Desktop-Optimisations"},"nextItem":{"title":"No Available Resources Error when attempting to connect to Azure Virtual Desktop","permalink":"/azure/no-available-resources-error-when-connecting-to-azure-virtual-desktop"}},"content":"If you have a few Azure Virtual Desktop machines, you need some way to keep user persistence\'s and application customisations, which would usually be stored in the user profile locally across multiple machines _(or even the same machine if using Ephemeral OS)_, this is where FSLogix Profile Containers can assist.\\n\\nWe are going to implement FSLogix using an Azure File Share, to store the profiles.\\n\\nI am going to assume you already have an Azure Virtual Desktop farm _(and Azure ADDS)_, if not you can check out my guide [here](https://luke.geek.nz/azure/create-a-azure-virtual-desktop-farm/ \\"How to create a Azure Virtual Desktop farm \\").\\n\\nThis article will be based on the Azure Virtual Desktop farm created in a previous article, however, you can just still along and replace the resource names and groups with your own.\\n\\n### Setup Storage Account\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on **Create a resource**\\n 3. Type in **Storage Account** and press Enter to search\\n 4. Select Storage account\\n 5. ![FSLogix - Azure Storage Account](/uploads/storageaccount.png \\"FSLogix - Azure Storage Account\\")\\n 6. Click **Create**\\n 7. If you already have a Resource Group, then **select** it, if not you can create a new **resource group**. I am going to put my resources user profiles in the same resource group as my utility server: aad_infra _(this is just personal preference, keeping the session hosts in their own resource groups)_.\\n 8. **Type in** a Storage **Account Name** _(the name needs to be globally unique across all of Azure, the field can contain only lowercase letters and numbers. Name must be between 3 and 24 characters.)_, in my case I have gone with: fslogixprofileslgnz.\\n 9. **Select** your **Region** _(the same region you have your Azure Virtual Desktop session hosts and Virtual Network)_\\n10. **Select** Standard **performance** _(Microsoft have recommendations, based on users on what Tier to select -_ [_https://learn.microsoft.com/en-us/azure/virtual-desktop/store-fslogix-profile_](https://learn.microsoft.com/en-us/azure/virtual-desktop/store-fslogix-profile?WT.mc_id=AZ-MVP-5004796 \\"https://learn.microsoft.com/en-us/azure/virtual-desktop/store-fslogix-profile?WT.mc_id=AZ-MVP-5004796\\")_)_\\n11. For Redundancy, I am going to **select LRS** storage _(I haven\'t built have any redundancy in my Azure Virtual Desktop farm)_.\\n12. _Note: Just a heads up, don\'t select Geo-Redundant if you are looking to create File Shares on this Storage account over 100TiB, it is only supported in LRS. If you do need this kind of large file size, I recommend using a completely different storage account from the one you are using for user profiles. My screenshot below has GRS, just ignore it!_\\n13. ![FSLogix - Azure Storage Account](/uploads/storageaccount_projectdetails.png \\"FSLogix - Azure Storage Account\\")\\n14. Click **Next: Advanced**\\n15. Leave everything as default and **select Next: Networking**\\n16. Now we need to configure a Private Endpoint for the Azure storage account to add onto the Virtual Network directly.\\n17. Select **Private endpoint** and click **+ Add Private endpoint**\\n18. **Verify** that your **Location** is **correct** and **type** in a **Name for** your **Private Endpoint** service, in my case: fslogixprofileslgnzPE\\n19. **Select** the drop-down for **Storage sub-resource** and select **file**\\n20. **Select** your **Virtual Network** and **subnet** _(I will be selecting my main resource subnet of aadds-subnet, where the Azure Virtual Desktop hosts are)_\\n21. Click **Ok**\\n22. ![FSLogix - Azure Storage Account](/uploads/storageaccount_privateendpoint.png \\"FSLogix - Azure Storage Account\\")\\n23. Select **Next: Data Protection**\\n24. **Untick** the **Enable soft delete for Blogs and Container\'s** _(we will only be using Azure Files in this storage account)_\\n25. Soft delete allows you to quickly recover a deleted file-share, even though we can backup the Azure Fileshare, my recommendation would be to leave this on for additional protection and \'7\' days is enough for me.\\n26. ![FSLogix - Azure Storage Account](/uploads/storageaccount_softdelete.png \\"FSLogix - Azure Storage Account\\")\\n27. Select **Review + Create**\\n28. **Validate** your **configuration** and select **Create**\\n\\n### Configure Storage Account\\n\\n 1. Once your storage account has been created, go to it.\\n 2. Navigate down the left-hand side Blade and select: **Networking**\\n\\n    Make sure: Selected networks are selected and the Private Endpoint connection is displaying.\\n 3. ![FSLogix - Azure Storage Account](/uploads/storageaccount_firewalls.png \\"FSLogix - Azure Storage Account\\")\\n 4. ![FSLogix - Azure Storage Account](/uploads/storageaccount_peapproved.png \\"FSLogix - Azure Storage Account\\")\\n 5. Now its time to join the Storage account to Microsoft Entra ID Domain Services, on the left-hand side Blade, click on **Configuration** _(under Settings)_\\n 6. Navigate to: **Identity-based access for file shares**\\n 7. Select **Enabled**\\n 8. Click **Save**\\n 9. ![FSLogix - Azure Storage Account](/uploads/storageaccount_adds_identity.png \\"FSLogix - Azure Storage Account\\")\\n10. Now its time to create the File Share, On the left-hand side Blade, navigate to **File Shares** (under Data Storage)\\n11. Select **+ File Share**\\n12. Give this **File share** a **name**: fslogixprofiles\\n13. Even though you don\'t need to have a Quota _(the Fileshare will grow)_, I will add one in stop any surprises and make sure that I have an ongoing task to review and optimize the profiles\\n14. Because user profiles are generally a lot of read/write activity, select **Transaction Optimized** _(take a look at the_ [_https://azure.microsoft.com/en-us/pricing/details/storage/files/_](https://azure.microsoft.com/en-us/pricing/details/storage/files/?WT.mc_id=AZ-MVP-5004796 \\"https://azure.microsoft.com/en-us/pricing/details/storage/files/?WT.mc_id=AZ-MVP-5004796\\") _)_\\n15. Click **Create**\\n16. ![FSLogix - File Share](/uploads/storageaccount_newfileshare.png \\"FSLogix - File Share\\")\\n17. One last thing we can do on the Storage Account is **enable backups** for your **Azure File Share** - [https://learn.microsoft.com/en-us/azure/backup/backup-afs?WT.mc_id=AZ-MVP-5004796](https://learn.microsoft.com/en-us/azure/backup/backup-afs?WT.mc_id=AZ-MVP-5004796 \\"https://learn.microsoft.com/en-us/azure/backup/backup-afs?WT.mc_id=AZ-MVP-5004796\\")\\n\\n### Configure File Share\\n\\nNow that the Microsoft Entra ID rights have been assigned and the File Share has been created, we now need to set up the NTFS permissions on the FSLogix share.\\n\\n 1. Navigate to **File Shares** _(under Data Storage)_\\n 2. **Click** on **your** file-**share**\\n 3. Click on **Properties**\\n 4. **Copy** the **URL**\\n 5. ![FSLogix - File Share](/uploads/storageaccount_fslogixprofiles.png \\"FSLogix - File Share\\")\\n 6. **Remove http** and replace the forward slashes with backslashes so it looks like this: \\\\\\\\\\\\\\\\fslogixprofileslgnz.file.core.windows.net\\\\\\\\fslogixprofiles\\n 7. **Using** a **user** that is a **member of** the \'**AVD Admins\'** group and can **log into** the **Azure Virtual Desktop** farm _(it\u2019s a good chance to test connectivity to the Storage account through the private endpoint from your Azure Virtual Desktop session host)_\\n 8. Open **Computer**\\n 9. Select the Computer Tab and select **Map network drive**\\n10. ![FSLogix - Mapped Drive](/uploads/computermappeddrive.png \\"FSLogix - Mapped Drive\\")\\n11. **Select a drive letter** that isn\'t in use and paste in the **UNC path** created earlier _(step 6)_.\\n12. ![FSLogix - Mapped Drive](/uploads/computermappingdrive.png \\"FSLogix - Mapped Drive\\")\\n13. Hopefully, you should **successfully** have **mapped** a **drive**!\\n14. Once the drive is mapped, **open** up a **Command Prompt**\\n\\n    _Note: Don\'t run the Command Prompt as Administrator, as this runs in a separate context and doesn\'t have permissions to the mapped drive._\\n15. **Run** the **following command** to **set** the necessary **NTFS permissions** _(change the Drive mapping and AVD Users group to your own group)_:\\n\\n        icacls z: /grant \\"AVD Users\\":(M)\\n        \\n        icacls z: /grant \\"Creator Owner\\":(OI)(CI)(IO)(M)\\n        \\n        icacls z: /remove \\"Authenticated Users\\"\\n        \\n        icacls z: /remove \\"Builtin\\\\Users\\"\\n16. ![FSLogix - Security Permissions](/uploads/setfslogixpermissions.png \\"FSLogix - Security Permissions\\")\\n17. The permissions should look similar to:\\n18. ![FSLogix - Security Permissions](/uploads/setfslogixpermissions2.png \\"FSLogix - Security Permissions\\")\\n\\n### Configure FSLogix policies\\n\\nNow that you have successfully created a Storage Account and granted it the proper permissions, we now need to configure Group Policy for FSLogix.\\n\\n 1. **Connect to** your Microsoft Entra ID **Utility server**, that has **Group Policy management** installed using an account in the: AAD DC Administrators group\\n 2. **Download** the latest **FSLogix Agent** - [https://aka.ms/fslogix_download](https://aka.ms/fslogix_download \\"https://aka.ms/fslogix_download?WT.mc_id=AZ-MVP-5004796\\") onto the Utility server\\n 3. **Extract** the FSLogix agent **zip** file to a folder\\n 4. Now we will **create** a **Central Store** to manage the Group Policy consistently\\n 5. On your Utility server, browse to **C:\\\\\\\\Windows** _(If you are primarily using Azure Virtual Desktop, it may be best to copy the PolicyDefinitions folder from an Azure Virtual Desktop session host to make sure you can edit all the latest Windows 10 policies)_\\n 6. **Copy** the **PolicyDefinitions** folder\\n 7. Copy **the PolicyDefinitions** folder to your **Policies** folder on your **domain**: \\\\\\\\luke.geek.nz\\\\\\\\SYSVOL\\\\\\\\luke.geek.nz\\\\\\\\Policies\\\\\\n  _(replace luke.geek.nz, with your ADDS DNS name)_\\n 8. ![FSLogix - Group Policy](/uploads/sysvolpolicies.png \\"FSLogix - Group Policy\\")\\n 9. **Go to** your **extracted** FSLogix **folder** and **copy**:\\n    * fslogix.admx to: \\\\\\\\luke.geek.nz\\\\\\\\SYSVOL\\\\\\\\luke.geek.nz\\\\\\\\Policies\\\\\\\\PolicyDefinitions\\\\\\n    * fslogix.adml to: \\\\\\\\luke.geek.nz\\\\\\\\SYSVOL\\\\\\\\luke.geek.nz\\\\\\\\Policies\\\\\\\\PolicyDefinitions\\\\\\\\en-US\\\\\\n10. This will allow us to use Group Policy to manage FSLogix using Group Policy, **Open Group Policy Management**\\n11. **Navigate** to your **Hosts OU**\\n12. Right-click the OU and select: **Create a GPO in this domain, and Link it here\u2026**\\n13. Name it according to your naming standards (this is a Computer-based policy) - in my example, I am using: AVD_ComputerPolicy\\n14. Click **Ok**\\n15. ![FSLogix - Group Policy](/uploads/gpo_management_createpolicy.png \\"FSLogix - Group Policy\\")\\n16. Right-click the GPO you have just created and select **Edit\u2026**\\n17. Because this is a Computer-based policy, to speed up processing, **right-click** the **Policy heading** and select **Properties**\\n18. Tick: **Disable User Configuration Settings**\\n19. Confirm that you want to do it and select **Yes**\\n20. Click **Apply**\\n21. While you have the screen open, **click on** **Comment**, and **add** in some **details** about the GPO for future reference then click **Apply** and **Ok**\\n22. ![FSLogix - Group Policy](/uploads/gpo_avd_computerpolicy.png \\"FSLogix - Group Policy\\")\\n23. Now it\'s time to actually configure the FSLogix Group Policy settings.\\n24. **Navigate** to: Computer Configuration\\\\\\\\Policies\\\\\\\\Administrative Templates\\\\\\\\FSLogix\\\\\\\\**Profile Containers**\\n25. Open up **Enabled** and select: **Enabled** and **Apply**\\n26. Open: **VHD Location** and **copy** in your Profiles **UNC share** _(for example, mine is:_ \\\\[_\\\\\\\\\\\\\\\\fslogixprofileslgnz.file.core.windows.net\\\\\\\\fslogixprofiles)_ click **Ok**\\n27. Select: **Delete local profile when FSLofix profile should apply**, click **Enabled** and check to **Delete local profile when FSLogix Profile should apply** _(don\'t blindly follow this, I am making the assumption this is a new farm, with no user-based profile stored on it. You may need to create a separate GPO to test this setting on, or you could lose valuable data)_.\\n28. Open: **Set Outlook cached mode on successful container attach** to **Enabled**.\\n29. Now in Group Policy Management console, click on **Container and Directory Naming** and select **Virtual Disk type**\\n30. Click **Enabled** and change the Option to **VHDX**, click **Ok**\\n31. Click on: **Swap directory name components setting** and click **Enabled**, **check** the s**wap directory name components** and click **Apply**\\n32. **Restart** the Azure Virtual Desktop **session hosts** to pick up the new policies.\\n33. **You have now set up FSLogix profiles! If you map the drive you should see your user profile folders!**\\n34. ![FSLogix - Mapped Profiles](/uploads/computermappingdrivelast.png \\"FSLogix - Mapped Profiles\\")"},{"id":"azure/no-available-resources-error-when-connecting-to-azure-virtual-desktop","metadata":{"permalink":"/azure/no-available-resources-error-when-connecting-to-azure-virtual-desktop","source":"@site/blog/2021-06-29-no-available-resources-error-when-connecting-to-azure-virtual-desktop.md","title":"No Available Resources Error when attempting to connect to Azure Virtual Desktop","description":"When connecting to Azure Virtual Desktop, you may get a \\"We couldn\'t connect because there are currently no available resources. Try again later or contact tech support for help if this keeps happening.\\"","date":"2021-06-28T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.73,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-06-28T12:00:00.000Z","title":"No Available Resources Error when attempting to connect to Azure Virtual Desktop","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/no-available-resources-error-when-connecting-to-azure-virtual-desktop"},"unlisted":false,"prevItem":{"title":"How to setup FSLogix profiles for Azure Virtual Desktop","permalink":"/azure/how-to-setup-fslogix-profiles-for-azure-virtual-desktop"},"nextItem":{"title":"How to create a Azure Virtual Desktop farm","permalink":"/2021/06/27/create-a-azure-virtual-desktop-farm"}},"content":"When connecting to Azure Virtual Desktop, you may get a _\\"We couldn\'t connect because there are currently no available resources. Try again later or contact tech support for help if this keeps happening.\\"_\\n\\n![We couldn\'t connect because there are currently no available resources.](/uploads/noresourcesavd.png \\"We couldn\'t connect because there are currently no available resources.\\")\\n\\n### Check your Max Session Count\\n\\nOn your Azure Virtual Desktop Host Pool, check your Max Session Count, which hasn\'t been exceeded. \\n\\nIn my screenshot below, even one connection to my Azure Virtual Desktop farm couldn\'t connect; this was fixed when I raised this.\\n\\n![Host Pool - Max Session Count](/uploads/maxsessionlimitavd.png \\"Host Pool - Max Session Count\\")\\n\\n### Check your Host Pool sessions are available\\n\\nCheck your Azure Virtual Desktop Host pool; Session Hosts are: \\n\\n* Available\\n* Not in Drain Mode\\n\\n![Host Pool - Host Pool Status](/uploads/avd_sessionhots.png \\"Host Pool - Host Pool Status\\")"},{"id":"/2021/06/27/create-a-azure-virtual-desktop-farm","metadata":{"permalink":"/2021/06/27/create-a-azure-virtual-desktop-farm","source":"@site/blog/2021-06-27-create-a-azure-virtual-desktop-farm.md","title":"How to create a Azure Virtual Desktop farm","description":"Previously known as Windows Virtual Desktop, Azure Virtual Desktop is the successor of Microsoft Remote Desktop; although compatible with Server OS (Operating System), it is the first to support Windows 10(and soon Windows 11)_ multisession, reducing application compatibility issues and giving consistent user experience.","date":"2021-06-26T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":16.575,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-06-26T12:00:00.000Z","title":"How to create a Azure Virtual Desktop farm","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"No Available Resources Error when attempting to connect to Azure Virtual Desktop","permalink":"/azure/no-available-resources-error-when-connecting-to-azure-virtual-desktop"},"nextItem":{"title":"Remove old PowerShell modules versions using PowerShell","permalink":"/2021/06/18/remove-old-powershell-modules-versions-using-powershell"}},"content":"Previously known as Windows Virtual Desktop, [Azure Virtual Desktop](https://learn.microsoft.com/en-us/azure/virtual-desktop/overview?WT.mc_id=AZ-MVP-5004796 \\"What is Azure Virtual Desktop?\\") is the successor of Microsoft Remote Desktop; although compatible with Server OS (Operating System), it is the first to support Windows 10_(and soon Windows 11)_ multisession, reducing application compatibility issues and giving consistent user experience.\\n\\nIn this guide, I will run you through creating Azure Virtual Desktop from scratch, along with some prerequisites that will help you manage AVD after you create it.\\n\\nBefore I begin, I recommend reading the Azure Virtual Desktop Azure product page \\"[here](https://azure.microsoft.com/en-us/services/virtual-desktop/?WT.mc_id=AZ-MVP-5004796 \\" Azure Virtual Desktop -   Enable a secure remote desktop experience from virtually anywhere.\\")\\" to understand the pricing model, features and additional resources that could help you in your journey.\\n\\nWhen selecting a region for your Session Hosts _(Virtual Machines)_, I recommend you have a look at the: [Azure Virtual Desktop Experience Estimator](https://azure.microsoft.com/en-us/services/virtual-desktop/assessment/?WT.mc_id=AZ-MVP-5004796 \\" Azure Virtual Desktop Experience Estimator\\") to help validate the proper region for your Session Hosts and the round trip time _(I am in New Zealand, so my recommended region is: Australia East, which is what I will be using for this guide)_.\\n\\n_If you don\'t already have a Microsoft Azure subscription, you can sign up for a Free subscription \\"_[_here_](https://azure.microsoft.com/en-us/free/?WT.mc_id=AZ-MVP-5004796 \\"Create your Azure free account today\\")_\\"._\\n\\nAssuming you already have an Azure subscription and the appropriate access to create resources in that subscription, gets begin!\\n\\n## Create Microsoft Entra ID Domain Services\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on **Create a resource**\\n 3. Search for: **Azure AD Domain Services**. You can change the Publisher Type to Microsoft, so it doesn\'t display any other marketplace offerings.\\n    ![Azure AD Domain - Marketplace](/uploads/azureaddssearch.png \\"Azure AD Domain - Marketplace\\")\\n    ![Azure AD Domain - Marketplace](/uploads/azureaddsmarketplace.png \\"Azure AD Domain - Marketplace\\")\\n 4. Click **Create**\\n 5. If you already have a **Resource Group**, select it - in this Demo, we are going to create one: aad_prod\\n 6. Type in the **DNS** domain **name** - this is the **FQDN** of your **domain**; in my demo, I will choose internal.luke.geek.nz.\\n 7. Because I am in New Zealand, the closest region to me is Australia East, so that\u2019s the region I will select. Make sure you **select** the **appropriate region** for where your Azure Virtual Desktop workloads are.\\n 8. **Select** the **SKU** and **Resource Type**; you can see the Pricing Calculator and the \\"Help Me choose..\\" links to verify your SKU and Forest type _(however, in most cases, such as Azure Virtual Desktop, your Forest Type will be \'User\')_.\\n    ![Azure AD Domain Services - Basic Config](/uploads/adds_basics.png \\"Azure AD Domain Services - Basic Config\\")\\n 9. Click **Next**\\n10. We will set up the **Networking**; if you have an already existing Virtual Network, select it.\\n    _Azure AD Domain Services uses a dedicated subnet within a virtual network to hold all of its resources. If using an existing network, ensure that the network configuration does not block the ports required for Azure AD Domain Services to run._ [_Learn more_](https://learn.microsoft.com/en-us/azure/active-directory-domain-services/tutorial-create-instance?WT.mc_id=AZ-MVP-5004796)\\n11. I will let it create a **Virtual Network** and its Subnet (/24); click **Next**.\\n    ![Azure AD Domain Services - Networking Config](/uploads/adds_networking.png \\"Azure AD Domain Services - Networking Config\\")\\n12. **Azure AD Domain Services** will **create** a new Azure AD **Group** called: **AAD DC Administrators** - this group will be used for Administrator level permissions on the Azure AD Domain Services domain _(it automatically adds the account you are using to create Azure AD Domain Services into this group)_.\\n13. You can **configure Membership** of this **group** now and configure who gets alerted if there are issues with Azure AD Domain Services.\\n14. When you are ready, select **Next**.\\n    ![Azure AD Domain Services - Administration Config](/uploads/adds_admin.png \\"Azure AD Domain Services - Administration Config\\")\\n15. Depending on the amount of Microsoft Entra ID users you have in your organisation, and whether they will need Azure AD Domain Services, you can choose to synchronise **ALL Azure AD Groups and Users**, or specific groups of users _(this can be changed later)_, because my Azure AD Organisation is fairly low, I am going to Sync everything, click **Next**.\\n16. One thing to note here is the recommendation on the number of Objects _(Users, Groups)_ that will get synced to Azure AD Domain Services; for the Standard SKU, the suggested Object Count is 0 to 25,000 - for the Enterprise SKU, it is 25,000 to 100,000. So although there is no hard limit, it might be worth upgrading the SKU you are running for the additional backups and authentication if fit in the Enterprise space.\\n    ![Azure AD Domain Services - Syncronisation Config](/uploads/adds_sync.png \\"Azure AD Domain Services - Syncronisation Config\\")\\n17. We can now **configure** the **Security Setting**s, the only setting I am going to change here is **TLS 1.2 Only Mode** to **Enable**\\n    ![Azure AD Domain Services - Security Config](/uploads/adds_securitysettings.png \\"Azure AD Domain Services - Security Config\\")\\n18. Enter any applicable Tags and click **Review & Create** to **validate** your **configuration**.\\n19. Review your configuration, and if you are happy with it: Select **Create**.\\n20. Confirm that you are happy with the following and click **Ok**\\n    ![Azure AD Domain Services](/uploads/adds_youshouldknow.png \\"Azure AD Domain Services\\")\\n    **Note: Azure AD Domain Services can take up to an hour to provision.**\\n21. Once your Azure AD Domain Services has been configured, we must make some final configuration changes to point the Virtual Network DNS to use the Azure AD Domain Services. So first, **open** your newly created **Azure AD Domain Services**.\\n22. Click on **Overview** and: **Configuration** issues for your managed domain were detected. **Run configuration Diagnostics**\\n    ![Azure AD Domain Services](/uploads/adds_configissues.png \\"Azure AD Domain Services\\")\\n23. Click on **Run**\\n24. It should find a DNS record issue; click **Fix** to **set** the **DNS settings** of the Virtual Network to use the Azure AD Domain Services.\\n    Please **be careful here**, especially if you have already existing DNS settings; you might have to add it manually.\\n\\n## Create a Utility server to help Administer Azure Virtual Desktop\\n\\nWe need to create a Virtual Machine to help manage the AAD Domain and deploy Group Policies to help manage and configure the Azure Virtual Desktop farm.\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on **Create a resource.**\\n 3. Search for: **Windows Server 2019 Datacenter** and select **Create**\\n 4. If you already have a **Resource Group**, select it - in this Demo, we are going to create one: aad_infra\\n 5. **Specify** a **name** for the **Virtual Machine** _(I am going to use: UTILITY-P01)_\\n 6. **Select** a **Region** _(use the same Region as the Azure AD Domain Services and Azure Virtual Desktop resources)_\\n 7. For the **Image**, you can select either Windows Server 2019 Datacenter -Gen 1 or **Gen 2**; in my case, I am going with Gen2 _(although it doesn\'t matter)_.\\n 8. I am a firm believer in **selecting** the **smallest size** possible for the size, then scaling up when/where needed; I am going to go with a Standard_B2ms.\\n    ![Azure - Create VM](/uploads/createvm1.png \\"Azure - Create VM\\")\\n 9. Now we need to enter in the **Administrator** (local account) **Username** and **Password**.\\n10. Select \'**None**\' for **Public** inbound **ports**\\n11. If you have existing Windows Server licenses, you can select Hybrid Use Benefit; if not, select **Next: Disks**.\\n    ![Azure - Create VM](/uploads/createvm2.png \\"Azure - Create VM\\")\\n12. For the disks, I only need the OS disk, so I don\'t need to add a Data Disk _(although you could use this to store your Application install files etc.)_; however, to reduce cost, I am going to change the **Disk type** to **Standard SSD** _(locally-redundant storage)_ and select **Next: Networking**.\\n    ![Azure - Create VM](/uploads/createvmdisks.png \\"Azure - Create VM\\")\\n13. For the Virtual Network, make sure you **select** the **same** Virtual **Network** that the **Azure AD Domain Services** has been **installed to**; I will select the: aadds-subnet created earlier for my Utility server.\\n14. Set \'**None**\' for the **Public IP** and select **Next: Management**\\n    ![Azure - Create VM](/uploads/createvmnetworkinterface.png \\"Azure - Create VM\\")\\n15. Feel free to **leave** this all **as Default**\\n16. Just **be wary** of the **Auto-shutdown** settings, which will automatically shut down the VM daily _(I will keep mine selected as this is just a demo, and I only need the UTILITY server for initial configuration, it doesn\'t need to be running 24.7)_.\\n17. If you have a **Recovery Services Vault**, now is a good time to **add** the Utility **server** to **Backups**, so you don\'t forget it later, select Review & Create\\n18. **Verify** the **configuration** is correct and select **Create**\\n\\n## Create Azure Bastion to connect to the Utility server\\n\\nOnce the VM has been created, we now need to connect to it securely, so we will create a Bastion instance, which will allow us to connect to it without publishing the RDP (Remote Desktop Protocol) over the internet.\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on **Create a resource**\\n 3. Search for: **Bastion**\\n    ![Azure - Bastion](/uploads/bastionmarketplace.png \\"Azure - Bastion\\")\\n 4. Click **Create**\\n 5. This is a Networking resource to place it in the same Resource Group as my Virtual Network.\\n 6. Please type in a **Name** for the **Bastion** instance; I will call mine: Bastion\\n 7. **Select** the **Region** that **matches** the Virtual **Network** region\\n 8. Select the **Virtual Network**\\n 9. It now warns you about creating an: AzureBastionSubnet with a prefix of at least /27, so we need to create one; click on **Manage Subnet Configuration**.\\n10. Click **+ Subnet**\\n11. For the Name type in: **AzureBastionSubnet**\\n12. For the **Subnet** address range: **10.0.1.0/27**\\n    _If you get an error that indicates the address is overlapping with the aadds-subnet, it may be because the Address space is only a /24; click Cancel and click on Address Space in the Virtual Network and change the /24 to/16 to increase the address range._\\n13. Click **Save** to create the subnet\\n    ![Azure - Bastion](/uploads/az_subnet.png \\"Azure - Bastion\\")\\n14. Up the Top, click **Create a Bastion**. To go back to the Bastion setup, your Subnet should be selected automatically.\\n15. You do need a **Public IP** for Bastion, so **confirm** the **name** is appropriate, then click **Review + Create**\\n    ![Azure - Bastion](/uploads/bastionsetup.png \\"Azure - Bastion\\")\\n16. Click on **Create** to create your Bastion instance!\\n\\n**Note: Bastion may take 10-20 minutes to provision.**\\n\\n## Configure the Utility server\\n\\nNow that we have a Bastion instance, it is time to connect and configure the Utility server and create a new Azure AD user for Azure Virtual Desktop configuration.\\n\\n 1. First thing I am going to create a separate Azure AD account to manage the Utility server and join the Azure Virtual Desktop session hosts to the domain; this is to separate my own account. Azure AD Domain Services relies on password hash. So you won\'t be able to log in using Azure AD Domain Services unless you and the people using it have reset their passwords AFTER Azure AD Domain Services has been created.\\n 2. Navigate to the Azure Portal and open **Microsoft Entra ID**\\n 3. Click on **Users**\\n 4. Click on **+ New User**\\n 5. **Type** in the **username** of a **user**, I am going to use: \'avdjoin\'\\n 6. Type in an **easily identifiable name**\\n 7. Generate or put in a secure **password**\\n 8. **Add** to the **AAD DC Administrators** group\\n 9. Click **Ok** to create the user\\n    ![Azure AD - Users](/uploads/avdjoin.png \\"Azure AD - Users\\")\\n10. Once the account has been created, make sure to **login** with it **to** the **Azure Portal** or Office portal to **force** a final **password reset**, or you won\'t be able to use it in the next steps as it will be waiting for a password reset.\\n11. Once that account has been created, it\'s time to **join** your utility **server** to the Microsoft Entra ID **Domain**, navigate to your Utility **server** and click **Connect**.\\n12. Select **Bastion**\\n13. Select **Use Bastion**\\n14. **Type** in the **username** and **password** of the **LOCAL account** created when the Virtual Machine was created and click **Connect**\\n    _Note: If you are running a popup blocker, you need to allow it to open, as Bastion opens up the connection in a new window._\\n    ![Azure Bastion](/uploads/azurebastionconnect.png \\"Azure Bastion\\")\\n15. You should now be logged in to the server successfully.\\n16. Now it\'s time to join the server to the domain _(make sure that DNS is configured for AD Domain Services on the Virtual Network, see the last step in the AD Domain Services section, or you won\'t be able to domain join anything)_.\\n17. In **Server Management**, click on **Local Server**\\n18. Select **WORKGROUP**\\n19. Click **Change\u2026**\\n20. Select **Domain**\\n21. Please **type** in the DNS name of your **domain**; in my demo, it is: luke.geek.nz\\n22. **Type** in the **username** and **password** of the account we created earlier and clicked **Ok**\\n    ![Azure - Domain Join](/uploads/jointodomain.png \\"Azure - Domain Join\\")\\n23. Once you see, Welcome to the domain, click **Ok** a few times to **restart** the **server**.\\n24. Once the server has been restarted, you can now close your **bastion** window and **reconnect** using your **Azure AD** credentials (in my case, avdjoin), a **member** of the **ADDC Administrators group**.\\n    ![Azure - Connect to Bastion](/uploads/azurebastionconnect2.png \\"Azure - Connect to Bastion\\")\\n25. You have now successfully connected using an Azure AD account to the AD Services domain.\\n26. Now it\'s time to install some base Active Directory tools\\n27. Open Windows **PowerShell** as **Administrator**\\n28. **Type** in the following PowerShell **commands**:\\n\\n        Add-WindowsFeature RSAT-Role-Tools\\n        \\n        Install-WindowsFeature \u2013Name GPMC\\n\\n_Note: You can use the little arrows on the left-hand side of your Remote Desktop window to copy and paste text to and from your Bastion connection._\\n\\n1. This will now install the base Active Directory remote management tools, including Group Policy Management, so you can now create and manage the Group Policy objects for your Azure Virtual Desktop hosts.\\n   ![Server Tools](/uploads/utility_servertools.png)\\n2. We will now set up some base configurations to **create** a custom **OU** for the Azure Virtual Desktops **hosts** to go into:\\n\\n* Open **Active Directory Users & Computers**\\n* **Expand** out the **Domain** and right-click (at the Top Level)\\n* Select **New, Organisational Unit**\\n\\n![Server Tools](/uploads/utility_newou.png)\\n\\n* Type in: AVD\\n* In the AVD OU, **create** a new **OU** called: Hosts\\n* Now that we have an OU for the hosts, we will need to tell Azure what OU the hosts go into, so while we have Active Directory Users and Computers open, click on View.\\n* Select **Advanced Features**\\n* **Right-click** the Hosts **OU**\\n* Select **Properties**\\n* Click on **Attribute Editor**\\n* Find the **distinguishedName attribute**\\n\\n![Server Tools](/uploads/utility_serverdn.png)\\n\\n* Open and **Copy** the **Value** for future _(in my case: OU=Hosts,OU=AVD,DC=luke,DC=geek,DC=nz)_ for future reference.\\n* Now that we have the AVD Hosts OU, you can also open Group Policy Management and create your Computer policies.\\n\\n## Deploy Azure Virtual Desktop\\n\\nNow we are ready to deploy Azure Virtual Desktop finally!\\n\\n 1. Log in to the **Azure Portal**\\n 2. Click on **Create a resource**\\n 3. Find and select **Host pool**\\n    ![Azure Virtual Desktop - Host Pool](/uploads/avdhostpoolmarketplace.png \\"Azure Virtual Desktop - Host Pool\\")\\n 4. Click **Create**\\n 5. Please **create** a new **Resource Group** to help resources separately, and I am going to name mine: avd_prod\\n 6. **Type** in a **Host Pool Name**, I will call mine: avd-pooled\\n 7. Please select the **location** of the **Metadata** _(this is NOT the location of your session hosts, it\u2019s the gateway, select the Region closet to you as possible)_\\n 8. For **Host Pool Type**, if you want everyone to have a Virtual Machine each, you can select Personal; however, I want people to be shared across my servers, so I will select **Pooled**.\\n 9. For the Load balancing algorithm, we can choose to spread people over available hosts or fill up one host before moving connections to the next; we are going with **Breadth-first**.\\n10. Click **Next: Virtual Machines**\\n    ![Azure Virtual Desktop - Host Pool](/uploads/avdhostpoolsetup1.png \\"Azure Virtual Desktop - Host Pool\\")\\n11. Now we can **add** your Session **hosts** to the **Pool**.\\n12. By default, it has defaulted the Resource Group to the same Resource Group as the Host pool; however, you can separate them.\\n13. Please **select** a **Name prefix** for your session hosts, and it must be unique. Azure will automatically add a number to it as you build out more sessions hosts. I will put avdhost.\\n14. As I am based in New Zealand, I will be using the Australia East region.\\n15. We are going to use a **Gallery Image** of **Windows 10 Enterprise multi-session, Version 20H2 + M365 Apps** _(select the newest image at the time of your deployment)_\\n16. Select your **Virtual machine size**\\n17. **Select** the **number** of Virtual **Machines** you need\\n18. Select the **OS disk type**\\n    ![Azure Virtual Desktop - Host Pool](/uploads/avdhostpoolsetup2.png \\"Azure Virtual Desktop - Host Pool\\")\\n19. Select your **Virtual Machine** and **subnet**\\n20. Select **Yes** to specify your **domain** or unit\\n21. Type in your A**D Domain Services domain name**\\n22. If you don\'t **specify** an **OU**, it will create it in the: AADDC Computers OU. I had previously created a separate OU for my hosts so that I will enter the OU information.\\n23. For the **Domain Administrator** account, I will use the AVDJoin account I created earlier.\\n24. When the Virtual Machines get created, a **local Administrator** account will be created for each machine, and you can **specify** the **username** and **password** of what you want this account to be.\\n    ![Azure Virtual Desktop - Host Pool](/uploads/avdhostpoolsetup3.png \\"Azure Virtual Desktop - Host Pool\\")\\n25. Click **Next: WorkSpace**\\n26. Select **Yes** to **Register Desktop App Group**\\n27. We haven\u2019t created an Azure Virtual Desktop Workspace yet, so select **Create New**.\\n28. **Create** a **name** for your **Workspace**; my example is: avd_workspace\\n29. Click **Ok**\\n    ![Azure Virtual Desktop - Workspace](/uploads/avdworkspacesetup.png \\"Azure Virtual Desktop - Workspace\\")\\n30. Click on **Review + Create**\\n31. Confirm everything looks ok and click Create\\n    **Note: This may take 10-20 minutes to create your Azure Virtual Desktop resources:**\\n\\n* Host Pool\\n* Workspace\\n* Session hosts\\n\\n1. Once the resources have been created, you should now have an **Application group** for the Session Desktop.\\n2. **Open** the Application **Group** and click **Applications**; you should confirm the SessionDesktop application is listed. ![Azure Virtual Desktop - Application Group](/uploads/avdapplications.png \\"Azure Virtual Desktop - Application Group\\")\\n3. Click on the **SessionDesktop** to **change** the **Display name** _(this is the resource people will see when they go to your Azure Virtual Desktop_), and I changed mine to AVD Desktop. ![Azure Virtual Desktop - Application Group](/uploads/avddesktop.png \\"Azure Virtual Desktop - Application Group\\")\\n4. Click on **Assignments**\\n5. These are the Users & Groups that are allowed to access your Azure Virtual Desktop.\\n6. My recommendation would be to **add** a **Group** that contains your users, but in my demo, I will add in my: \'avdjoin\' account. ![Azure Virtual Desktop - Application Group](/uploads/avddesktopassignment.png \\"Azure Virtual Desktop - Application Group\\")\\n7. Using an assigned account, you can now **navigate** to**:** [https://rdweb.wvd.microsoft.com/arm/webclient/index.html](https://rdweb.wvd.microsoft.com/arm/webclient/index.html \\"https://rdweb.wvd.microsoft.com/arm/webclient/index.html\\") ![Azure Virtual Desktop - RD Web](/uploads/avdremotewebapp.png \\"Azure Virtual Desktop - RD Web\\")\\n8. You can now **launch** your **Desktop**.\\n9. **Congratulations**, you have now created and connected to Azure Virtual Desktop! ![Azure Virtual Desktop](/uploads/avddesktopfull.png \\"Azure Virtual Desktop\\")\\n\\n## Additional Configuration\\n\\n* You can Navigate to your Host Pool; under Settings, you can restrict or allow RDP settings, Device redirections and configure Display sessions.\\n* Configure [Start VM On Connect](https://luke.geek.nz/azure/start-vm-on-connect-for-azure-virtual-desktop/ \\"Start VM on Connect for Azure Virtual Desktop\\") to help reduce your spend.\\n* If you click on Session hosts, you can add additional hosts to your pool or Drain them to prevent logins.\\n* If you click Application Groups, you can add RemoteApp groups to allow users to connect directly to an Application versus a Full Desktop.\\n* Configure [FSLogix](https://luke.geek.nz/azure/how-to-setup-fslogix-profiles-for-azure-virtual-desktop/ \\"How to setup FSLogix profiles for Azure Virtual Desktop\\") profiles for user persistance.\\n* Set Disconnected Session Time limits in Group Policy, to automatically log off Disconnected sessions after \'x\' period of time."},{"id":"/2021/06/18/remove-old-powershell-modules-versions-using-powershell","metadata":{"permalink":"/2021/06/18/remove-old-powershell-modules-versions-using-powershell","source":"@site/blog/2021-06-18-remove-old-powershell-modules-versions-using-powershell.md","title":"Remove old PowerShell modules versions using PowerShell","description":"Did you know, that if you update PowerShell modules, the old versions can sometimes get left behind?","date":"2021-06-18T00:00:00.000Z","tags":[{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":0.495,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-06-18 00:00:00 +1200","title":"Remove old PowerShell modules versions using PowerShell","authors":["Luke"],"tags":["PowerShell"],"toc":false,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"How to create a Azure Virtual Desktop farm","permalink":"/2021/06/27/create-a-azure-virtual-desktop-farm"},"nextItem":{"title":"Add Log Analytics to Monitoring Agent with PowerShell","permalink":"/2021/06/13/add-log-analytics-workspace-to-microsoft-monitoring-agent-via-powershell"}},"content":"Did you know, that if you update PowerShell modules, the old versions can sometimes get left behind?\\n\\nThis little snippet with remove any old PowerShell modules _(that are not the latest version)_, which are installed.\\n\\n    #requires -Version 2.0 -Modules PowerShellGet\\n    function Remove-OldModules\\n    {\\n      <#\\n    <#\\n        authors: [Luke] Murray (Luke.Geek.NZ)\\n        Version: 0.1\\n        Purpose: Basic function to remove old PowerShell modules which are installed\\n    #>\\n    \\n      #>\\n      $Latest = Get-InstalledModule \\n      foreach ($module in $Latest) { \\n        \\n        Write-Verbose -Message \\"Uninstalling old versions of $($module.Name) [latest is $( $module.Version)]\\" -Verbose\\n        Get-InstalledModule -Name $module.Name -AllVersions | Where-Object {$_.Version -ne $module.Version} | Uninstall-Module -Verbose \\n      }\\n    }\\n    \\n    Remove-OldModules"},{"id":"/2021/06/13/add-log-analytics-workspace-to-microsoft-monitoring-agent-via-powershell","metadata":{"permalink":"/2021/06/13/add-log-analytics-workspace-to-microsoft-monitoring-agent-via-powershell","source":"@site/blog/2021-06-13-add-log-analytics-workspace-to-microsoft-monitoring-agent-via-powershell.md","title":"Add Log Analytics to Monitoring Agent with PowerShell","description":"Have you ever wanted to add a Log Analytics workspace to multiple Microsoft Monitoring Agent (MMA)\'s before?","date":"2021-06-13T00:00:00.000Z","tags":[],"readingTime":1.04,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-06-13 00:00:00 +1200","title":"Add Log Analytics to Monitoring Agent with PowerShell","authors":["Luke"],"Tags":["PowerShell"],"toc":false,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Remove old PowerShell modules versions using PowerShell","permalink":"/2021/06/18/remove-old-powershell-modules-versions-using-powershell"},"nextItem":{"title":"Update your Azure WebApp to be Always On","permalink":"/azure/update-your-azure-webapp-to-be-always-on"}},"content":"Have you ever wanted to add a Log Analytics workspace to multiple Microsoft Monitoring Agent (MMA)\'s before? \\n\\nMaybe you are setting up Windows Defender or wanting to redirect to collect event or performance logs. \\n\\nThis little quick script will help get you started on automating adding a Log Analytics workspace to the MMA agent, even through a proxy.\\n\\n_Note:  It is recommended to have the latest MMA Agent installed, this is not compatible with SCOM 2012 R2 agents, but the latest agent is supported by SCOM._\\n\\n```powershell title=\\"Add_LogAnalyticsWorkspace.ps1\\"\\n\\n<#\\n\\n    Author: Luke Murray (Luke.Geek.NZ)\\n    Version: 0.1\\n    Version History:\\n\\n    Purpose: Add an MMA agent to a Log Analytics workspace using a proxy with no user authentication.\\n    Notes:\\n    Find more options about the MMA Agent Object:\\n    #$healthServiceSettings = New-Object -ComObject \'AgentConfigManager.MgmtSvcCfg\'\\n    #$proxyMethod = $healthServiceSettings | Get-Member -Name \'SetProxyInfo\'\\n\\n    If script is being published by Configuration as a package, create a Command Line installer:\\n    \\"%Windir%\\\\sysnative\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\\" -ExecutionPolicy Bypass -Command  .\\\\Add_LogAnalyticsWorkspace.ps1\\n\\n    If the proxy requires authentication, then the following null entries need to be replaced with user,password: $mma.SetProxyInfo(\\"$proxy\\",\\"$null\\",\\"$null\\"). If you aren\'t using a proxy then you can remove the entire mma.SetProxyInfo line.\\n\\n    Location: https://github.com/lukemurraynz/PowerOfTheShell/blob/master/OperationsMgr/Add_LogAnalyticsWorkspace.ps1\\n#>\\n\\n$workspaceId = \\"INSERTLOGANALYTICSWORKSPACEIDHERE\\"\\n$workspaceKey = \\"INSERTLOGANALYTICSWORKSPACEKEY\\"\\n$proxy = \'ProxyIP:PORT\'\\n$mma = New-Object -ComObject \'AgentConfigManager.MgmtSvcCfg\'\\n$mma.AddCloudWorkspace($workspaceId, $workspaceKey)\\n$mma.SetProxyInfo(\\"$proxy\\",\\"$null\\",\\"$null\\")\\n$mma.ReloadConfiguration()\\n\\n```"},{"id":"azure/update-your-azure-webapp-to-be-always-on","metadata":{"permalink":"/azure/update-your-azure-webapp-to-be-always-on","source":"@site/blog/2021-05-14-update-your-azure-webapp-to-be-always-on.md","title":"Update your Azure WebApp to be Always On","description":"By default, Azure Web Apps are unloaded if they are idle for a set period of time (20 minutes). This way, the system can conserve resources.","date":"2021-05-13T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.655,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-05-13T12:00:00.000Z","title":"Update your Azure WebApp to be Always On","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/update-your-azure-webapp-to-be-always-on"},"unlisted":false,"prevItem":{"title":"Add Log Analytics to Monitoring Agent with PowerShell","permalink":"/2021/06/13/add-log-analytics-workspace-to-microsoft-monitoring-agent-via-powershell"},"nextItem":{"title":"Update your Azure WebApp to use your timezone","permalink":"/azure/update-your-azure-webapp-to-use-your-timezone"}},"content":"By default, Azure Web Apps are unloaded if they are idle for a set period of time _(20 minutes)_. This way, the system can conserve resources.\\n\\nThe downside is that the response to the first request after the web app is unloaded is longer, as the Web App has to load into memory and present itself, which could lead to a bad user experience.\\n\\nTo ensure that the Azure App Service Web App is running and always available to respond to incoming HTTP(S) requests, can be set the \\"Always On\\" configuration feature to \\"On\\".\\n\\n## Overview\\n\\nBy setting the \\"Always On\\" feature of the App Service Web App to \u201cOn\u201d, will ensure that Azure will always keep an instance of the Web App running at all times. This way when a user/client hits the Azure Front Door endpoint, the back-end Web App will always be ready to respond to that request without timing out. This will ensure the application is always available even during times of low usage or inactivity. Azure will continuously ping the website to keep the website alive.\\n\\nEnabling \\"Always On\\" keeps your Apps always loaded, even when there is no traffic. It\u2019s required for example when using continuous Web Jobs or for Web Jobs that are triggered using a [CRON](https://en.wikipedia.org/wiki/Cron \\"Wikipedia - cron\\") expression, this feature is similar to the Internet Information Services (IIS) idle time-out property.\\n\\nDisabling \\"Always On\\" makes your Apps unloaded if they are idle for a set period of time. This way, the system can conserve resources. This is the reason \u201cAlways On\u201d is set as disabled by default.\\n\\nYou need a minimum of Basic and Standard App Service Tiers to enable \\"Always On\\"\\n\\nHaving \\"Always On\\" off or on does not affect your billing or pricing, the Azure App Service billing is done at the App Service Plan level, which is charged per hour that the App Service Plan exists and is running, this is charged whether you have a WebApp or its set to \\"Always On\\" or not.\\n\\n## Configure Always On\\n\\nNote: Any changes to applications settings and connection strings could restart your application, so make sure that you schedule this during a period you can have an intermittent outage.\\n\\n1. Log in to the **Azure Portal**\\n2. **Find** your [**App Service**](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2Fsites \\"Azure Portal - App Service\\") and open it\\n3. In the left hand Blade, under **Configuration**, select **General Settings**\\n   ![App Service - General Settings](/uploads/app-service_configurationsettings.png \\"App Service - General Settings\\")\\n4. Navigate down to **Platform Settings** and click \'**On**\' to **Always On**\\n   ![App Service - Always On](/uploads/app-service_alwayson.png \\"App Service - Always On\\")\\n5. Click **Save**\\n\\n## Configure Always On through Azure Policy\\n\\nThe Azure policy below, contains a set of 3 policies which used in combination should ensure that your web app is always on. Since the alwaysOn property is optional in the ARM deployment, a combination of 3 policies are required to ensure true compliance in the environment.\\n\\n* [Web App Always On Policy](https://github.com/Azure/Community-Policy/tree/master/Policies/WebApps/web-app-always-on \\"https://github.com/Azure/Community-Policy/tree/master/Policies/WebApps/web-app-always-on\\")\\n\\n## References\\n\\nFor more information about the \\"Always On\\" feature please see the documentation below:\\n\\n* [Azure Web Sites adds Always On](https://azure.microsoft.com/en-us/updates/azure-web-sites-adds-always-on/?WT.mc_id=AZ-MVP-5004796 \\"https://azure.microsoft.com/en-us/updates/azure-web-sites-adds-always-on/?WT.mc_id=AZ-MVP-5004796\\")\\n* [Application performance FAQs for Web Apps in Azure](https://learn.microsoft.com/en-us/troubleshoot/azure/general/web-apps-performance-faqs?WT.mc_id=AZ-MVP-5004796 \\"https://learn.microsoft.com/en-us/troubleshoot/azure/general/web-apps-performance-faqs?WT.mc_id=AZ-MVP-5004796\\")"},{"id":"azure/update-your-azure-webapp-to-use-your-timezone","metadata":{"permalink":"/azure/update-your-azure-webapp-to-use-your-timezone","source":"@site/blog/2021-05-09-update-your-azure-webapp-to-use-your-timezone.md","title":"Update your Azure WebApp to use your timezone","description":"By default, the timezone in Microsoft Azure defaults to UTC (Universal Coordinated Time) as a standard, as a universal and consistent timezone, this makes sense - however when troubleshooting issues or attempting to schedule jobs, having the time in UTC may add additional confusion. An Azure WebApp is no exception to UTC as a standard, however, this can be changed.","date":"2021-05-09T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.9,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-05-09 00:00:00 +1200","title":"Update your Azure WebApp to use your timezone","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/update-your-azure-webapp-to-use-your-timezone"},"unlisted":false,"prevItem":{"title":"Update your Azure WebApp to be Always On","permalink":"/azure/update-your-azure-webapp-to-be-always-on"},"nextItem":{"title":"Run PowerShell App Deployment Toolkit in Datto RMM","permalink":"/2021/05/09/run-powershell-app-deployment-toolkit-in-datto-rmm"}},"content":"By default, the timezone in Microsoft Azure defaults to UTC (Universal Coordinated Time) as a standard, as a universal and consistent timezone, this makes sense - however when troubleshooting issues or attempting to schedule jobs, having the time in UTC may add additional confusion. An Azure WebApp is no exception to UTC as a standard, however, this can be changed.\\n\\nAs I am in \'New Zealand\', I will be setting my WebApp (which is hosted in Australia East) to NZ time from UTC.\\n\\nI will be using a Windows-based App Service, for this article.\\n\\n## Find the Timezone\\n\\nThe Azure App Service uses the same naming standard as Windows.\\n\\n1. To find the correct name, run the following PowerShell snippet on a Windows PC:\\n\\n       Get-ChildItem -Path \'HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\Time Zones\' | Select-Object PSChildName\\n\\n![Windows Terminal - Timezone](/uploads/windowsterminal_timezone.png)\\n\\nThis will list all the compatible Timezone names, because I am in New Zealand, I now know that: \'New Zealand Standard Time\' is the correct syntax. \\n\\nFor future reference, I have exported the list of compatible timezones into a CSV file below:\\n\\n* [Export of Timezones](https://luke.geek.nz/uploads/files/Timezones.csv \\"Export of Timezones\\")\\n\\n## Set the Timezone\\n\\nMaking a change to the Application Settings, which includes setting the Timezone will restart the WebApp app pool, so make sure this is scheduled at a time it is acceptable for an intermittent outage.\\n\\n1. Log in to the [Azure Portal](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2Fsites \\"Azure Portal - App Services\\")\\n2. Find your Azure WebApp and open it up\\n3. On the left-hand side Blade, underneath Settings, click on Configuration\\n4. Click on + New Application Setting\\n5. Type in the following Key/Value pair:\\nWEBSITE_TIME_ZONE | New Zealand Standard Time\\n6. Click Ok\\n7. Click Save to confirm and save the change.\\n\\n![Azure WebApp - Timezone](/uploads/azurewebapp_appsettingstimezone.png \\"Azure WebApp - Timezone\\")\\n\\n## Test the Timezone\\n\\n1. Log in to the Azure Portal\\n2. Find your [Azure WebApp](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2Fsites \\"Azure Portal - App Services\\") and open it up\\n3. On the left-hand side Blade, underneath Development Tools, click on Console\\n4. Type the following into the console:\\n\\n* Time\\n* Date\\n\\nThese commands can help you confirm, that the Date and Time now your Timezone, the Azure WebApp logs etc will now be updated to use your timezone.\\n\\n![Azure App Service - Console](/uploads/azurewebapp_console_date.png \\"Azure App Service - Console\\")"},{"id":"/2021/05/09/run-powershell-app-deployment-toolkit-in-datto-rmm","metadata":{"permalink":"/2021/05/09/run-powershell-app-deployment-toolkit-in-datto-rmm","source":"@site/blog/2021-05-09-run-powershell-app-deployment-toolkit-in-datto-rmm.md","title":"Run PowerShell App Deployment Toolkit in Datto RMM","description":"The PowerShell App Deployment Toolkit provides a set of functions to perform common application deployment tasks and to interact with the user during deployment. It simplifies the complex scripting challenges of deploying applications in the enterprise, provides a consistent deployment experience and improves installation success rates.","date":"2021-05-08T12:00:00.000Z","tags":[{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":1.425,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-05-08T12:00:00.000Z","title":"Run PowerShell App Deployment Toolkit in Datto RMM","authors":["Luke"],"tags":["PowerShell"],"toc":false,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Update your Azure WebApp to use your timezone","permalink":"/azure/update-your-azure-webapp-to-use-your-timezone"},"nextItem":{"title":"Full end to end encryption on an Azure WebApp using Cloudflare","permalink":"/2021/05/08/full-end-to-end-encryption-on-an-azure-webapp-using-cloudflare"}},"content":"The [PowerShell App Deployment Toolkit](https://psappdeploytoolkit.com/ \\"PowerShell App Deployment Toolkit\\") provides a set of functions to perform common application deployment tasks and to interact with the user during deployment. It simplifies the complex scripting challenges of deploying applications in the enterprise, provides a consistent deployment experience and improves installation success rates.\\n\\n![PowerShell App Deployment Toolkit](/uploads/powershell_app_deploymenttoolkit.png \\"PowerShell App Deployment Toolkit\\")\\n\\nAlthough the PowerShell App Deployment Toolkit, makes application installation a lot more visible and gives your users more control over how and when the Application is installed,  due to some technical limitations, you can\'t run the PowerShell App Deployment Toolkit, directly from the Datto RMM package store.\\n\\nThis is a brief article, intended to help other people who may be using the App Deployment Toolkit with Datto RMM.\\n\\n```powershell title=\\"DattoRMMpowerShellAppDeploymentToolkitCommand.ps1\\"\\n\\n#This is the name of the zip file in the component. Make sure that the PowerShell App Deployment Toolkit is zipped.\\n$ZipFile = \\"DesktopSOE.zip\\"\\n#This will create a folder called: C:\\\\Temp (these folders can be changed to suit your requirements)\\nMkdir c:\\\\Temp -Force\\n#This will create a folder called: C:\\\\Temp\\\\DesktopSOE\\\\ (these folders can be changed to suit your requirements)\\nMkdir C:\\\\Temp\\\\DesktopSOE\\\\ -Force\\n#This will then copy your PowerShellAppDeployment Toolkit to a folder, outside of the CentraStage Packagestore location. \\nCopy-Item -Path \\"$ZipFile\\" -Destination \\"C:\\\\Temp\\\\$ZipFile\\" -Recurse\\n$DestinationFolder = $ZipFile.Split(\\".\\")[0]\\n#This will then extract your PowerShell App Deployment Toolkit and run it.\\nExpand-Archive -Path \\"c:\\\\temp\\\\$ZipFile\\" -DestinationPath \\"C:\\\\Temp\\\\DesktopSOE\\\\\\" -Force\\nInvoke-Command { c:\\\\temp\\\\DesktopSOE\\\\Deploy-Application.exe }\\n\\n```\\n\\nNote: You may also need to navigate to: AppDeployToolkitConfig.xml, and change the: <Toolkit_RequireAdmin> attribute to False, to avoid issues with UAC (User Access Control).\\n\\nI also ran my component as:\\n\\n* Only Run when the user is logged in\\n* Only run if User has Administrator rights"},{"id":"/2021/05/08/full-end-to-end-encryption-on-an-azure-webapp-using-cloudflare","metadata":{"permalink":"/2021/05/08/full-end-to-end-encryption-on-an-azure-webapp-using-cloudflare","source":"@site/blog/2021-05-08-full-end-to-end-encryption-on-an-azure-webapp-using-cloudflare.md","title":"Full end to end encryption on an Azure WebApp using Cloudflare","description":"Cloudflare offers many capabilities; one of the capabilities it offers is SSL offloading and CNAME flattening.","date":"2021-05-08T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.345,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Full end to end encryption on an Azure WebApp using Cloudflare","authors":["Luke"],"tags":["Azure"],"date":"2021-05-08 00:00:00 +1300","toc":true,"header":{"teaser":"/uploads/cloudflare_azure_e2e_cert.png"}},"unlisted":false,"prevItem":{"title":"Run PowerShell App Deployment Toolkit in Datto RMM","permalink":"/2021/05/09/run-powershell-app-deployment-toolkit-in-datto-rmm"},"nextItem":{"title":"Azure Blob and Azure Lifecycle Management","permalink":"/2021/04/23/azure-blob-and-azure-lifecycle-management"}},"content":"Cloudflare offers many capabilities; one of the capabilities it offers is SSL offloading and CNAME flattening.\\n\\nWhen setting up an Azure Web App using default settings, it is set up using HTTP, not HTTPS, so we will set the WebApp to your custom domain, then use Cloudflare to protect traffic from your user\'s browsers to Cloudflare, then encrypt traffic from Cloudflare to your website.\\n\\nWe will go through both setups, with the result being full end-to-end encryption of your Azure WebApp using Cloudflare and your custom domain.\\n\\n**Using Cloudflare without a backend Certificate** \\n\\n![Using Cloudflare without a backend Certificate](/uploads/cloudflare_azure_brokensslchain.png \\"Using Cloudflare without a backend Certificate\\")\\n\\n**Using Cloudflare with a backend Certificate** \\n\\n![Using Cloudflare with a backend Certificate](/uploads/cloudflare_azure_e2e_cert.png \\"Using Cloudflare with a backend Certificate\\")\\n\\nBy default, Azure WebApps have a wildcard cert for the following domains:\\n\\n* *.azurewebsites.net\\n* With Subject alternative names for:\\n* *.scm.azurewebsites.net\\n* *.azure-mobile.net\\n* *.scm.azure-mobile.net\\n* *.sso.azurewebsites.net\\n\\n![badasscloud - azurewebsites.net secure](/uploads/badasscloudazurewebsitessl.png \\"badasscloud - azurewebsites.net secure\\")\\n\\nThis certificate allows you to use HTTPS using the default azurewebsites URL, which gets created when you create your Azure WebApp and is completely managed by Microsoft and the Azure ecosystem. Still, if you want to use your own Custom Domain, then these certificates won\'t work.\\n\\n## Prerequisites\\n\\n* Azure WebApp _(supports Custom Domain SSL support, Custom Domains/SSL support are available from \u2018B1\u2019 plans and upwards.)_\\n* [Cloudflare](https://www.cloudflare.com/en-gb/ \\"Cloudflare\\") _account (can be free)_\\n* Domain _(an actual custom domain to use for your website that is already setup to use Cloudflare nameservers)_\\n* [PfxCreator](https://github.com/georg-jung/PfxCreator \\"PFXCreator GitHub Repository\\")\\n\\n## Add a Custom Domain to your Azure WebApp using Cloudflare\\n\\n 1. Login into the [Azure Portal](https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2Fsites \\"Azure Portal - App Services\\")\\n 2. Navigate to your App Service.\\n 3. Underneath Settings on the left-hand side blade of the App Settings, look for Custom Domains and select it.\\n 4. Click on \u2018Add Custom Domain\u2019.\\n 5. type in your custom domain _(in my example, I am using a domain I own called: badasscloud.com)_\\n 6. Select Validate; you will have a similar seen to me below; select CNAME.\\n![Azure - Add Custom Domain](/uploads/AzureAppService_AddCustomDomain.png \\"Azure - Add Custom Domain\\")\\n 7. Now we need to validate that you are the one who owns the domains and can use it for your WebApp, so we will need to create some records to verify that you own the domain and redirect the website to the Azure Websites.\\n 8. Login to Cloudflare\\n 9. Select SSL/TLS and make sure that \u2018Flexible\u2019 SSL has been selected.\\n10. Select DNS \\n_Note: You may need to remove any A records for \u2018www\u2019 or the root domain \u2018@\u2019 you have set; please make sure you have a reference to them in case you need to roll back any changes because we will be redirecting the main URL to an Azure DNS alias, we will be using Cloudflare CNAME flattening at the root level, so anyone going to \u2018badasscloud.com\u2019 will be redirected to the Azure WebApp._\\n11. You can also use the txt record to validate the domain and do some reconfiguration without changing the domain and redirecting traffic ahead of your change to avoid downtime.\\n12. Add in the records to Cloudflare (please note that verification will fail if Cloudflare proxy is turned on, so make sure that the proxy status is set to DNS only)\\n![](/uploads/badassclouddns_azureverification.png)\\n13. Navigate back to the Azure Portal.\\n14. Click on Validate again and select CNAME.\\n15. Hostname availability and Domain ownership should be both green.\\n16. Add Custom Domain.\\n![Azure - Add Custom Domain](/uploads/AzureAppService_AddCustomDomain_VerificationComplete.png \\"Azure - Add Custom Domain\\")\\n17. If they are still Red, wait a few minutes for Cloudflare to replicate the changes across its Networks and Azure to clear any server-side caching, verification can fail if you try to verify straight away.\\n18. Now that Domain verification has been completed navigate Cloudflare and enable the Cloudflare proxy for your root domain and www record. ![](/uploads/badassclouddns_postazureverification.png)\\n19. Navigate and test your website. Now that the domain has been added to the Azure WebApp and Cloudflare proxy has been enabled, your website will now have a certificate supplied by Cloudflare. You have now set up Flexible SSL traffic to your website, so traffic between users\u2019 browsers to Cloudflare is now encrypted. ![badasscloud.com - Cloudflare Certificate](/uploads/badasscloud_Azure_Cloudflarefront.png \\"badasscloud.com - Cloudflare Certificate\\")\\n\\n## **Update your WebApp to support \u2018Full\u2019 end-to-end using Cloudflare origin certificate**\\n\\nAdding your domain to Cloudflare was only the first part of the puzzle; although traffic between the browser and Cloudflare is now encrypted, traffic between Cloudflare and your WebApp is not; to encrypt this traffic, we are going to use the Cloudflare origin certificate.\\n\\nCloudflare Origin Certificates are free SSL certificates issued by Cloudflare for installation on your origin server to facilitate end-to-end encryption for your visitors using HTTPS. Once deployed, they are [compatible with the Strict SSL mode](https://developers.cloudflare.com/ssl/origin-configuration/ssl-modes#strict). By default, newly generated certificates are valid for 15 years, but you can change this to 7 days.\\n\\n 1. Log in to Cloudflare\\n 2. Click on SSL/TLS\\n 3. Click on Origin Server\\n 4. Click on Create Certificate ![Cloudflare - Origin Certificate](/uploads/Cloudflare_OriginCert1.png \\"Cloudflare - Origin Certificate\\")\\n 5. Verify that the Private Key Type is RSA (2048)\\n 6. Make sure that the Hostnames you want to be covered under the origin cert is covered.\\n 7. Verify certificate validity, in my example, and I am going with 15 years; remember to keep this certificate validated and updated. ![Cloudflare - Origin Certificate](/uploads/Cloudflare_OriginCert2.png \\"Cloudflare - Origin Certificate\\")\\n 8. Click Create\\n 9. Cloudflare will now generate your Origin certificate and Private key (save these somewhere secure, the private key will not be shown again).\\n10. Now we need to create a certificate PFX file to upload to the Azure WebApp, run PfxCreator.exe (see Prerequisites for download link)\\n11. Paste the Origin Certificate into the: Certificate (PEM)\\n12. Paste the Private Key into the Private Key (PEM) ![PfxCreator](/uploads/PfxCreator.png \\"PfxCreator\\")\\n13. Type in a password for the certificate\\n14. Click Save PFX\u2026 and save your certificate.\\n15. Login into the Azure Portal\\n16. Navigate to your App Service.\\n17. Underneath Settings on the left-hand side blade of the App Settings, look for Custom Domains and select it.\\n18. You should see the SSL state of your domain as \u2018Not Secure\u2019, and under SSL Binding, you will have an option to Add Binding, click on Add Binding.\\n19. Select your Custom Domain and click Upload PFX Certificate\\n20. Click File and browse for your certificate.\\n21. Type in the password you entered PFXCreator earlier. ![Azure Portal - Add Private Certificate](/uploads/AzureWebApp-Cloudflare_OriginCert_AddBinding1.png.png \\"Azure Portal - Add Private Certificate\\")\\n22. Click on Upload.\\n23. Once uploaded, select your Custom Domain.\\n24. Select the Cloudflare Origin Certificate\\n25. Make sure the TLS/SSL type is: SNI SSL and click Add Binding. ![Azure Portal - Add Private Certificate](/uploads/AzureWebApp-Cloudflare_OriginCert_AddBinding3.png.png \\"Azure Portal - Add Private Certificate\\")\\n26. The SSL State of your Custom Domain should now have been changed to Secure.\\n27. Click on HTTPS Only\\n _Note: You may see constant redirect issues with your website until the following Cloudflare changes have been made._\\n![Azure Portal - Enable HTTPS](/uploads/AzureWebApp-Cloudflare_OriginCert_AddBinding4.png \\"Azure Portal - Enable HTTPS\\")\\n28. Login to Cloudflare\\n29. Select SSL/TLS and make sure that \u2018Full (Strict)\u2019 has been selected.\\n30. Give it 30 seconds to a minute to take effect, and you have now successfully encrypted traffic end-to-end on your website, from the browser to Cloudflare and from Cloudflare to your Azure WebApp.\\n\\n\\\\#ProTip - If you want to be more secure, you can look into blocking access to your website from Cloudflare and a few select IPs for testing only to avoid traffic from bypassing Cloudflare and going to the azure websites URL."},{"id":"/2021/04/23/azure-blob-and-azure-lifecycle-management","metadata":{"permalink":"/2021/04/23/azure-blob-and-azure-lifecycle-management","source":"@site/blog/2021-04-23-azure-blob-and-azure-lifecycle-management.md","title":"Azure Blob and Azure Lifecycle Management","description":"Azure Blob storage (Platform-as-a-service (PaaS)) is used for streaming and storing documents, videos, pictures, backups, and other unstructured text or binary data\u2026 however the functionality extends beyond just a place to \u201cstore stuff\u201d, it can save you money and time by automating the lifecycle of your data using Azure Blob Lifecycle Management and access tiers.","date":"2021-04-22T12:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.965,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-04-22T12:00:00.000Z","title":"Azure Blob and Azure Lifecycle Management","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"/uploads/AzureBlobBanner.png"}},"unlisted":false,"prevItem":{"title":"Full end to end encryption on an Azure WebApp using Cloudflare","permalink":"/2021/05/08/full-end-to-end-encryption-on-an-azure-webapp-using-cloudflare"},"nextItem":{"title":"Azure DevOps and creating your Cloud Adoption Framework","permalink":"/2021/04/18/azure-devops-and-creating-your-cloud-adoption-framework"}},"content":"Azure Blob storage (Platform-as-a-service (PaaS)) is used for streaming and storing documents, videos, pictures, backups, and other unstructured text or binary data\u2026 however the functionality extends beyond just a place to \u201cstore stuff\u201d, it can save you money and time by automating the lifecycle of your data using Azure Blob Lifecycle Management and access tiers.\\n\\nAs of January 2021, Blob storage now supports the Network File System (NFS) 3.0 protocol. This support provides Linux file system compatibility at object storage scale and prices and enables Linux clients to [mount a container](https://learn.microsoft.com/en-us/azure/storage/blobs/network-file-system-protocol-support?WT.mc_id=AZ-MVP-5004796 \\"Network File System (NFS) 3.0 protocol support in Azure Blob storage (preview)\\") in Blob storage from an Azure Virtual Machine (VM) or a computer on-premises.\\n\\n# First up what is a Blob?\\n\\nBlobs - _\u201cHighly scalable, REST-based cloud object store\u201d_\\n\\n* Data sharing, Big Data, Backups\\n* Block Blobs: Read and write data in blocks. Optimized for sequential IO. Most cost-effective Storage. Ideal for files, documents & media.\\n* Page Blobs: Optimized for random access and can be up to 8 TB in size. IaaS VM OS & data disks and backups are of this type.\\n* Append Blobs: Like block blobs and optimized for append operations. Ideal for logging scenarios and total size can be up to 195 GB.\\n\\n# Aren\u2019t there only 2 access tiers?\\n\\nWhen you create an Azure Storage account, you get presented with 2 options for the Access Tier:\\n\\n* Hot\\n* Cool\\n\\n**Hot access tier**\\n\\nThe hot access tier has higher storage costs than cool and archive tiers, but the lowest access costs. Example usage scenarios for the hot access tier include:\\n\\n* Data that is in active use or is expected to be read from and written to frequently.\\n* Data that is staged for processing and eventual migration to the cool access tier\\n\\n**Cool access tier**\\n\\nThe cool access tier has lower storage costs and higher access costs compared to hot storage. This tier is intended for data that will remain in the cool tier for at least 30 days. Example usage scenarios for the cool access tier include:\\n\\n* Short-term backup and disaster recovery\\n* Older data not used frequently but expected to be available immediately when accessed.\\n* Large data sets need to be stored cost-effectively, while more data is being gathered for future processing.\\n\\nThese options are set globally for your Azure Storage account blobs, however, there is a third tier, the Archive Access Tier:\\n\\n**Archive access tier**\\n\\nThe Archive access tier has the lowest storage cost, but higher data retrieval costs compared to hot and cool tiers.\\n\\nData must remain in the archive tier for at least 180 days or be subject to an early deletion charge. Data in the archive tier can take several hours to retrieve depending on the specified rehydration priority.\\n\\nWhile a blob is in archive storage, the blob data is offline and cannot be read or modified. To read or download a blob in the archive, you must first rehydrate it to an online tier.\\n\\n# How is this charged?\\n\\nDepending on which tier your data is in, depends on the costs, Azure Blob Storage is charged on Read/Write and list operation and other factors, for example:\\n\\n* Hot Tier: Lower access prices for frequent use\\n* Cool Tier: Lower storage prices for high volume\\n* The volume of data stored per month.\\n* Quantity and types of operations performed, along with any data transfer costs.\\n* Data redundancy option selected.\\n\\nMore information here: [https://azure.microsoft.com/en-us/pricing/details/storage/blobs/](https://azure.microsoft.com/en-us/pricing/details/storage/blobs/?WT.mc_id=AZ-MVP-5004796 \\"Azure Blobs\\")\\n\\n# What is data lifecycle management?\\n\\nThere are many versions of it, but at its core, there are 5 stages to simple data lifecycle management:\\n\\n* **Creation** \u2013 When the data is first created.\\n* **Storage** -Where the data is stored.\\n* **Usage** \u2013 When the data is useful and relevant and used.\\n* **Archival** \u2013 When the data is not as useful, but still helpful to have around due to knowledge or legal requirements.\\n* **Destruction** \u2013 When the data is completely irrelevant and there is no need to store or use it anymore.\\n\\n# Right... so, tell me more about the Azure Blob Lifecycle Management?\\n\\nAzure Blob Storage has a lifecycle management feature built-in. Azure Blob Storage lifecycle management offers a rich, rule-based policy for General Purpose v2 and blob (and Premium Block blob) storage accounts.\\n\\n* Imagine you working on a project, such as purchasing a new company you not only want somewhere to store that data, but you want to make sure it is accessible quickly, so you put it in an Azure Blob Storage account under the Hot Tier.\\n* You\u2019ve then spent some time working on new documents using the data you acquired when you purchased the \u2018new\u2019 company, but don\u2019t touch them anymore, you don\u2019t want them sitting on fast storage costing you additional money, so they get migrated to a \u2018Cool\u2019 access tier.\\n* A few months later, you realized that you needed some of the original data from the company acquisition, you find the files and use them, it took a bit longer to open as the data needed to be migrated to the \u2018hot tier\u2019 but you are happy because the data that you want was there.\\n* A year later, you are onto acquiring another company and the data from the company acquisition which seemed a lifetime ago is forgotten about, however, you know you might need it for legal or finance auditing purposes, the data goes into the Archive tier, costing you less than the cool tier, but could be reacquired at a later date if needed (for an extra charge).\\n* 7 years down the track, you\u2019re now a multi-million-dollar firm, and have completely forgotten or no longer need the data from your original acquisition, the data then gets deleted, saving you money and data management costs.\\n\\nMicrosoft Azure and Lifecycle Management for Blob Storage automate the entire lifecycle for you.\\n\\n# How do I enable or configure Azure Blob Lifecycle Management?\\n\\n1. Log in to the [Azure Portal](https://portal.azure.com/#home \\"Azure Portal\\")\\n2. Find the Azure storage account you want to configure Lifecycle Management on\\n3. On the Storage account left-hand side Blade, under Blob Service click on Lifecycle Management\\n4. Click on Add a rule\\n5. Enter in a Rule name any name that suits your naming standards, for example, AzureBlobLifecyclePolicy.\\n![Azure Blob Lifecycle Policy](/uploads/azurebaseblobrules1.png \\"Azure Blob Lifecycle Policy\\")\\n_Note: Make sure Append Blobs is unselected, this is un-supported for moving access tiers (however supports being deleted after x amount of days)._\\n 6. Click Next\\n 7. This is where the magic happens, we are going to go with the following:\\n    ![Azure Base Blob Policies](/uploads/azurebaseblobrules.png \\"Azure Base Blob Policies\\")\\n 8. Base Blobs that were last modified 90 days ago will be moved to Cool storage.\\n 9. Click on + Add if-then block, now we will select the Archive Storage, the example we will now archive data that has been in Cool storage for 90 days, so we enter in: 180 days.\\n    _Note: Migrating the data between Access Tiers, does not change the last modified date of the file, so it\'s 90 days for migrating to Cool, then another 90 days to move to archive._\\n10. Click on + Add if-then block, now we will select the Delete the blob, data that has been in Archive storage for 90 days will now be deleted, so we enter in: 270 days.\\n11. Click Next and do the same for Snapshots and versions and click Save.\\n12. Congratulations, you have now created an Azure Blob Lifecycle policy!\\n\\nOnce the Policy has been saved, it is Enabled by default. You can disable it by selecting the Policy and select Disable on the top banner.\\n\\n!Azure Blob Lifecycle Management[](/uploads/lifecyclepolicydisable.png \\"Azure Blob Lifecycle Management\\")\\n\\n\\\\#ProTip - You can also view the policy as Code in Code View, which is a simple and quick way of documenting and modifying your lifecycle policy.\\n\\n\\\\#ProTip - You can have multiple Lifecycle Policies on a single storage account.\\n\\n\\\\#ProTip - You can learn more about Lifecycle policies by going to the Microsoft documentation here: [Optimize costs by automating Azure Blob Storage access tiers](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview?tabs=azure-portal&WT.mc_id=AZ-MVP-5004796).\\n\\n\\\\#ProTip - If you are looking for integration with Azure AD or Active Directory NTFS permissions, replicating data from fileservers, you are better off looking at Azure File Shares and not blob storage."},{"id":"/2021/04/18/azure-devops-and-creating-your-cloud-adoption-framework","metadata":{"permalink":"/2021/04/18/azure-devops-and-creating-your-cloud-adoption-framework","source":"@site/blog/2021-04-18-azure-devops-and-creating-your-cloud-adoption-framework.md","title":"Azure DevOps and creating your Cloud Adoption Framework","description":"Do you want to make a start on Azure Adoption and Governance, Server Migration or Azure Virtual Desktop and do not know where to start, or whether you are asking the right questions?","date":"2021-04-17T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":4.445,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-04-17 00:00:00 +1200","title":"Azure DevOps and creating your Cloud Adoption Framework","authors":["Luke"],"tags":["Azure"],"toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Azure Blob and Azure Lifecycle Management","permalink":"/2021/04/23/azure-blob-and-azure-lifecycle-management"},"nextItem":{"title":"Azure Resource Graph Explorer and the PowerShell Azure Resource Graph","permalink":"/2021/04/09/azure-resource-graph-explorer-and-the-powershell-azure-resource-graph"}},"content":"Do you want to make a start on Azure Adoption and Governance, Server Migration or Azure Virtual Desktop and do not know where to start, or whether you are asking the right questions?\\n\\nIf you want to create a framework for your cloud adoption or migration plans, you can look at... using [Azure DevOps Demo Generator](https://azuredevopsdemogenerator.azurewebsites.net/ \\"Azure DevOps Generator\\")\u2026\\n\\nAzure DevOps is not only a continuous integration and deployment tool, along with the Repos, Pipelines, Test plans and Artifacts \u2013 there is Azure Boards, with Boards you can plan and track your work items and use the Kanban board functionality to easy update or track your work in progress items and add to the backlog, although Agile squads and sprint planning organizations primarily use Azure Boards \u2013 it does not have to be. \\n\\nThe Azure DevOps Board\u2019s come with your MSDN license or free under the Basic plan for the first 5 users.\\n\\nThe Azure DevOps Demo Generator can create projects in your Azure DevOps organization, already prepopulated with relevant Epics, Features and Tasks that can help you on your cloud journey!\\n\\n![Azure DevOps Demo Generator](/uploads/AzureDevOpsGeneratorPage.png \\"Azure DevOps Demo Generator\\")\\n\\nThere are many prepopulated projects in the Demo Generator, from Security to Learning; you can even import prepopulated templates from other people.\\n\\nThe ones we are going to concentrate on is the: Cloud Adoption Framework projects.\\n\\n![Azure DevOps Generator - Choose a template](/uploads/AzureDevOpsCloudAdoptionTemplates.png \\"Azure DevOps Generator - Choose a template\\")\\n\\nThe following projects are available under the Cloud Adoption Framework heading to help you on your journey *(as of the date this article was published)*:\\n\\n| Project | Description |\\n| --- | --- |\\n| Cloud Adoption Plan | The Cloud Adoption Plan template creates a backlog for managing cloud adoption efforts based on the guidance in the Microsoft Cloud Adoption Framework. |\\n| CAF Strategy-Plan-Ready-Gov | In this checklist we share all the decision points needed to successfully build a Cloud Adoption Plan as well as the Landing Zone with Governance |\\n| ServerMigration_CAF_DevOps_ProjectTaskList | Server migration has many different activities. In the Azure DevOps Project we will provide the steps necessary to go from zero to a complete Server migration and management. |\\n| AKS_CAF_DevOps_Project_TaskList | AKS deployment has many different activities. In the Azure DevOps Project we will provide the steps necessary to go from zero to a complete AKS deployment and management. |\\n| SQL Migration | SQL migration has many different activities. In the Azure DevOps Project we will provide the steps necessary to go from zero to a complete SQL migration and management. |\\n| Windows Virtual Desktop | Project work plan templates in Azure DevOps that provide the steps necessary to go from zero to a complete WVD deployment with ongoing management |\\n| Knowledge Mining | Knowledge project simplifies the process of accessing the latent insights contained within structured and unstructured data. Use this project to help you address all the steps. |\\n| Azure Governance Readiness | The standalone Azure governance project provides guidance and tools on how to ensure that your Azure environment is governed in the correct way. |\\n| Modern Data Warehouse | Build your modern data warehouse using this ADO checklist of items, in this checklist we have links to assets, code and learning material. |\\n| Retail Recommender with Azure Synapse | This Solution Accelerator is an end-to-end example on how to enable personalized customer experiences for retail scenarios by leveraging Azure Synapse Analytics, Azure Machine Learning Services, and other Azure Big Data services. |\\n| Modern IOT | Connected sensors, devices, and intelligent operations can transform businesses and enable new growth opportunities. In this project you will get the work items needed to plan and implement your IOT solution using the Azure IoT Platform. |\\n\\nOnce the project has been created, you can go into Azure Board and click on: Work Items.\\n\\nIf we take a look at the CAF Strategy-Plan-Ready-Gov Team one, we can see the Epics, Features and Tasks associated with Cloud Adoption:\\n\\n![Azure DevOps - Cloud Adoption Strategy](/uploads/AzureDevOps_CloudAdoptionWorkItems.png \\"Azure DevOps - Cloud Adoption Strategy\\")\\n\\nIf we click Boards, we can see the Kanban board, the state of the Epics, features etc. and where they are.\\n\\n![Azure DevOps - Kanban](/uploads/AzureDevOps_CloudAdoptionKanban.png \\"Azure DevOps - Kanban\\")\\n\\nDepending on the Tasks, it may have a description of the task with links to the relevant documentation, such as this SQL Deployment and Migration testing:\\n\\n![Azure DevOps - Kanban](/uploads/AzureDevOps_Task.png)\\n\\nAs you can see, the Azure DevOps Generator offers not only a place to track your progress but relevant data to help you put a framework around your cloud journey, and these projects work well with the Microsoft Cloud Adoption and Azure Well Architected Framework!\\n\\nThese are guidelines, and they do not need to be followed to the letter; however, in my opinion, they offer an excellent base to build your cloud adoption and implementations upon.\\n\\nI have extracted the following work items from the projects as CSV, in case you prefer to start with excel or want to take a look at the epics, features and tasks that come with these projects:\\n\\n* [CAF Strategy-Plan-Ready-Gov Team - Epics](https://luke.geek.nz/uploads/files/AzureDevOpsDemoGenerator/CAF Strategy-Plan-Ready-Gov Team - Epics.csv \\"CAF Strategy-Plan-Ready-Gov Team - Epics\\")\\n* [Windows Virtual Desktop Guidance](https://luke.geek.nz/uploads/files/AzureDevOpsDemoGenerator/WVD_Guidance.csv \\"Windows Virtual Desktop Guidance\\")\\n* [Cloud Adoption](https://luke.geek.nz/uploads/files/AzureDevOpsDemoGenerator/CloudAdoption.csv \\"Cloud Adoption\\")\\n* [SQL Migration](https://luke.geek.nz/uploads/files/AzureDevOpsDemoGenerator/SQLMigration.csv \\"SQL Migration\\")"},{"id":"/2021/04/09/azure-resource-graph-explorer-and-the-powershell-azure-resource-graph","metadata":{"permalink":"/2021/04/09/azure-resource-graph-explorer-and-the-powershell-azure-resource-graph","source":"@site/blog/2021-04-09-azure-resource-graph-explorer-and-the-powershell-azure-resource-graph.md","title":"Azure Resource Graph Explorer and the PowerShell Azure Resource Graph","description":"Every now and again you come across something that you pay little attention to until you actually spend the time to sit down, work through and try to break stuff! The Azure Resource Graph was that for me!","date":"2021-04-09T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.525,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Azure Resource Graph Explorer and the PowerShell Azure Resource Graph","authors":["Luke"],"tags":["Azure"],"date":"2021-04-09 00:00:00 +1300","toc":true,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Azure DevOps and creating your Cloud Adoption Framework","permalink":"/2021/04/18/azure-devops-and-creating-your-cloud-adoption-framework"},"nextItem":{"title":"Keep up to date with Azure changes using PowerShell","permalink":"/2021/04/03/keep-up-to-date-with-latest-changes-on-azure-using-powershell"}},"content":"Every now and again you come across something that you pay little attention to until you actually spend the time to sit down, work through and try to break stuff! The Azure Resource Graph was that for me!\\n\\nThe idea was to create an export of Azure Recommendations, directly from the Azure Advisor into PowerShell, Microsoft Azure has this functionality out of the box with a few tools:\\n\\n* Azure Resource Graph Explorer\\n* The [Az.ResourceGraph](https://learn.microsoft.com/en-us/azure/governance/resource-graph/first-query-powershell?WT.mc_id=AZ-MVP-5004796) PowerShell module\\n\\n### Azure Graph Resource Explorer\\n\\nThe Azure Graph Resource Explorer is built into the Azure Portal, it can be found by going to [https://portal.azure.com/#blade/HubsExtension/ArgQueryBlade](https://portal.azure.com/#blade/HubsExtension/ArgQueryBlade \\"https://portal.azure.com/#blade/HubsExtension/ArgQueryBlade\\")\\nor by logging into the [Azure Portal](https://portal.azure.com) and typing in \'Resource Graph\' and select Explorer.\\n\\n![Azure Resource Graph](/uploads/azureresourcegraphsearch.png)\\n\\nThe Azure Resource Graph Explorer, allows you to explore the Microsoft Azure Resource Graph, using inbuilt Sample Queries and the Kusto Query language. \\n\\nThe Powershell queries mentioned in the section below, started by clicking on the \'microsoft.advisor/recommendations\' field and selecting Run Query.\\n\\n    advisorresources\\n    | where type == \\"microsoft.advisor/recommendations\\"\\n\\n![Azure Resource Graph Explorer](/uploads/azureresourcegraph.png \\"Azure Resource Graph Explorer\\")\\n\\nI then clicked on the \'See Details\' on the right-hand side to see all the details that were being brought in, in each object or row. Example below:\\n\\n    {\\n        \\"recommendationTypeId\\": \\"7262dc51-c168-41b5-b99b-b5b98f8fe50a\\",\\n        \\"extendedProperties\\": {\\n            \\"assessmentKey\\": \\"7262dc51-c168-41b5-b99b-b5b98f8fe50a\\",\\n            \\"score\\": \\"0\\"\\n        },\\n        \\"resourceMetadata\\": {\\n            \\"resourceId\\": \\"/subscriptions/0673a0bd-0c9b-483f-9aee-c44795ae739f\\",\\n            \\"singular\\": null,\\n            \\"plural\\": null,\\n            \\"action\\": null,\\n            \\"source\\": \\"/subscriptions/0673a0bd-0c9b-483f-9aee-c44795ae739f/providers/Microsoft.Security/assessments/7262dc51-c168-41b5-b99b-b5b98f8fe50a\\"\\n        },\\n        \\"shortDescription\\": {\\n            \\"solution\\": \\"Subscriptions should have a contact email address for security issues\\",\\n            \\"problem\\": \\"Subscriptions should have a contact email address for security issues\\"\\n        },\\n        \\"suppressionIds\\": null,\\n        \\"impactedField\\": \\"Microsoft.Subscriptions/subscriptions\\",\\n        \\"impactedValue\\": \\"0673a0bd-0c9b-483f-9aee-c44795ae739f\\",\\n        \\"lastUpdated\\": \\"2021-04-08T13:15:54.2870000Z\\",\\n        \\"category\\": \\"Security\\",\\n        \\"metadata\\": null,\\n        \\"impact\\": \\"Low\\"\\n    }\\n\\nAnd no, that isn\'t my real Subscription ID etc, I\'ve replaced the field with randomly generated GUIDs.\\n\\nWe can see that there is a good amount of actionable data here such as:\\n\\n* This is a Security Category recommendation\\n* It is Low Impact\\n* The problem is that the Azure subscription should have a contact email address to be used for Security alerts and it does not have one set up (Oops!)\\n\\nSo we need to turn it into something a bit more useable, I know that the Azure Advisor has the following categories:\\n\\n* Cost\\n* HighAvailability\\n* OperationalExcellence\\n* Performance\\n* Security\\n\\nThe same syntax can be used for any of these categories, for my example, we will continue with Security, Looking at the Details (or Example above) we can see that Category is simply listed on its own at the top level, inside the \'microsoft.advisor/recommendations\' field, so we now need to add another pipe to the query:\\n\\n    | where properties[\'category\'] == \'Security\'\\n\\nThis will now only select the \'Security\' category. However as you can see below, it\'s hardly something you can action on or read.\\n\\n![Azure Resource Graph - Category \'Security\'](/uploads/azureresourcegraph_category.png \\"Azure Resource Graph - Category \'Security\'\\")\\n\\nThe next step is to look into making it a bit more readable because we know this is a Kusto Language, its time to hit the Microsoft Docs page and read up about the \'Project Operator\' - [https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/projectoperator](https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/projectoperator \\"https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/projectoperator?WT.mc_id=AZ-MVP-5004796\\"). Project = \\"Select the columns to include, rename or drop, and insert new computed columns.\\" That sounds like what we want.\\n\\nIf we take a gander back at the \'Full Details\' (or Example above) there are 3 fields I am looking at that would add the most value to a report or digest for the security posture of my Azure ecosystem:\\n\\n* Solution\\n* impactedField\\n* impactedValue\\n\\nWe now need to add our final pipe to remove everything we don\'t want and add the properties that make the most sense to use, because we are using multiple properties we will do it separated by commas. It\'s worth noting that unlike the \'Security\' property above (and the impactedField, impactedValue), which was a top-level property, the Solution property is a sub-properties of \'shortDescription\', so we have to select the shortdescription property and then expand out to the extended solution property like below:\\n\\n    | project properties.shortDescription.solution\\n\\nThat now gives us a list of the security alerts on the subscription, but without a heading that makes sense:\\n\\n![Azure Resource Graph](/uploads/azureresourcegraphheader.png)\\n\\nTo add a header called: Recommendation, we need to do the following\\n\\n    | project Recommendation=tostring(properties.shortDescription.solution)\\n\\nNow we are ready to add the impactedField and impactedValue.\\n\\nThe final query should look like this:\\n\\n    advisorresources\\n    | where type == \'microsoft.advisor/recommendations\'\\n    | where properties[\'category\'] == \'Security\'\\n    | project Recommendation=tostring(properties.shortDescription.solution), ImpactedType=tostring(properties.impactedField), ImpactedResources=tostring(properties.impactedValue )\\n\\nand the Azure Resource Graph Explorer should display something like this:\\n\\n![Azure Resource Graph](/uploads/azuregraphexplorerfinalquery.png)\\n\\nProtip, on the Azure Resource Graph Explorer page, click on \'Get Started\', underneath the Query window to view Example Queries, such as Listing all Public IP addresses or even getting the Security Center Recommendations. They are really good to use as a base and see how they work.\\n\\n### Azure Graph PowerShell\\n\\nUsing the Azure Resource Graph Explorer is a good way to create the Kusto queries you want, which you can then run the queries in PowerShell and turn them into PowerShell objects, which opens up a few possibilities for things like:\\n\\n* Automated Reporting on Cost, Security etc\\n* Proactive remediation actions.\\n\\nFirst things first you need to install the Az.ResourceGraph module, then you can use the Search-AzGraph to run the queries that you created above. I am going to rely on the gist below to give you a few examples.\\n\\n![Azure Resource Graph](/uploads/azuregraphpowershell.png)\\n\\n```powershell title=\\"AzGraph.ps1\\"\\n\\n   <#\\n      .SYNOPSIS\\n      Installs the Az.ResourceGraph Module and has example queries\\n      .NOTES\\n      Version:        1.0\\n      Author:         Luke Murray (Luke.Geek.NZ) \\n      Website:        https://luke.geek.nz/azure-resource-graph-explorer-and-the-powershell-azure-resource-graph\\n      Creation Date:  09.04.21\\n      Change History: \\n      09.04.21 - Intital script development\\n\\n  #>\\n  \\n  # Install the Resource Graph module from PowerShell Gallery\\nInstall-Module -Name Az.ResourceGraph -Scope CurrentUser\\n\\n# Imports the Resource Graph module into the PowerShell session\\nImport-Module -Name Az.ResourceGraph\\n\\n#Connects to Microsoft Azure\\nConnect-AzAccount\\n\\n#Grabs the acount of all recommendations under each Category that the Azure Advisor Has\\n\\nSearch-AzGraph -Query \\"advisorresources | summarize Count=count() by Category=tostring(properties.category) | where Category!=\'\' | sort by Category asc\\"\\n\\n#Following on from the Blog post, this is the query we created to list all Security recommendations, their resource type and what resources were impacted\\n\\nSearch-AzGraph -Query \\"advisorresources\\n| where type == \'microsoft.advisor/recommendations\'\\n| where properties[\'category\'] == \'Security\'\\n| project Recommendation=tostring(properties.shortDescription.solution), ImpactedType=tostring(properties.impactedField), ImpactedResources=tostring(properties.impactedValue )\\"\\n\\n#List of Performance recommendations\\n\\nSearch-AzGraph -Query \\"advisorresources | where type == \'microsoft.advisor/recommendations\' and properties.category == \'Performance\' | project Solution=tostring(properties.shortDescription.solution) | summarize Count=count() by Solution | sort by Count\\"\\n\\n#List of Cost recommendations\\n\\nSearch-AzGraph -Query \\"advisorresources | where type == \'microsoft.advisor/recommendations\' and properties.category == \'Cost\' | summarize Resources = dcount(tostring(properties.resourceMetadata.resourceId)), Savings = sum(todouble(properties.extendedProperties.savingsAmount)) by Solution = tostring(properties.shortDescription.solution), Currency = tostring(properties.extendedProperties.savingsCurrency) | project Solution, Resources, Savings = bin(Savings, 0.01), Currency | order by Savings desc\\"\\n\\n```"},{"id":"/2021/04/03/keep-up-to-date-with-latest-changes-on-azure-using-powershell","metadata":{"permalink":"/2021/04/03/keep-up-to-date-with-latest-changes-on-azure-using-powershell","source":"@site/blog/2021-04-03-keep-up-to-date-with-latest-changes-on-azure-using-powershell.md","title":"Keep up to date with Azure changes using PowerShell","description":"Keeping up with what is happening with changes and previews in Microsoft Azure is difficult, change happens all the time - and being able to stay informed on what is happening with the Azure ecosystem is half the battle, whether it is a new feature or security fix.","date":"2021-04-03T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"},{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.68,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Keep up to date with Azure changes using PowerShell","authors":["Luke"],"tags":["Misc","PowerShell","Azure"],"date":"2021-04-03 00:00:00 +1300","toc":true,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Azure Resource Graph Explorer and the PowerShell Azure Resource Graph","permalink":"/2021/04/09/azure-resource-graph-explorer-and-the-powershell-azure-resource-graph"},"nextItem":{"title":"Microsoft Teams Recommendations","permalink":"/2021/03/30/microsoft-teams-recommendations"}},"content":"Keeping up with what is happening with changes and previews in Microsoft Azure is difficult, change happens all the time - and being able to stay informed on what is happening with the Azure ecosystem is half the battle, whether it is a new feature or security fix.\\n\\nMicrosoft publishes the latest updates on Azure Products and features to their Azure Updates blog: [https://azure.microsoft.com/en-us/updates/](https://azure.microsoft.com/en-us/updates/ \\"https://azure.microsoft.com/en-us/updates/?WT.mc_id=AZ-MVP-5004796\\")\\n\\nSo you can browse the website each week, or... monitor the RSS feeds. Sometimes this isn\'t enough, you may want to do something with this information such as:\\n\\n* Create Alerts or Notifications to specific teams who may work with Azure SQL, or Azure Automation and not care about any other product.\\n* Not have to go to the website to keep up-to-date with what is happening, maybe your happy with it popping up in your PowerShell session each time you open it.\\n* Publish the information to Microsoft Teams channels to keep people informed.\\n\\nI have created a basic PowerShell function, that will retrieve the latest updates from the Microsoft Azure Updates RSS Feed and turn it into a PowerShell object you can actually use to keep informed.\\n\\n## The Script - Get-AzureBlogUpdates\\n\\nThe script is hosted on my Github repository. Feel free to clone/recommend improvements or fork, I can add parameter sets instead of relying on the PowerShell methods listed in the examples section - if you find this script useful:\\n\\n```powershell title=\\"Get-AzureBlogUpdates.ps1\\"\\n\\nfunction Get-AzureBlogUpdates {\\n    <#\\n      .SYNOPSIS\\n      Retrieves the latest Updates of Azure, from the Azure Blog RSS feed.\\n      .DESCRIPTION\\n      Retrieves the latest Updates of Azure, from the Azure Blog RSS feed.\\n      .NOTES\\n      Version:        1.0\\n      Author:         Luke Murray (Luke.Geek.NZ) \\n      Website: https://luke.geek.nz/keep-up-to-date-with-latest-changes-on-azure-using-powershell\\n      Creation Date:  03.04.21\\n      Purpose/Change: \\n      03.04.21 - Intital script development\\n      .EXAMPLE\\n      Get-AzureBlogUpdate\\n\\n  #>\\n  #Retrieving RSS Feed Content - as XML, then converting into PSObject\\n    $xml = [xml](Invoke-WebRequest -Uri \'https://azurecomcdn.azureedge.net/en-us/updates/feed/\').content\\n    $Array = @()\\n    foreach ($y in $xml.rss.channel.selectnodes(\'//item\'))\\n    {\\n        $PSObject = New-Object -TypeName PSObject\\n        $Date = [datetime]$y.pubdate\\n        $PSObject | Add-Member NoteProperty \'Title\'  $y.title\\n        $PSObject | Add-Member NoteProperty \'Date\' $Date\\n        $PSObject | Add-Member NoteProperty \'Category\'  $y.category\\n        $PSObject | Add-Member NoteProperty \'Description\'  $y.content.InnerText\\n        $PSObject | Add-Member NoteProperty \'Link\'   $y.link\\n    \\n    \\n        $Array += $PSObject\\n    } \\n    #Some article had multiple categories, to make it easier for reporting, joined the categories together and got rid of duplicates.\\n\\n    $results = @()\\n    ForEach ($item in $Array) {\\n        $Category = Foreach ($title in $item.Title)\\n        {\\n            $results += [pscustomobject]@{\\n                \'Title\'          = $item.Title\\n                \'Category\'       = $item.Category -join \',\' | Select-Object -Unique\\n                \'Published Date\' = $item.Date\\n                \'Description\'    = $item.Description\\n                \'Link\'           = $item.Link\\n            }\\n        }\\n    }\\n    $results\\n}\\n\\n```\\n\\n## Examples\\n\\n    #Runs the actual Function:\\n    Get-AzureBlogUpdates\\n\\n![Get-AzureBlogUpdates](/uploads/windowsterminal_5oqnizj8ko.png)\\n\\n    #EXAMPLE - Gets Azure Blog Updates, that have been published in the last 7 days.\\n    $PublishedIntheLastDays = (Get-Date).AddDays(-7)\\n    Get-AzureBlogUpdates | Where-Object \'Published Date\' -GT $PublishedIntheLastDays\\n\\n![Get-AzureBlogUpdates](/uploads/windowsterminal_duphvuiqpz.png)\\n\\n    #EXAMPLE - Gets all Azure Blog Updates, and displays it as a Table, organised by Category\\n    Get-AzureBlogUpdates | Sort-Object Category -Descending | Format-Table\\n\\n![Get-AzureBlogUpdates](/uploads/windowsterminal_xrskcraov0.png)\\n\\n    #EXAMPLE -Gets the latest 10 Azure Blog Articles\\n    Get-AzureBlogUpdates | Select -Last 10\\n\\n![Get-AzureBlogUpdates - Select Last 10 Articles](/uploads/windowsterminal_bxxy0lnrjc.png \\"Get-AzureBlogUpdates - Select Last 10 Articles\\")\\n\\n    #EXAMPLE - Gets the Azure Blog Update articles, where the title has Automation in it.\\n    Get-AzureBlogUpdates | Where-Object Title -match \'Automation\'\\n\\n![Get-AzureBlogUpdates - Title matches Automation](/uploads/windowsterminal_qitgwrqfm9.png \\"Get-AzureBlogUpdates - Title matches Automation\\")"},{"id":"/2021/03/30/microsoft-teams-recommendations","metadata":{"permalink":"/2021/03/30/microsoft-teams-recommendations","source":"@site/blog/2021-03-30-microsoft-teams-recommendations.md","title":"Microsoft Teams Recommendations","description":"In the age of remote working and collaboration, Microsoft Teams is one of the most popular tools being used to increase communication and productivity.","date":"2021-03-30T00:00:00.000Z","tags":[],"readingTime":12.31,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Teams Recommendations","authors":["Luke"],"Tags":["M365"],"date":"2021-03-30 00:00:00 +1300","toc":false,"header":{"teaser":"images/Office-365-Banner2.png","classes":"wide"}},"unlisted":false,"prevItem":{"title":"Keep up to date with Azure changes using PowerShell","permalink":"/2021/04/03/keep-up-to-date-with-latest-changes-on-azure-using-powershell"},"nextItem":{"title":"Transfer Ownership of an Azure Subscription","permalink":"/azure/transfer-ownership-of-azure-subscription"}},"content":"In the age of remote working and collaboration, Microsoft Teams is one of the most popular tools being used to increase communication and productivity.\\n\\nEspecially those undergoing implementation and migrations from Skype for Business to Microsoft Teams - it is a good opportunity to take a step back and evaluate and clarify your implementation, the recommendations below as good as a place to start as any.\\n\\nPlease keep in mind that like any recommendations, do not blindly follow them, make sure to determine the impact on your users on enabling some of this functionality, there may also be recommendations that you will not be able to apply, do to business constraints.\\n\\n**Recommendation**|**Description**\\n-----|-----\\nAdd the Microsoft Teams SMTP domain as an allowed list in Microsoft Exchange Online  Spam filter protection|Whether you create an Office 365 Group in the admin console or by using Outlook, Exchange Online is used to send notifications of a team member being added to a Group. These messages are generated from your tenant as they represent your default domain SMTP FQDN.Teams uses Microsoft Exchange Online as well to send notifications to team members when they\u2019ve been added. The difference being the domain FQDN of the SMTP message is \u201c@email.teams.microsoft.com\u201d and could be caught by spam filtering. Outlook considers message from Teams as an external sender which is subject to standard security features such as blocking images and certain content.\\nAllow the following User Agent Strings for Microsoft Teams within the EWS configuration|Teams users may not be able to access Teams meetings/connectors though their mailboxes are in Exchange Online.\\nAssign a valid security group that can be used for controlling who can create Office 365 groups as well as Office 365 services that depends on groups such as Teams, Planner, etc|A security group is configured to restrict which users are allowed to create groups. However this security group does not exist anymore which prevents the creation of new groups.\\nAssign Teams Meeting Room license to your Teams meeting room account |Without the proper license, you may have some Teams Meeting room features that are not working properly or not available such as the ability to dial-out attendees into your meeting.\\nAssociate registered SBC with Office domain|Check as part of the Direct Routing configuration is missing the domain name associated with one of your SBCs.\\nCheck  Skype for Business to Microsoft Teams meeting migration failures|Some of your user\u2019s meeting may not have been successfully migrated  from Skype for Business to Teams.  Users might be unable to join the affected meetings.\\nCheck Microsoft Stream license is assigned to users if cloud recording is allowed |Your users who can do Teams meeting and recordings may not have the necessary Microsoft Stream license to store /upload meeting recordings / playback to Microsoft Stream.\\nCheck the SBC gateway(s) associated with voice routes|Makse sure that none or more of your SBC gateway(s) defined in Voice Routes are in disabled state. This could cause unexpected call failures.\\nConfigure your Meeting Room accounts with the recommended setting of AddAdditionalResponse|Microsoft Teams Rooms will only work in a properly configured Microsoft Teams or Skype for Business environment where the device accounts are set up correctly.  To provide optimal meeting experience, you should configure your meeting room accounts meeting the recommendations.\\nConfigure your Meeting Room accounts with the recommended setting of AddOrganizerToSubject|Microsoft Teams Rooms will only work in a properly configured Microsoft Teams or Skype for Business environment where the device accounts are set up correctly.  To provide optimal meeting experience, you should configure your meeting room accounts meeting the recommendations.\\nConfigure your Meeting Room accounts with the recommended setting of DeleteComments|Microsoft Teams Rooms will only work in a properly configured Microsoft Teams or Skype for Business environment where the device accounts are set up correctly.  To provide optimal meeting experience, you should configure your meeting room accounts meeting the recommendations.\\nConfigure your Meeting Room accounts with the recommended setting of RemovePrivateProperty|Microsoft Teams Rooms will only work in a properly configured Microsoft Teams or Skype for Business environment where the device accounts are set up correctly.  To provide optimal meeting experience, you should configure your meeting room accounts meeting the recommendations.\\nCreate meeting room lists for room mailboxes to allow for searching and booking rooms with Microsoft Teams |You need to create room list distribution group to be able to specify a meeting room when you schedule a Teams meeting.\\nCreate multiple Microsoft Teams IP Phone Policies to cater for the different phones and meeting rooms devices that you have in the organization |To provide more tailored user interfaces to different phones and meeting room devices that you\'ve, it is recommended to create different IP Phone policies to them.\\nCreate Office 365 Groups Classification|You can create classifications that the users in your organization can set when they create an Office 365 group. For example, you can allow users to set \\"Standard\\", \\"Secret\\", and \\"Top Secret\\" on groups they create. Group classifications aren\'t set by default and you need to create it in order for your users to set it. Use Microsoft Entra ID PowerShell to point your users to your organization\'s usage guidelines for Office 365 groups.\\nDefine Office 365 Group naming policy|To enforce consistent naming conventions for Office 365 groups created or edited by your users, set up a group naming policy for your tenants in Microsoft Entra ID (Azure AD). For example, you could use the naming policy to communicate the function of a group, membership, geographic region, or who created the group. You could also use the naming policy to help categorize groups in the address book. You can use the policy to block specific words from being used in group names and aliases.\\nEnable Advance Threat Protection for Teams|People regularly share files and collaborate using SharePoint, OneDrive, and Microsoft Teams. With Office 365 Advanced Threat Protection (ATP), your organization can collaborate in a safer manner. ATP helps detect and block files that are identified as malicious in team sites and document libraries.\\nEnable connectors in your Exchange Online environment|When connectors are disabled in Exchange Online environment this is impacting connectors in Microsoft Teams. Users who are trying to add a connector in both Teams desktop client and a web app version will get the error: \u201cConnectors have been turned off for this mailbox by the admin. Contact your admin if you want to have connectors turned on: Access to Connectors is disabled.\u201d\\nEnable Teams license for some Office 365 Users |At the user level, access to Microsoft Teams can be enabled or disabled on a per-user basis by assigning or removing the Microsoft Teams product license. Once the license is disabled, the user access to Microsoft Teams will be prevented and the user will no longer be able to see Teams in the Office 365 app launcher and homepage. \\nEnable users SharePoint Online, OneDrive for Business and Exchange Online|For the full Microsoft Teams experience, every user should be enabled for Exchange Online, SharePoint Online, and Office 365 Group creation.SharePoint Online is required to share and store files in team conversations. OneDrive for Business is required to share and store files in private chats. If users aren\'t assigned and enabled with SharePoint Online licenses, they don\'t have OneDrive for Business storage in Office 365. File sharing will continue to work in Channels, but users are unable to share files in Chats without OneDrive for Business storage in Office 365. In Microsoft Teams, security and compliance features like eDiscovery, Content Search, archiving, and legal hold work best in Exchange Online and SharePoint Online environments. For channel conversations, messages are journaled to the group mailbox in Exchange Online, where they\'re available for eDiscovery. If SharePoint Online and OneDrive for Business (using work or school account) are enabled across the organization and for users, these compliance features are available for all files within Teams as well.\\nEnsure a public IP associated with FQDN of the SBCs|SBC needs to have valid public IP address to make it accessible from Internet by Teams Direct Routing components.\\nEnsure that the right ports and protocols are open across your network for optimum call experience|Skype for Business Online audio/video calls over TCP traffic do not perform as well as calls over UDP traffic.\\nGrant Teams Direct Routing users with appropriate Voice Routing Policy|List of users who are enabled for Teams DR/Hybrid Voice but not assigned with any OnlineVoiceRoutingPolicy\\nImplement Office 365 Groups governance|Office 365 Groups has a rich set of tools to implement any governance capabilities your organization might require.\\nImprove Network Performance for Skype for Business Online/Microsoft Teams |The quality of real-time media (audio, video, and application sharing) over IP is greatly impacted by the quality of end-to-end network connectivity. For optimal Skype for Business Online media quality, it is important for you to make sure there is a high-quality connection between your company network and Skype for Business Online. The best way to accomplish this is to set up your internal network and cloud connectivity based on the capacity of your network to accommodate for peak traffic volume for Skype for Business Online across all connections.\\nInfo: Teams which have external/guest users |You should review external users who had been invited to Teams in your environment.\\nLeverage the Teams RBAC to specify different levels of Teams administrative access |Using Microsoft Entra ID (Azure AD), you can designate administrators who need different levels of access for managing Microsoft Teams. Administrators can manage the entire Teams workload, or they can have delegated permissions for troubleshooting call quality problems or managing your organization\'s telephony needs.\\nLimit the number of Office 365 Global Administrators|Having too many Office 365 Global Administrators might indicate that you\u2019ve not assigned the right individuals to manage your overall Office 365 environment. This could result in unwanted configuration changes to Office 365 if some of these individuals does not have the right skills or capabilities.\\nMulti Factor Authentication (MFA) is not enabled for Skype for Business Administrators and/or Office 365 Global Administrators|Make sure that any account who is in the Global Administrators or Skype for Business Online Administrators group are not enabled for Multi Factor Authentication (MFA). It is recommended to enable MFA for these accounts to add an additional layer of security during the authentication process.\\nOffice 365 Groups usage guidelines has not been put in-place|When users create or edit a group, you can show them a link to your organization\'s usage guidelines. For example, if you require a specific prefix or suffix to be added to a group name.\\nOld version of Skype for Business Network Assessment Tool detected|Using an older version of the Skype for Business Network Assessment Tool will impact the data collection. It is recommended to update to the latest version of the tool and run another data collection.\\nReview Teams meeting policy assigned to your users|Meeting policies are used to control the features that are available to meeting participants for meeting that are scheduled by users in your organization. Different users across your organization might need different meeting features based of what they do and other things. By providing them with the right meeting policy, not only you facilitate them to accomplish their jobs but also you help to optimize the Teams environment and organization resources.\\nReview the ability for Team owners to invite external users to teams|Allowing Team owners to invite external users to teams could improve work productivity and drive collaboration with external users.\\nReview the Direct Routing Users whose Skype for Business accounts are hosted in on-premises Skype for Business Server|Microsoft Teams Direct Routing works only if SfB user accounts is hosted in Skype for Business Online.\\nReview the Teams user accounts which had some provisioning problems|Users may experience issue when using Skype for Business Online or Microsoft Teams when they\u2019re not properly provisioned.\\nReview the Teams users\u2019 calling policy|All users are configured with the default Teams calling policy.\\nReview your Teams Co-existence mode and upgrade settings |Your current Teams and Skype for Business Global co-existence mode may be set to Island mode which might not be the best co-existence mode for the organization and could be limiting features.\\nSet AllowGuestsToAccessGroups on unified group setting to True|This setting indicates whether or not a guest user can have access to Files or OneNote content in your Teams. This setting does not require an Microsoft Entra ID Premium P1 license. \\nSet the UsersPermissionToReadOtherUsersEnabled  to true in your Azure AD configuration|When this value is set to false in AAD, Teams owner is unable to add external/internal members in Microsoft Teams, and the following error message is displayed: \\"We couldn\'t add member. We ran into an issue. Please try again later.\\" However, members can be added directly to Office 365 groups.\\nSpecify a security group who can create Office 365 groups and its related services|Because it\'s so easy for users to create Office 365 Groups, you aren\'t inundated with requests to create them on behalf of other people. Depending on your business, however, you might want to control who has the ability to create groups.\\nTeams Upgrade Status: Candidate - Check the Teams Upgrade Status using Get-CsTeamsUpgradeStatus |Microsoft initiates and performs automatic upgrade to Teams to organizations that meet certain requirements. You need to understand what Teams upgrade means and the impact it would have to your organization.\\nTeams Upgrade Status: Deferred - Check the Teams Upgrade Status using Get-CsTeamsUpgradeStatus |Microsoft initiates and performs automatic upgrade to Teams to organizations that meet certain requirements. You need to understand what Teams upgrade means and the impact it would have to your organization.\\nTeams Upgrade Status: Downgraded - Check the Teams Upgrade Status using Get-CsTeamsUpgradeStatus |Microsoft initiates and performs automatic upgrade to Teams to organizations that meet certain requirements. You need to understand what Teams upgrade means and the impact it would have to your organization.\\nTeams Upgrade Status: Paused- Check the Teams Upgrade Status using Get-CsTeamsUpgradeStatus |Microsoft initiates and performs automatic upgrade to Teams to organizations that meet certain requirements. You need to understand what Teams upgrade means and the impact it would have to your organization.\\nTeams Upgrade Status: ScheduledForUpgrade - Check the Teams Upgrade Status using Get-CsTeamsUpgradeStatus |Microsoft initiates and performs automatic upgrade to Teams to organizations that meet certain requirements. You need to understand what Teams upgrade means and the impact it would have to your organization.\\nTeams Upgrade Status: Upgraded - Check the Teams Upgrade Status using Get-CsTeamsUpgradeStatus |Microsoft initiates and performs automatic upgrade to Teams to organizations that meet certain requirements. You need to understand what Teams upgrade means and the impact it would have to your organization.\\nValidate licenses assigned to Teams Room System|Without the proper license, you may have some Teams Meeting room features that are not working properly or not available such as the ability to dial-out attendees into your meeting"},{"id":"azure/transfer-ownership-of-azure-subscription","metadata":{"permalink":"/azure/transfer-ownership-of-azure-subscription","source":"@site/blog/2021-03-30-transfer-ownership-of-azure-subscription.md","title":"Transfer Ownership of an Azure Subscription","description":"Imagine you want to transfer Azure resources to another person or company? This could be because something may have been created in an external third-party subscription, to begin with, or you have created a product using Azure resources that you have just sold to the highest bidder!","date":"2021-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":3.16,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Transfer Ownership of an Azure Subscription","authors":["Luke"],"tags":["Azure"],"date":"2021-03-30 00:00:00 +1300","toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"},"slug":"azure/transfer-ownership-of-azure-subscription"},"unlisted":false,"prevItem":{"title":"Microsoft Teams Recommendations","permalink":"/2021/03/30/microsoft-teams-recommendations"},"nextItem":{"title":"Microsoft Entra ID Recommendations","permalink":"/2021/03/29/azure-active-directory-recommendations"}},"content":"Imagine you want to transfer Azure resources to another person or company? This could be because something may have been created in an external third-party subscription, to begin with, or you have created a product using Azure resources that you have just sold to the highest bidder!\\n\\nBefore you start rolling in that money bin of cash, you need to be able to give that person the Azure resources. The best way to do this is to transfer ownership of an Azure subscription.\\n\\nIt may be best to create a new Azure subscription, and then transfer _(using the Move Resources in the Azure Resource Group)_ the resources to that new subscription. That way it is clean, then the recipient can just migrate the resources to their own Production subscription later, etc as they see fit.\\n\\nJust a heads up IF you are selling services you have created in Microsoft Azure, whether freelance or professionally make sure you have spent time working on Azure governance to make sure you have a proper Azure Landing Zone stood up for standardization and naming conventions in place if you are a transferring a resource that has a Global Scope (ie these are usually Public-facing, the last thing you want is to transfer the resources to someone else and find out that you can\'t reuse the same unique name.\\n\\n**_Please read this carefully,_** there are certain limitations when transferring Subscription Ownership - especially across to another tenancy that you need to be aware of, these limitations are the Type of Subscription it is and the type of resources, encryption status, etc.[Transfer an Azure subscription to a different Azure AD directory](https://learn.microsoft.com/en-gb/azure/role-based-access-control/transfer-subscription?WT.mc_id=AZ-MVP-5004796#understand-the-impact-of-transferring-a-subscription) In some cases, you may need to look at alternative ways, such as redeploying or recreating the resources in the other subscription/tenancy manually - via redirecting an Azure DevOps deployment or manual backup export and import.\\n\\n## Transfer a Subscription\\n\\nOnce you are ready to transfer a subscription, you can do the rest, simply through the Azure Portal:\\n\\n1. In the Azure Portal, navigate to [Subscriptions](https://portal.azure.com/#blade/Microsoft_Azure_Billing/SubscriptionsBlade)\\n2. Click on the Subscription you want to migrate\\n3. Click on Transfer billing ownership\\n4. Type in the Recipient\'s email address, in the email address field\\n5. If you are moving the Azure subscription to another Azure AD tenancy (in this article, I am assuming we are), select the \'Move Subscription Tenant toggle: Yes\\n   ![Transfer Billing Ownership](/uploads/2021-03-28-18_56_27-transfer-billing-ownership-microsoft-azure-mozilla-firefox.png)\\n6. Click on Send Transfer Request, acknowledge the prompt and click Yes\\n7. This will send an email to the recipient with a link to transfer the Azure subscription and all the resources.\\n   ![Transfer Billing Ownership](/uploads/transferrequest.png)\\n\\nNote: The Transfer Request is not permanent, the recipient has only a few weeks to accept the transfer before you will need to it again, you can see the expires date in the screenshot above.\\n\\nNote: Something to be aware of, only the user in the new account who accepted the transfer request will have access to manage the resources, they will need to add the necessary groups and rights on their end.\\n\\n## Cancel an Azure Subscription Transfer\\n\\nIf the recipient hasn\'t accepted the transfer, you can revoke or cancel the transfer request. To do this, do the following:\\n\\n1. In the Azure Portal, navigate to [Subscriptions](https://portal.azure.com/#blade/Microsoft_Azure_Billing/SubscriptionsBlade)\\n2. Click on the Subscription you want to migrate\\n3. Click on Transfer billing ownership\\n4. You will now get a Window indicating the Transfer Request is pending\\n5. Click on Cancel the Transfer Request (bottom of the Blade)\\n   ![Transfer Billing Ownership](/uploads/transferrequest_cancel.png)\\n6. Accept the prompt to cancel the transfer request.\\n\\nNote: You can now click on the Transfer billing ownership, to confirm the request was canceled and if needed, open a new request. Just a heads up as well, that canceling the transfer, will also email the recipient."},{"id":"/2021/03/29/azure-active-directory-recommendations","metadata":{"permalink":"/2021/03/29/azure-active-directory-recommendations","source":"@site/blog/2021-03-29-azure-active-directory-recommendations.md","title":"Microsoft Entra ID Recommendations","description":"Microsoft Entra ID is the foundation, which Microsoft 365 is built-on.","date":"2021-03-29T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":8.58,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Microsoft Entra ID Recommendations","authors":["Luke"],"tags":["Azure"],"date":"2021-03-29 00:00:00 +1300","toc":false,"header":{"teaser":"images/Office-365-Banner2.png","classes":"wide"}},"unlisted":false,"prevItem":{"title":"Transfer Ownership of an Azure Subscription","permalink":"/azure/transfer-ownership-of-azure-subscription"},"nextItem":{"title":"How to restrict users to specific boards in Azure DevOps","permalink":"/azure/misc/how-to-restrict-users-to-specific-boards-in-azure-devops"}},"content":"Microsoft Entra ID is the foundation, which Microsoft 365 is built-on.\\n\\nIn the words of Microsoft:\\n\\n> Microsoft Entra ID (Azure AD) is Microsoft\u2019s cloud-based identity and access management service, which helps your employees sign in and access resources in:\\n>\\n> * External resources, such as Microsoft 365, the Azure portal, and thousands of other SaaS applications.\\n> * Internal resources, such as apps on your corporate network and intranet, along with any cloud apps developed by your own organization.\\n\\nMicrosoft Entra ID (AAD) is simply not set and forget, especially given the fact that AAD  services are constantly evolving in terms of features and improved security.\\n\\nBelow is a table of some Microsoft Entra ID and best practice recommendations.\\n\\nPlease keep in mind that like any recommendations, do not blindly follow them, make sure to determine the impact on your users on enabling some of this functionality, there may also be recommendations that you will not be able to apply, do to business constraints.\\n\\n| Recommendation | Why Consider This | Probability | Impact | Effort |\\n| --- | --- | --- | --- | --- |\\n| Change break glass accounts passwords every 90 days | Emergency access accounts are highly privileged, and they are not assigned to specific individuals. Emergency access accounts are limited to emergency or \\"break glass\\"\' scenarios where normal administrative accounts can\'t be used. We recommend that you maintain a goal of restricting emergency account use to only the times when it is absolutely necessary. | High | High | Low |\\n| Review possible stale Guest (B2B) accounts | Guest accounts do not exist by default and pose a potential data exposure vulnerability if left unused.  Guest accounts should only be used with a defined business need and closely monitored to ensure accounts are valid/legitimate. | High | Moderate | Low |\\n| Remove invited guests who have not accepted invite | Remove invited guests who have not accepted invite as it helps control the scope of identity and access management as it pertains to provisioning users in Azure AD.   In addition, removing stale invites and user from Azure AD is part of the recommended routine account maintenance. | High | Low | Low |\\n| Enable Windows Hello for Business PIN Reset Service | The Microsoft PIN reset services enables you to help users recover who have forgotten their PIN. Using Group Policy, Microsoft Intune or a compatible MDM, you can configure Windows 10 devices to securely use the Microsoft PIN reset service that enables users to reset their forgotten PIN through settings or above the lock screen without requiring re-enrollment. | Low to Moderate | Moderate | Low |\\n| Ensure security compliance notification mail is set | Managing security and compliance is a partnership. You are responsible for protecting your data, identities, and devices, while Microsoft vigorously protects Office 365 services. You can use Office 365 and Enterprise Mobility + Security (EMS) together to help you achieve the appropriate level of protection for your organization. | Low to Moderate | Moderate | Low |\\n| Add owner to legacy Service Principal | Legacy service principals without a defined owner create a challenge for management and accountability. | Low to Moderate | Moderate | Low |\\n| Add owner to application | Assigning an application owner provides an opportunity for delegation and establishes accountability for management of the resource. | Low to Moderate | Moderate | Low |\\n| Add owner to cloud-only groups | Assigning a group owner provides an opportunity for delegation and establishes accountability for management of the resource. | Low to Moderate | Moderate | Low |\\n| Require that users can create security groups is set to no | The creation and management of security groups should be restricted to administrators only to limit proliferation of this security principal.  The default setting is to prevent users from creating security groups in the Azure portal and it is recommended to maintain this configuration unless required by a defined business need. | Low to Moderate | Moderate | Low |\\n| Delete empty cloud-only groups | Cloud-only groups that contain no members and are not associated with Azure applications should be deleted as they serve no purpose. | Low to Moderate | Low | Low |\\n| Review Dynamic Groups with membershipRuleProcessingState not turned on | Sometimes you may want to stop the processing of a dynamic group, like when you\u2019re importing a large number of new users or reorganizing your group architecture. To do that, use the MembershipRuleProcessingState parameter to switch processing on and off. | Low to Moderate | Low | Low |\\n| Review and consider federating all domains | When a domain is federated with Azure AD, several properties are set on the domain in Azure. One important one is IssuerUri. This property is a URI that is used by Azure AD to identify the domain that the token is associated with. | Low to Moderate | Low | Low |\\n| Review applications with credentials about to expire or are expired | Applications with expired credentials will prevent its use and should be updated before expiration to avoid an outage. If the application\'s service principal already has newer credentials remove the no longer valid credentials. | Moderate | High | Low |\\n| Review applications granted with risky OAUTH2 permissions | Depending on the scope of permissions, it can pose a risk to the confidentiality, integrity, or availability of the organization\'s data.  Periodic review of application permission grants can help identity over-privileged applications and establish access controls that align with the principle of least privilege. | Moderate | High | Low |\\n| Configure user passwords to never expire | Requesting users to regularly change passwords will lead to weak password practices like patterns or sequential words and numbers. | Moderate | Moderate | Low |\\n| Review Service Principals using password based credentials | Protect and manage your confidential app credentials for web apps, web APIs and daemon apps. Use certificate credentials, not password credentials (client secrets). | Moderate | Moderate | Low |\\n| Review Azure AD Guest (B2B) accounts | Guest accounts do not exist by default and pose a potential data exposure vulnerability if left unused.  Guest accounts should only be used with a defined business need and closely monitored to ensure accounts are valid/legitimate. | Moderate | Moderate | Low |\\n| Review applications consented by admins | Review applications granted consent by admins to ensure this global configuration is desired, which results in authorization for applications to data for all users in the Azure AD tenant. | Moderate | Moderate | Low |\\n| Review applications consented by one user | Review applications granted consent by a single users to ensure the configuration is desired, which results in authorization for applications to data for individual users as compared to admin consent which is global for the tenant. | Moderate | Moderate | Low |\\n| Review domain password policies that do not match defaults. | Only passwords for user accounts that are not synchronized through directory synchronization can be configured for password policies.  By default users do not have a password policy defined. | Moderate | Moderate | Low |\\n| Specify the usage location property for users | Some Microsoft services aren\'t available in all locations because of local laws and regulations. Before you can assign a license to a user, you must specify the Usage location property for the user. | Moderate | Moderate | Low |\\n| Require that users can consent to apps accessing company data on their behalf is set to no | Allowing users to provide consent for third-party applications risks exfiltration of personally identifiable information (PII) such as email and phone number, as it\'s associated with the user\'s profile. | High | High | Moderate |\\n| Review group license errors | These errors should be resolved and all users should be assigned expected licenses, for avoiding any loss of productivity. | High | Moderate | Moderate |\\n| Remove email / mailbox from directory role admins | To help separate internet risks (phishing attacks, unintentional web browsing) from administrative privileges, create dedicated accounts for each user with administrative privileges with no mail enabled to make sure they do not inadvertently open emails or run programs associated with their admin accounts. | Moderate | High | Moderate |\\n| Remove Skype address from directory role admins | To help separate internet risks (phishing attacks, unintentional web browsing) from administrative privileges, create dedicated accounts for each user with administrative privileges with no Skype Enabled to make sure they do not inadvertently open emails or run programs associated with their admin accounts. | Moderate | High | Moderate |\\n| Develop plan to migrate or remove legacy Service Principals | ServicePrincipals with ServicePrincipalType of legacy are not associated with an application and should be migrated to an application to improve manageability. | Moderate | Moderate | Moderate |\\n| Federated domains in Azure AD must have SupportsMFA enabled if ADFS MFA is used | When the configured conditional access policy requires multi-factor authentication, Azure AD defaults to using Azure MFA. If you use the federation service for MFA, you can configure Azure AD to redirect to the federation service when MFA is needed by setting -SupportsMFA to $true in PowerShell. This setting works for federated authentication services that support the MFA challenge request issued by Azure AD | Moderate | Moderate | Moderate |\\n| Verify all root level domains | Every new Azure AD tenant comes with an initial domain name, domainname.onmicrosoft.com. You can\'t change or delete the initial domain name, but you can add your organization\'s names to the list. Adding custom domain names helps you to create user names that are familiar to your users | Moderate | Moderate | Moderate |\\n| Review user objects no longer syncing with on-premises | Users present in Windows Server AD and no longer syncing to Azure AD impacts users ability to use services provided by Azure AD (Password reset, access to O365 services and cloud based apps etc.) and it also poses administrative challenge in managing the account. | Moderate | Moderate | Moderate |"},{"id":"azure/misc/how-to-restrict-users-to-specific-boards-in-azure-devops","metadata":{"permalink":"/azure/misc/how-to-restrict-users-to-specific-boards-in-azure-devops","source":"@site/blog/2021-03-28-how-to-restrict-users-to-specific-boards-in-azure-devops.md","title":"How to restrict users to specific boards in Azure DevOps","description":"Do you ever want to add external Microsoft Entra ID or other users to specific boards in a project, but not want to give them access to the entire Azure DevOps Project?","date":"2021-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.69,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-03-28 00:00:00 +1300","title":"How to restrict users to specific boards in Azure DevOps","authors":["Luke"],"tags":["Azure","Misc"],"toc":false,"slug":"azure/misc/how-to-restrict-users-to-specific-boards-in-azure-devops"},"unlisted":false,"prevItem":{"title":"Microsoft Entra ID Recommendations","permalink":"/2021/03/29/azure-active-directory-recommendations"},"nextItem":{"title":"Azure WebApp 500 Errors reporting from AspNetCoreModule","permalink":"/azure/azure-webapp-500-errors-reporting-from-aspnetcoremodule"}},"content":"Do you ever want to add external Microsoft Entra ID or other users to specific boards in a project, but not want to give them access to the entire Azure DevOps Project?\\n\\nUsing the steps below, we can restrict users to a specific board.\\n\\n1. Invite external users to DevOps org with Stakeholder access.\\n2. In the project, create a new Team and do not add it to the existing security group to inherit permissions.\\n![Azure DevOps - Boards](/uploads/azdevopsboard1.jpg)\\n3. Add external users to created Team.\\n4. Set permission for created Team properly. In this case, it\u2019s to set View project-level information to Allow.\\n![Azure DevOps - Boards](/uploads/azdevopsboard2.jpg)\\n5. Create a new area path and set the permission for the created Team in Security\\n![Azure DevOps - Boards](/uploads/azdevopsboard3.jpg)\\n6. Assign the area path to the newly created Team."},{"id":"azure/azure-webapp-500-errors-reporting-from-aspnetcoremodule","metadata":{"permalink":"/azure/azure-webapp-500-errors-reporting-from-aspnetcoremodule","source":"@site/blog/2021-03-27-azure-webapp-500-errors-reporting-from-aspnetcoremodule.md","title":"Azure WebApp 500 Errors reporting from AspNetCoreModule","description":"Issue Description","date":"2021-03-27T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.88,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-03-27 00:00:00 +1300","title":"Azure WebApp 500 Errors reporting from AspNetCoreModule","authors":["Luke"],"tags":["Azure"],"toc":false,"slug":"azure/azure-webapp-500-errors-reporting-from-aspnetcoremodule"},"unlisted":false,"prevItem":{"title":"How to restrict users to specific boards in Azure DevOps","permalink":"/azure/misc/how-to-restrict-users-to-specific-boards-in-azure-devops"},"nextItem":{"title":"Allow Azure DevOps Microsoft Hosted Agent to communicate with Azure KeyVault","permalink":"/2021/03/26/allow-azure-devops-microsoft-hosted-agent-to-communicate-with-azure-keyvault"}},"content":"## Issue Description\\n\\nIntermittent issues with Azure WebApp constantly stop functioning, a Stop/Start operation brings it back online.\\n\\n## Root Cause\\n\\nFurther investigation using Azure Application Insights, reveals the Azure WebApp was experiencing a few FailedRequestCount, with HTTP 500 Errors. An exception was thrown by a TaskScheduler. Exception of type \'System.OutOfMemoryException\' was thrown.\\n\\n## Resolution\\n\\nIn my case, the service that was running on the Azure WebApp was using .NET Core 2.0, the fix was to upgrade to the latest version.\\n\\n.NET Core 2.0 is an unsupported version and we highly recommend upgrading to the latest version (3.1). Please take a look at this information of the .NET Core official support policy: [https://dotnet.microsoft.com/platform/support/policy/dotnet-core](https://dotnet.microsoft.com/platform/support/policy/dotnet-core \\"https://dotnet.microsoft.com/platform/support/policy/dotnet-core\\")\\n\\nFor .NET Core applications I suggest enabling the stdout logs, as those will capture some important errors: [https://learn.microsoft.com/en-us/aspnet/core/test/troubleshoot-azure-iis?view=aspnetcore-2.2#aspnet-core-module-stdout-log-azure-app-service-1](https://learn.microsoft.com/en-us/aspnet/core/test/troubleshoot-azure-iis?view=aspnetcore-2.2#aspnet-core-module-stdout-log-azure-app-service-1 \\"https://learn.microsoft.com/en-us/aspnet/core/test/troubleshoot-azure-iis?view=aspnetcore-2.2&WT.mc_id=AZ-MVP-5004796#aspnet-core-module-stdout-log-azure-app-service-1\\")\\n\\nIf those OutOfMemory exceptions come with a 5xx status code, I would suggest as well using the AutoHeal feature as it will allow setting rules based on that status code to capture a Memory Dump, you can check more information here: [https://azure.github.io/AppService/2018/09/10/Announcing-the-New-Auto-Healing-Experience-in-App-Service-Diagnostics.html](https://learn.microsoft.com/en-us/aspnet/core/test/troubleshoot-azure-iis?view=aspnetcore-2.2#aspnet-core-module-stdout-log-azure-app-service-1 \\"https://learn.microsoft.com/en-us/aspnet/core/test/troubleshoot-azure-iis?view=aspnetcore-2.2&WT.mc_id=AZ-MVP-5004796#aspnet-core-module-stdout-log-azure-app-service-1\\")"},{"id":"/2021/03/26/allow-azure-devops-microsoft-hosted-agent-to-communicate-with-azure-keyvault","metadata":{"permalink":"/2021/03/26/allow-azure-devops-microsoft-hosted-agent-to-communicate-with-azure-keyvault","source":"@site/blog/2021-03-26-allow-azure-devops-microsoft-hosted-agent-to-communicate-with-azure-keyvault.md","title":"Allow Azure DevOps Microsoft Hosted Agent to communicate with Azure KeyVault","description":"It is best practice to lock down Azure resources to be accessible by location and services that is only to what\'s required and, the Azure Key vault is no exception.","date":"2021-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":2.11,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-03-26 00:00:00 +1300","title":"Allow Azure DevOps Microsoft Hosted Agent to communicate with Azure KeyVault","authors":["Luke"],"tags":["Azure"],"toc":false},"unlisted":false,"prevItem":{"title":"Azure WebApp 500 Errors reporting from AspNetCoreModule","permalink":"/azure/azure-webapp-500-errors-reporting-from-aspnetcoremodule"},"nextItem":{"title":"Failed to delete the private endpoint. Error: Call to Microsoft.Storage/storageAccounts failed","permalink":"/2021/03/26/failed-to-delete-the-private-endpoint.error-call-to-microsoft.storage-storageaccounts-failed"}},"content":"It is best practice to lock down Azure resources to be accessible by location and services that is only to what\'s required and, the Azure Key vault is no exception.\\n\\nWhen using Microsoft Hosted Agents in Azure DevOps, you need to make sure that the AzureCloud IPs for the Azure DevOps regions are opened on the Firewall.\\n\\nIn my case, I was in the: AustraliaEast region and needed to identify and add the following \'AzureCloud\' Address Ranges to the KeyVault firewall:\\n\\n* \\"name\\": \\"**_AzureCloud.australiaeast_**\\",\\n* \\"id\\": \\"AzureCloud.australiaeast\\",\\n* \\"properties\\": {\\n* \\"changeNumber\\": 13,\\n* \\"region\\": \\"australiaeast\\",\\n* \\"regionId\\": 3,\\n* \\"platform\\": \\"Azure\\",\\n* \\"systemService\\": \\"\\",\\n* \\"addressPrefixes\\": \\\\[\\n* \\"13.70.64.0/18\\",\\n* \\"13.72.224.0/19\\",\\n* \\"13.73.192.0/20\\",\\n* \\"13.75.128.0/17\\",\\n* \\"13.104.211.128/26\\",\\n* \\"13.105.16.192/26\\",\\n* \\"13.105.20.128/26\\",\\n* \\"13.105.52.192/26\\",\\n* \\"13.105.53.128/26\\",\\n* \\"20.37.192.0/19\\",\\n* \\"20.38.112.0/23\\",\\n* \\"20.40.64.0/20\\",\\n* \\"20.40.80.0/21\\",\\n* \\"20.40.120.0/21\\",\\n* \\"20.40.176.0/20\\",\\n* \\"20.42.192.0/19\\",\\n* \\"20.43.96.0/20\\",\\n* \\"20.47.37.0/24\\",\\n* \\"20.47.122.0/23\\",\\n* \\"20.53.32.0/28\\",\\n* \\"20.53.40.0/21\\",\\n* \\"20.53.64.0/18\\",\\n* \\"20.53.128.0/17\\",\\n* \\"20.58.128.0/18\\",\\n* \\"20.60.72.0/22\\",\\n* \\"20.60.182.0/23\\",\\n* \\"20.70.128.0/17\\",\\n* \\"20.135.120.0/21\\",\\n* \\"20.150.66.0/24\\",\\n* \\"20.150.92.0/24\\",\\n* \\"20.150.117.0/24\\",\\n* \\"20.157.44.0/24\\",\\n* \\"20.188.128.0/17\\",\\n* \\"20.190.142.0/25\\",\\n* \\"20.190.167.0/24\\",\\n* \\"20.191.192.0/18\\",\\n* \\"20.193.0.0/18\\",\\n* \\"20.193.64.0/19\\",\\n* \\"23.101.208.0/20\\",\\n* \\"40.79.160.0/20\\",\\n* \\"40.79.211.0/24\\",\\n* \\"40.82.32.0/22\\",\\n* \\"40.82.192.0/19\\",\\n* \\"40.87.208.0/22\\",\\n* \\"40.90.18.0/28\\",\\n* \\"40.90.30.0/25\\",\\n* \\"40.90.130.80/28\\",\\n* \\"40.90.130.208/28\\",\\n* \\"40.90.140.32/27\\",\\n* \\"40.90.142.160/27\\",\\n* \\"40.90.147.64/27\\",\\n* \\"40.90.150.0/27\\",\\n* \\"40.112.37.128/26\\",\\n* \\"40.126.14.0/25\\",\\n* \\"40.126.39.0/24\\",\\n* \\"40.126.224.0/19\\",\\n* \\"52.108.40.0/23\\",\\n* \\"52.108.83.0/24\\",\\n* \\"52.109.112.0/22\\",\\n* \\"52.111.224.0/24\\",\\n* \\"52.113.88.0/22\\",\\n* \\"52.113.103.0/24\\",\\n* \\"52.114.16.0/22\\",\\n* \\"52.114.58.0/23\\",\\n* \\"52.114.192.0/23\\",\\n* \\"52.115.98.0/24\\",\\n* \\"52.120.158.0/23\\",\\n* \\"52.121.108.0/22\\",\\n* \\"52.143.199.0/24\\",\\n* \\"52.143.200.0/23\\",\\n* \\"52.147.0.0/19\\",\\n* \\"52.156.160.0/19\\",\\n* \\"52.187.192.0/18\\",\\n* \\"52.232.136.0/21\\",\\n* \\"52.232.154.0/24\\",\\n* \\"52.237.192.0/18\\",\\n* \\"52.239.130.0/23\\",\\n* \\"52.239.226.0/24\\",\\n* \\"52.245.16.0/22\\",\\n* \\"104.44.90.64/26\\",\\n* \\"104.44.93.96/27\\",\\n* \\"104.44.95.48/28\\",\\n* \\"104.46.29.0/24\\",\\n* \\"104.46.30.0/23\\",\\n* \\"104.209.80.0/20\\",\\n* \\"104.210.64.0/18\\",\\n* \\"191.238.66.0/23\\",\\n* \\"191.239.64.0/19\\",\\n* \\"2603:1010::/46\\",\\n* \\"2603:1010:5::/48\\",\\n* \\"2603:1010:6::/48\\",\\n* \\"2603:1016:1400:60::/59\\",\\n* \\"2603:1016:2402::/48\\",\\n* \\"2603:1016:2500:c::/64\\",\\n* \\"2603:1017:0:60::/59\\"\\n\\nYou only need to add the IP ranges of the Region that your Azure DevOps instance sits in.\\n\\nYou can find the region that your Azure DevOps instance sits in by following the article below:\\n\\n* [Microsoft-hosted agents for Azure Pipelines - Azure Pipelines](https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml&WT.mc_id=AZ-MVP-5004796#networking)\\n\\nYou can retrieve the list of Azure IP Ranges and Service Tags from the following Microsoft JSON file:\\n\\n* [Download Azure IP Ranges and Service Tags \u2013 Public Cloud from Official Microsoft Download Center](https://www.microsoft.com/en-us/download/details.aspx?id=56519)\\n\\n_Note: These IP ranges can change and update, depending on Microsoft removing and adding new datacenter capability, it is always worth rechecking the list if you find you start having problems with intermittent connectivity to check whether new ranges have been added that haven\'t been whitelisted._"},{"id":"/2021/03/26/failed-to-delete-the-private-endpoint.error-call-to-microsoft.storage-storageaccounts-failed","metadata":{"permalink":"/2021/03/26/failed-to-delete-the-private-endpoint.error-call-to-microsoft.storage-storageaccounts-failed","source":"@site/blog/2021-03-26-failed-to-delete-the-private-endpoint.error-call-to-microsoft.storage-storageaccounts-failed.md","title":"Failed to delete the private endpoint. Error: Call to Microsoft.Storage/storageAccounts failed","description":"Issue Description","date":"2021-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":0.735,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-03-26 00:00:00 +1300","title":"Failed to delete the private endpoint. Error: Call to Microsoft.Storage/storageAccounts failed","authors":["Luke"],"tags":["Azure"],"toc":false},"unlisted":false,"prevItem":{"title":"Allow Azure DevOps Microsoft Hosted Agent to communicate with Azure KeyVault","permalink":"/2021/03/26/allow-azure-devops-microsoft-hosted-agent-to-communicate-with-azure-keyvault"},"nextItem":{"title":"The Cloud Frame of Mind","permalink":"/2021/03/26/the-cloud-frame-of-mind"}},"content":"## Issue Description\\n\\nFailed to delete the private endpoint. Error: Call to Microsoft.Storage/storageAccounts failed\\n\\n## Root Cause\\n\\nAzure Backup locks the storage account when you configure protection for any file share in the corresponding account. This provides protection against accidental deletion of a storage account with backed-up file shares.\\n\\n## Resolution\\n\\nIn my case, the Storage account I was attempting to remove the Private Endpoint from was an Azure File Sync storage account, that had Azure File Shares that were getting Backuped Up.\\n\\n* Found and removed the lock on the storage account\\n* Then successfully delete the private endpoint\\n\\n## More info\\n\\nGenerally, it is recommended that keep the lock taken on the storage account by Azure Backup. If you delete the lock, your storage account will be prone to accidental deletion and if it\'s deleted, you\'ll lose your snapshots or backups.\\n\\n[https://learn.microsoft.com/en-us/azure/backup/backup-afs#best-practices](https://learn.microsoft.com/en-us/azure/backup/backup-afs?WT.mc_id=AZ-MVP-5004796#best-practices \\"https://learn.microsoft.com/en-us/azure/backup/backup-afs?WT.mc_id=AZ-MVP-5004796#best-practices\\")\\n\\n[https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources \\"https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json&WT.mc_id=AZ-MVP-5004796\\")"},{"id":"/2021/03/26/the-cloud-frame-of-mind","metadata":{"permalink":"/2021/03/26/the-cloud-frame-of-mind","source":"@site/blog/2021-03-26-the-cloud-frame-of-mind.md","title":"The Cloud Frame of Mind","description":"Note: Warning buzzwords to follow. Yes, it\'s that type of article. The views expressed are purely my own.","date":"2021-03-26T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":6.335,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"The Cloud Frame of Mind","authors":["Luke"],"tags":["Misc"],"date":"2021-03-26 00:00:00 +1300","toc":false,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Failed to delete the private endpoint. Error: Call to Microsoft.Storage/storageAccounts failed","permalink":"/2021/03/26/failed-to-delete-the-private-endpoint.error-call-to-microsoft.storage-storageaccounts-failed"},"nextItem":{"title":"Installing RSAT Tools with PowerShell","permalink":"/2021/03/25/installing-the-rsat-remote-server-administration-tools-for-windows-10-tools-using-powershell"}},"content":"> Note: Warning buzzwords to follow. Yes, it\'s that type of article. The views expressed are purely my own.\\n\\nWe are at a turning point in history, where technology is enabling us to do more with less and faster, human life expectancy is longer than what it has ever been, and the world has never been as connected - We are in the 4th industrial revolution.\\n\\nIn an effort to continue to make things better, stronger, and faster we have developed technological aids to assist to drive us forward, in areas such as (but not only) Health, space exploration, travel, and business transformation, one could almost call it the evolution of the first hammer or axe as a tool to help us survive and thrive.\\n\\nAlthough there are many technologies in play around the world, artificial intelligence, machine learning, virtual and augmented reality, biotechnology, robotics, and the internet of things to name a few \u2013 faster computer processing capability and datacenters all connected to each other and built into the fabric of connectivity across the backbone of the internet, is what I am here to talk about today.\\n\\nCompanies such as Microsoft, in just over 11 years have revolutionized the world of Information Technology. The Microsoft Azure ecosystem is not just \u2018_someone else computer\u2019_, it is so much more than that.\\n\\nTraditional data centers or on-premises equipment use to require specialized knowledge around areas such as Networking and Hardware, every business function needed to have a physical server, taking up space in a specialized air-conditioned room or just sitting under someone\u2019s desk, running critical functions needed by businesses. I am not saying, this is still not the case and that there are no excuses for such implementations (other than running under someone\u2019s desk), what I am saying is that \u2018Information Technology was about Information **Technology**\u2019 \u2013 the \u2018Information\u2019 portion of that was a bit harder to access than it is today and was a lot more hands-on to drive value.\\n\\nCloud is disrupting traditional IT faster than we think. Today, with 80% of businesses deploying or fully embracing the cloud, we have \u2018crossed the chasm\u2019 and are in the \u2018early majority stage of the adoption curve.\\n\\n> \u201cA ship is safe in harbor, but that\'s not what ships are for.\u201d - William Shedd\\n\\nUsing and treating the Microsoft Azure ecosystem, like a normal on-premises datacenter in a world where \\"_a kid working in a garage anywhere in the world, can put you out of business\\"_ will slowly but surely limit your potential, with the global scale of the Azure platform, its now possible for businesses, charities and similarly minded individuals to have a global and multi-regional presence.\\n\\nIn the world of digital transformation, technology has become the source of competitive differentiation \u2013 If you haven\u2019t realized that your company is a technology company, you have already lost.\\n\\nInformation Technology functional requirements have changed from thinking of performance in terms of the central processing unit (CPU), and Random-access memory (RAM), to thinking about user experience (useability), portability, and scalability.\\n\\n![Azure DevOps](/uploads/devops.png \\"Azure DevOps\\")\\n\\nCloud-based thinking is migrating workloads from IaaS (Infrastructure as a Service) to PaaS (Platform as a Service), or from PaaS to SaaS (Software as a Service), instead of you working for the technology, the technology works for you. \\n\\nThose previous Information Technology professionals that once worked till the early hours of the morning replacing hardware, keeping systems up and running, are now free to automate, simplify and understand how the technology can work for the consumer. They can now finally concentrate on helping you to deliver and concentrate on the **Information** that is now at your disposal.\\n\\n![Azure Built-in Controls](/uploads/azurebuiltincontrols.png)\\n\\nThe perimeter for security is no longer some black box, running in a dark room blinking into the night \u2013 it is your identity, your phone, your password (or password less). Security is everyone\u2019s concern and education of security and how to use technologies is just not the \u2018IT guys\u2019 responsibility.\\n\\nWhile the cloud can bring greater business value and agility, it can also bring new concerns, including cloud sprawl.\\n\\nWith the pace of change brought on by cloud-based digital technology, your business needs to be highly attuned to the capabilities, skills, and processes necessary from a people perspective to stay relevant and competitive.\\n\\nWhat can and cannot be achieved has now been limited by people\u2019s imaginations and the way that people work.\\n\\nWith companies now operating at a global stage and remote working, retaining talent has never become so important, the importance of a company mission statement, strategic priorities, and their \u2018Why\u2019 - to use the words of Simon Sinek _\u2018People don\'t buy what you do, they buy why you do it._\u2019 Has become a lot more important and visible as employees follow vision, leadership, and technology.\\n\\nLong story short so, what does this all mean?\\n\\nIn a few bullet points \u2013 this is some of what comes to mind when I think of having a Cloud frame of mind means:\\n\\n* Collaboration across Information Technology professionals (as the enablers) and Business needs have never been so important.\\n* Try, Try, Fail and Try again \u2013 Experiment!\\n* Think outside of the traditional box, into using technology across Cloud ecosystems such as Azure to drive outcomes.\\n* Partnerships with Microsoft and other businesses globally are important to learn, adapt and avoid reinventing the wheel.\\n* Shift from Captial to Operational expenses, subscription-based modelling and pay for what you use and consume.\\n* Enable, Trust and empower employees.\\n* Do not aim for perfection before moving forward or you will never get there.\\n* Use Analytics, Integration, and Machine Learning engines to help drive data-driven business decisions.\\n* Adopt a continuous learning culture.\\n* Embrace Chaos\\n* Remember that employee Utilisation does not equal maximum throughput.\\n* Build what you cannot buy. Buy what you can\'t live without\\n* Log what is useful, monitor what matters, alert on what\'s actionable.\\n* Empower others (ie shift left) while making sure that everything is auditable, standardized.\\n* Develop and promote an \u2018everything as code\u2019, \u2018everything is automated\u2019 mindset.\\n* Test and develop roadmaps to get the most out of upcoming Cloud capabilities.\\n* Educate employees on Security and the use of technology to get the most out of it.\\n* Remember that some of your clients\u2019 employees are not \u2018bogged\u2019 down by what they deem as not possible in IT.\\n* Take advantage of the variable cost model of the Cloud\\n* Design efficient use of resources via such activities as rightsizing (the process of resizing cloud resources to better match the workload requirements), allocating container costs, finding unused storage and compute, and identifying whether spending anomalies are expected.\\n* Automate what is trivial, boring, mundane, and belittling.\\n* The Cloud can offer cost savings with resiliency, high-availability, and security automation strategies, you just need to take advantage of it.\\n* Champion improvements in People/Process and ways of working\\n* Using the Cloud, does not always equal cost savings, however the real value is decreased time-to-deployment, increased agility to adapt to changes and opportunies for innovation, security and modernisation.\\n* Concentrate and continue on operational improvements, such as Incident and Problem Management\\n* Just as you have to have a push of changes, you have to have a pull of changes to keep the environment lean and avoid waste.\\n\\nAnd finally, Revisit, Improve and Reinvest... and enjoy the challenge and opportunities that being in the 4th industrial revolution has to offer! \\n\\nIt is surely an exciting time and is only just the beginning...\\n\\n![Cloud Word](/uploads/map.png \\"Word Cloud\\")"},{"id":"/2021/03/25/installing-the-rsat-remote-server-administration-tools-for-windows-10-tools-using-powershell","metadata":{"permalink":"/2021/03/25/installing-the-rsat-remote-server-administration-tools-for-windows-10-tools-using-powershell","source":"@site/blog/2021-03-25-installing-the-rsat-remote-server-administration-tools-for-windows-10-tools-using-powershell.md","title":"Installing RSAT Tools with PowerShell","description":"Installing the RSAT (Remote Server Administration Tools for Windows 10) tools using PowerShell. This is just a quick article, written purely as an easy reference.","date":"2021-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":1.355,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2021-03-25 00:00:00 +1300","title":"Installing RSAT Tools with PowerShell","authors":["Luke"],"tags":["Misc","PowerShell"],"toc":false,"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"The Cloud Frame of Mind","permalink":"/2021/03/26/the-cloud-frame-of-mind"},"nextItem":{"title":"Unable to start Windows Azure Guest Agent (it\'s in a disabled state)","permalink":"/2021/03/25/unable-to-start-windows-azure-guest-agent-it-s-in-a-disabled-state."}},"content":"Installing the RSAT (Remote Server Administration Tools for Windows 10) tools using PowerShell. This is just a quick article, written purely as an easy reference.\\n\\nIn the age of the cloud and work from anywhere, Windows 10 allows you easily, install the Remote Server Administration Tools using PowerShell, sometimes (like me) you need these tools not to actually use them - but for the PowerShell modules that come with them to work on scripts locally.\\n\\nNote: This needs to be run from an elevated PowerShell console (ie ran as Administrator). You can check this using the following:\\n\\n    $currentPrincipal = New-Object Security.Principal.WindowsPrincipal([Security.Principal.WindowsIdentity]::GetCurrent())\\n    $currentPrincipal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)\\n\\nIf it returns:\\n\\n* False - You are not in an elevated PowerShell window and will have to relaunch as Administrator\\n* True - You are all set to go and can continue...\\n\\nTo get a list of all the Remote Server Administration tools you can install run the following:\\n\\n    Get-WindowsCapability -Name RSAT.* -Online\\n\\nThe versions as of the time this article was written are:\\n\\n* Rsat.ActiveDirectory.DS-LDS.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.BitLocker.Recovery.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.CertificateServices.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.DHCP.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.Dns.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.FailoverCluster.Management.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.FileServices.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.GroupPolicy.Management.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.IPAM.Client.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.LLDP.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.NetworkController.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.NetworkLoadBalancing.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.RemoteAccess.Management.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.RemoteDesktop.Services.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.ServerManager.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.Shielded.VM.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.StorageMigrationService.Management.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.StorageReplica.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.SystemInsights.Management.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.VolumeActivation.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n* Rsat.WSUS.Tools\\\\~\\\\~\\\\~\\\\~0.0.1.0\\n\\nTo install ALL the RSAT Tools run the following:\\n\\n    Get-WindowsCapability -Name RSAT.* -Online | Add-WindowsCapability -Online\\n\\nTo only install ONLY the Active Directory Users & Computers Remote Administration tool run the following command:\\n\\n    Get-WindowsCapability -Name RSAT.ActiveDirectory* -Online | Add-WindowsCapability -Online\\n\\nTo only install ONLY the Group Policy Management Remote Administration tool run the following command:\\n\\n    Get-WindowsCapability -Name RSAT.GroupPolicy* -Online | Add-WindowsCapability -Online"},{"id":"/2021/03/25/unable-to-start-windows-azure-guest-agent-it-s-in-a-disabled-state.","metadata":{"permalink":"/2021/03/25/unable-to-start-windows-azure-guest-agent-it-s-in-a-disabled-state.","source":"@site/blog/2021-03-25-unable-to-start-windows-azure-guest-agent-it-s-in-a-disabled-state..md","title":"Unable to start Windows Azure Guest Agent (it\'s in a disabled state)","description":"Azure Backup Overview","date":"2021-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":1.93,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Unable to start Windows Azure Guest Agent (it\'s in a disabled state)","authors":["Luke"],"tags":["Azure"],"date":"2021-03-25 00:00:00 +1300","toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Installing RSAT Tools with PowerShell","permalink":"/2021/03/25/installing-the-rsat-remote-server-administration-tools-for-windows-10-tools-using-powershell"},"nextItem":{"title":"First look at Universal Automation Desktop","permalink":"/2020/02/29/First look at Universal Automation Desktop"}},"content":"![Azure Backup Overview](https://csharpcorner.azureedge.net/article/an-overview-of-azure-backup/Images/An%20Overview%20Of%20Azure%20Backup01.png)\\n\\n## Issue Description\\n\\nUnable to start Windows Azure Guest Agent (it\'s in a disabled state). When trying and set the service to auto the following error occurs \'The specified service has been marked for deletion.\'\\n\\nVM Agent is unable to communicate with the Azure Backup service.\\n\\n## Root Cause\\n\\nThis may occur if Windows Communication Framework (WCF) profiling is enabled. WCF profiling should only be enabled while debugging a WCF issue. It should not be left enabled while running a production workload.\\n\\n## Resolution #1\\n\\n1\\\\. Restart your workload, I would recommend to Stop (deallocate first) to make sure that the workload starts correctly on a new hypervisor, the Azure Backup agent starts and checks for agent updates during the boot process.\\n\\n## Resolution #2\\n\\nDisable WCF profiling:\\n\\n1\\\\. Launch an elevated CMD prompt.\\n2\\\\. Run the following commands to back up the existing: C:\\\\\\\\Windows\\\\\\\\Microsoft.NET\\\\\\\\Framework\\\\\\\\v4.0.30319\\\\\\\\Config\\\\\\\\machine.config file:\\n\\n       cd C:\\\\Windows\\\\Microsoft.NET\\\\Framework\\\\v4.0.30319\\\\Config\\n\\n       copy machine.config machine.config.bak\\n3\\\\. Run notepad machine.config to edit the file in Notepad.\\n\\nRemove this text, being careful not to also remove any additional text that may be on the same line:\\n\\n    <add name=\\"Microsoft.VisualStudio.Diagnostics.ServiceModelSink.Behavior\\" type=\\"Microsoft.VisualStudio.Diagnostics.ServiceModelSink.Behavior, Microsoft.VisualStudio.Diagnostics.ServiceModelSink, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a\\"/>\\n\\nAlso remove this text, being careful not to also remove any additional text that may be on the same line:\\n\\n    <commonBehaviors><endpointBehaviors><Microsoft.VisualStudio.Diagnostics.ServiceModelSink.Behavior/></endpointBehaviors><serviceBehaviors><Microsoft.VisualStudio.Diagnostics.ServiceModelSink.Behavior/></serviceBehaviors></commonBehaviors>\\n\\n4\\\\. Save and close the file.\\n5\\\\. Restart the guest agent services:\\n\\n    net stop Rdagent\\n    \\n    net stop WindowsAzureGuestAgent\\n    \\n    net stop WindowsAzureTelemetryService\\n    \\n    net start Rdagent\\n\\n6\\\\. In some cases the VM may need to be restarted for the WCF disablement to take effect.\\n\\n## Resolution #3\\n\\nFrom time to time the Azure backup agent may fail. Sometimes this will self-resolve but on the odd occasion, additional steps may be needed.\\n\\n1\\\\. Uninstall the agent via the Control Panel.\\n2\\\\. Open CMD as Admin.\\n3\\\\. Stop the following services:\\n\\n    net stop rdagent\\n    \\n    net stop WindowsAzureGuestAgent\\n    \\n    net stop WindowsAzureTelemetryService \\n\\n4\\\\. Delete all the services of the agent:\\n\\n    sc delete rdagent\\n    \\n    sc delete WindowsAzureGuestAgent\\n    \\n    sc delete WindowsAzureTelemetryService \\n   \\n5\\\\. Create a folder called OLD in \\"C:\\\\ WindowsAzure\\" and move the old version of the agent to it and the folders that say Packages.\\n6\\\\. Install the service again using the link: [https://go.microsoft.com/fwlink/?LinkID=394789&clcid=0x409](https://go.microsoft.com/fwlink/?LinkID=394789&clcid=0x409?WT.mc_id=AZ-MVP-5004796 \\"https://go.microsoft.com/fwlink/?LinkID=394789&clcid=0x409\\") or the latest agent available.  \\n7\\\\. Restart the server.\\n\\n## Resolution #4\\n\\n1\\\\. Migrate the Pagefile to a new disk\\n2\\\\. Set a limit on the pagefile"},{"id":"/2020/02/29/First look at Universal Automation Desktop","metadata":{"permalink":"/2020/02/29/First look at Universal Automation Desktop","source":"@site/blog/2020-02-29-First look at Universal Automation Desktop.md","title":"First look at Universal Automation Desktop","description":"There are many ways to do automation scheduling \u2013 whether its Jenkins or even","date":"2020-02-29T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":6.92,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2020-02-29","title":"First look at Universal Automation Desktop","authors":["Luke"],"tags":["Azure","PowerShell"]},"unlisted":false,"prevItem":{"title":"Unable to start Windows Azure Guest Agent (it\'s in a disabled state)","permalink":"/2021/03/25/unable-to-start-windows-azure-guest-agent-it-s-in-a-disabled-state."},"nextItem":{"title":"Update your Azure Local Network Gateway IP with PowerShell","permalink":"/2019/09/08/Update your Azure Local Network Gateway IP with PowerShell"}},"content":"There are many ways to do automation scheduling \u2013 whether its Jenkins or even\\nWindows Task Scheduler. Each toolset has its place or specialization\\ntoday we are looking at the son of Ironman Software\u2019s PowerShell [Universal\\nAutomation](https://universalautomation.io/) platform \u2013 the Desktop Edition!\\n\\nThe Desktop edition replicates some of the same functionality as the Universal\\nAutomation platform, however, aimed more at being able to drive automation\\nscheduling from your Desktop! Entirely for someone like me who likes to do a\\ncertain amount of automation from the Desktop but has a particular distaste for\\nscheduled tasks \u2013 like the Universal Automation platform this is entirely driven\\nfor PowerShell!\\n\\n>   \u201cDesktop edition comes packaged as an Electron app that provides all the great\\n>   automation features of UA without role-based access, remote access or\\n>   authentication.\u201d\\n\\nYou can use Universal Automation Desktop for free as a trial:\\n\\n-   25 Jobs per day\\n\\n-   Up to 2 concurrent jobs\\n\\nWe are going to be using the Trial here \u2013 however, per-user pricing can be found\\nat the following link:  [Universal Automation](https://ironmansoftware.com/universal-automation/)\\n\\nThe toolset is quite intuitive a lot of below isn\u2019t worth going into how do use\\nit - as it seems to be easy to pick up but its always pleasant to have it documented\\nand referable! In my example below, we are going to\\ncreate a Resource Group in Azure.\\n\\n* TOC\\n{:toc}\\n\\n\\n## Install Universal Automation Desktop:\\n\\nUnlike Universal Automation & Universal Dashboard, installation of the Universal\\nAutomation Desktop is packaged into an executable.\\n\\n1.  Download the latest [Universal Automation\\n    Desktop](https://ironmansoftware.com/downloads/) installer (bottom of the\\n    download pages \u2013 at time of writing the installer is 120MB and version\\n    1.0.0)\\n2.  Installation of Universal Automation Desktop is pretty straight forward,\\n    just run the downloaded installer:\\n![UniversalAutomationInstall](/images/posts/Universal_Automation_Installer.png)\\n3.  Once complete, Universal Automation Desktop will load.\\n\\n## Use & Configure Universal Automation Desktop:\\n\\nAdd Scripts\\n-----------\\n\\nUniversal Automation supports git, so a Repository folder is created\\nautomatically \u2013 any scripts that you add will automatically be added to it:\\n\\n%LOCALAPPDATA%\\\\\\\\UniversalAutomation\\\\\\\\Repository\\n\\n1. On the Scripts pane select Add Scripts\\n![UniversalAutomationScriptsPane](/images/posts/Universal_Automation_ScriptsPane.png)\\n2. Select the script you want to upload \u2013 in my example; I am using\\n    \u2018New-AzureResourceGroup.ps1\u2019 the script I created for quickly testing some\\n    of the functionality.\\n>   Gist of script found below, but its also in my GitHub Repository under\\n>   Azure (GitHub link on the site menu).\\n3. Once added you should see the script appear and you should be able to see it\\n    in the Repository folder now:\\n![UniversalAutomationScriptsPanePopulated](/images/posts/Universal_Automation_ScriptsPanePopulated.png)\\n\\n## Add variables\\n\\nUniversal Automation Desktop supports variables.\\n\\n1. Click on the Variables menu item\\n2. Select Add Variable\\n3. In my example, I am adding the location that the Resource Group will be\\n    created, so it is going to be the following Key = Value pair: Location = Australia East\\n4. Click Ok to save\\n\\nNote: Location is a variable in my script, I also tested manually setting the\\nname of the Resource Group as well with the Name value as well and worked well.\\n\\n![UniversalAutomationVariablesPopulated](/images/posts/Universal_Automation_Variables.png)\\n\\nNote: The Variables are not encrypted! They are in plain text under:\\n\\nRepository\\\\\\\\.ua\\\\\\\\variables.ps1\\n\\nI did have a few issues with the UI freezing on me, so also discovered that I\\ncan manually add variables to this file and after a restart, it seemed to be\\npicked up by Universal Automation as well.\\n\\n![UniversalAutomationVariablesVSCode](/images/posts/Universal_Automation_VariablesVSCode.png)\\n\\n## Change PowerShell version\\n\\nThis is an interesting feature, that allows you to specify what Version of\\nPowerShell you can have the scripts run under (in this example I will be adding\\nPowerShell 7 preview). We do not need this for my example.\\n\\nNote: If you do not see the below, you may need to update \u2013 Automation Desktop\\nwill update automatically and should notify you \u2013 close and restart Automation\\nDesktop to continue (if you get an error message \u2013 navigate to your notification\\ntray by the time and Quit any open Universal Automation Desktop applications you\\nhave open and then relaunch).\\n\\n1. Click Settings\\n2. Navigate down to PowerShell versions\\n3. Click Add New Version\\n4. A new Table row will appear (Version\\\\\\\\Path)\\n5. In Version we are going to type in: PowerShell 7-preview (x64)\\n6. In path type in: C:\\\\\\\\Program Files\\\\\\\\PowerShell\\\\\\\\7-preview\\\\\\\\pwsh.exe\\n7. Press Enter\\n![UniversalAutomationPowerShellVersions](/images/posts/Universal_Automation_PowerShellVersions.png)\\n\\nNow when you run your scripts, you can now specify what Version of PowerShell to\\nuse!\\n\\n## Run the script\\n\\nNow that the variables have been set up and the script has been added, we can\\nthen Run it.\\n\\n1. On the scripts pane select \u2018New-AzureResourceGroup.ps1\u2019 and select Run\\n![Universal_Automation_ScriptsRun](/images/posts/Universal_Automation_ScriptsRun.png)\\n2. Specify the PowerShell version \u2013 I believe automating this selection is\\n    currently in the backlog: - and click Run\\n![Universal_Automation_ScriptsRunVersion](/images/posts/Universal_Automation_ScriptsRunVer.png)\\n3. The script will now go to the Jobs screen:\\n![Universal_Automation_ScriptsRunJob](/images/posts/Universal_Automation_ScriptsRunJob.png)\\n4. Usually, the script would just run \u2013 but in my case, I have a parameter in\\n    my PowerShell script to request the name of the Resource Group we are going\\n    to create, click on Response to Feedback icon\\n5. Type in the name of the Resource Group we are going to create \u2013 in my\\n    example I am going with: UAutomationRGTest and click Ok\\n![Universal_Automation_VariablesFeedback](/images/posts/Universal_Automation_VariablesFeedback.png)\\n6. It will now run the script:\\n![Universal_Automation_Script Runs](/images/posts/Universal_Automation_ScriptsRunJob2.png)\\n7. My new Resource Group has been created in Azure, using the name specified in\\n    the Parameter (UAutomationRGTest) and the Location (Australia East) that was\\n    set in the Variables!\\n\\n![Universal_Automation_Azure Resource Group created](/images/posts/Universal_Automation_AzureResourceGroupCreated.png)\\n\\n## Scheduling scripts\\n\\nAlthough at this stage, I am not scheduling any of my scripts to run \u2013 it is a\\ncore function of the toolset.\\n\\n1. Click on Scripts\\n2. Select the script you want to schedule and select View\\n3. On the right-hand side blade next to Edit, click on the ellipsis (i.e.\u2026)\\n  ![Universal_Automation_Schedule](/images/posts/Universal_Automation_Schedule1.png)\\n4. Select Schedule\\n  ![Universal_Automation_Schedule](/images/posts/Universal_Automation_Schedule2.png)\\n5. Specify the schedule you want and click Ok\\n\\nYou should now see the Schedule under Schedules and view the Job history under\\nJobs.\\n\\n## Overall opinion\\nLong story short - Universal Automation has a place and is a toolset I will be looking at more closely and using!\\n\\nI see myself using it to utilize PowerShell and automation a bit more in completion of general day to day activities (both personal and professional) and service requests - without having to worry about moving to the next step with a bigger toolset.\\n\\nIf I use a script often enough \u2013 then there will be a definite need to move to another team based toolset with RBAC tools such as the Universal Automation offering by Adam Driscoll of Ironman Software.\\n\\nWord of warning \u2013 and it should go without saying :\\n\\n**DO NOT RUN UNIVERSAL AUTOMATION DESKTOP ON YOUR DOMAIN COMPUTER FOR PRODUCTION\\nOR SHARED SCRIPTS! PLEASE LOOK AT UNIVERSAL AUTOMATION FOR THAT! YOU DON\u2019T WANT\\nTO GO HOME OR SHUTOFF YOUR PC OR LEAVE FOR BETTER AND BRIGHTER THINGS AND GET\\nCALLED UP BECAUSE SOME VERY IMPORTANT PROCESS DIDN\u2019T RUN!** \\n\\n## My Test Script - New-AzureResourceGroup\\n\\nI created this function to quickly test 2 things:\\n\\n-   How does Universal Automation work with 3rd party modules?\\n\\n-   How does Universal Automation work with parameters and variables?\\n\\nUniversal Automation Desktop does not touch your scripts, in fact depending on\\nwhat your use case is your git repository should be inline with Automation\\nDesktop and you can sync the Variables across multiple installs.\\n\\nMy script is using 2 modules:\\n\\n-   CredentialManager\\n\\n-   Azure (AZ)\\n\\nI thought CredentialManager would be a good test here as Universal Automation is\\nintended to be run from your Desktop (in my case Windows 10) and using\\nCredential Manager to store my Azure SPN details \u2013 without revealing it in plan\\ntext was a good test. More information can be found below:\\n\\n [ToastIT - Safe Credentials](https://toastit.dev/2018/07/10/safe-credentials/)\\n\\n```powershell title=\\"New-AzureResourceGroup.ps1\\"\\n\\n#requires -Version 2.0 -Modules Az.Accounts, Az.Resources, CredentialManager\\n\\n\\nfunction New-AzureResourceGroup\\n{\\n  <#\\n      .SYNOPSIS\\n      Creates Azure Resource Group\\n      .DESCRIPTION\\n      Creates Azure Resource Group function, created as a test function for Universal Automation Desktop\\n      .EXAMPLE\\n      New-AzureResourceGroup\\n  #>\\n  param\\n  ([Parameter(Mandatory = $true, HelpMessage = \'Enter the name of the Resource Group you want to create\', Position = 0)]\\n    [ValidateNotNullorEmpty()]\\n    [string] $Name,\\n    [Parameter(Position = 1)]\\n    [string]\\n    $Location = \'Australia East\'\\n\\n  )\\n     \\n  $tenantId = (Get-StoredCredential -Target \'MSDN SPN Demo\').GetNetworkCredential().UserName \\n  $pscredential = (Get-StoredCredential -Target \'MSDN SPN Demo Key\')\\n  \\n  Connect-AzAccount -ServicePrincipal -Credential $pscredential -Tenant $tenantId\\n  \\n  New-AzResourceGroup -Name $Name -Location $Location -Force\\n}\\n\\nNew-AzureResourceGroup\\n\\n```\\n\\n [Luke - GitHub ](https://github.com/lukemurraynz)"},{"id":"/2019/09/08/Update your Azure Local Network Gateway IP with PowerShell","metadata":{"permalink":"/2019/09/08/Update your Azure Local Network Gateway IP with PowerShell","source":"@site/blog/2019-09-08-Update your Azure Local Network Gateway IP with PowerShell.md","title":"Update your Azure Local Network Gateway IP with PowerShell","description":"One of the issues you face with setting up an Azure Site to Site VPN is making sure that your Azure Local Network Gateway always has your Public/On-premises IP.","date":"2019-09-08T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":2.215,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2019-09-08T00:00:00.000Z","title":"Update your Azure Local Network Gateway IP with PowerShell","authors":["Luke"],"tags":["Azure","PowerShell"],"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"First look at Universal Automation Desktop","permalink":"/2020/02/29/First look at Universal Automation Desktop"},"nextItem":{"title":"How to manage cost in Microsoft Azure","permalink":"/2018/12/19/How to manage cost in Microsoft Azure"}},"content":"One of the issues you face with setting up an Azure [Site to Site VPN](https://learn.microsoft.com/en-us/azure/vpn-gateway/tutorial-site-to-site-portal?WT.mc_id=AZ-MVP-5004796) is making sure that your Azure Local Network Gateway always has your Public/On-premises IP.\\n\\nThis setup is fine when used in environments that have Static IPs (and yes if setting this up for a Business or Production, it is highly recommended to have a static IP!).\\n\\nHowever, when used in environments like my home network or lab environments - which has a Dynamic IP that could change at any time it will cause connectivity issues if your IP changes and the Local Network Gateway is not updated.\\n\\nThe script below \u2013 intended to be run on as a Daily scheduled task, will find\\nyour Public IP and connect to Azure and if needed \u2013 will update the IP of your\\nLocal Network Gateway.\\n\\nPrerequisites:\\n\\n* [Az PowerShell Module](https://learn.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.5.0&WT.mc_id=AZ-MVP-5004796)\\n* [Azure Service Principal](https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal?WT.mc_id=AZ-MVP-5004796) (with Contributor rights to the Azure Local Network Gateway)\\n\\nOnce you have the Azure Service Principal and Az Module installed, you need to\\nedit the following variables to suit your environment:\\n\\n* $ResourceGroup = \'RESOURCE GROUP OF LOCAL NETWORK GATEWAY\'\\n* $LocalNetworkGateway = \u2018NAME OF AZURE LOCAL NETWORK GATEWAY\u2019\\n* $azureAplicationId =\u2019AZURE AD APPLICATION ID\u2019\\n* $azureTenantId= \u2018AZURE AD TENANCY/DIRECTORY ID\u2019\\n* $azureAPI = \u2018AZURE AD APPLICATION API/CLIENT SECRET\u2019\\n\\n```powershell title=\\"Update-LocalNetGatewayIP.ps1\\"\\n\\n#requires -Version 3.0 -Modules Az.Network\\n<#\\n      .SYNOPSIS\\n      Custom script to update your Azure Local Network Gateway with your Public IP\\n      .DESCRIPTION\\n      Updates the Azure Local Network Gateway with your Public IP\\n      Version: 1.0\\n      Author:  Luke Murray (Luke.Geek.NZ)\\n      If no Public IP parameter is set, it will automatically grab the Public IP of the computer running it and set it.\\n      The intention of this script is to run as as a scheduled task on your network, which connects to Azure and updates. Intended for Homelabs and scenarios which have Dynamic IPs.\\n\\n  #>\\n#---------------------------------------------------------[Initialisations]--------------------------------------------------------\\n  \\n$ErrorActionPreference = \'Stop\'\\n\\n[Object]$PublicIP = (Invoke-WebRequest -Uri \'http://ifconfig.me/ip\').Content \\n[string]$ResourceGroup = \'z_Network\'\\n[string]$LocalNetworkGateway = \'Prod-SiteToSite-VLAN-LNGateway\'\\n\\n# Use the application ID as the username, and the secret as password\\n$azureAplicationId =\\"Azure AD Application Id\\"\\n$azureTenantId= \\"Your Tenant Id\\"\\n$azureAPI = \\"Your API Key\\"\\n$azurePassword = ConvertTo-SecureString \\"$azureAPI\\" -AsPlainText -Force\\n$psCred = New-Object System.Management.Automation.PSCredential($azureAplicationId , $azurePassword)\\nConnect-AzAccount -Credential $psCred -TenantId $azureTenantId  -ServicePrincipal \\n\\n<#\\n\\nBefore adding the Azure Service Principal in and testing as a Scheduled Task, it is recommended that you just test the script first by connecting manually using:\\n\\nConnect-AzAccount\\n\\n #>\\n  \\n#-----------------------------------------------------------[Execution]------------------------------------------------------------  \\n\\n \\n  \\n$a = Get-AzLocalNetworkGateway -ResourceGroupName $ResourceGroup -Name $LocalNetworkGateway\\n$GatewayIP = $a.GatewayIpAddress\\n \\n\\nIf ($PublicIP -ne $GatewayIP) {\\n\\n    $a.GatewayIpAddress = $PublicIP\\n    Set-AzLocalNetworkGateway -LocalNetworkGateway $a\\n\\n}\\n  \\nElse {\\n\\n   $null\\n    \\n}\\n\\n```\\n\\n_Note: Script is also hosted on my Github repository. Feel free to clone/recommend improvements or fork._"},{"id":"/2018/12/19/How to manage cost in Microsoft Azure","metadata":{"permalink":"/2018/12/19/How to manage cost in Microsoft Azure","source":"@site/blog/2018-12-19-How to manage cost in Microsoft Azure.md","title":"How to manage cost in Microsoft Azure","description":"Microsoft has built up the Microsoft Azure ecosystem to offer scale and performance as and when needed; this gives customers the ability to not only remain competitive by lead and disrupt their industries without having to worry about on-premises datacentre capacity, redundancy, and manual processes, this, however, comes at a cost, where engineers may have in the past not cared or been responsible for cost \u2013 the Cloud has enabled us not only to consume valuable resources but also drive the cost up which has now shifted left to System Administrators or DevOps Engineers to keep things running and keeping cost low.","date":"2018-12-19T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"}],"readingTime":6.48,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2018-12-19T00:00:00.000Z","title":"How to manage cost in Microsoft Azure","authors":["Luke"],"tags":["Azure"],"toc":true,"header":{"teaser":"images/iazure-marketplace-banner.png"}},"unlisted":false,"prevItem":{"title":"Update your Azure Local Network Gateway IP with PowerShell","permalink":"/2019/09/08/Update your Azure Local Network Gateway IP with PowerShell"},"nextItem":{"title":"Using PowerShell to start the DFS Namespace service","permalink":"/2018/06/11/dfs-namespace-service-not-starting-reboot"}},"content":"Microsoft has built up the Microsoft Azure ecosystem to offer scale and performance as and when needed; this gives customers the ability to not only remain competitive by lead and disrupt their industries without having to worry about on-premises datacentre capacity, redundancy, and manual processes, this, however, comes at a cost, where engineers may have in the past not cared or been responsible for cost \u2013 the Cloud has enabled us not only to consume valuable resources but also drive the cost up which has now shifted left to System Administrators or DevOps Engineers to keep things running and keeping cost low.\\n\\nMicrosoft has built (and continuously improving) some great tools to help customers not only be aware of their cost but make data driven decisions; this blog post is intended to help you gain an understanding of various cost management functions in the Microsoft Azure environment to help you drive productivity up but keep costs low.\\n\\nMicrosoft Azure and other Cloud/Software as a Service product operate in the Opex (or Operational Expenditure) model, meaning that you are essentially paying for subscription-based services and resources on a Monthly or as consumed basis \u2013 versus the traditional Capex (or Capital Expenditure) model where you may pay for something up-front whether you use it or not and usually leads to waste in the capital due to oversizing machines or purchasing more than is needed, using Opex gives greater flexibility in terms of what you are currently (and not) consuming and what you want to consume, one thing to remember is that if something is scaled up, it can always be scaled down again with little (or none in some cases) downtime or risk.\\n\\n* TOC \\n{:toc}\\n\\n## Roles & Responsibilities & Azure Governance\\n\\nBefore I start through some of the Products that can help you on your managing cost journey \u2013 I need to cover off the People aspect first; what I mean by this are Roles & Responsibilities; you need to have some clear clarity in your organisations around who is responsible for what costs are not only created but anything running, whether you have a Product Owner approving and managing the financials or entire Agile Squads dedicated to keeping their resources lean and responsibly for any costs incurred you need to be transparent around Responsibility and Accountability of your costs to help manage (and pay for) the costs and avoid waste. Microsoft Azure has a \u2018[Billing Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/rbac-and-directory-admin-roles?WT.mc_id=AZ-MVP-5004796)\u2019 role which may be helpful to give someone visibility of costs in Microsoft Azure, also be aware of what roles users have, and what they can do with it, you don\u2019t want to give someone Contributor rights over a subscription and have them create a ton of resources when all they needed was Reader rights to view someone else work or skip internal processes for deploying resources.\\n\\nIf you haven\u2019t already \u2013 I highly recommend getting an [Azure Scaffold](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/resources/azure-scaffold?WT.mc_id=AZ-MVP-5004796) or Governance model in place to help determine how you not only use Azure, but how you manage the costs \u2013 whether it\u2019s per subscription-based or Resource Groups, the use of Tags to tag Azure resources based on cost center or department etc is invaluable.\\n\\n## Pricing Calculator\\n\\nThe first place to start in the journey of managing technical costs in Azure is the Pricing Calculator\\n\\n[https://azure.microsoft.com/en-us/pricing/calculator/](https://azure.microsoft.com/en-us/pricing/calculator/?WT.mc_id=AZ-MVP-5004796)\\n\\nThe Pricing Calculator allows you to select various Azure services (Virtual Machines, App Services Plans, IoT Hubs) and work out estimated pricing vs criteria which may be service-specific such as runtime, geo-redundancy, the amount of storage required etc. You need to know whether a service is costing money if it is running, how many read/writes is allowed, etc. I recommend spending some time looking through the Pricing Calculator and various services that Microsoft Azure offers to get a feel of cost \u2013 if you are in NZ (make sure to select \u2018New Zealand Dollar\u2019), the default is US.\\n\\n## Cost Visibility across Subscriptions & Resource Groups\\n\\nThe Microsoft Azure Portal offers a lot of visibility into the cost of running resources in Azure, whether you want to look at the costs per subscription or in this example the Resource Group.\\n\\nYou can find the cost information under the \u2018Resource Costs\u2019 blade in the Resource Group or \u2018Cost Analysis\u2019 at a Resource Group level.\\n\\n![Azure Resource Group Costs](/images/posts/appservice_resourcecosts.png)\\n\\nUsing the built-in Azure Portal tools you filter costs based on Time/Date and Tags.\\n\\nOn the Overview Blade of an Azure Subscription, you can see the highest costs by Resource type and estimated cost based on current consumption.\\n\\n![Azure Subscription Forecast](/images/posts/subscription_costs_forecast.png)\\n\\n## Invoices & Consumption Insights\\n\\nFor those of you who pay by Credit Card or want to know the Pre-tax costs of your Azure resources, you can go to your subscription and click on the Invoices blade to review current and previous invoices, if not set up already I recommend you configure the Email Invoice to send the invoice to your Billing Administrator automatically.\\n\\nIf you are on an Enterprise Agreement, you can setup [Microsoft Azure Consumption Insights](https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-connect-azure-consumption-insights?WT.mc_id=AZ-MVP-5004796) PowerBI pack to help give visibility of Azure costs through the use of the API key from your enrolment portal.\\n\\n## Azure Policies\\n\\nAlong with your Cloud Scaffold or Governance framework, you need to be able to force or guide your environment, [Azure Policies](https://learn.microsoft.com/en-us/azure/governance/policy/overview?WT.mc_id=AZ-MVP-5004796) can be used to not only keep your Azure ecosystem in alignment but stop the creation of unapproved resources, high costing Virtual Machines, or resources in specific regions, you can use Azure Policies to Audit resources or completely prevent them from being created. Azure Policy samples can be at the [azure-policy](https://github.com/Azure/azure-policy/tree/master/samples) git repository.\\n\\n## Hybrid Use Benefit\\n\\nIf you already own on-premises Windows Server/SQL server licenses \u2013 you may be eligible for [HUB (Hybrid Use Benefit)](https://azure.microsoft.com/en-us/pricing/hybrid-benefit/?WT.mc_id=AZ-MVP-5004796) which allows you to run certain machines, based on licensed cores in Azure under the same Windows Server licensing for no additional cost, this can be enabled in the Azure Portal on a Virtual Machine with no downtime.\\n\\n## Azure Advisor & Reserved Instances\\n\\nMicrosoft has built an\xa0Advisor into Azure, this Advisor not only gives Security & Performance recommendations, it also includes Cost Recommendations\\n\\n![Azure Advisor](/images/posts/azureadvisorbutton.png)\\n\\n![Azure Reserved Instance](/images/posts/azurerihub.png)\\n\\nThese Cost recommendations, are based on CPU usage of workloads (and can be adjusted) \u2013 such as workloads running at 5% CPU utilization for the past 14 days and recommends Cost Savings by resizing down the Virtual Machine.\\n\\nThe Azure Advisor also recommends [Reserved Instances](https://azure.microsoft.com/en-us/pricing/reserved-vm-instances/?WT.mc_id=AZ-MVP-5004796), instead of \u2018Pay As You Go\u2019 or \u2018Pay As You Consume\u2019 Opex model, you pay for the Virtual Machines up-front (think capital expenditure) for 1 or 3-year terms, in some cases, you can get 72% savings and if you have machines you know will be on 24/7 and won\u2019t need resizing \u2013 think Infrastructure servers, such as Domain Controllers then Reserved Instances are a good idea.\\n\\n## Last Considerations\\n\\nYou need to be aware that pricing in the Azure Portal or Pricing Calculator, may not show all costs. These price indications are only based on the service or size of a resource, they do not take into consideration Network dependencies such as data egress (data leaving Azure), storage replication, etc \u2013 the reality is, is sometimes the only way to know how much something will cost is to provision it, use it and monitor the costs.\\n\\nA side bit of information also is that although a service might cost x in AustraliaEast it may be a lot cheaper to run it in the US for example. Hence, if you need to spin something up quickly or test it and not concerned about latency or data sovereignty then check out [Azure Price](https://azureprice.net/?currency=NZD), this website lists the cost of the Virtual Machines, their costs, and recommended regions to get in some cases 30-40% cost savings."},{"id":"/2018/06/11/dfs-namespace-service-not-starting-reboot","metadata":{"permalink":"/2018/06/11/dfs-namespace-service-not-starting-reboot","source":"@site/blog/2018-06-11-dfs-namespace-service-not-starting-reboot.md","title":"Using PowerShell to start the DFS Namespace service","description":"Distributed File System (DFS) has some service dependencies - so if those don\'t start the DFS Namespace service will also not start.","date":"2018-06-11T00:00:00.000Z","tags":[{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":2.5,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using PowerShell to start the DFS Namespace service","tags":["PowerShell"],"date":"2018-06-11 00:00:00 +1300","header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"How to manage cost in Microsoft Azure","permalink":"/2018/12/19/How to manage cost in Microsoft Azure"},"nextItem":{"title":"Using PowerShell to connect to Microsoft Azure","permalink":"/2018/06/10/Using PowerShell to connect to Azure"}},"content":"Distributed File System (DFS) has some service dependencies - so if those don\'t start the DFS Namespace service will also not start.\\n\\n<img class=\\"alignnone\\" src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0835_DFSNamespac1.png?resize=584%2C112\\" alt=\\"DFS Namespace\\" width=\\"584\\" height=\\"112\\" data-recalc-dims=\\"1\\" />\\n\\nThe dependencies are:\\n\\n * Remote Registry\\n * Security Accounts Manager\\n * Server\\n * Workstation\\n\\nI have seen the Remote Registry service become the culprit of the DFS-N service not starting.\\n\\nIn my experience, this had been caused by antivirus software changing the Remote Registry service to Disabled start-up type so when the DFS-N server restarts, one of the dependency services:\\n\\nRemote Registry does not start so if you have issues with the DFS-N service not starting \u2013 check the Remote Registry Start-up type is configured to Automatic and click Start to confirm there are no errors and try starting the DFS-N service again.\\n\\n*Note: RemoteRegistry \u2013 although it is Automatic, will only Start when it is being used so don\'t be alarmed if it is in a \'Stopped\' state.*\\n  \\n<img class=\\"alignnone\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0835_DFSNamespac2.png?resize=377%2C267\\" alt=\\"Remote Registry\\" width=\\"377\\" height=\\"267\\" data-recalc-dims=\\"1\\" />\\n\\nI have also created a PowerShell script to do some general checking for the DFS namespace service \u2013 which sets the Remote Registry service to Automatic startup then gets the other DFS dependency services and changes the startup type to Automatic and starts them and finally tries to start the DFS Namespace service.\\n\\n```powershell title=\\"Start-DFS.ps1\\"\\n\\n#requires -Version 2.0\\n\\n<#\\n.SYNOPSIS\\n  Starts the DFS service\\n\\n.DESCRIPTION\\n  Changes the Remote Registry service to Automatic start-up and Start the DFS NameSpace service dependencies, then start the DFS namespace service. \\n  If the service does not start, it will retrieve the last 10 event log items from the DFS log.\\n\\n.NOTES\\n  Version:        1.0\\n  Author:         Luke Murray (Luke.Geek.NZ)\\n  Creation Date:  20/03/17\\n  Purpose/Change: \\n  20/03/17 - Initial script development\\n  11/06/18 - Updated script formatting\\n\\n.EXAMPLE\\n  ./Start-DFS-Service.ps1\\n  \\n#>\\n\\n#---------------------------------------------------------[Script Parameters]------------------------------------------------------\\n\\n$ServiceName = \'DFS\'\\n$ErrorActionPreference = \'Stop\'\\n\\n#-----------------------------------------------------------[Execution]------------------------------------------------------------\\n\\nTry \\n{\\n  Get-Service -Name RemoteRegistry | Set-Service -StartupType Automatic\\n}\\nCatch \\n{\\n  Write-Verbose -Message \'There is an issue changing the Remote Registry Service to Automatic Startup Type\' -Verbose\\n}\\nTry\\n{\\n  $ServiceDependency = Get-Service -Name $ServiceName -DependentServices\\n  $ServiceDependency | Set-Service -StartupType Automatic | Start-Service\\n  Write-Verbose -Message \\"$ServiceName dependencies have started. Will now try starting the $ServiceName service..\\" -Verbose\\n}\\ncatch [Microsoft.PowerShell.Commands.ServiceCommandException]\\n{\\n  [Management.Automation.ErrorRecord]$e = $_\\n\\n  $info = New-Object -TypeName PSObject -Property @{\\n    Exception = $e.Exception.Message\\n    Reason    = $e.CategoryInfo.Reason\\n    Target    = $e.CategoryInfo.TargetName\\n    Line      = $e.InvocationInfo.ScriptLineNumber\\n    Column    = $e.InvocationInfo.OffsetInLine\\n  }\\n  Write-Verbose -Message \'Opps! There was an error:\' -Verbose\\n  $info\\n}\\nCatch \\n{\\n  Write-Verbose -Message \\"There was an issue starting $ServiceName dependencies\\" -Verbose\\n}\\n\\ntry\\n{\\n  Try\\n  {\\n    Start-Service -Name $ServiceName\\n    Write-Verbose -Message \\"The $ServiceName service has started.\\" -Verbose\\n  }\\n  Catch \\n  {\\n    Get-WinEvent -LogName Microsoft-Windows-DFSN-Server/Operational | Select-Object -Last 10\\n  }\\n}\\n\\ncatch\\n{\\n  [Management.Automation.ErrorRecord]$e = $_\\n\\n  $info = New-Object -TypeName PSObject -Property @{\\n    Exception = $e.Exception.Message\\n    Reason    = $e.CategoryInfo.Reason\\n    Target    = $e.CategoryInfo.TargetName\\n    Line      = $e.InvocationInfo.ScriptLineNumber\\n    Column    = $e.InvocationInfo.OffsetInLine\\n  }\\n  Write-Verbose -Message \'Opps! There was an error:\' -Verbose\\n  $info\\n}\\n```\\n\\n*Note: Script is also hosted on my Github repository. Feel free to\\nclone/recommend improvements or fork.*"},{"id":"/2018/06/10/Using PowerShell to connect to Azure","metadata":{"permalink":"/2018/06/10/Using PowerShell to connect to Azure","source":"@site/blog/2018-06-10-Using PowerShell to connect to Azure.md","title":"Using PowerShell to connect to Microsoft Azure","description":"Microsoft Azure has a good user portal where you can do most things, however","date":"2018-06-10T00:00:00.000Z","tags":[{"inline":true,"label":"Azure","permalink":"/tags/azure"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":1.09,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"date":"2018-06-10","title":"Using PowerShell to connect to Microsoft Azure","authors":["Luke"],"tags":["Azure","PowerShell"],"header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Using PowerShell to start the DFS Namespace service","permalink":"/2018/06/11/dfs-namespace-service-not-starting-reboot"},"nextItem":{"title":"Stuck at a Black Screen and Connecting on a vCloud console","permalink":"/2017/01/01/stuck-black-screen-connecting-vcloud-console"}},"content":"Microsoft Azure has a good user portal where you can do most things, however\\nwhen it comes to automation, gathering a lot of information at once and more\\nin-depth scenarios that the Portal doesn\u2019t quite offer \u2013 PowerShell is used.\\n\\nBefore you can use PowerShell to connect to Microsoft Azure, you need to install\\nthe Azure Resource Manager modules first \u2013 follow the guide below:\\n\\n- [Install and configure Azure\\n    PowerShell](https://learn.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-7.5.0&viewFallbackFrom=azps-5.7.0&WT.mc_id=AZ-MVP-5004796)\\n\\n![Disable SMB1](/images/posts/InstallAzureRMModule.gif)\\n\\nOnce the Az modules has been installed \u2013 you can now connect to Azure.\\n\\nUsually you would have to go through the process of logging in to Azure, finding\\nwhat subscription you need to connect to and then selecting that manually,\\nhowever I have created a little function that will connect to Azure and\\nautomatically populate a list of the subscriptions that your account has access\\nto in a window which you can then select to connect to which makes the process\\neasier without having to remember different Azure subscription names or ids.\\nThis function can easily be used in any environment. I have it loaded as part of\\nmy PowerShell profile script so the function can be run from the second I open\\nup a new PowerShell prompt.\\n\\n*Note: Script is also hosted on my Github repository. Feel free to\\nclone/recommend improvements or fork.*"},{"id":"/2017/01/01/stuck-black-screen-connecting-vcloud-console","metadata":{"permalink":"/2017/01/01/stuck-black-screen-connecting-vcloud-console","source":"@site/blog/2017-01-01-stuck-black-screen-connecting-vcloud-console.md","title":"Stuck at a Black Screen and Connecting on a vCloud console","description":"On a computer running Windows 7 x64 SP1 Enterprise running Internet Explorer 11 browser, I had issues connecting to the console of Virtual Machines hosted by VMWare vCloud.","date":"2017-01-01T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.365,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Stuck at a Black Screen and Connecting on a vCloud console","image":"/wp-content/uploads/2016/12/123116_0852_StuckataBla1.jpg","tags":["Windows"],"date":"2017-01-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Using PowerShell to connect to Microsoft Azure","permalink":"/2018/06/10/Using PowerShell to connect to Azure"},"nextItem":{"title":"Capturing Windows boot performance with the Windows Performance Toolkit","permalink":"/win/capturing-windows-boot-performance-windows-performance-toolkit"}},"content":"On a computer running Windows 7 x64 SP1 Enterprise running Internet Explorer 11 browser, I had issues connecting to the console of Virtual Machines hosted by VMWare vCloud.\\n\\n<img class=\\"alignnone\\" src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/123116_0852_StuckataBla1.jpg?resize=636%2C572&#038;ssl=1\\" alt=\\"vCloud Console black screen\\" width=\\"636\\" height=\\"572\\" data-recalc-dims=\\"1\\" />\\n\\nThis was resolved by completing the following:\\n\\n  1. Adding the website to the Trusted Site list\\n  2. Adding the website to Internet Explorer&#8217;s compatibility list\\n  3. Updating Java on the workstation to the newest"},{"id":"win/capturing-windows-boot-performance-windows-performance-toolkit","metadata":{"permalink":"/win/capturing-windows-boot-performance-windows-performance-toolkit","source":"@site/blog/2016-12-28-capturing-windows-boot-performance-windows-performance-toolkit.md","title":"Capturing Windows boot performance with the Windows Performance Toolkit","description":"The Windows Performance Toolkit, developed by Microsoft has 3 separate tools and are key to solving a lot of boot and general performance issues:","date":"2016-12-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":2.84,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Capturing Windows boot performance with the Windows Performance Toolkit","image":"/wp-content/uploads/2016/12/122816_0751_Howtocaptur5.png","tags":["Windows"],"date":"2016-12-28 00:00:00 +1300","slug":"win/capturing-windows-boot-performance-windows-performance-toolkit"},"unlisted":false,"prevItem":{"title":"Stuck at a Black Screen and Connecting on a vCloud console","permalink":"/2017/01/01/stuck-black-screen-connecting-vcloud-console"},"nextItem":{"title":"Database error when making changes to DHCP reservations","permalink":"/2016/12/28/database-error-making-changes-dhcp-reservations"}},"content":"The Windows Performance Toolkit, developed by Microsoft has 3 separate tools and are key to solving a lot of boot and general performance issues:\\n\\n\u2022\xa0\xa0\xa0\xa0Windows Performance Recorder\\n\\n\u2022\xa0\xa0\xa0\xa0Windows Performance Analyzer\\n\\n\u2022\xa0\xa0\xa0\xa0Xperf\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Download_Windows_Performance_Toolkit\\"><span class=\\"toc_number toc_depth_1\\">1</span> Download Windows Performance Toolkit</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Run_Windows_Performance_Recorder\\"><span class=\\"toc_number toc_depth_1\\">2</span> Run Windows Performance Recorder</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Run_Windows_Performance_Analyzer\\"><span class=\\"toc_number toc_depth_1\\">3</span> Run Windows Performance Analyzer</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Useful_resources\\"><span class=\\"toc_number toc_depth_1\\">4</span> Useful resources</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n### <span id=\\"Download_Windows_Performance_Toolkit\\">Download Windows Performance Toolkit</span>\\n\\nThis can be downloaded by going to the Microsoft website and looking for the latest Windows Assessment and Deployment Toolkit for the operating system you want to analyze \u2013 for example: Windows 10\\n\\n<a href=\\"https://developer.microsoft.com/en-us/windows/hardware/windows-assessment-deployment-kit\\" target=\\"_blank\\">https://developer.microsoft.com/en-us/windows/hardware/windows-assessment-deployment-kit</a>\\n\\nDownload and run, we only need the Windows Performance Toolkit portion of the ADK:\\n\\n<img class=\\"alignnone\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/122816_0751_Howtocaptur1.png?resize=697%2C404&#038;ssl=1\\" alt=\\"Windows Performance Toolkit\\" width=\\"697\\" height=\\"404\\" data-recalc-dims=\\"1\\" />\\n\\nOnce installed navigate to: C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit\\n\\nTip: You can copy the Redistributables folder if you need to install the Windows Performance Toolkit only on another computer.\\n  \\n### <span id=\\"Run_Windows_Performance_Recorder\\">Run Windows Performance Recorder</span>\\n\\nWe only need too: WPRUI.EXE \u2013 Windows Performance Recorder & WPA.EXE \u2013 Windows Performance Analyzer.\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/122816_0751_Howtocaptur2.png?w=1500&#038;ssl=1\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nRun WPRUI to launch Windows Performance Recorder\\n\\nChange the Performance Scenario to Boot, and File\\n\\nSelect Resource Analysis and click CPU usage\\n\\n_Tip: You can add more: File I/O, Networking I/O, GPU usage etc if you know what in particular may be causing your boot slowness, the more you add the more data and complexity is added. I would recommend to only add additional resource analytics when required._\\n\\n<img class=\\"alignnone\\" src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/122816_0751_Howtocaptur3.png?resize=659%2C518&#038;ssl=1\\" alt=\\"Windows Performance Recorder\\" width=\\"659\\" height=\\"518\\" data-recalc-dims=\\"1\\" />\\n\\nClick Start to select where your boot traces will go and click Ok to start your boot traces, this will restart your computer 3 times.\\n\\n_Tip: If you need to login, please make sure you login quickly during each trace as the longer you leave it unattended the more data and delay it will collect._ \\n\\n### <span id=\\"Run_Windows_Performance_Analyzer\\">Run Windows Performance Analyzer</span>\\n\\nOnce the computer has been restarted 3 times and your traces have been complete navigate back to: C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit and click on WPR.EXE to open the Windows Performance Analyzer or click Open in WPA from the Windows Performance Recorder dialog.\\n\\nThe Windows Performance Analyzer will be our canvas in analyzing issues, you can expand areas like System Activity to dig into Processes, Services. Just drag the data onto the Analysis screen to go through it\\n\\n<img src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/122816_0751_Howtocaptur4.png?w=1500&#038;ssl=1\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nFrom here you can drill down into the data to find the cause or improvements for your login time, from here I can see one of the delays of my system is the CortanaUI.\\n\\nI am running this on a 16GB i7 4GHZ machine running on an SSD so it isn&#8217;t really a good example, but have used this in the past to work out that my login was slow because Lync had logging turned on.\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/122816_0751_Howtocaptur5.png?w=1500&#038;ssl=1\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nHopefully this gets you through the first steps in diagnosis and resolving your performance issues.\\n\\n### <span id=\\"Useful_resources\\">Useful resources</span>\\n\\nTroubleshooting Windows Internals when unexpected events happen &#8211; <a href=\\"https://channel9.msdn.com/events/Ignite/New-Zealand-2016/M405\\" target=\\"_blank\\">https://channel9.msdn.com/events/Ignite/New-Zealand-2016/M405</a>\\n\\nInvestigating website performance with Windows Performance Toolkit &#8211; <a href=\\"https://github.com/MicrosoftEdge/MicrosoftEdge-Documentation/tree/master/performance-analysis/windows-performance-toolkit\\" target=\\"_blank\\">https://github.com/MicrosoftEdge/MicrosoftEdge-Documentation/tree/master/performance-analysis/windows-performance-toolkit</a>\\n\\nSlow Boot Slow Logon (SBSL), A Tool Called XPerf and Links You Need To Read &#8211; <a href=\\"https://blogs.technet.microsoft.com/askpfeplat/2012/06/09/slow-boot-slow-logon-sbsl-a-tool-called-xperf-and-links-you-need-to-read/\\" target=\\"_blank\\">https://blogs.technet.microsoft.com/askpfeplat/2012/06/09/slow-boot-slow-logon-sbsl-a-tool-called-xperf-and-links-you-need-to-read/</a>"},{"id":"/2016/12/28/database-error-making-changes-dhcp-reservations","metadata":{"permalink":"/2016/12/28/database-error-making-changes-dhcp-reservations","source":"@site/blog/2016-12-28-database-error-making-changes-dhcp-reservations.md","title":"Database error when making changes to DHCP reservations","description":"\'An error occurred while accessing the DHCP database.\'","date":"2016-12-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.785,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Database error when making changes to DHCP reservations","tags":["Windows"],"date":"2016-12-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Capturing Windows boot performance with the Windows Performance Toolkit","permalink":"/win/capturing-windows-boot-performance-windows-performance-toolkit"},"nextItem":{"title":"Upgrade MDT 2013 to MDT Current Branch","permalink":"/2016/12/13/upgrade-mdt-2013-mdt-current-branch"}},"content":"\'An error occurred while accessing the DHCP database.\'\\n  \\n<img class=\\"alignnone\\" src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/122816_0625_Databaseerr1.png?resize=515%2C179&#038;ssl=1\\" alt=\\"an error occurred while accessing the DHCP database\\" width=\\"515\\" height=\\"179\\" data-recalc-dims=\\"1\\" />\\n\\nOne of the issues I have ran into since an upgrade to Windows Server 2012 R2 DHCP servers \u2013 was due to multiple processes access the DHCP database when I was attempting to Create or Modify a DHCP v4 reservation.\\n\\n  1. Adding an exclusion to DHCP.MDB file for Real Time scanning on my antivirus product.\\n  2. Disabling the Windows indexing service from indexing the DHCP folder.\\n\\nTo disable the Windows indexing service from indexing the DHCP server follow the quick steps below:\\n\\n  1. Open Windows Explorer \u2013 or Computer: on the DHCP server\\n  2. Navigate to: c:\\\\Windows\\\\System32\\\\DHCP\\n  3. Click on the General Tab up the top and click Advanced\\n  4. Uncheck &#8216;For Fast Searching, Allow Indexing Service to Index this Folder.&#8217;\\n  5. Click Ok\\n\\nUseful Resources:\\n\\nMicrosoft Anti-Virus Exclusion List &#8211; <a href=\\"https://social.technet.microsoft.com/wiki/contents/articles/953.microsoft-anti-virus-exclusion-list.aspx?WT.mc_id=AZ-MVP-5004796\\" target=\\"_blank\\">https://social.technet.microsoft.com/wiki/contents/articles/953.microsoft-anti-virus-exclusion-list.aspx</a>"},{"id":"/2016/12/13/upgrade-mdt-2013-mdt-current-branch","metadata":{"permalink":"/2016/12/13/upgrade-mdt-2013-mdt-current-branch","source":"@site/blog/2016-12-13-upgrade-mdt-2013-mdt-current-branch.md","title":"Upgrade MDT 2013 to MDT Current Branch","description":"Upgrading MDT (Microsoft Deployment Toolkit) is generally not an issue \u2013 the main points are:","date":"2016-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":4.265,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Upgrade MDT 2013 to MDT Current Branch","tags":["Windows"],"date":"2016-12-13 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Database error when making changes to DHCP reservations","permalink":"/2016/12/28/database-error-making-changes-dhcp-reservations"},"nextItem":{"title":"Microsoft Ignite NZ 2016! What a blast!","permalink":"/2016/10/29/microsoft-ignite-nz-2016-what-a-blast"}},"content":"Upgrading MDT _(Microsoft Deployment Toolkit)_ is generally not an issue \u2013 the main points are:\\n\\n  * Upgrade the Windows ADK before upgrading MDT.\\n  * Make sure you have a backup _(or can restore to a pre-upgraded MDT)_ of the Deployment Share \u2013 the Upgrade will upgrade the schema of the MDT database \u2013 including allowing new ADK features for your Deployment Share.\\n\\nNow that we have a backup it is now time to go through the Windows ADK _(Windows Assessment and Deployment Kit)_ upgrade on the MDT server and MDT current branch update. Follow the guide below to complete.\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Upgrade_Windows_ADK\\"><span class=\\"toc_number toc_depth_1\\">1</span> Upgrade Windows ADK</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Upgrade_MDT\\"><span class=\\"toc_number toc_depth_1\\">2</span> Upgrade MDT</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n## <span id=\\"Upgrade_Windows_ADK\\">Upgrade Windows ADK</span>\\n\\nI will be using Windows ADK for Windows 10, version 1607 in my example.\\n\\n  1. Download the latest Windows ADK \u2013 <a href=\\"https://developer.microsoft.com/en-us/windows/hardware/windows-assessment-deployment-kit\\" target=\\"_blank\\">https://developer.microsoft.com/en-us/windows/hardware/windows-assessment-deployment-kit</a> and save the setup file to your MDT server.\\n\\n<img class=\\"alignnone\\" src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT21.png?resize=624%2C333\\" alt=\\"Windows ADK\\" width=\\"624\\" height=\\"333\\" data-recalc-dims=\\"1\\" />\\n\\n  1. If you try to install the ADK without upgrading, you will get the following error:\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT22.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  1. So open Program and Features and select Windows Assessment and Deployment Kit \u2013 Windows 10 and select Uninstall to uninstall the old ADK (in this example I am uninstalling the v1511 Windows 10 ADK).\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT23.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  1. Select Yes to uninstall the Windows ADK and Close when the uninstall has been completed\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT24.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  1. Now that the old Windows ADK has been uninstalled you can now launch the new Windows ADK downloaded in Step 1 and make sure Install the Windows Assessment and Deployment Kit \u2013 Windows 10 to this computer is selected and the install path is correct and click Next\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT25.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n6. You can either select Yes or No to allow Microsoft to collect usage data \u2013 I am just going to select No and click Next to proceed with the install\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT26.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n7. Click Accept on the license agreement\\n\\n8. You will now get greeted by a dialog for installing the features of the Windows ADK \u2013 you need: Deployment Tools, Windows Preinstallation Environment (Windows PE) and User State Migration Tool (USMT) and select Install\\n\\n<img class=\\"alignnone\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT27.png?resize=690%2C508\\" alt=\\"Windows ADK\\" width=\\"690\\" height=\\"508\\" data-recalc-dims=\\"1\\" />\\n\\n9. Once the Windows Assessment and Deployment Kit installation has been completed, restart your MDT server (this is not required \u2013 but I prefer to do it to make sure any registered DLLs or registry changes have taken affect and it is in a clean state).\\n\\n## <span id=\\"Upgrade_MDT\\">Upgrade MDT</span>\\n\\n1. Now that the Windows ADK has been updated \u2013 it is time to download the Microsoft Deployment Toolkit &#8211; <a href=\\"https://www.microsoft.com/en-us/download/details.aspx?id=54259\\" target=\\"_blank\\">https://www.microsoft.com/en-us/download/details.aspx?id=54259</a> by selecting Download\\n\\n<img class=\\"alignnone\\" src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT28.png?resize=639%2C525\\" alt=\\"MDT Download\\" width=\\"639\\" height=\\"525\\" data-recalc-dims=\\"1\\" />\\n\\n2. We are upgrading the x64 version so select this and click Next _(same process for x32 \u2013 just download that instead)_.\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT29.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />12. Select Run to start the install\\n\\n<img src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT210.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n3. You will now have the Install Microsoft Deployment Toolkit Setup wizard \u2013 select Next to start the install\\n\\n<img class=\\"alignnone\\" src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT211.png?resize=508%2C396\\" alt=\\"MDT install\\" width=\\"508\\" height=\\"396\\" data-recalc-dims=\\"1\\" />\\n\\n4. Accept the License Agreement and select Next\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT212.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n5. Make sure that Microsoft Deployment Toolkit \u2013 Documents and Tools and Templates are select and the install path is correct (matches your current MDT install) and click Next\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT213.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n6. Select Yes or No to joining the Customer Experience Improvement Program and select Next\\n\\n7. Finally \u2013 click Install to start the MDT install\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT214.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n8. Once installed click Finish\\n\\n9. Open the Deployment Workbench\\n\\n<img src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT215.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n10. It should automatically have your Deployment Share listed under Deployment Shares \u2013 if you will need to click File, Add Deployment Share to add your deployment share. Right click your deployment share and select Upgrade Deployment Share\\n\\n<img class=\\"alignnone\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT216.png?resize=476%2C240\\" alt=\\"Upgrade Deployment Share\\" width=\\"476\\" height=\\"240\\" data-recalc-dims=\\"1\\" />\\n\\n11. Verify that the information is correct and click Next to start the Upgrade\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT217.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n12. This will start the upgrade of the Deployment Share\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT218.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n13. Once the Upgrade of the Deployment Share has been completed \u2013 it is time to upgrade the Boot Image\\n\\n14. Right click your Deployment Share again \u2013 and instead of Upgrade, there will be Update. Select Update Deployment Share\\n\\n<img src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT219.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n15. You will be greeted by the Update Deployment Share Wizard \u2013 select completely regenerate the boot images and click Next\\n\\n<img class=\\"alignnone\\" src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT220.png?resize=653%2C542\\" alt=\\"Update Boot Image\\" width=\\"653\\" height=\\"542\\" data-recalc-dims=\\"1\\" />\\n\\n16. You will then be forwarded to a review page, verify the permissions are correct and click Next to start regenerating the boot images.\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT221.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n17. Once completed you are finished. You have now hopefully successfully upgraded the ADK, MDT and Boot Images.\\n\\n<img src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/12/121316_0538_UpgradeMDT222.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />"},{"id":"/2016/10/29/microsoft-ignite-nz-2016-what-a-blast","metadata":{"permalink":"/2016/10/29/microsoft-ignite-nz-2016-what-a-blast","source":"@site/blog/2016-10-29-microsoft-ignite-nz-2016-what-a-blast.md","title":"Microsoft Ignite NZ 2016! What a blast!","description":"Microsoft Ignite NZ 2016! What a blast! #NinjaCat #MSIgniteNZ","date":"2016-10-29T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":18.955,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Microsoft Ignite NZ 2016! What a blast!","tags":["Misc"],"date":"2016-10-29 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Upgrade MDT 2013 to MDT Current Branch","permalink":"/2016/12/13/upgrade-mdt-2013-mdt-current-branch"},"nextItem":{"title":"Hyper-V 2012 R2 on an Intel NUC (NUC5i7RYH)","permalink":"/2016/03/06/hyper-v-2012-r2-intel-nuc-nuc5i7ryh"}},"content":"Microsoft Ignite NZ 2016! What a blast! _#NinjaCat #MSIgniteNZ_\\n\\n[<img class=\\"alignnone\\" title=\\"image\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image_thumb.png?resize=714%2C37\\" alt=\\"Microsoft Ignite NZ 2016\\" width=\\"714\\" height=\\"37\\" data-recalc-dims=\\"1\\" />](https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image.png)\\n\\nI was lucky enough to attend Microsoft Ignite this year, so I thought I would do a high level blog post about my experience at this event.\\n\\n\x3c!--more--\x3e\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Prologue\\"><span class=\\"toc_number toc_depth_1\\">1</span> Prologue:</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Day_1_Tuesday_25th_October_2016\\"><span class=\\"toc_number toc_depth_1\\">2</span> Day 1 (Tuesday 25th October 2016):</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Day_2_Wednesday_26th_October_2016\\"><span class=\\"toc_number toc_depth_1\\">3</span> Day 2 (Wednesday 26th October 2016):</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Day_3_Thursday_27th_October_2016\\"><span class=\\"toc_number toc_depth_1\\">4</span> Day 3 (Thursday 27th October 2016):</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Day_4_Friday_28th_October_2016\\"><span class=\\"toc_number toc_depth_1\\">5</span> Day 4 (Friday 28th October 2016):</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n### <span id=\\"Prologue\\">Prologue:</span>\\n\\nFor those that don\u2019t know \u2013 Microsoft Ignite NZ is an event hosted by Microsoft for IT Professionals \u2013 whether they are System Administrators or Developers \u2013 there is stuff for everyone! This event features sessions from key speakers, usually the top of their fields on their respective technologies covering the entire Microsoft stack and soft skill sessions such as dealing with stress etc. You can find out more \u2013 at the MS Ignite webpage &#8211; <a href=\\"https://msignite.nz/why-attend\\" target=\\"_blank\\">https://msignite.nz/why-attend</a>\\n\\nThis event was held at Sky City in Auckland, New Zealand between the 25<sup>th</sup> \u2013 28<sup>th</sup> of October 2016.\\n\\nThis was an event I had been eager to attend for a few years, so finally being privileged enough to attend and experience it all was well worth the wait!\\n\\n### <span id=\\"Day_1_Tuesday_25th_October_2016\\">Day 1 (Tuesday 25<sup>th</sup> October 2016):</span>\\n\\nDay 1 consisted of getting to Sky City early for the Exam Cram sessions hosted by Auldhouse _(and of course coffee!)_\\n\\nThe exam cram sessions were a high level overview of the Microsoft Azure platform for: **70-533 &#8211; Implementing Microsoft Infrastructure Solutions & 70-534 &#8211; Architecting Microsoft Azure Solutions**. This content was intended to turn the normal 5-day course offered by Auldhouse into 60 minute slots with the intention of giving an overview of features like ARM (Azure Resource Manager) and of course Azure WebApps and Virtual Machines.\\n\\nAs someone who supports the VMWare virtualization stack \u2013 I don\u2019t have the privilege of working with the Microsoft Azure stack on a BAU basis however having stood up Azure Virtual Machines and WebApps in the past it was useful in seeing other aspects of the Azure Cloud and the advancements that Microsoft have made to their Cloud solution and how they are disrupting the market and pushing technology to its potential. It is definitely an exciting time!\\n\\nThese sessions were well worth attending and I gained huge value attending \u2013 thanks to Auldhouse for putting them on.\\n\\nAfter the Exam Cram sessions \u2013 was **the Keynote**!\\n\\nThe keynote was the opening salvo for the rest of the conference! It certainly set the pace and the right attitude!\\n\\n> _\u201cWe are living in one of the most extraordinary times in the history of our planet. Where transformation is at the very heart of the matter. We must transform or we will perish. Our Flame will die! Transform the way we do things, the way we do business, the way we live, the way we create, innovate and inspire&#8221; \u2013 Hilary Barry_\\n> \\n> _\u201cOur industry does not respect tradition \u2014 it only respects innovation\u201d \u2013 Hilary Barry/Satya Nadella_\\n> \\n> _\u201cThe art of the possible\u201d \u2013 Hilary Barry_\\n\\nThis keynote featured a few awesome demos \u2013 that included but not limited to \u2013 The Microsoft Surface Hub \u2013 an awesome touch capable \u2018whiteboard\u2019 device running a variant of the Windows 10 operating system that leans towards team collaboration and sharing. A presentation by Donovan Brown on DevOps! As a developer he demonstrated Visual Studio and Visual Studio Team Services using the Build and Deploy release management! He was able to add changes to a mobile application _(OS independent)_ _\u2018People Tracker\u2019_ during the demo and build the new version live \u2013 this was awesome and from an IT Professional perspective this was great insight into the DevOps pipeline. This was a great lead up to Regan Murphy from Microsoft New Zealand, displaying the Azure DevTest Lab \u2013 using the power of the Azure Cloud to stand up development/test environments in a standard and quick way and they now have automatic shutdown and start up at designated time \u2013 saving businesses a lot of money and allowing the environments to be accessible when required. Dona Sarkar the Windows Insider Program Lead from Microsoft Redmond also had a huge section of the Keynote! Her speech was more soft skills then technical but was definitely worth listening too! Dona was very inspiring with \u201c_hustle the humans_\u201d and using words like Experiment and not Project to try out new ideas and add value!\\n\\n> _\u201cWe are all creative!\u201d \u2013 Dona Sarkar_\\n> \\n> _\u201cThe only real failure is not starting at all!\u201d \u2013 Dona Sarkar_\\n\\n<div style=\\"width: 740px\\" class=\\"wp-caption alignnone\\">\\n  <a href=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image-1.png\\"><img title=\\"image\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image_thumb-1.png?resize=730%2C233\\" alt=\\"Keynote\\" width=\\"730\\" height=\\"233\\" data-recalc-dims=\\"1\\" /></a>\\n  \\n  <p class=\\"wp-caption-text\\">\\n    MS Ignite NZ 2016 Keynote\\n  </p>\\n</div>\\n\\nAfter the keynote I began my technical sessions _(each technical session was 60 minutes)!_\\n\\nAs the primary subject matter expert in System Centre Configuration Manager (SCCM) at work \u2013 it was natural that my first session was:\\n\\n**30 things you should never do with System Center Configuration Manager!** This was a pretty light and hilarious but at the same time serious session detailing how not to do deployment \u2013 such deploy to All Systems and how dangerous the software is! It was also clarification and a push that I needed to take a fresh look at the role-based access control (RBAC) for the users and teams using the SCCM environment and re-evaluate what permissions they actual need to do their job to reduce configuration drift against the SCCM standards and to reduce the accidental collection deletion or untoward deployment that would be a resume ending event!\\n\\nAfter some afternoon tea \u2013 my next session was: **Reducing the network impact of Windows 10 feature and quality updates** &#8211; the speaker for this session was Michael Niehaus one of the top experts in Windows deployment and management. This session detailed \u2018_Windows as a service_\u2019 Microsoft push with Windows 10 management \u2013 in terms of Feature updates and their size, and getting into the rhythm of maintaining and keeping the Windows Desktop and image up to date. With rough Feature update sizes about 3.5GB every 6 months this needs awareness and some changes to SCCM cache size on the endpoint devices \u2013 such as changing this to 10GB and using technologies such as Branch Cache Distributed Hosts or Configuration Manager Peer Cache to distribute the updates and allow them to be distributed from endpoint devices to other endpoint devices on the same subnet reducing the hit to the distribution points \u2013 which works well with the fluid nature of the SCCM client check-in _(one client will check in and store the update to the local cache, allowing the next client on the subnet that checks in to retrieve the content from that endpoint device without having to connect back to the datacentre and hit the distribution points)_.\\n\\n### <span id=\\"Day_2_Wednesday_26th_October_2016\\">Day 2 (Wednesday 26<sup>th</sup> October 2016):</span>\\n\\nDay 2 \u2013 After the first day \u2013 I now knew where the rooms were for the sessions and was able to get myself into the rhythm. With my sessions and their locations added to my Calendar I was easily able to go from session to session without an issue \u2013 which was good timing considering this was the first full day of sessions!\\n\\nI started the day with a soft-skills session &#8211; **Stay in control when everything is out of control, and other brain hacks for techies** this was held in the Sky City Theatre which and was very fascinating! The session revealed a few tricks to do while stress *_breathe_* and ways of thinking such as being the devil\u2019s advocate for yourself as everyone is bias with their own opinions because of their personal and professional experiences, seeing how the other people may think differently and why they may thing differently. An interesting fact \u2013 because of the glucose levels \u2013 judges were much more likely to accept prisoners\u2019 requests for parole at the beginning of the day than the at end after they had a break or a fresh start!\\n\\nNext up was another soft session which was again quite interesting &#8211; **Microsoft&#8217;s trusted and secure cloud services** this session was led by Join Russell Craig, Microsoft NZ National Technology Officer and Michael Brick from Microsoft\u2019s NZ Legal counsel and their discussion revolved around Azure Cloud Security. This was interesting in the development and the security certifications the Microsoft Azure platforms gets audited upon a daily basis. It is aligned to just about all security standards including Cloud Security Alliance and the GCIO and that New Zealand Clinical data was authorized and was getting stored on the Azure datacentres in Australia with the New Zealand government having a \u2018_cloud first_\u2019 approach which includes the Azure stack where applicable. This potentially opens up the ability for Office 365 and other Microsoft services for the New Zealand Health Industry outside of just identity management.\\n\\n**From 0 to DevOps** was the next one I attended \u2013 this was an awesome session by Donovan Brown who used Visual Studio and Team Services in Azure to create a release pipeline from Visual Studio on his workstation device, uploaded to Team Services in Azure. He showed automated builds and how it could be used to build to Dev, UAT or Production at the same time using any language. He demonstrated using the Node JS language on an Ubuntu platform _(also running PowerShell \u2013 how cool is that)_ and then deployed to Team Services. The full stack that Donovan uses the following DevOps processes:\\n\\n**Plan** _(Plan Work)_ **->** **Infra** _(Configure/Provision Environment)_ **->** **Test** (_Manual/Automated Test) **-> Approve** (Notify/Collect Approvals) **-> Monitor** (Diagnostics/Performance)_\\n\\n<div style=\\"width: 760px\\" class=\\"wp-caption alignnone\\">\\n  <a href=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image-2.png\\"><img title=\\"image\\" src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image_thumb-2.png?resize=750%2C327\\" alt=\\"From 0 to DevOps\\" width=\\"750\\" height=\\"327\\" data-recalc-dims=\\"1\\" /></a>\\n  \\n  <p class=\\"wp-caption-text\\">\\n    \u201cDevOps is the union of people, process, and products to enable continuous delivery of value to our end users.\u201d \u2013 Donovan Brown\\n  </p>\\n</div>\\n\\nThis was an awesome session \u2013 especially from an Infrastructure Engineer point of view, this helped me gain insight into the Developer world and some of the processes that should be used in conjunction with PowerShell scripting, as DevOps becomes more supported and Operations and Development mingle. Source Control people!\\n\\nAfter some lunch supplied by Microsoft and Sky City it was time to head to my next session:\\n\\n**Customizing Windows 10: Image Creation Tips and Tricks** \u2013 another session by Michael Niehaus. This session was a lot of value in relation to doing image engineering with MDT 2013 Update 2. Having experience in MDT image creation for Windows 7 and Windows 8.1 \u2013 I gained a lot of value in understanding about best practices in terms of where to place the AppX package removal steps in the reference task sequence (just after the WinPE phase \u2013 before the OS starts) and how Windows 10 1607 will allow the Task Bar to be customized using the StartLayout export XML! About time! There was also a demo about adding in a language pack requires multiple downloads and merged into an MDT Application to add as part of the base WIM.\\n\\n<div style=\\"width: 339px\\" class=\\"wp-caption alignnone\\">\\n  <a href=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image-3.png\\"><img title=\\"image\\" src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image_thumb-3.png?resize=329%2C255\\" alt=\\"Customizing Windows 10: Image Creation Tips and Tricks\\" width=\\"329\\" height=\\"255\\" data-recalc-dims=\\"1\\" /></a>\\n  \\n  <p class=\\"wp-caption-text\\">\\n    Microsoft Ignite NZ 2016 &#8211; Windows 10: Image Creation Tips and Tricks!\\n  </p>\\n</div>\\n\\nThe next session I attended with was **The 9 Group Policy Principals that all IT Administrators should know!** This session was useful to reinforce best practice in terms of not using Block inheritance, Loop Back processing or of course to not use login scripts and wait for network! He also touched on plain text passwords in the group policy and of course the recent Microsoft patch (MS16-072) that resulted in some Group Policies not applying after Authenticated users had been removed The speaker was Alan Burchill who is a Group Policy MVP! So was right on track! Sometimes it\u2019s good to take this kind of session \u2013 that allows you to take a step back to look at what may be setup or in use in a \u201cfriends\u201d organization and what may and should be done to remedy it! Was a highly valuable session which was something that everyone running Active Directory would have got value out of it straight away, that doesn\u2019t cost money!\\n\\nLast official session of the day was: **Windows 10 Deployments: What I wish I knew before I started** by a New Zealand Microsoft consultant, Brendan Byrne. I got a lot of value from this session as it was down to earth, wasn\u2019t a sales pitch and revealed a few things to keep in mind going forward \u2013 such as not thinking of Windows 10 as Windows 7. Thinking about using features such as Credential Guard and moving to UE-V for user profile management.\\n\\nAfter that \u2013 and the Hub Happy hour \u2013 I attended mini session at the Citrix booth that had a demo and slides for using Citrix AppDNA for Application Migration and compatibility testing,\\n\\n### <span id=\\"Day_3_Thursday_27th_October_2016\\">Day 3 (Thursday 27<sup>th</sup> October 2016):</span>\\n\\nThis day was an awesome day \u2013 I did miss the Windows Device Smack down! session as I was using the session time to catch up with a few people! One of the best things about Microsoft Ignite NZ 2016 was being able to catch up with people \u2013 whether they are people you have worked with or vendors! It was definitely a social geek fest! I also missed Demystifying the Windows Firewall \u2013 Learn how to irritate attackers without crippling your network because I had to do some work but will definitely check it out later on the Channel9 website once the session has been uploaded _(link is below this post)_!\\n\\n**Configuring and Deploying Just Enough and Just-In-Time Administration** \u2013 ****Awesome session, and was very interesting! The speaker for this session was: Orin Thomas who is an MVP and awesome technical writer (own a few of his books)! This session details using Just in Time Administration \u2013 Privileged Access Management which is a technology by Microsoft that utilizes Microsoft Identity Manager, to have a one-way forest trust with a privileged forest. The idea basically is a user goes to the Microsoft Identity Manager webpages and requests access such as \u2018Domain Admin\u2019 once that access is approved \u2013 MIM elevates a user\u2019s system admin account for a designated time (for example 30 minutes) which then the user does the work and then MIM removes the elevated rights from the user after this time. The idea is no one is a member of an AD group unless they actually need it and then for a certain amount of time. This would work in well with Change Control, as part of the implementation steps you need to figure out how long the Change would be and add in a step to request the specified access for the time of that action that is then automatically removed. PAM supports any Active Directory group which is also handy \u2013 it doesn\u2019t have to be an Active Directory Built-in group! Just Enough Administration (JEA) is all about granting a virtual token/account to a user based on a specific role that will only allow them to run very specific PowerShell commands \u2013 such as the demo Orin used as the ability to restart DNS servers, add DNS A records but not able to restart any other server or touch them! Very cool technology that I can see becoming the standard in terms of Role Based Access Control and PowerShell! This session gave me some concepts to think over!\\n\\n**Righting the Right Rights &#8211; Active Directory & Domain Security, Administration & Maintenance** was my next session (which was luckily in the same room!) The speaker for this session was Jess Dodson from Australia! An MVP! This was a very fascinating and awesome session regarding Active Directory replication, Active Directory security which included Delegation \u2013 being more proactive in terms of AD event log monitoring and collecting and included maintenance and recommendations for Microsoft Entra ID and checking sync replication. This was another of those sessions that I can instantly get the most value from without having to spend money from \u2013 well worth a watch!! A Healthy Active Directory \u2013 keeps the Users away (I mean happy)!\\n\\nSecond to last session of the day was: **Everything you forgot to ask about Certificate Services** this was another session by Orin Thomas! Orin pretty much went back to basics on this session \u2013 what a RootCA is, Subordinates and explained CRLS. Certificate Services is usually something very important but often neglected so was great to attend this session which reminded me about how it all worked and gave me an idea that I had never considered before \u2013 Using System Center Operations Manager (SCOM) to monitor for expiring certificates! Definitely something that needs looking into!\\n\\nMy last session was then &#8211; **Enabling DevOps from an IT Pro perspective** this was a very interesting session using Team Services \u2013 this session had x2 speakers &#8211; Callum Lowe a Microsoft New Zealand &#8211; Premier Field Engineer (more from an Infrastructure/Operational background) and Daniel Larsen \u2013 another Field Engineer that was more from the Development side of the house. They simulated the build and deployment \u2013 Daniel gave his requirements to Callum who then used Team Services and Visual Studio to develop the necessary ARM templates to spin up the resources in Azure. This was a session that helped give some context to DevOps and how the people is a big part of the 3 ways.\\n\\nThursday night of course \u2013 was the MS Ignite New Zealand 2016 party! The food was awesome, with selections of Seafood, BBQ, Asian! Revera Cloud Services had custom made Beer made for the event which was awesome and the entertainment of the night was New Zealand band Shapeshifter! Which had me head banging and fist pumping! This was held at Shed 10 on the Auckland harbor \u2013 band was on the bottom floor and a few Xboxes, Table Tennis, VR gaming was at the top floor! Was #Epic! I hadn\u2019t had that much fun in a while!\\n\\n<div style=\\"width: 756px\\" class=\\"wp-caption alignnone\\">\\n  <a href=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image-4.png\\"><img title=\\"image\\" src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image_thumb-4.png?resize=746%2C190\\" alt=\\"Thursday Night - Ignite NZ Party\\" width=\\"746\\" height=\\"190\\" data-recalc-dims=\\"1\\" /></a>\\n  \\n  <p class=\\"wp-caption-text\\">\\n    Microsoft Ignite NZ 2016 Party!\\n  </p>\\n</div>\\n\\n### <span id=\\"Day_4_Friday_28th_October_2016\\">Day 4 (Friday 28<sup>th</sup> October 2016):</span>\\n\\nLast day! It seemed the week went really quickly and I had mixed feelings about heading back home or wanting to stay! The last day was only a half a day\\n\\nAfter grabbing a coffee \u2013 it was straight into **Deploy, configure, and remotely manage Nano Server,** hosted by Benjamin Armstrong a Principle Program Manager at Microsoft this was definitely a good session! Last time I had stood up a Nano Server it was in technical preview and the provisioning of said Nano Server has come a long way \u2013definite project for the weekend to stand up Nano Server 2016 \u2013 Hyper-V host on my Intel NUC. Nano Server will follow the same update \u2018servicing model\u2019 as Windows 10 with feature updates roughly every year. Benjamin explained that not all Roles & Features are supported in Nano server and what and why they removed parts of Windows that we have taken for granted and that has become bloated over the years such as MSI and the Plug & Play engine. This operating system \u2013 severely reduces attack surface, disk image space and reduces the need for restarts during Windows updates! This platform is definitely worth looking into and they have paid allot of attention to the \u2018Voice of the Customer\u2019 in what the pain points are.\\n\\n<div style=\\"width: 403px\\" class=\\"wp-caption alignnone\\">\\n  <a href=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image-5.png\\"><img title=\\"image\\" src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/10/image_thumb-5.png?resize=393%2C298\\" alt=\\"Deploy, configure, and remotely manage Nano Server, hosted by Benjamin Armstrong\\" width=\\"393\\" height=\\"298\\" data-recalc-dims=\\"1\\" /></a>\\n  \\n  <p class=\\"wp-caption-text\\">\\n    Deploying Nano Server with Nano Guy!\\n  </p>\\n</div>\\n\\nAfter Nano server it was &#8211; **Migrating to Windows 10, make sure you don\u2019t end up with Windows 7 again** another presentation by Brendan Byrne &#8211; another interesting session! This session was all about making sure to use or at least consider the Windows 10 features \u2013 so you don\u2019t end up with a Windows 7 version of Windows 10 and miss out on modern features and security advancements made on the Desktop platform. There was also examples of a few companies who had deployed Windows 10 in different ways \u2013 1 using in Place Upgrade and another allowing Self-Service upgrade from Windows 7 to Windows 10, which allowed users to upgrade to Windows 10 on demand \u2013 reducing the IT resourcing requirement and user impact which was an interesting concept. Brendan also touched on Application Testing putting your applications into certain baskets \u2013 as not everything can be tested.\\n\\n**Discover -> Rationalize -> Prioritize -> Test**\\n\\nHe also touched on Windows as a Service and getting into the rhythm and managing it on a day to day basis \u2013 setting up the CB, CBB and Windows Insider rings inside the organization and using tools like UE-V and Enterprise State Roaming!\\n\\n> _\u201cIf people aren\u2019t more productive with just the OS along, how successful is the project?\u201d &#8211; Brendan Byrne_\\n\\nThe last session of the event \u2013 was **30 terrible habits of Server and Cloud Administrators** by Orin Thomas. This was a useful session regarding how lazy we as system administrations have become and how dangerous it is, such as Windows Firewall being disabled, passwords stored in an excel spreadsheet, Service accounts that are set to never expired and part of Domain Admin!\\n\\nUseful session that again I can get a lot of value and actions out of straight away!\\n\\n> _\u201cBuilders often live in unfinished houses?\u201d \u2013 Orin Thomas_\\n> \\n> _\u201cHack the nerd is simpler and often more effective than hack the system\u201d \u2013 Orin Thomas_\\n\\nAfter some Lunch \u2013 the event was over and was time to head back home! My time at Microsoft Ignite NZ 2016 was fun, valuable, eventful and just all round awesome! I thank Microsoft and all the Speakers and Sponsors for this event! It is good to know that as an IT professional there are others having similar or same issues \u2013 the Pale Blue Dot does not look as small as it was a week ago!\\n\\n**Resources:**\\n\\nSessions & Slides: <a href=\\"https://channel9.msdn.com/Events/Ignite/2016\\" target=\\"_blank\\">https://channel9.msdn.com/Events/Ignite/2016</a>\\n\\nWhat is DevOps by Donovan Brown &#8211; <a href=\\"http://donovanbrown.com/post/2015/09/01/what-is-devops\\" target=\\"_blank\\">http://donovanbrown.com/post/2015/09/01/what-is-devops</a>\\n\\nAzure IaaS Operations Guidance &#8211; <a href=\\"https://blogs.technet.microsoft.com/tangent_thoughts/2015/10/06/azure-iaas-operations-guidance/\\" target=\\"_blank\\">https://blogs.technet.microsoft.com/tangent_thoughts/2015/10/06/azure-iaas-operations-guidance/</a>"},{"id":"/2016/03/06/hyper-v-2012-r2-intel-nuc-nuc5i7ryh","metadata":{"permalink":"/2016/03/06/hyper-v-2012-r2-intel-nuc-nuc5i7ryh","source":"@site/blog/2016-03-06-hyper-v-2012-r2-intel-nuc-nuc5i7ryh.md","title":"Hyper-V 2012 R2 on an Intel NUC (NUC5i7RYH)","description":"One of the problems with Hyper-V 2012 R2 on an Intel NUC &#8211; NUC5i7RYH was because of the &#8216;Client&#8217; network card chipset. Windows Server 2012 would not install the Ethernet Adapter for the Server operating system.","date":"2016-04-30T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":5.905,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Hyper-V 2012 R2 on an Intel NUC (NUC5i7RYH)","tags":["Windows"],"date":"2016-04-30 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Microsoft Ignite NZ 2016! What a blast!","permalink":"/2016/10/29/microsoft-ignite-nz-2016-what-a-blast"},"nextItem":{"title":"Using PowerShell to set up Automatic Login on Windows Servers","permalink":"/2016/04/30/using-powershell-setup-automatic-logon-windows-servers"}},"content":"One of the problems with Hyper-V 2012 R2 on an Intel NUC &#8211; NUC5i7RYH was because of the &#8216;Client&#8217; network card chipset. Windows Server 2012 would not install the Ethernet Adapter for the Server operating system.\\n\\nOnce I made the manual adjustments to the Network drivers to get this going, I then slipstreamed the drivers and Server 2012 R2 updates (as of Feb 2016) into a Hyper-V 2012 R2 ISO which can then be made bootable for future Hyper-V installations.\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Changing_the_Intel_Network_Drivers_Manually\\"><span class=\\"toc_number toc_depth_1\\">1</span> Changing the Intel Network Drivers Manually</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Downloading_Intel_Network_Drivers_8211_NUC5i7RYH_Compatible\\"><span class=\\"toc_number toc_depth_1\\">2</span> Downloading Intel Network Drivers &#8211; NUC5i7RYH Compatible</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Intel_NUC5i7RYH_Hyper-V_2012_R2_ISO\\"><span class=\\"toc_number toc_depth_1\\">3</span> Intel NUC5i7RYH Hyper-V 2012 R2 ISO</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n## <span id=\\"Changing_the_Intel_Network_Drivers_Manually\\">Changing the Intel Network Drivers Manually</span>\\n\\n7-Zip or WinRAR will be needed to\xa0extract the EXE to a folder location. In these examples, I am using 7-Zip.\\n\\n  * Download the Intel x64 Network Drivers here: <a href=\\"https://downloadcenter.intel.com/download/23071/Network-Adapter-Driver-for-Windows-8-1-\\" target=\\"_blank\\">https://downloadcenter.intel.com/download/23071/Network-Adapter-Driver-for-Windows-8-1-</a>\\n  \\n    _(20.7 Date: 2/11/2016)_\\n  * Once downloaded. Right click the &#8216;PROWinx64.exe&#8217; and select 7-Zip and Extract to &#8220;PROWinx64\\\\&#8221; this will extract the driver executable to a folder\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R1.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  * Once extracted navigate to: \\\\PROWinx64\\\\PRO1000\\\\Winx64\\\\NDIS64Find &#8216;e1d64x64.INF&#8217; and right click and select Open to open with Notepad\\n  * Find the Heading &#8216;[Intel.NTamd64.6.3.1]&#8217; and copy the devices in this section\\n\\n\x3c!--more--\x3e\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R2.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  * Navigate to the Heading &#8216;[Intel.NTamd64.6.3]&#8217; and paste underneath the Devices (for example \u2013 underneath: PCI\\\\VEN\\\\8086&DEV\\\\_15B7&SUBSYS\\\\_00011179 \u2013 the last line):\\n\\n&nbsp;\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R3.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  * So it looks like the below with the devices from the top copied onto the bottom. You may have some duplicate Device IDs on the bottom section, you can remove these if you want to. I left mine in there. Once confirmed, save the file: e1d64x64.INF.\\n\\n&nbsp;\\n\\n<img src=\\"https://i2.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R4.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  * Once completed, copy the: NDIS64 folder to a USB flash drive and plug it into one of the Intel NUCs Flash drive slots.\\n  * Open the command prompt and type the following commands to allow unsigned/driver testing drivers:\\n\\n<pre class=\\"lang:default decode:true\\">bcdedit /set LOADOPTIONS DISABLE_INTEGRITY_CHECKS\\nbcdedit /set TESTSIGNING ON\\nbcdedit /set nointegritychecks ON</pre>\\n\\n  * Once ran, restart the NUC.\\n  * Once restarted and logged in navigate back to the Command Prompt again and navigate to your USB Flash drive \u2013 time to install the drivers.\\n  * Type in the below and press Enter:\\n\\n<pre class=\\"lang:default decode:true \\">pnputil\xa0-i -a\xa0e1d64x64.inf</pre>\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R5.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\n  * Wait a minute while it installs the driver. Once you have the Windows Security Dialog &#8220;Windows can&#8217;t verify the publisher of this driver software&#8221; select &#8220;Install this software anyway&#8221;\\n  * Once installed. Restart your Hyper-V server the Network Adapter should now be installed.\\n  * You can then disable the unsigned/driver testing drivers if needed in the Command Prompt window:\\n\\n<pre class=\\"lang:default decode:true \\">bcdedit /set LOADOPTIONS ENABLE_INTEGRITY_CHECKS\\nbcdedit /set TESTSIGNING OFF\\nbcdedit /set nointegritychecks OFF</pre>\\n\\n  * Back at the Server Configuration window _(type sconfig in the Command Prompt if missing it)_ you can now select 8 to configure your Network settings, remember to specify the Computer Name an add it to a Domain if needed.\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R6.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nReferences: Thanks to an article written by Jay &#8220;<a href=\\"http://jayrbarrios.com/2014/11/19/intel-nuc-d54250wykh-installing-lan-driver-on-windows-hyper-v-server-2012-r2\\" target=\\"_blank\\">Intel NUC D54250WYKH: Installing LAN Driver on Windows Hyper-V Server 2012 R2</a>&#8221; for a lot of assistance in getting this going, was able to easily adapt it for the NUC5i7RYH.\\n\\n## <span id=\\"Downloading_Intel_Network_Drivers_8211_NUC5i7RYH_Compatible\\">Downloading Intel Network Drivers &#8211; NUC5i7RYH Compatible</span>\\n\\nI have bundled the tweaked driver packed I used into a ZIP file \u2013 You can download it here: <a href=\\"https://mega.nz/#!5ZtQUTqZ!iKWih9idK-jil6dT4Q7C_D16N5MQiCF9vr6qws6CmFc\\" target=\\"_blank\\">Intel_NUC_2012R2_NUC5i7RYH_Drivers.zip</a>\\n  \\n_(20.7 Date: 2/11/2016_). This will take you to Step 8 of the guide above. This driver pack should also be able to be used in a normal Windows Server 2012 R2 x64 environment.\\n\\n## <span id=\\"Intel_NUC5i7RYH_Hyper-V_2012_R2_ISO\\">Intel NUC5i7RYH Hyper-V 2012 R2 ISO</span>\\n\\nAs mentioned earlier. I have created an ISO _(which I then used to create my Hyper-V server on my NUC)_ which I have added the recommended _(excluding .Net)_ Windows Updates _(as of Feb 2016)_ and added the Intel NUC drivers too. This allowed me to easily rebuild my Hyper-V server on the Intel NUC NUC5i7RYH a few times during testing, minimizing the need to do the Network drivers manually and reduced the amount of Windows Updates I had to install.\\n\\nThe ISO I used as a source was Hyper-V Core RTM _(9600.16384.WINBLUE\\\\_RTM.130821-1623\\\\_X64FRE\\\\_SERVERHYPERCORE\\\\_EN-US-IRM\\\\_SHV\\\\_X64FRE\\\\_EN-US\\\\_DV5)_ downloaded directly from Microsoft.\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R7.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nI then used a WSUS server to download the 2012 R2 updates and slipstreamed them into the Retail ISO. Then I added the NUC Intel Drivers.\\n\\nOther than the above: 2012 R2 Updates, Intel Ethernet Drivers for the Intel NUC and enabling the .Net 3.5 feature this is untouched.\\n\\n<img src=\\"https://i0.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R8.png?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nThis can be downloaded here: <a href=\\"https://mega.nz/#!wclQwJKB!l6pM0XU5sv63mgSu7rYaPIHvQRYfXaMDvzBBJCVX9Gw\\" target=\\"_blank\\">SERVERHYPERVCORE_EN-US-NUC_NUCi57RYH_Feb2016.ISO</a>\\n\\n_README\\n  \\n_ \\n\\n_&#8212;&#8212;&#8212;&#8212;\\n  \\n_ \\n\\n_This Hyper-V Core DVD ISO was created by: Luke Murray (Luke.Geek.NZ) on the: 05/03/16\\n  \\n_ \\n\\n_The following adjustments have been made (no other adjustments have been made and should be RTM)\\n  \\n_ \\n\\n_This 2012 R2 server core ISO has the latest Windows Updates slipstreamed (except .net updates) to: Feb 2016.\\n  \\n_ \\n\\n_This 2012 server core also has the following Intel drivers for the Intel NUC system:\\n  \\n_ \\n\\n_Note: Please note Intel does not fully support Windows Server 2012 R2 on the NUC5i7RYH due to using a client Network chipset.\\n  \\n_ \\n\\n_The Network driver&#8217;s configurations had to be tweaked to allow installation.\\n  \\n_ \\n\\n_Intel(R) Network Drivers 20.7 (January 21, 2016)\\n\\n _Intel\xae 82599EB 10 Gigabit Ethernet Controller\\n \\n_Intel\xae 82580EB Gigabit Ethernet Controller\\n\\n_Intel\xae Ethernet Server Adapter I340-F4\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-SR1\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-DA2\\n\\n_Intel\xae Ethernet Controller I350-BT2\\n\\n_Intel\xae Ethernet Controller I350-AM2\\n\\n_Intel\xae Ethernet Controller I350-AM4\\n\\n_Intel\xae Ethernet Controller I210-IT\\n\\n_Intel\xae Ethernet Converged Network Adapter X540-T2\\n\\n_Intel\xae Ethernet Server Adapter I350-F2\\n\\n_Intel\xae Ethernet Server Adapter I350-F4\\n\\n_Intel\xae Ethernet Controller X540-AT2\\n\\n_Intel\xae Ethernet Connection I217-V\\n\\n_Intel\xae Ethernet Connection I218-V\\n\\n_Intel\xae Ethernet Connection I218-LM\\n\\n_Intel\xae Ethernet Controller I210-AT\\n\\n_Intel\xae Ethernet Controller I210-IS\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-DA4\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-QDA1\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-LR1\\n\\n_Intel\xae 82599ES 10 Gigabit Ethernet Controller\\n\\n_Intel\xae 82579LM Gigabit Ethernet PHY\\n\\n_Intel\xae Ethernet Server Adapter I340-T4\\n\\n_Intel\xae Ethernet Server Adapter I350-T2\\n\\n_Intel\xae Ethernet Server Adapter I350-T4\\n\\n_Intel\xae Ethernet Connection I217-LM\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-T2\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-SR2\\n\\n_Intel\xae 82579V Gigabit Ethernet PHY\\n\\n_Intel\xae Ethernet Converged Network Adapter X540-T1\\n\\n_Intel\xae Ethernet Controller I211-AT\\n\\n_Intel\xae 82599EN 10 Gigabit Ethernet Controller\\n\\n_Intel\xae Ethernet Converged Network Adapter X520-DA1\\n\\n_Intel\xae Ethernet Controller I210-AS\\n\\n_Intel\xae Ethernet Controller I210-CS\\n\\n\\n_Note: I take no liability for loss or damage following these guides or using my driver pack or ISO. This is merely intended to help people\\n  \\n_ \\n\\nI recommend using <a href=\\"https://rufus.akeo.ie/\\" target=\\"_blank\\">Rufus</a> to load the ISO to a USB flash drive using the settings below:\\n\\n<img src=\\"https://i1.wp.com/luke.geek.nz/wp-content/uploads/2016/03/030516_2351_HyperV2012R9.jpg?w=1500\\" alt=\\"\\" data-recalc-dims=\\"1\\" />\\n\\nOnce completed, put the USB drive into the NUC and restart. On startup press F10 to navigate to the Boot Menu and select the USB drive."},{"id":"/2016/04/30/using-powershell-setup-automatic-logon-windows-servers","metadata":{"permalink":"/2016/04/30/using-powershell-setup-automatic-logon-windows-servers","source":"@site/blog/2016-04-30-using-powershell-setup-automatic-logon-windows-servers.md","title":"Using PowerShell to set up Automatic Login on Windows Servers","description":"Some server based applications require to be logged into\xa0a service account to allow an Application or service to run, These applications usually require manual intervention by systems\xa0administrators\xa0to login to the account manually after a server restart.","date":"2016-04-30T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using PowerShell to set up Automatic Login on Windows Servers","tags":["Windows"],"date":"2016-04-30 00:00:00 +1300","header":{"teaser":"images/powershell-blog-feature-banner.png"}},"unlisted":false,"prevItem":{"title":"Hyper-V 2012 R2 on an Intel NUC (NUC5i7RYH)","permalink":"/2016/03/06/hyper-v-2012-r2-intel-nuc-nuc5i7ryh"},"nextItem":{"title":"How to install .NET Framework 3.5 on Windows Server 2012 and Windows Server 2012 R2","permalink":"/2015/01/22/install-net-framework-3-5-windows-server-2012-windows-server-2012-r2"}},"content":"Some server based applications require to be logged into\xa0a service account to allow an Application or service to run, These applications usually require manual intervention by systems\xa0administrators\xa0to login to the account manually after a server restart.\\n\\nThere are many ways to setup Automatic Logon, using _&#8220;control userpasswords2_&#8221; via the Run Prompt, using Third Party utilities like _LogonExpert_ or_\xa0Sysinternals Autologon for Windows \xa0_this\xa0simply using RegEdit and setting them manually.\\n\\nI have created a PowerShell script for editing the registry to set this manually in a standardized way and could be run remotely. It is pretty simple and only requires version 1 of PowerShell.\\n\\n#authors: [Luke] Murray (Luke.Geek.NZ)\\n#Version: 0.1\\n#Purpose:\\n\\n#The purpose of this PowerShell script is to set the AutoLogon and WinLogon registry strings using PowerShell, to setup a Windows Server #or Workstation for Automatic Logon.\\n#The $UserName and $Password variables need to be configuration for your environment. Please make sure the $UserName variables follows #DOMAINNAME\\\\SAMACCOUNTNAME format.\\n#This needs to be ran using an Elevated PowerShell ISE or PowerShell window (with Admin access on the computer you are running this on).\\n\\n$usrname = \'DOMAINNAME\\\\SAMACCOUNTNAME\'\\n$password = \'PASSWORD\'\\n$RegistryLocation = \'HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\\\\Winlogon\'\\nSet-ItemProperty $RegistryLocation -Name \'AutoAdminLogon\' -Value \'1\'\\nSet-ItemProperty $RegistryLocation -Name \'DefaultUsername\' -Value \\"$usrname\\"\\nSet-ItemProperty $RegistryLocation -Name \'DefaultPassword\' -Value \\"$password\\"\\n\\n\\nNote: Also note following this method, the username and password are not encrypted and stored as plain text in the registry."},{"id":"/2015/01/22/install-net-framework-3-5-windows-server-2012-windows-server-2012-r2","metadata":{"permalink":"/2015/01/22/install-net-framework-3-5-windows-server-2012-windows-server-2012-r2","source":"@site/blog/2015-01-22-install-net-framework-3-5-windows-server-2012-windows-server-2012-r2.md","title":"How to install .NET Framework 3.5 on Windows Server 2012 and Windows Server 2012 R2","description":".NET Framework 3.5 needs the source files off the Windows Server 2012 DVD in order to install. You can do this online and without needing a restart using DISM.","date":"2015-01-22T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install .NET Framework 3.5 on Windows Server 2012 and Windows Server 2012 R2","tags":["Windows"],"date":"2015-01-22 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Using PowerShell to set up Automatic Login on Windows Servers","permalink":"/2016/04/30/using-powershell-setup-automatic-logon-windows-servers"},"nextItem":{"title":"How to disable Internet Explorer Enhanced Security in Windows Server 2012 R2","permalink":"/2015/01/02/disable-internet-explorer-enhanced-security-windows-server-2012-r2"}},"content":".NET Framework 3.5 needs the source files off the Windows Server 2012 DVD in order to install. You can do this online and without needing a restart using DISM.\\n\\n1.\xa0Go to a command prompt and enter this:\\n\\n    dism /online /enable-feature /featurename:NetFX3 /all /Source:d:sourcessxs /LimitAccess\\n\\n_Note: Source should be the Windows DVD location, ie d:\xa0**/Source:x**_ if x is your DVD mount point."},{"id":"/2015/01/02/disable-internet-explorer-enhanced-security-windows-server-2012-r2","metadata":{"permalink":"/2015/01/02/disable-internet-explorer-enhanced-security-windows-server-2012-r2","source":"@site/blog/2015-01-02-disable-internet-explorer-enhanced-security-windows-server-2012-r2.md","title":"How to disable Internet Explorer Enhanced Security in Windows Server 2012 R2","description":"Note: This is not best practice and is not recommended to do in a production environment.","date":"2015-01-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.205,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to disable Internet Explorer Enhanced Security in Windows Server 2012 R2","tags":["Windows"],"date":"2015-01-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to install .NET Framework 3.5 on Windows Server 2012 and Windows Server 2012 R2","permalink":"/2015/01/22/install-net-framework-3-5-windows-server-2012-windows-server-2012-r2"},"nextItem":{"title":"Windows 10 Technical How to switch from Start Menu to Start Screen","permalink":"/2014/10/02/windows-10-technical-switch-start-menu-start-screen"}},"content":"Note: This is not best practice and is not recommended to do in a production environment.\\n\\n  1. Open Server Manager\\n  2. Click on Local Server (to the left)\\n  3. Select IE Enhanced Security Configuration\\n  4. Select Turn off and click Ok"},{"id":"/2014/10/02/windows-10-technical-switch-start-menu-start-screen","metadata":{"permalink":"/2014/10/02/windows-10-technical-switch-start-menu-start-screen","source":"@site/blog/2014-10-02-windows-10-technical-switch-start-menu-start-screen.md","title":"Windows 10 Technical How to switch from Start Menu to Start Screen","description":"Windows 10 Technical- How to switch from Start Menu to Start Screen","date":"2014-10-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.255,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows 10 Technical How to switch from Start Menu to Start Screen","tags":["Windows"],"date":"2014-10-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to disable Internet Explorer Enhanced Security in Windows Server 2012 R2","permalink":"/2015/01/02/disable-internet-explorer-enhanced-security-windows-server-2012-r2"},"nextItem":{"title":"How to configure Active Directory delegation to allow users to move computers between OUs","permalink":"/2014/09/10/configure-active-directory-delegation-allow-users-move-computers-ous"}},"content":"Windows 10 Technical- How to switch from Start Menu to Start Screen\\n\\n1. Click the Start button\\n\\n2. Right click a blank space on the menu and click properties\\n\\n3. Click the Start Menu tab\\n\\n4. Uncheck or check; Use the Start Menu instead of the Start Screen\\n\\n5. Click Apply"},{"id":"/2014/09/10/configure-active-directory-delegation-allow-users-move-computers-ous","metadata":{"permalink":"/2014/09/10/configure-active-directory-delegation-allow-users-move-computers-ous","source":"@site/blog/2014-09-10-configure-active-directory-delegation-allow-users-move-computers-ous.md","title":"How to configure Active Directory delegation to allow users to move computers between OUs","description":"1. Open Active Directory Users & Computers with AD rights","date":"2014-09-10T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.55,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to configure Active Directory delegation to allow users to move computers between OUs","tags":["Windows"],"date":"2014-09-10 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows 10 Technical How to switch from Start Menu to Start Screen","permalink":"/2014/10/02/windows-10-technical-switch-start-menu-start-screen"},"nextItem":{"title":"How to install Telnet client on Windows 7","permalink":"/2014/09/06/how-to-install-telnet-client-on-windows-7"}},"content":"1. Open Active Directory Users & Computers with AD rights\\n  2. Right click on the organisation unit you want to give access to and click Delegate Control\\n  3. Add the group want to give this access to, for example \u201c\\"IT HelpDesk\\"\\n  4. Select \u201cCreate a custom task to delegate\u201d and click Next\\n  5. Select \\"Only the following objects in this folder\\"\\n  6. Check \\"Computer Objects\\"\\n  7. Check \\"Create selected objects in this folder\\"\\n  8. Check \\"Delegate selected objects...\\" and click Next\\n  9. Make sure Write is checked and click Next\\n 10. Click Finish\\n\\nRepeat steps 2 to 10 again on other OUs you would like to delegate move rights to."},{"id":"/2014/09/06/how-to-install-telnet-client-on-windows-7","metadata":{"permalink":"/2014/09/06/how-to-install-telnet-client-on-windows-7","source":"@site/blog/2014-09-06-how-to-install-telnet-client-on-windows-7.md","title":"How to install Telnet client on Windows 7","description":"1. Click Start","date":"2014-09-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.4,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install Telnet client on Windows 7","tags":["Windows"],"date":"2014-09-06 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to configure Active Directory delegation to allow users to move computers between OUs","permalink":"/2014/09/10/configure-active-directory-delegation-allow-users-move-computers-ous"},"nextItem":{"title":"How to see what users are currently logged into a Windows system","permalink":"/2014/09/06/how-to-see-what-users-are-currently-logged-into-a-windows-system"}},"content":"1. <span style=\\"color:black;font-family:Arial;font-size:9pt\\">Click <strong>Start</strong><br /> </span>\\n  2. <span style=\\"color:black;font-family:Arial;font-size:9pt\\">Click <strong>Control Panel.</strong><br /> </span>\\n  3. <span style=\\"color:black;font-family:Arial;font-size:9pt\\">Click <strong>Programs</strong>.<br /> </span>\\n  4. <span style=\\"color:black;font-family:Arial;font-size:9pt\\">Click <strong>Turn Windows Features On Or Off.<br /> </strong></span>\\n  5. <span style=\\"color:black;font-family:Arial;font-size:9pt\\">In the Windows Features dialog <strong>select</strong> the <strong>Telnet Client</strong> check box.<br /> </span>\\n  6. <span style=\\"color:black;font-family:Arial;font-size:9pt\\">Click <strong>OK</strong>.<br /> </span>\\n\\n<span style=\\"color:black;font-family:Arial;font-size:9pt\\">You can also install the Telnet client \u2013 using the command line by running the following command from an elevated command prompt:<br /> </span>\\n\\n**start /w pkgmgr /iu:&#8221;TelnetClient&#8221;**"},{"id":"/2014/09/06/how-to-see-what-users-are-currently-logged-into-a-windows-system","metadata":{"permalink":"/2014/09/06/how-to-see-what-users-are-currently-logged-into-a-windows-system","source":"@site/blog/2014-09-06-how-to-see-what-users-are-currently-logged-into-a-windows-system.md","title":"How to see what users are currently logged into a Windows system","description":"Being able to determine who is currently logged into a particular Windows system \u2013 such as a server can be useful for identify performance problems and notifying the relevant users of an impending restart.","date":"2014-09-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.645,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to see what users are currently logged into a Windows system","tags":["Windows"],"date":"2014-09-06 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to install Telnet client on Windows 7","permalink":"/2014/09/06/how-to-install-telnet-client-on-windows-7"},"nextItem":{"title":"What Antivirus should I get for my computer?","permalink":"/2014/09/01/what-antivirus-should-i-get-for-my-computer"}},"content":"Being able to determine who is currently logged into a particular Windows system \u2013 such as a server can be useful for identify performance problems and notifying the relevant users of an impending restart.\\n\\n### <span id=\\"Method_1\\">Method 1</span>\\n\\n  1. Open a command prompt on the machine\\n  2. Type in: **quser**\\n  3. This will list the users currently logged in and what type of session are they using.\\n\\nNote: If you have opened an elevated command prompt you can use: **quser /server:Servername** to show logged on users on remote machines.\\n\\n### <span id=\\"Method_2\\">Method 2</span>\\n\\n  1. Open Task Manager (by clicking CTRL+ALT+DEL) on the PC\\n  2. Click on the Users Tab \u2013 up the top.\\n  3. This will list the users currently logged in and what type of session are using."},{"id":"/2014/09/01/what-antivirus-should-i-get-for-my-computer","metadata":{"permalink":"/2014/09/01/what-antivirus-should-i-get-for-my-computer","source":"@site/blog/2014-09-01-what-antivirus-should-i-get-for-my-computer.md","title":"What Antivirus should I get for my computer?","description":"Its a common question &#8211; what antivirus should I get for my computer? That&#8217;s a good question &#8211; here is my recommended list:","date":"2014-09-01T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.59,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"What Antivirus should I get for my computer?","tags":["Misc"],"date":"2014-09-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to see what users are currently logged into a Windows system","permalink":"/2014/09/06/how-to-see-what-users-are-currently-logged-into-a-windows-system"},"nextItem":{"title":"How to schedule Windows Memory Diagnostic to Start","permalink":"/2014/08/06/schedule-windows-memory-diagnostic-to-start"}},"content":"<span lang=\\"en-nz\\"><span style=\\"color: #000000;font-size: small\\">Its a common question &#8211; what antivirus should I get for my computer? That&#8217;s a good question &#8211; here is my recommended list:</span></span>\\n\\n  1. If it\u2019s Windows 7 &#8211; get <a title=\\"Microsoft Security Essentials\\" href=\\"http://windows.microsoft.com/en-NZ/windows/security-essentials-download\\" target=\\"_blank\\">Microsoft Security Essentials</a>!\\n  2. If it\u2019s Windows 8 &#8211; don&#8217;t worry. Windows Defender should be good enough!\\n  3. If it\u2019s Windows XP &#8211; get <a title=\\"AVG Free Antivirus\\" href=\\"http://free.avg.com/au-en/free-antivirus-download\\" target=\\"_blank\\">AVG Antivirus</a>!\\n  4. If it\u2019s Linux Distribution &#8211; get <a title=\\"Clam Antivirus\\" href=\\"http://www.clamav.net/download.html\\" target=\\"_blank\\">Clam Antivirus</a>!\\n  5. If it\u2019s OSX \u2013 get <a title=\\"ClamXav\\" href=\\"http://www.clamxav.com/download.html\\" target=\\"_blank\\">ClamXav</a>!\\n\\nNote: Bear in mind this is my own personal opinion and I am not responsible for any virus that may occur from following this recommendation!"},{"id":"/2014/08/06/schedule-windows-memory-diagnostic-to-start","metadata":{"permalink":"/2014/08/06/schedule-windows-memory-diagnostic-to-start","source":"@site/blog/2014-08-06-schedule-windows-memory-diagnostic-to-start.md","title":"How to schedule Windows Memory Diagnostic to Start","description":"Scheduling Windows Memory Diagnostic to run at next start-up is useful as it means you can easily save your work and restart your computer when you are ready without having to go into the F8 menu to start it manually. In order to schedule windows memory diagnostic to start at next boot follow the quick guide below.","date":"2014-08-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.395,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to schedule Windows Memory Diagnostic to Start","tags":["Windows"],"date":"2014-08-06 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"What Antivirus should I get for my computer?","permalink":"/2014/09/01/what-antivirus-should-i-get-for-my-computer"},"nextItem":{"title":"Enterprise Vault Archive Explorer Blank","permalink":"/2014/07/14/enterprise-vault-archive-explorer-blank"}},"content":"Scheduling Windows Memory Diagnostic to run at next start-up is useful as it means you can easily save your work and restart your computer when you are ready without having to go into the F8 menu to start it manually. In order to schedule windows memory diagnostic to start at next boot follow the quick guide below.\\n\\n  1. Click **Start**\\n  2. Click **Run**\\n  3. Type in: **mdsched**\\n  4. Select &#8220;_**Check for problems the next time I start my computer**_&#8220;"},{"id":"/2014/07/14/enterprise-vault-archive-explorer-blank","metadata":{"permalink":"/2014/07/14/enterprise-vault-archive-explorer-blank","source":"@site/blog/2014-07-14-enterprise-vault-archive-explorer-blank.md","title":"Enterprise Vault Archive Explorer Blank","description":"Note: This needs to be done from the Enterprise Vault archiving server.","date":"2014-07-14T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.265,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Enterprise Vault Archive Explorer Blank","tags":["Windows"],"date":"2014-07-14 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to schedule Windows Memory Diagnostic to Start","permalink":"/2014/08/06/schedule-windows-memory-diagnostic-to-start"},"nextItem":{"title":"How to get the Lync client to redownload the address book","permalink":"/2014/07/08/lync-address-book"}},"content":"Note: This needs to be done from the Enterprise Vault archiving server.\\n\\n  1. Stop the EnterpriseVaultIndexingService <em style=\\"font-family: sans-serif;font-size: medium\\">(this will stop the other services as well)</em>\\n  2. Kill the IndexBroker.exe process running on the server if running.\\n  3. Start EnterpriseVaultIndexingService\\n  4. Run an elevated\xa0command prompt and type: IISRESET /RESTART and press Ok"},{"id":"/2014/07/08/lync-address-book","metadata":{"permalink":"/2014/07/08/lync-address-book","source":"@site/blog/2014-07-08-lync-address-book.md","title":"How to get the Lync client to redownload the address book","description":"Lync 2010","date":"2014-07-08T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.565,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to get the Lync client to redownload the address book","tags":["Windows"],"date":"2014-07-08 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Enterprise Vault Archive Explorer Blank","permalink":"/2014/07/14/enterprise-vault-archive-explorer-blank"},"nextItem":{"title":"How to clear the local workstations Group Policy cache","permalink":"/2014/07/01/clear-local-workstations-group-policy-cache"}},"content":"**Lync 2010**\\n\\nLog off of the Lync client and delete the profile from:\\n\\n**%userprofile%/AppData/Local/Microsoft/Communicator**\\n\\nThen run the following command into an elevated\xa0command prompt to add the registry that forces the Lync client to immediately download the address book:\\n\\n**reg add HKLM/Software/Policies/MicrosoftCommunicator /v GalDownloadInitialDelay /t REG_DWORD /d 0 /f**\\n\\n**Lync 2013**\\n\\nLog off of the Lync client and delete the profile from:\\n\\n**%userprofile%/appdata/Local/Microsoft/Office/15.0/Lync**\\n\\nThen run the following command into an elevated\xa0command prompt\xa0to add the registry that forces the Lync client to immediately download the address book:\\n\\n**reg add HKLMSoftwarePoliciesMicrosoftOffice15.0Lync /v GalDownloadInitialDelay /t REG_DWORD /d 0 /f**\\n\\nNote: The Lync address book is saved in the following files on the local workstations: \xa0GalContacts.db & GalContacts.db.idx."},{"id":"/2014/07/01/clear-local-workstations-group-policy-cache","metadata":{"permalink":"/2014/07/01/clear-local-workstations-group-policy-cache","source":"@site/blog/2014-07-01-clear-local-workstations-group-policy-cache.md","title":"How to clear the local workstations Group Policy cache","description":"Option 1","date":"2014-07-01T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"},{"inline":true,"label":"PowerShell","permalink":"/tags/power-shell"}],"readingTime":0.7,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to clear the local workstations Group Policy cache","tags":["Windows","PowerShell"],"date":"2014-07-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to get the Lync client to redownload the address book","permalink":"/2014/07/08/lync-address-book"},"nextItem":{"title":"How to refresh Printer Installer configuration on a workstation","permalink":"/2014/06/26/refresh-printer-installer-configuration-workstation"}},"content":"## Option 1\\n\\n  1. Open My Computer/Computer\\n  2. In the URL or address bar paste: %windir%\\\\system32\\\\GroupPolicy\\n  3. Right click and delete the: Machine and User folders to clear local group policy cache\\n  4. Restart the computer to reapply the group policies\\n\\nNote: You can also run: gpupdate /force on the machine to force the policy to reapply.\\n\\nYou can also run the little PowerShell oneliner as Administrator to remove the Group Policy folder and all files below:\\n\\n    #requires -Version 1.0\\n    #Requires -RunAsAdministrator\\n\\n    Remove-Item \\"$env:windir\\\\system32\\\\GroupPolicy\\" -Force -Recurse\\n\\n## Option 2\\n\\n  1. Delete the &#8220;HKLM\\\\Software\\\\Policies\\\\Microsoft Key (looks like a folder).\\n  2. Delete the &#8220;HKCU\\\\Software\\\\Policies\\\\Microsoft Key\\n  3. Delete the &#8220;HKCU\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Group Policy Objects Key.\\n  4. Delete the &#8220;HKCU\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies Key.\\n\\n## Option 3\\n\\n  1. Remove the computer from the domain (change to a Workgroup)\\n  2. Restart computer\\n  3. Run gpupdate /force\\n  4. Rejoin the domain"},{"id":"/2014/06/26/refresh-printer-installer-configuration-workstation","metadata":{"permalink":"/2014/06/26/refresh-printer-installer-configuration-workstation","source":"@site/blog/2014-06-26-refresh-printer-installer-configuration-workstation.md","title":"How to refresh Printer Installer configuration on a workstation","description":"Sometimes\xa0making adjustments with Printer Installer can be a nightmare &#8211; especially when you need to wait for the change to\xa0replicate out to users.","date":"2014-06-26T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to refresh Printer Installer configuration on a workstation","tags":["Windows"],"date":"2014-06-26 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to clear the local workstations Group Policy cache","permalink":"/2014/07/01/clear-local-workstations-group-policy-cache"},"nextItem":{"title":"Wildstar Client version does not match the game server version","permalink":"/2014/06/03/wildstar-client-version-match-game-server-version"}},"content":"Sometimes\xa0making adjustments with Printer Installer can be a nightmare &#8211; especially when you need to wait for the change to\xa0replicate out to users.\\n\\nUsing this method you can easily use Printer Installer to update the configuration on the local computer without having to wait for replication.\\n\\n  1. Click **Start**\\n  2. Click **All Programs**\\n  3. Click **Printer Installer**\\n  4. Click **Refresh Configurations**"},{"id":"/2014/06/03/wildstar-client-version-match-game-server-version","metadata":{"permalink":"/2014/06/03/wildstar-client-version-match-game-server-version","source":"@site/blog/2014-06-03-wildstar-client-version-match-game-server-version.md","title":"Wildstar Client version does not match the game server version","description":"This error seems to be happening mostly to us New Zealand players.. looks like a DNS issue. The fix is to use a VPN tunnel for\xa0when you first open a launcher or edit the host file.","date":"2014-06-03T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.515,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Wildstar Client version does not match the game server version","tags":["Windows"],"date":"2014-06-03 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to refresh Printer Installer configuration on a workstation","permalink":"/2014/06/26/refresh-printer-installer-configuration-workstation"},"nextItem":{"title":"WildStar How to install addons","permalink":"/2014/06/02/wildstar-install-addons"}},"content":"This error seems to be happening mostly to us New Zealand players.. looks like a DNS issue. The fix is to use a VPN tunnel for\xa0when you first open a launcher or edit the host file.\\n\\nTo edit the Host file\xa0do the following:\\n\\n  1. Click Start\\n  2. All Programs\\n  3. Accessories\\n  4. Right click Notepad and select Run as administrator\\n  5. Click Continue on the &#8220;Windows needs your permission&#8221; UAC window.\\n  6. When Notepad opens Click File -> Open\\n  7. In the filename field type C:WindowsSystem32Driversetchosts\\n  8. Click Open\\n  9. Paste in and save the host file:\\n\\n`174.35.56.164 wildstar.patcher.ncsoft.com``<br />\\n` `174.35.56.164 wildstar.patcher.ncsoft.com.cdngc.net`"},{"id":"/2014/06/02/wildstar-install-addons","metadata":{"permalink":"/2014/06/02/wildstar-install-addons","source":"@site/blog/2014-06-02-wildstar-install-addons.md","title":"WildStar How to install addons","description":"As most MMORG out these days; Wildstar allows addons.","date":"2014-06-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.26,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"WildStar How to install addons","tags":["Windows"],"authors":["Luke"],"date":"2014-06-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Wildstar Client version does not match the game server version","permalink":"/2014/06/03/wildstar-client-version-match-game-server-version"},"nextItem":{"title":"Using exchange to automatically accept meeting requests","permalink":"/2014/05/20/using-exchange-automatically-accept-meeting-requests"}},"content":"As most MMORG out these days; Wildstar allows addons.\\n\\n  1. The first and\xa0easiest way is to use the Curse Gaming Client to install addons.\\n  2. The second is to download the addons and extract the ZIP file to:\xa0%appdata%/NCSOFT/Wildstar/Addons\\n\\nNote: If the Addons folder does not exist &#8211; you\xa0will need to create it."},{"id":"/2014/05/20/using-exchange-automatically-accept-meeting-requests","metadata":{"permalink":"/2014/05/20/using-exchange-automatically-accept-meeting-requests","source":"@site/blog/2014-05-20-using-exchange-automatically-accept-meeting-requests.md","title":"Using exchange to automatically accept meeting requests","description":"Having multiple mailboxes and calendars can be hard to manage &#8211; especially when meeting requests don&#8217;t automatically accept themselves.","date":"2014-05-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.265,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using exchange to automatically accept meeting requests","tags":["Windows"],"date":"2014-05-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"WildStar How to install addons","permalink":"/2014/06/02/wildstar-install-addons"},"nextItem":{"title":"Common Protocols and their ports","permalink":"/2014/05/18/common-protocols-and-their-ports"}},"content":"Having multiple mailboxes and calendars can be hard to manage &#8211; especially when meeting requests don&#8217;t automatically accept themselves.\\n\\nUsing a quick powershell cmdlet from the Exchange Management Shell you can quickly set a calender to AutoAccept meeting requests\\n\\n    Set-MailboxCalendarSettings \\"mailbox name\\" -AutomateProcessing AutoAccept\\n\\nNote: Remember to run the Management Shell as Administrator."},{"id":"/2014/05/18/common-protocols-and-their-ports","metadata":{"permalink":"/2014/05/18/common-protocols-and-their-ports","source":"@site/blog/2014-05-18-common-protocols-and-their-ports.md","title":"Common Protocols and their ports","description":"Networking is a marvel of interconnected devices and languages \u2013 part of that inter connectivity is allowing devices to talk to one another without getting any crosstalk or confusion this is done by separating network traffic into ports \u2013 allowing traffic to be managed easier and allows for overall greater security.","date":"2014-05-18T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Common Protocols and their ports","tags":["Misc"],"date":"2014-05-18 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Using exchange to automatically accept meeting requests","permalink":"/2014/05/20/using-exchange-automatically-accept-meeting-requests"},"nextItem":{"title":"How to reset the Surface RT","permalink":"/2014/05/17/reset-the-surface-rt"}},"content":"<p class=\\"style1\\">\\n  Networking is a marvel of interconnected devices and languages \u2013 part of that inter connectivity is allowing devices to talk to one another without getting any crosstalk or confusion this is done by separating network traffic into ports \u2013 allowing traffic to be managed easier and allows for overall greater security.\\n</p>\\n\\n<p class=\\"style1\\">\\n  <strong>HTTP (Hypertext Transfer Protocol) </strong><br /> HTTP uses port 80 \u2013 this is usually used for web traffic \u2013 for webpages.<br /> <strong>HTTPS (Hypertext Transfer Protocol SSL) </strong><br /> Similar to port 80 although this is using port 443 \u2013 this is used for secure and encrypted traffic and used for website logon pages and secure VPN connections.<br /> <strong>FTP (File Transfer Protocol)</strong><br /> Using port 20 and 21 \u2013 this is used for FTP traffic to and from.. used to upload webpages and files and folders from one host to another .<br /> <strong>POP (Post Office Protocol) \u2013 Also POP3</strong><br /> POP and POP3 uses the port 110 \u2013 this provides the ability to receive emails \u2013 commonly used by ISPs email service.<br /> <strong>SMTP (Simple Mail Transfer Protocol)</strong><br /> SMTP uses the port 25 \u2013 this provides the ability to send emails \u2013 commonly used by ISPs email service.<br /> <strong>IMAP</strong><br /> Uses TCP and UDP port 143 \u2013 IMAP is another email service that is commonly used by email providers that hold emails on their servers \u2013 instead of downloading them to the local computer.<br /> <strong>Telnet</strong><br /> Telnet uses port 23 \u2013 and is commonly used for remote administration of devices using a command line.\\n</p>"},{"id":"/2014/05/17/reset-the-surface-rt","metadata":{"permalink":"/2014/05/17/reset-the-surface-rt","source":"@site/blog/2014-05-17-reset-the-surface-rt.md","title":"How to reset the Surface RT","description":"Without any bootable media &#8211; such as the Windows 8 CD. You have to reset the Surface RT at the operating system level.","date":"2014-05-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.51,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to reset the Surface RT","tags":["Windows"],"date":"2014-05-17 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Common Protocols and their ports","permalink":"/2014/05/18/common-protocols-and-their-ports"},"nextItem":{"title":"ASUS thunderbolt driver working in Windows 8","permalink":"/2014/05/17/thunderbolt-driver-win8"}},"content":"Without any bootable media &#8211; such as the Windows 8 CD. You have to reset the Surface RT at the operating system level.\\n\\n  1. Attach your keyboard\xa0cover\\n  2. Power on your Surface\\n  3. <span style=\\"color: #333333\\">Press and hold the\xa0</span><b style=\\"color: #333333\\">Left Shift</b><span style=\\"color: #333333\\">\xa0key at the logon\xa0screen</span>\\n  4. <span style=\\"color: #333333\\">Press the\xa0</span><b style=\\"color: #333333\\">Power\xa0</b><span style=\\"color: #333333\\">icon</span>\\n  5. <span style=\\"color: #333333\\">Press\xa0</span><b style=\\"color: #333333\\">Restart</b>\\n  6. <em style=\\"font-family: sans-serif;font-size: medium\\"><span style=\\"color: #333333\\">Your surface should boot to a light blue screen with some options on it.</span></em>\\n  7. <span style=\\"color: #333333\\">Click on\xa0</span><b style=\\"color: #333333\\">Troubleshoot</b>\\n  8. <span style=\\"color: #333333\\">Click on\xa0</span><b style=\\"color: #333333\\">Reset your PC</b>\\n  9. Follow the prompts."},{"id":"/2014/05/17/thunderbolt-driver-win8","metadata":{"permalink":"/2014/05/17/thunderbolt-driver-win8","source":"@site/blog/2014-05-17-thunderbolt-driver-win8.md","title":"ASUS thunderbolt driver working in Windows 8","description":"Unfortunately ASUS hasn&#8217;t released driver updates for the\xa0Rampage 3 Black edition for Windows 8 or Windows 8.1 and although the on-board Realtek\xa0chipset drivers are available through Windows Update &#8211;\xa0one of the selling points of this motherboard combo was the Thunderboard audio interface!","date":"2014-05-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.125,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"ASUS thunderbolt driver working in Windows 8","tags":["Windows"],"date":"2014-05-17 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to reset the Surface RT","permalink":"/2014/05/17/reset-the-surface-rt"},"nextItem":{"title":"How to show Administrative Tools in Windows 8","permalink":"/2014/04/28/administrative-tools-windows-8"}},"content":"Unfortunately ASUS hasn&#8217;t released driver updates for the\xa0Rampage 3 Black edition for Windows 8 or Windows 8.1 and although the on-board Realtek\xa0chipset drivers are available through Windows Update &#8211;\xa0one of the selling points of this motherboard combo was the Thunderboard audio interface!\\n\\nNote: Tested in Windows 8.1 64 bit.\\n\\n  1. First you need to download the Windows 7 thunderbolt drivers &#8211; &#8220;<a style=\\"font-family: sans-serif;font-style: normal\\" title=\\"ASUS Rampage 3 Black Edition Drivers/Utilities\\" href=\\"http://support.asus.com/download.aspx?SLanguage=en&m=Rampage%20III%20Black%20Edition&os=30\\" target=\\"_blank\\">here</a>&#8220;\\n  2. Download and extract the drivers\xa0to a folder\\n  3. Press the Windows Key + Pause/Break at the same time to open up your computers System properties\\n  4. Click Device Manager on the left hand menu (you need full Administrator rights)\\n  5. Navigate down to the Thunderbolt device &#8211; _it should have an exclamation on it to indicate driver problems_ &#8211; Right click and select\xa0Update Driver Software\\n  6. Select Browse my computer for driver software\\n  7. Select Let me pick from a list of device drivers on my computer and click Next\\n  8. Navigate down to Sound Devices and click next\\n  9. Click Have Disk and navigate to the extracted driver path and select &#8220;USBUAA.inf&#8221; and click Next\\n 10. Select USB device in the Model window and select Next to\xa0force Windows 8 to install the Windows 7\xa0Thunderbolt audio driver\\n 11. Once installed &#8211; restart your computer and you should now have Thunderbolt audio."},{"id":"/2014/04/28/administrative-tools-windows-8","metadata":{"permalink":"/2014/04/28/administrative-tools-windows-8","source":"@site/blog/2014-04-28-administrative-tools-windows-8.md","title":"How to show Administrative Tools in Windows 8","description":"Windows 8 (and Windows 8.1) \u2013 allows for RSAT (Remote Server Administration Tools) just like the previous versions of Windows for remote management of Server 2012 &#8211; however you need to reveal or show\xa0it.","date":"2014-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.495,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to show Administrative Tools in Windows 8","tags":["Windows"],"date":"2014-04-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"ASUS thunderbolt driver working in Windows 8","permalink":"/2014/05/17/thunderbolt-driver-win8"},"nextItem":{"title":"Create VHD in Windows 7 - Windows","permalink":"/2014/04/28/create-vhd-windows-7"}},"content":"Windows 8 (and Windows 8.1) \u2013 allows for RSAT (Remote Server Administration Tools) just like the previous versions of Windows for remote management of Server 2012 &#8211; however you need to reveal or show\xa0it.\\n\\nBy default however \u2013 this is hidden from the Metro Title menu and you have to enable it \u2013 follow the quick guide below to show\xa0the Administrative Tools in Windows 8.\\n\\n  1. Open the **Start** screen.\\n  2. Hit **Windows Key + C** to bring up the charms bar.\\n  3. Click **Settings** on the right charms bar.\\n  4. Click **Tiles**.\\n  5. Click the **Show Administrative Tools**"},{"id":"/2014/04/28/create-vhd-windows-7","metadata":{"permalink":"/2014/04/28/create-vhd-windows-7","source":"@site/blog/2014-04-28-create-vhd-windows-7.md","title":"Create VHD in Windows 7 - Windows","description":"Want to create a VHD in Windows 7?\xa0Why? You may ask&#8230; would you use a VHD?","date":"2014-04-28T00:00:00.000Z","tags":[],"readingTime":0.665,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Create VHD in Windows 7 - Windows","date":"2014-04-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to show Administrative Tools in Windows 8","permalink":"/2014/04/28/administrative-tools-windows-8"},"nextItem":{"title":"How to enable numlock on startup in Windows 8","permalink":"/2014/04/28/enable-numlock-windows8"}},"content":"Want to create a VHD in Windows 7?\xa0Why? You may ask&#8230; would you use a VHD?\\n\\nUsing a VHD as a \u201cvirtual disk\u201d you can easily use it as a self-contained \u201cfolder\u201d or storage mechanism, you can also use it to install operating systems onto allowing to run dual booted operating systems with only one hard drive and not having to worry about partition ids. Creating VHD files in Windows 7 is remarkably easy \u2013 follow the guide below:\\n\\n1. Click the Start button in Windows 7, type in diskmgmt.msc, and hit Enter.\\n  \\n2. Click the **Action** menu and select **Create VHD**.\\n  \\n3. Enter a location, such as C:VHDWindows.vhd. Set the size.\xa0Dynamic VHD files start out small and only grow with data written\xa0to the virtual disk.\\n4. Click **OK**\xa0and the file is created."},{"id":"/2014/04/28/enable-numlock-windows8","metadata":{"permalink":"/2014/04/28/enable-numlock-windows8","source":"@site/blog/2014-04-28-enable-numlock-windows8.md","title":"How to enable numlock on startup in Windows 8","description":"If you are like me \u2013 you rely heavily on the Num Lock key and having it disabled on boot is extremely annoying \u2013 using a quick registry change you can enable Numlock by default in Windows 8.","date":"2014-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.82,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to enable numlock on startup in Windows 8","tags":["Windows"],"date":"2014-04-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Create VHD in Windows 7 - Windows","permalink":"/2014/04/28/create-vhd-windows-7"},"nextItem":{"title":"Changing your user photo in Windows 8","permalink":"/2014/04/28/user-photo-in-windows-8"}},"content":"If you are like me \u2013 you rely heavily on the Num Lock key and having it disabled on boot is extremely annoying \u2013 using a quick registry change you can enable Numlock by default in Windows 8.\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#HOW_TO_enableNUMLOCK_ON_STARTUP\\"><span class=\\"toc_number toc_depth_1\\">1</span> HOW TO enable\xa0NUMLOCK ON STARTUP</a>\\n    </li>\\n    <li>\\n      <a href=\\"#How_to_disable_Numlock_on_Startup\\"><span class=\\"toc_number toc_depth_1\\">2</span> How to disable Numlock on Startup</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n#### <span id=\\"HOW_TO_enableNUMLOCK_ON_STARTUP\\">HOW TO enable\xa0NUMLOCK ON STARTUP</span>\\n\\n  1. Press the Windows Key to open the Tile (Metro) screen\\n  2. Type: regedit to search for the Windows Registry Editor and press Enter.\\n  3. When the Registry Editor loads &#8211; navigate through:\\n  4. HKEY_USERS,.DEFAULT, Control Panel, and Keyboard.\\n  5. Locate the InitialKeyboardIndicators entry and right-click it, select Modify.\\n  6. Enter 2 into the box.\\n  7. Click OK to save the changes.\\n\\n\\n#### <span id=\\"How_to_disable_Numlock_on_Startup\\">How to disable Numlock on Startup</span>\\n\\nFollow the same steps till: Step 6 than instead of 2 type: 2147483648."},{"id":"/2014/04/28/user-photo-in-windows-8","metadata":{"permalink":"/2014/04/28/user-photo-in-windows-8","source":"@site/blog/2014-04-28-user-photo-in-windows-8.md","title":"Changing your user photo in Windows 8","description":"Changing your user photo in Windows 8 is remarkably easy and helps make a standard Windows 8 installation&#8230; More&#8230; You!","date":"2014-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.345,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Changing your user photo in Windows 8","tags":["Windows"],"date":"2014-04-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to enable numlock on startup in Windows 8","permalink":"/2014/04/28/enable-numlock-windows8"},"nextItem":{"title":"Windows 8 Hotkeys","permalink":"/2014/04/28/windows-8-hotkeys"}},"content":"Changing your user photo in Windows 8 is remarkably easy and helps make a standard Windows 8 installation&#8230; More&#8230; You!\\n\\n  1. Open the Start screen and **click** your **user** **picture**\\n  2. Click **Change** Account **Picture**.\\n  3. Click the **Browse** button.\\n  4. **Navigate** the browser until you **find** an **image** you want to use and\xa0click it.\\n  5. Select **Choose** **Image**.\\n  6. Done \u2013 you have now changed your Account Picture."},{"id":"/2014/04/28/windows-8-hotkeys","metadata":{"permalink":"/2014/04/28/windows-8-hotkeys","source":"@site/blog/2014-04-28-windows-8-hotkeys.md","title":"Windows 8 Hotkeys","description":"Navigating around Windows 8 can be a lot faster and more efficient with the use of keyboard shortcuts called Hotkeys \u2013 the below guide is a list of Windows 8 hotkeys for navigating the user interface.","date":"2014-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.73,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows 8 Hotkeys","tags":["Windows"],"date":"2014-04-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Changing your user photo in Windows 8","permalink":"/2014/04/28/user-photo-in-windows-8"},"nextItem":{"title":"Windows Networking Protocols and their uses","permalink":"/2014/04/28/windows-networking-protocols-uses"}},"content":"<p class=\\"Style1\\">\\n  Navigating around Windows 8 can be a lot faster and more efficient with the use of keyboard shortcuts called Hotkeys \u2013 the below guide is a list of Windows 8 hotkeys for navigating the user interface.\\n</p>\\n\\n<p class=\\"Style1\\">\\n  <b>Windows Key</b> &#8211; Opens the Start screen<br /> <b>Windows Key+Tab</b> &#8211; Metro app switcher<br /> <b>Windows Key+C</b> &#8211; System charms<br /> <b>Windows Key+Z</b> &#8211; Application charms<br /> <b>Windows Key+I</b> &#8211; Settings menu<br /> <b>Windows Key+K</b> &#8211; Devices menu<br /> <b>Windows Key+H</b> &#8211; Share menu<br /> <b>Windows Key+F</b> &#8211; Search files on Start screen<br /> <b>Windows Key+Q</b> &#8211; Application search on Start screen<br /> <b>Windows Key+W</b> &#8211; Settings search on Start screen<br /> <b>Windows Key+. (Period)</b> &#8211; Snap Metro app to the right side of the\xa0screen<br /> <b>Windows Key+Shift+. (Period)</b> &#8211; Snap Metro app to the left side of the screen (only works on wide-screen\xa0monitors)\\n</p>"},{"id":"/2014/04/28/windows-networking-protocols-uses","metadata":{"permalink":"/2014/04/28/windows-networking-protocols-uses","source":"@site/blog/2014-04-28-windows-networking-protocols-uses.md","title":"Windows Networking Protocols and their uses","description":"Networking is a marvel of interconnected devices and languages &#8211; Windows networking protocols help that happen by making sure 2 devices are sharing the same method of communicating. This is a quick reference guide on what each Protocol and Client on the Windows OS does.","date":"2014-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Networking Protocols and their uses","tags":["Windows"],"date":"2014-04-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows 8 Hotkeys","permalink":"/2014/04/28/windows-8-hotkeys"},"nextItem":{"title":"Finding a MAC address through PXE boot","permalink":"/2014/04/27/finding-a-mac-address-pxe"}},"content":"<p class=\\"Style1\\">\\n  Networking is a marvel of interconnected devices and languages &#8211; Windows networking protocols help that happen by making sure 2 devices are sharing the same method of communicating. This is a quick reference guide on what each Protocol and Client on the Windows OS does.\\n</p>\\n\\n<p class=\\"Style1\\">\\n  <strong>Client for Microsoft Networks</strong><br /> Used to access other shared resources on\xa0your local network running the File and\xa0Printer Sharing for Microsoft Networks\xa0protocol.<br /> <strong>QOS Packet Scheduler</strong><br /> Used to provide traffic management on your\xa0network for applications that support the\xa0protocol.<br /> <strong>File and Printer Sharing for\xa0Microsoft Networks</strong><br /> Used to share your printer and files on your\xa0computer with other computers on your local\xa0network.<br /> <strong>Microsoft Network Adapter\xa0Multiplexor Protocol</strong><br /> provides the ability to load balance between two or more network cards.<br /> <strong>Microsoft LLDP Protocol Driver</strong><br /> Used to create the network map used in\xa0the Network browser and Networking and\xa0Sharing Centre.<br /> <strong>Link Layer Topology Discovery\xa0Mapper I/O Driver</strong><br /> Used to discover other computers connected\xa0to your local network.<br /> <strong>Link Layer Topology Responder</strong><br /> Used to identify your computer to other computers\xa0connected to your local network.<br /> <strong>Internet Protocol Version 6\xa0(TCP/IPv6)</strong><br /> A new version of the IPv4 protocol. Unless you are connected to\xa0an IPv6 network (most of you are not), you can\xa0safely disable this protocol.<br /> <strong>Internet Protocol Version 4\xa0(TCP/IPv4)</strong><br /> Primary network communication protocol. Do\xa0not disable this protocol.\\n</p>"},{"id":"/2014/04/27/finding-a-mac-address-pxe","metadata":{"permalink":"/2014/04/27/finding-a-mac-address-pxe","source":"@site/blog/2014-04-27-finding-a-mac-address-pxe.md","title":"Finding a MAC address through PXE boot","description":"Imaging computers using Windows Deployment Services or System Center \u2013 can be painful when you don\u2019t know the MAC address of a computer finding a\xa0MAC address through PXE boot can be quite useful \u2013 this needs to be done from the physical workstation.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.445,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Finding a MAC address through PXE boot","tags":["Windows"],"date":"2014-04-27 00:00:00 +1300","authors":["Luke"]},"unlisted":false,"prevItem":{"title":"Windows Networking Protocols and their uses","permalink":"/2014/04/28/windows-networking-protocols-uses"},"nextItem":{"title":"Accessing the Ricoh printer maintenance shell","permalink":"/2014/04/27/how-to-access-the-ricoh-printer-maintenance-shell"}},"content":"Imaging computers using Windows Deployment Services or System Center \u2013 can be painful when you don\u2019t know the MAC address of a computer finding a\xa0MAC address through PXE boot can be quite useful \u2013 this needs to be done from the physical workstation.\\n\\n  1. **Start** the **computer**\\n  2. Tap **F12** to start network boot as soon as the computer starts to boot\\n  3. You should get to a black screen with some white writing \u2013 select Pause/Break on your keyboard to pause this screen the MAC address should display."},{"id":"/2014/04/27/how-to-access-the-ricoh-printer-maintenance-shell","metadata":{"permalink":"/2014/04/27/how-to-access-the-ricoh-printer-maintenance-shell","source":"@site/blog/2014-04-27-how-to-access-the-ricoh-printer-maintenance-shell.md","title":"Accessing the Ricoh printer maintenance shell","description":"Most Ricoh network capable printers include a Telnet server that is used by Ricoh for maintenance of their printers allowing you to access the Ricoh Printer maintenance shell. Using telnet or in my guide \u2013 I am using the third party tool pUTTY (as in my domain environment \u2013 telnet hasn\u2019t been installed on most workstations) you can access the Maintenance Shell.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.655,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Accessing the Ricoh printer maintenance shell","tags":["Misc"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Finding a MAC address through PXE boot","permalink":"/2014/04/27/finding-a-mac-address-pxe"},"nextItem":{"title":"Adjusting java security settings","permalink":"/2014/04/27/java-security-settings"}},"content":"Most Ricoh network capable printers include a Telnet server that is used by Ricoh for maintenance of their printers allowing you to access the Ricoh Printer maintenance shell. Using telnet or in my guide \u2013 I am using the third party tool pUTTY _(as in my domain environment \u2013 telnet hasn\u2019t been installed on most workstations)_ you can access the Maintenance Shell.\\n\\n  1. Download and run [PuTTY](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\\n  2. **Open** **PuTTY** and choose **Telnet** under Connection Type.\\n  3. **Type** **in** the IP **address** or hostname in the Host Name (or IP address field).\\n  4. Click **Open**.\\n  5. This will then start the RICOH Maintenance Shell \u2013 type in the login name _(default is using admin as the username \u2013 and a blank password).\\n  6. You will be greeted with the maintenance shell."},{"id":"/2014/04/27/java-security-settings","metadata":{"permalink":"/2014/04/27/java-security-settings","source":"@site/blog/2014-04-27-java-security-settings.md","title":"Adjusting java security settings","description":"Adjusting Java Security settings can be used to fix a variety of problems \u2013 especially around certification validation and problems with java security updates. However although not recommended for security reasons it can be used as a \u201cquick fix\u201d or \u201cwork around\u201d until a permanent solution can be found and also just takes a few clicks of the mouse to change.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.535,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Adjusting java security settings","tags":["Windows"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Accessing the Ricoh printer maintenance shell","permalink":"/2014/04/27/how-to-access-the-ricoh-printer-maintenance-shell"},"nextItem":{"title":"Automatically accept Meeting Requests in Outlook","permalink":"/2014/04/27/meeting-requests-in-outlook"}},"content":"Adjusting Java Security settings can be used to fix a variety of problems \u2013 especially around certification validation and problems with java security updates. However although not recommended for security reasons it can be used as a \u201cquick fix\u201d or \u201cwork around\u201d until a permanent solution can be found and also just takes a few clicks of the mouse to change.\\n\\n  1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click on the View by: option \u2013 top right and select Small icons\\n  4. Click **Java (32-Bit)**\\n  5. Up the top \u2013 click **Security**\\n  6. **Change** the Security Level **slider** to **Medium**\\n  7. Click **Apply** (down the bottom)"},{"id":"/2014/04/27/meeting-requests-in-outlook","metadata":{"permalink":"/2014/04/27/meeting-requests-in-outlook","source":"@site/blog/2014-04-27-meeting-requests-in-outlook.md","title":"Automatically accept Meeting Requests in Outlook","description":"Have more emails in your inbox than you can read? Sometimes those all important Meeting requests can be missed \u2013 still waiting to be accepted or declined. Outlook includes Resource Scheduling options to automatically accept meeting requests in Outlook or to wait for the request to be accepted or declined manually.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.5,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Automatically accept Meeting Requests in Outlook","tags":["Windows"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Adjusting java security settings","permalink":"/2014/04/27/java-security-settings"},"nextItem":{"title":"Optimal Taleo Settings","permalink":"/2014/04/27/optimal-taleo-settings"}},"content":"Have more emails in your inbox than you can read? Sometimes those all important Meeting requests can be missed \u2013 still waiting to be accepted or declined. Outlook includes Resource Scheduling options to automatically accept meeting requests in Outlook or to wait for the request to be accepted or declined manually.\\n\\n  1. Open **Outlook**\\n  2. Click **File**\\n  3. Click **Options**\\n  4. Click **Calendar**\\n  5. Scroll down to the bottom and select: **Resource Scheduling**\\n  6. Tick **automatically accept meeting requests and remove cancelled meetings**.\\n\\nNote: Un-ticking automatically accept meeting requests and remove cancelled meetings for setting your calendar meeting requests manually."},{"id":"/2014/04/27/optimal-taleo-settings","metadata":{"permalink":"/2014/04/27/optimal-taleo-settings","source":"@site/blog/2014-04-27-optimal-taleo-settings.md","title":"Optimal Taleo Settings","description":"Taleo is used for many organisations as its tool for managing job applications. These settings are Internet Explorer recommended settings for displaying Taleo with no problems.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":2.99,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Optimal Taleo Settings","tags":["Misc","Windows"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Automatically accept Meeting Requests in Outlook","permalink":"/2014/04/27/meeting-requests-in-outlook"},"nextItem":{"title":"Remote assist Xenapp users","permalink":"/2014/04/27/remote-assist-xenapp-users"}},"content":"<p class=\\"Style1\\">\\n  Taleo is used for many organisations as its tool for managing job applications. These settings are Internet Explorer recommended settings for displaying Taleo with no problems.\\n</p>\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Magnification\\"><span class=\\"toc_number toc_depth_1\\">1</span> Magnification</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Configuring_Cookie_Policy_for_Internet_Explorer\\"><span class=\\"toc_number toc_depth_1\\">2</span> Configuring Cookie Policy for Internet Explorer</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Enabling_JavaScript\\"><span class=\\"toc_number toc_depth_1\\">3</span> Enabling JavaScript</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Troubleshooting_JavaScript\\"><span class=\\"toc_number toc_depth_1\\">4</span> Troubleshooting JavaScript</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Setting_Browser_to_Cache_Web_Files\\"><span class=\\"toc_number toc_depth_1\\">5</span> Setting Browser to Cache Web Files</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Optimizing_Cache_Settings\\"><span class=\\"toc_number toc_depth_1\\">6</span> Optimizing Cache Settings</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Configuring_the_Printer\\"><span class=\\"toc_number toc_depth_1\\">7</span> Configuring the Printer</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n#### <span id=\\"Magnification\\">Magnification</span>\\n\\nThe magnification level of your Internet browser must be set to 100%.\\n  \\nUsing a setting other than 100% might produce less than optimal results depending on the Taleo product you are\\n  \\nusing and the action you are performing. For this reason, 100% is the only recommended magnification level.\\n\\n#### <span id=\\"Configuring_Cookie_Policy_for_Internet_Explorer\\">Configuring Cookie Policy for Internet Explorer</span>\\n\\nPrerequisite\\n  \\nThe default Privacy setting for Internet Explorer is Medium. This setting is normally sufficient to ensure the\\n  \\nproper handling of cookies used by Taleo Enterprise.\\n  \\nInternet Explorer > Tools > Internet Options > Privacy\\n  \\nSteps\\n  \\n1. In the Privacy tab, click Advanced.\\n  \\n2. In the Advanced Privacy Settings window, clear the Override automatic cookie handling option.\\n  \\n3. Click OK.\\n  \\n4. In the Privacy tab, click Sites.\\n  \\n5. In the Per Site Privacy Actions window, add &#8220;taleo.net&#8221; (without quotes) in the Address of Web site field.\\n  \\n6. Click Allow.\\n  \\n7. Click Done.\\n\\n#### <span id=\\"Enabling_JavaScript\\">Enabling JavaScript</span>\\n\\nJavaScript is required for the proper function and use of Taleo Enterprise.\\n  \\nPrerequisite\\n  \\nInternet Explorer > Tools > Internet Options\\n  \\nSteps\\n  \\n1. In the Security tab, click the Internet icon.\\n  \\n2. Click Default Level.\\n  \\n3. Click OK.\\n\\n#### <span id=\\"Troubleshooting_JavaScript\\">Troubleshooting JavaScript</span>\\n\\nAllows users to have the application work properly after enabling JavaScript.\\n  \\nPrerequisite\\n  \\nEnabling JavaScript must have been performed.\\n  \\nInternet Explorer > Tools > Internet Options > Security tab\\n  \\nInternet and Browser Settings\\n\\nSteps\\n  \\n1. In the Security tab, click the Internet icon.\\n  \\n2. Click Custom Level.\\n  \\n3. Under Active Scripting, under the Scripting section, click Enable or Prompt.\\n  \\n4. Click OK.\\n  \\n5. Click Yes.\\n  \\n6. Click OK.\\n  \\nResult\\n  \\nApplication should run as expected.\\n\\n#### <span id=\\"Setting_Browser_to_Cache_Web_Files\\">Setting Browser to Cache Web Files</span>\\n\\nPrerequisite\\n  \\nInternet Explorer > Tools > Internet Options > Advanced\\n  \\nSteps\\n  \\n1. Deselect Empty Temporary Internet Files Folder When Browser Is Closed.\\n  \\n2. Deselect Do not save encrypted files to disk.\\n  \\n3. Click OK.\\n  \\n4. Restart browser.\\n\\n#### <span id=\\"Optimizing_Cache_Settings\\">Optimizing Cache Settings</span>\\n\\nMany browsers retain Web site files in the \u201ccache\u201d for a certain period of time. This saves browsing time by\\n  \\naccessing the file directly from your computer&#8217;s hard drive rather than gathering it from the Internet, thereby\\n  \\nincreasing overall performance.\\n  \\nPrerequisite\\n  \\nInternet Explorer > Tools > Internet Options > General tab\\n  \\nSteps\\n  \\n1. For Internet Explorer 7 or 8, click Settings under the Browsing History section.\\n  \\n2. In the Temporary Internet and History Settings window, select Automatically, for the Check for newer\\n  \\nversion of stored pages option.\\n  \\n3. For the Disk space to use setting, select a value greater than100 MB but less than 500 MB. Setting the\\n  \\ncache size higher than 500 MB might actually reduce performance.\\n  \\n4. Click OK.\\n  \\n5. In the Internet Options window, click OK.\\n\\n#### <span id=\\"Configuring_the_Printer\\">Configuring the Printer</span>\\n\\nAllows users to configure the browser for faster printing results.\\n  \\nPrerequisite\\n  \\nInternet Explorer > Tools > Internet Options > Advanced\\n  \\nSteps\\n  \\n1. Scroll down to the Printing settings.\\n  \\n2. Select Print background colors and images.\\n  \\n3. Click OK."},{"id":"/2014/04/27/remote-assist-xenapp-users","metadata":{"permalink":"/2014/04/27/remote-assist-xenapp-users","source":"@site/blog/2014-04-27-remote-assist-xenapp-users.md","title":"Remote assist Xenapp users","description":"Due to inconsistencies in Microsoft\u2019s remote assistance and dual monitor technologies \u2013 there is an incompatibility between Server 2008 R2 and some Windows client operating systems \u2013 not allowing you to use Citrix tools to shadow and control XenApp sessions &#8211; follow the quick guide below to remote assist Xenapp users.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.255,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Remote assist Xenapp users","tags":["Windows"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Optimal Taleo Settings","permalink":"/2014/04/27/optimal-taleo-settings"},"nextItem":{"title":"Unable to Remote Desktop into Computer","permalink":"/2014/04/27/unable-to-remote-desktop-into-computer"}},"content":"Due to inconsistencies in Microsoft\u2019s remote assistance and dual monitor technologies \u2013 there is an incompatibility between Server 2008 R2 and some Windows client operating systems \u2013 not allowing you to use Citrix tools to shadow and control XenApp sessions &#8211; follow the quick guide below to remote assist Xenapp users.\\n\\n&nbsp;\\n\\nA way around this is to \u2013 publish Remote Assistance for Helpdesk and System administrator access.\\n\\n&nbsp;\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Configure_Remote_Assistance_in_XenApp\\"><span class=\\"toc_number toc_depth_1\\">1</span> Configure Remote Assistance in XenApp</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Using_Remote_Assistance_Citrix_AppCenter_to_Remote_Assist_users\\"><span class=\\"toc_number toc_depth_1\\">2</span> Using Remote Assistance & Citrix AppCenter to Remote Assist users</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n#### <span id=\\"Configure_Remote_Assistance_in_XenApp\\">Configure Remote Assistance in XenApp</span>\\n\\n\\n  1. Lal Mohan (<a title=\\"Configure-windows-remote-assistance-in-citrix-xenapp-6-5\\" href=\\"http://lalmohan.co.nz/2013/07/02/configure-windows-remote-assistance-in-citrix-xenapp-6-5-for-multi-monitor-shadowing/\\" target=\\"_blank\\">http://lalmohan.co.nz/2013/07/02/configure-windows-remote-assistance-in-citrix-xenapp-6-5-for-multi-monitor-shadowing/</a> ) has written a great blog post on setting this up \u2013 just follow the guide to configure Remote Assistance in the XenApp environment.\\n\\n\\n#### <span id=\\"Using_Remote_Assistance_Citrix_AppCenter_to_Remote_Assist_users\\">Using Remote Assistance & Citrix AppCenter to Remote Assist users</span>\\n\\n\\n  1. Log into the Citrix Application Portal\\n  2. Launch Citrix **AppCenter**\\n  3. Select **XenAppFarm**\\n  4. **Click** the **Users** tab\\n  5. **Find** the **user** you want to connect to and **check** the **Server** they are on.\\n  6. Go back to the **Application Portal** and **click** the **Remote Assistance** application\\n  7. In the Type a computer name or IP address\xa0 &#8211; **type** in the **Server** name the user is connected to and press Next\\n  8. **Select** the name of the **user** you want to connect to and select **Next** to **start** the **Remote Assistance.**"},{"id":"/2014/04/27/unable-to-remote-desktop-into-computer","metadata":{"permalink":"/2014/04/27/unable-to-remote-desktop-into-computer","source":"@site/blog/2014-04-27-unable-to-remote-desktop-into-computer.md","title":"Unable to Remote Desktop into Computer","description":"There can be various reasons why people are unable to Remote Desktop into a Computer \u2013 one of the reasons could be they are not in the Remote Desktop users group on the local machine or the necessary services \u2013 for Remote Desktop connectivity aren\u2019t enabled. Follow the guide below to make sure the services are fully enabled and up and running you can test one at a time.","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.88,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Unable to Remote Desktop into Computer","tags":["Windows"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Remote assist Xenapp users","permalink":"/2014/04/27/remote-assist-xenapp-users"},"nextItem":{"title":"Windows cannot connect to the domain","permalink":"/2014/04/27/windows-cannot-connect-to-the-domain"}},"content":"There can be various reasons why people are unable to Remote Desktop into a Computer \u2013 one of the reasons could be they are not in the Remote Desktop users group on the local machine or the necessary services \u2013 for Remote Desktop connectivity aren\u2019t enabled. Follow the guide below to make sure the services are fully enabled and up and running you can test one at a time.\\n\\n  1. Open **Run**\\n  2. Type in: **services.msc**\\n  3. Click **Action**\\n  4. Click **Connect to another computer**\\n  5. **Type** in the **hostname** of the **PC** you are connecting to and press **Enter** to connect\\n  6. In the Services list **navigate** **to** the **following** and make sure they are started by double clicking to open the Services Property box \u2013changing the Start-up type to Automatic and select Start:\\n  \\n    \u2022\xa0\xa0\xa0 Remote Access Auto Connection Manager\\n  \\n    \u2022\xa0\xa0\xa0 Remote Access Connection Manager\\n  \\n    \u2022\xa0\xa0\xa0 Routing and Remote Access\\n  7. Once those have been changed navigate to the: Windows Firewall/Internet Connection Sharing (ICS) service and change Start-up type to: Disabled and stop the service."},{"id":"/2014/04/27/windows-cannot-connect-to-the-domain","metadata":{"permalink":"/2014/04/27/windows-cannot-connect-to-the-domain","source":"@site/blog/2014-04-27-windows-cannot-connect-to-the-domain.md","title":"Windows cannot connect to the domain","description":"1. Open Active Directory Users & Computers","date":"2014-04-27T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.835,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows cannot connect to the domain","tags":["Windows"],"date":"2014-04-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Unable to Remote Desktop into Computer","permalink":"/2014/04/27/unable-to-remote-desktop-into-computer"},"nextItem":{"title":"Reset Nokia Windows phone to factory settings","permalink":"/2014/04/25/reset-nokia-windows-phone"}},"content":"1. Open **Active Directory Users & Computers**\\n  2. Right click **Computers**\\n  3. **Search** for the **computer** **name** you would like to remove and **right** **click** and select **Reset**\\n  4. Now on the **computer** **logon\xa0**using a local computer account.\\n  5. **Right** **click** Computer/My **Computer**\\n  6. Select **Properties**\\n  7. **Click** **Computer** **Name** tab and select **Change**\\n  8. **Click** on **Member of** &#8211; and **change** from **Domain** to **Workgroup** and click Ok\\n  9. Windows will prompt you to restart your Computer &#8211; select **Restart** **later**\\n 10. **Select** **Domain** and enter in the domain name you wish to reconnect to and click **Ok**\\n 11. **Enter** in your domain **credentials** to add the workstation back to the domain and click **Ok** **to** **restart** the computer.\\n\\nNote: The benefit of doing the Reset in Active Directory first &#8211; allows you to maintain the computer groups and security identifier without loosing it.\\n\\nYou can also use the following Powershell command on the local PC to restore the secure channel to the domain: `Test-ComputerSecureChannel -Repair`"},{"id":"/2014/04/25/reset-nokia-windows-phone","metadata":{"permalink":"/2014/04/25/reset-nokia-windows-phone","source":"@site/blog/2014-04-25-reset-nokia-windows-phone.md","title":"Reset Nokia Windows phone to factory settings","description":"As part of my role at work \u2013 we continuously swap the company\u2019s mobile phone \u2013 a Nokia windows phone so being able to reset Nokia windows phone to factory settings is quite useful as we store our contacts on the SIM card so restoring the phone back to factory settings before handing them on is a useful occurrence.","date":"2014-04-25T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"},{"inline":true,"label":"Windows Phone","permalink":"/tags/windows-phone"}],"readingTime":0.62,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Reset Nokia Windows phone to factory settings","tags":["Mobile","Windows Phone"],"date":"2014-04-25 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows cannot connect to the domain","permalink":"/2014/04/27/windows-cannot-connect-to-the-domain"},"nextItem":{"title":"Lync Server Control Panel is blank","permalink":"/2014/04/21/lync-server-control-panel-is-blank"}},"content":"As part of my role at work \u2013 we continuously swap the company\u2019s mobile phone \u2013 a Nokia windows phone so being able to reset Nokia windows phone to factory settings is quite useful as we store our contacts on the SIM card so restoring the phone back to factory settings before handing them on is a useful occurrence.\\n\\n**Warning:**\xa0This erases all personal content on your phone and restores the factory settings._\\n\\n_Make sure the charger is disconnected, and switch your phone off.\\n\\n  1. Press and hold the\xa0**volume down**\xa0key, and connect the charger. The screen will show an exclamation mark (**!**).\\n  2. Press the keys in the following order:\xa0**volume up**,\xa0**volume down**,\xa0**power**, and\xa0**volume down**. Your phone resets, and boots up. This may take several minutes."},{"id":"/2014/04/21/lync-server-control-panel-is-blank","metadata":{"permalink":"/2014/04/21/lync-server-control-panel-is-blank","source":"@site/blog/2014-04-21-lync-server-control-panel-is-blank.md","title":"Lync Server Control Panel is blank","description":"The Lync Server Control panel is used to control most Lync user policies and settings so when the Lync Server control is blank \u2013 it causes an issue with maintaining and configuring the Lync server.","date":"2014-04-21T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Lync Server Control Panel is blank","tags":["Windows"],"date":"2014-04-21 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Reset Nokia Windows phone to factory settings","permalink":"/2014/04/25/reset-nokia-windows-phone"},"nextItem":{"title":"Acronis 2014 How to set compression level","permalink":"/2014/04/20/acronis-2014-how-to-set-compression-level"}},"content":"The Lync Server Control panel is used to control most Lync user policies and settings so when the Lync Server control is blank \u2013 it causes an issue with maintaining and configuring the Lync server.\\n\\n  1. The Lync Server control panel\xa0relies on\xa0the [Silverlight](http://www.microsoft.com/silverlight/) dependency in order to display properly &#8211; if your Lync Server Control Panel is blank &#8211; install Silverlight and restart the control panel."},{"id":"/2014/04/20/acronis-2014-how-to-set-compression-level","metadata":{"permalink":"/2014/04/20/acronis-2014-how-to-set-compression-level","source":"@site/blog/2014-04-20-acronis-2014-how-to-set-compression-level.md","title":"Acronis 2014 How to set compression level","description":"On the Performance tab you can configure the following settings:","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.6,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Acronis 2014 How to set compression level","tags":["Windows"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Lync Server Control Panel is blank","permalink":"/2014/04/21/lync-server-control-panel-is-blank"},"nextItem":{"title":"Acronis 2014 How to setup email notifications","permalink":"/2014/04/20/acronis-2014-how-to-setup-email-notifications"}},"content":"On the Performance tab you can configure the following settings:\\n\\nCompression level\\n\\nYou can choose the compression level for a backup:\\n\\n * None &#8211; the data will be copied without any compression, which may significantly increase the backup file size.\\n * Normal &#8211; the recommended data compression level (set by default).\\n * High &#8211; higher backup file compression level, takes more time to create a backup.\\n * Maximum &#8211; maximum backup compression, but takes a long time to create a backup.\\n\\nThe optimal data compression level depends on the type of files stored in the backup. For example, even maximum compression will not significantly reduce the backup size, if the backup contains essentially compressed files, like .jpg, .pdf or .mp3"},{"id":"/2014/04/20/acronis-2014-how-to-setup-email-notifications","metadata":{"permalink":"/2014/04/20/acronis-2014-how-to-setup-email-notifications","source":"@site/blog/2014-04-20-acronis-2014-how-to-setup-email-notifications.md","title":"Acronis 2014 How to setup email notifications","description":"To configure the email notifications in Acronis:","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.455,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Acronis 2014 How to setup email notifications","tags":["Windows"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Acronis 2014 How to set compression level","permalink":"/2014/04/20/acronis-2014-how-to-set-compression-level"},"nextItem":{"title":"Acronis 2014 Validating Backup Archive","permalink":"/2014/04/20/acronis-2014-validating-backup-archive"}},"content":"To configure the email notifications in Acronis:\\n\\n1. Select the Send e-mail notifications about the operation state check box.\\n  \\n2. Configure email settings:\\n\\n* Enter the email address in the To field. You can enter several email addresses in a semicolon-delimited format.\\n* Enter the outgoing mail server (SMTP) in the Outgoing mail server (SMTP) field.\\n* Set the port of the outgoing mail server. By default the port is set to 25.\\n* If required, select the SMTP authentication check box, and then enter the user name and password in the corresponding fields.\\n\\n3. To check whether your settings are correct, click the Send test message button.\\n\\nIf the test message sending fails, then perform the following:\\n\\n1. Click Show extended settings.\\n\\n2. Configure additional email settings:\\n\\nEnter the e-mail sender address in the From field. If you are not sure what address to specify, then type any address you like in a standard format, for example aaa@bbb.com.\\n\\nChange the message subject in the Subject field, if necessary.\\n\\nSelect the Log on to incoming mail server check box.\\n\\nEnter the incoming mail server (POP3) in the POP3 server field.\\n\\nSet the port of the incoming mail server. By default the port is set to 110.\\n\\n3. Click the Send test message button again.\\n\\nAdditional notification settings:\\n\\n* To send a notification concerning process completion, select the Send notification upon operation&#8217;s successful completion check box.\\n* To send a notification concerning process failure, select the Send notification upon operation failure check box.\\n* To send a notification with operation messages, select the Send notification when user interaction is required check box.\\n* To send a notification with full log of operations, select the Add full log to the notification check box."},{"id":"/2014/04/20/acronis-2014-validating-backup-archive","metadata":{"permalink":"/2014/04/20/acronis-2014-validating-backup-archive","source":"@site/blog/2014-04-20-acronis-2014-validating-backup-archive.md","title":"Acronis 2014 Validating Backup Archive","description":"1. Open Acronis 2014","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Acronis 2014 Validating Backup Archive","tags":["Windows"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Acronis 2014 How to setup email notifications","permalink":"/2014/04/20/acronis-2014-how-to-setup-email-notifications"},"nextItem":{"title":"Common ITIL V3 Abbreviations","permalink":"/2014/04/20/common-itil-abbreviations"}},"content":"1. Open Acronis 2014\\n  2. Click on the Recovery tab\\n  3. Right-click a backup and select Validate Archive.\\n\\nTo validate a backup automatically before recovery, on the Options step of the Recovery Wizard, select the Validate backup archive before recovery check box."},{"id":"/2014/04/20/common-itil-abbreviations","metadata":{"permalink":"/2014/04/20/common-itil-abbreviations","source":"@site/blog/2014-04-20-common-itil-abbreviations.md","title":"Common ITIL V3 Abbreviations","description":"ITIL (Information Technology Infrastructure Library)\xa0and IT (Information Technology) in general is filled with lots\xa0of\xa0abbreviations which sometimes get ahead of you &#8211;\xa0I have combined a useful list of ITIL abbreviations\xa0for\xa0quickly determining what something means. Use CTRL+F to\xa0quickly open the Find dialog of your internet browser and type in the abbreviation to display the meaning.","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":2.565,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Common ITIL V3 Abbreviations","tags":["Misc"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Acronis 2014 Validating Backup Archive","permalink":"/2014/04/20/acronis-2014-validating-backup-archive"},"nextItem":{"title":"Connecting OSX 10.9 to Active Directory","permalink":"/2014/04/20/connecting-osx-10-9-to-active-directory"}},"content":"ITIL _(Information Technology Infrastructure Library)_\xa0and IT _(Information Technology)_ in general is filled with lots\xa0of\xa0abbreviations which sometimes get ahead of you &#8211;\xa0I have combined a useful list of ITIL abbreviations\xa0for\xa0quickly determining what something means. Use CTRL+F to\xa0quickly open the Find dialog of your internet browser and type in the abbreviation to display the meaning.\\n\\n**ACD** automatic call distribution\\n  \\n**AM** availability management\\n  \\n**AMIS** availability management information system\\n  \\n**ASP** application service provider\\n  \\n**AST** agreed service time\\n  \\n**BCM** business continuity management\\n  \\n**BCP** business continuity plan\\n  \\n**BIA** business impact analysis\\n  \\n**BMP** Best Management Practice\\n  \\n**BRM** business relationship manager\\n  \\n**BSI** British Standards Institution\\n  \\n**CAB** change advisory board\\n  \\n**CAPEX** capital expenditure\\n  \\n**CCM** component capacity management\\n  \\n**CFIA** component failure impact analysis\\n  \\n**CI** configuration item\\n  \\n**CMDB** configuration management database\\n  \\n**CMIS** capacity management information system\\n  \\n**CMM** capability maturity model\\n  \\n**CMMI** Capability Maturity Model Integration\\n  \\n**CMS** configuration management system\\n  \\n**COBIT** Control OBjectives for Information and related Technology\\n  \\n**COTS** commercial off the shelf\\n  \\n**CSF** critical success factor\\n  \\n**CSI** continual service improvement\\n  \\n**CTI** computer telephony integration\\n  \\n**DIKW** Data-to-Information-to-Knowledge-to-Wisdom\\n  \\n**DML** definitive media library\\n  \\n**ECAB** emergency change advisory board\\n  \\n**ELS** early life support\\n  \\n**eSCM**&#8211;**CL** eSourcing Capability Model for Client Organizations\\n  \\n**eSCM-SP** eSourcing Capability Model for Service Providers\\n  \\n**FTA** fault tree analysis\\n  \\n**IRR** internal rate of return\\n  \\n**ISG** IT steering group\\n  \\n**ISM** information security management\\n  \\n**ISMS** information security management system\\n  \\n**ISO** International Organization for Standardization\\n  \\n**ISP** internet service provider\\n  \\n**IT** information technology\\n  \\n**ITSCM IT** service continuity management\\n  \\n**ITSM IT** service management\\n  \\n**itSMF IT** Service Management Forum\\n  \\n**IVR** interactive voice response\\n  \\n**KEDB** known error database\\n  \\n**KPI** key performance indicator\\n  \\n**LOS** line of service\\n  \\n**MIS** management information system\\n  \\n**M\\\\_o\\\\_R** Management of Risk\\n  \\n**MTBF** mean time between failures\\n  \\n**MTBSI** mean time between service incidents\\n  \\n**MTRS** mean time to restore service\\n  \\n**MTTR** mean time to repair\\n  \\n**NPV** net present value\\n  \\n**OLA** operational level agreement\\n  \\n**OPEX** operational expenditure\\n  \\n**PBA** pattern of business activity\\n  \\n**PDCA** Plan-Do-Check-Act\\n  \\n**PFS** prerequisite for success\\n  \\n**PIR** post-implementation review\\n  \\n**PMBOK** Project Management Body of Knowledge\\n  \\n**PMI** Project Management Institute\\n  \\n**PMO** project management office\\n  \\n**PRINCE2** Projects IN Controlled Environments\\n  \\n**PSO** projected service outage\\n  \\n**QA** quality assurance\\n  \\n**QMS** quality management system\\n  \\n**RACI** responsible, accountable, consulted and informed\\n  \\n**RCA** root cause analysis\\n  \\n**RFC** request for change\\n  \\n**ROA** return on assets\\n  \\n**ROI** return on investment\\n  \\n**RPO** recovery point objective\\n  \\n**RTO** recovery time objective\\n  \\n**SAC** service acceptance criteria\\n  \\n**SACM** service asset and configuration management\\n  \\n**SAM** software asset management\\n  \\n**SCM** service capacity management\\n  \\n**SCMIS** supplier and contract management information system\\n  \\n**SDP** service design package\\n  \\n**SFA** service failure analysis\\n  \\n**SIP** service improvement plan\\n  \\n**SKMS** service knowledge management system\\n  \\n**SLA** service level agreement\\n  \\n**SLM** service level management\\n  \\n**SLP** service level package\\n  \\n**SLR** service level requirement\\n  \\n**SMART** specific, measurable, achievable, relevant and time-bound\\n  \\n**SMIS** security management information system\\n  \\n**SMO** service maintenance objective\\n  \\n**SoC** separation of concerns\\n  \\n**SOP** standard operating procedure\\n  \\n**SOR** statement of requirements\\n  \\n**SOX** Sarbanes-Oxley (US law)\\n  \\n**SPI** service provider interface\\n  \\n**SPM** service portfolio management\\n  \\n**SPOF** single point of failure\\n  \\n**TCO** total cost of ownership\\n  \\n**TCU** total cost of utilization\\n  \\n**TO** technical observation\\n  \\n**TOR** terms of reference\\n  \\n**TQM** total quality management\\n  \\n**UC** underpinning contract\\n  \\n**UP** user profile\\n  \\n**VBF** vital business function\\n  \\n**VOI** value on investment\\n  \\n**WIP** work in progress"},{"id":"/2014/04/20/connecting-osx-10-9-to-active-directory","metadata":{"permalink":"/2014/04/20/connecting-osx-10-9-to-active-directory","source":"@site/blog/2014-04-20-connecting-osx-10-9-to-active-directory.md","title":"Connecting OSX 10.9 to Active Directory","description":"Connecting OSX 10.9 to Active Directory domain is very easy. Follow the quick guide below:","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.345,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Connecting OSX 10.9 to Active Directory","tags":["Mac OSX"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Common ITIL V3 Abbreviations","permalink":"/2014/04/20/common-itil-abbreviations"},"nextItem":{"title":"How to add videos on Windows Media Center","permalink":"/2014/04/20/how-to-add-videos-on-windows-media-center"}},"content":"Connecting OSX 10.9 to Active Directory domain is very easy. Follow the quick guide below:\\n\\n  1. Click on the **Apple** **Menu**\\n  2. Click **System Preferences**\\n  3. Select **Users & Groups**\\n  4. Click **Login Options**\\n  5. Click **Join**\\n  6. Click **Add** and type in the name of the Active Directory domain\\n  7. **Enter** **in** an **Active** **Directory** user **credentials** with domain rights to add workstations to the\xa0domain\\n  8. Click **Ok**"},{"id":"/2014/04/20/how-to-add-videos-on-windows-media-center","metadata":{"permalink":"/2014/04/20/how-to-add-videos-on-windows-media-center","source":"@site/blog/2014-04-20-how-to-add-videos-on-windows-media-center.md","title":"How to add videos on Windows Media Center","description":"1. From the Media Center Start menu, select My Videos.","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.4,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to add videos on Windows Media Center","tags":["Windows"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Connecting OSX 10.9 to Active Directory","permalink":"/2014/04/20/connecting-osx-10-9-to-active-directory"},"nextItem":{"title":"How to use Lync in Windows 8","permalink":"/2014/04/20/lync-in-windows-8"}},"content":"1. From the Media Center Start menu, select My Videos.\\n  2. Select Find Videos.\\n  3. Select Add folders, and then select Next.\\n  4. Do one, or both, of the following:\\n\\n  * To add videos that are on the Media Center PC, select Add folders on this computer.\\n  * To add videos\xa0that are on another computer on the network, select Add shared folders from another\xa0computer.\\n\\n5. Select the folder or folders that contain the videos, select Next, and then select Finish."},{"id":"/2014/04/20/lync-in-windows-8","metadata":{"permalink":"/2014/04/20/lync-in-windows-8","source":"@site/blog/2014-04-20-lync-in-windows-8.md","title":"How to use Lync in Windows 8","description":"Microsoft have released a Lync Metro App &#8211; available free through the Windows\xa0Store\xa0&#8211; Lync in Windows 8.","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to use Lync in Windows 8","tags":["Windows"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to add videos on Windows Media Center","permalink":"/2014/04/20/how-to-add-videos-on-windows-media-center"},"nextItem":{"title":"How to setup an Exchange mailbox on a mac","permalink":"/2014/04/20/setup-exchange-email-account-on-mac"}},"content":"Microsoft have released a Lync Metro App &#8211; available free through the <a title=\\"Microsoft Store - Lync\\" href=\\"http://apps.microsoft.com/windows/en-nz/app/lync/ba4b9485-8712-41ff-a9ea-6243a3e07682\\" target=\\"_blank\\">Windows\xa0Store</a>\xa0&#8211; Lync in Windows 8.\\n\\nOn first run &#8211; select if you want to run Lync in the background &#8211; this means that Lync will run regardless of whether you have opened it up first.\\n\\nYou can also use this Lync Application to join as an attendee to an existing Lync Meeting.\\n\\n  1. Type in your sign-in address &#8211; example@example.com\\n  2. It will then launch your username and password for example &#8211; domainusername.\\n  3. Type in your organisations domain name and your username and password and click Ok to login."},{"id":"/2014/04/20/setup-exchange-email-account-on-mac","metadata":{"permalink":"/2014/04/20/setup-exchange-email-account-on-mac","source":"@site/blog/2014-04-20-setup-exchange-email-account-on-mac.md","title":"How to setup an Exchange mailbox on a mac","description":"Having access to an exchange mailbox on a Mac or iMac is very useful \u2013 especially if you are a designer in a prominent Windows domain environment \u2013 setup exchange email account on mac is incredibly easy on the client site \u2013 as long as the relevant ports have been opened up allowing you to connect from outside a Windows domain.","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.9,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to setup an Exchange mailbox on a mac","date":"2014-04-20 00:00:00 +1300","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to use Lync in Windows 8","permalink":"/2014/04/20/lync-in-windows-8"},"nextItem":{"title":"Share files on a Mac with Windows","permalink":"/2014/04/20/share-files-on-a-mac-with-windows"}},"content":"Having access to an exchange mailbox on a Mac or iMac is very useful \u2013 especially if you are a designer in a prominent Windows domain environment \u2013 setup exchange email account on mac is incredibly easy on the client site \u2013 as long as the relevant ports have been opened up allowing you to connect from outside a Windows domain.\\n\\n  1. Open the\xa0**Apple** menu\\n  2. Click\xa0**System** **Preferences**\\n  3. Click **Internet Accounts**.\\n  4. Click the **Add (+)** button.\\n  5. Click **Exchange** in the list of services.\\n  6. **Enter** the **information**.\\n  7. Click **Continue**.\\n  8. **Enter** a **description** **for** the **account** (for example, Work or Exchange).\\n  9. In the **Server** **Address** field, **enter** the **fully** **qualified** **domain** **name** (FQDN) for the companies\xa0exchange server.\\n 10. Click **Continue**.\\n 11. Make sure the Contacts and Calendar checkboxes are selected to automatically set up those apps.\\n 12. Click **Done**.\\n\\nNote: To access an Exchange server from a Mac outside the organization\u2019s network, confirm with the server administrator that port 443 is open on the firewall and Exchange Web Services (EWS) is enabled on the server."},{"id":"/2014/04/20/share-files-on-a-mac-with-windows","metadata":{"permalink":"/2014/04/20/share-files-on-a-mac-with-windows","source":"@site/blog/2014-04-20-share-files-on-a-mac-with-windows.md","title":"Share files on a Mac with Windows","description":"Share files on a Mac with Windows can be extremely useful and luckily for us extremely easy to setup and manage.","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Share files on a Mac with Windows","tags":["Mac OSX"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to setup an Exchange mailbox on a mac","permalink":"/2014/04/20/setup-exchange-email-account-on-mac"},"nextItem":{"title":"Update caching on OSX Server","permalink":"/2014/04/20/update-caching-on-osx-server"}},"content":"Share files on a Mac with Windows can be extremely useful and luckily for us extremely easy to setup and manage.\\n\\n  1. Choose **Apple** menu\\n  2. Select **System Preferences**, then click **Sharing**.\\n  3. Select **File Sharing**, then click **Options**.\\n  4. Select \u201c**Share files and folders using SMB (Windows)**\u201d\\n  5. **Select** a\xa0**user** **group** or **users** you would like to share with and click **Ok**\\n  6. Click **Done**."},{"id":"/2014/04/20/update-caching-on-osx-server","metadata":{"permalink":"/2014/04/20/update-caching-on-osx-server","source":"@site/blog/2014-04-20-update-caching-on-osx-server.md","title":"Update caching on OSX Server","description":"Update caching on OSX server can be easily managed through the Server application.","date":"2014-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Update caching on OSX Server","tags":["Mac OSX"],"date":"2014-04-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Share files on a Mac with Windows","permalink":"/2014/04/20/share-files-on-a-mac-with-windows"},"nextItem":{"title":"How to point Active Directory Administrative Center to another domain in Server 2012","permalink":"/2014/04/05/how-to-point-active-directory-administrative-center-to-another-domain-in-server-2012"}},"content":"Update caching on OSX server can be easily managed through the Server application.\\n\\nIt is recommended to turn on updating caching to allow osx clients to be able to draw Apple updates from the server without having to branch out onto the internet \u2013 decreasing broadband data usage\xa0and network traffic.\\n\\n  1. Navigate to:\xa0**Applications**\\n  2. Open **Server**\\n  3. Select **Caching**\\n  4. Click **On/Off** to turn on the Software Update caching service"},{"id":"/2014/04/05/how-to-point-active-directory-administrative-center-to-another-domain-in-server-2012","metadata":{"permalink":"/2014/04/05/how-to-point-active-directory-administrative-center-to-another-domain-in-server-2012","source":"@site/blog/2014-04-05-how-to-point-active-directory-administrative-center-to-another-domain-in-server-2012.md","title":"How to point Active Directory Administrative Center to another domain in Server 2012","description":"1. Click Start","date":"2014-04-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.395,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to point Active Directory Administrative Center to another domain in Server 2012","tags":["Windows"],"date":"2014-04-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Update caching on OSX Server","permalink":"/2014/04/20/update-caching-on-osx-server"},"nextItem":{"title":"Outlook 2010 People Pane not showing information","permalink":"/2014/03/30/outlook-2010-people-pane-not-showing-information"}},"content":"1. Click **Start**\\n  2. Click **Administrative Tools**\\n  3. Click\xa0**Active Directory Administrative Center**\\n  4. Click on **Add Navigation Node**\\n  5. Click **Connect to other domains..** _(on the lower right of the window)\\n  6. Type the **domain name** you want to connect to and click **ok**.\\n\\nNote: This is only valid for trusted federated domains; this method of connecting uses your local credentials and in order to run as separate credentials you will need to launch the Administrative Center using Runas."},{"id":"/2014/03/30/outlook-2010-people-pane-not-showing-information","metadata":{"permalink":"/2014/03/30/outlook-2010-people-pane-not-showing-information","source":"@site/blog/2014-03-30-outlook-2010-people-pane-not-showing-information.md","title":"Outlook 2010 People Pane not showing information","description":"This fix is usually valid for an Exchange domain setup. The fix is to enable Cached mode on the Outlook client.","date":"2014-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.44,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Outlook 2010 People Pane not showing information","tags":["Windows"],"date":"2014-03-30 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to point Active Directory Administrative Center to another domain in Server 2012","permalink":"/2014/04/05/how-to-point-active-directory-administrative-center-to-another-domain-in-server-2012"},"nextItem":{"title":"How to export local group policy settings into a HTML report","permalink":"/win/how-to-export-local-group-policy-settings-into-a-html-report"}},"content":"This fix is usually valid for an Exchange domain setup. The fix is to enable Cached mode on the Outlook client.\\n\\n  1. Open Outlook\\n  2. Click File\\n  3. Click Account Settings\\n  4. Click Account Settings\\n  5. In the email tab select your email account and select Change\\n  6. Select Use Cached Exchange Mode\\n  7. Select Next and Next\\n\\nThe Outlook client will begin to sync and create an offline cache of the exchange email and address book. Allowing for the People Pane to gather and display appropriate information."},{"id":"win/how-to-export-local-group-policy-settings-into-a-html-report","metadata":{"permalink":"/win/how-to-export-local-group-policy-settings-into-a-html-report","source":"@site/blog/2014-03-28-how-to-export-local-group-policy-settings-into-a-html-report.md","title":"How to export local group policy settings into a HTML report","description":"1. Press the Windows Key + R to open the Run dialogue box","date":"2014-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to export local group policy settings into a HTML report","tags":["Windows"],"date":"2014-03-28 00:00:00 +1300","slug":"win/how-to-export-local-group-policy-settings-into-a-html-report"},"unlisted":false,"prevItem":{"title":"Outlook 2010 People Pane not showing information","permalink":"/2014/03/30/outlook-2010-people-pane-not-showing-information"},"nextItem":{"title":".NET Framework not installing through SCCM","permalink":"/2014/03/28/net-framework-not-installing-through-sccm"}},"content":"1. Press the Windows Key + R to open the Run dialogue box\\n  2. Type in cmd and press Enter\\n  3. This will load the Windows Command Prompt\\n  4. Using the CD _(change directory)_ command \u2013 change your directory to where you want to save the report&#8230; for example, \u201ccd Desktop\u201d will change to your Desktop.\\n  5. Type in: **gpresult\xa0/h LocalGrpPolReport.html** and press Enter\\n\\nThis will then load the group policy snap-in in the background and generate an HTML page called: &#8220;LocalGrpPolReport.html&#8221; that you will be able to open and see what policies have been applied to that local machine and which policies are taking precedence over others."},{"id":"/2014/03/28/net-framework-not-installing-through-sccm","metadata":{"permalink":"/2014/03/28/net-framework-not-installing-through-sccm","source":"@site/blog/2014-03-28-net-framework-not-installing-through-sccm.md","title":".NET Framework not installing through SCCM","description":"1. Click Start","date":"2014-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.245,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":".NET Framework not installing through SCCM","tags":["Windows"],"authors":["Luke"],"date":"2014-03-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to export local group policy settings into a HTML report","permalink":"/win/how-to-export-local-group-policy-settings-into-a-html-report"},"nextItem":{"title":"Unable to right click in Microsoft Excel","permalink":"/2014/03/28/unable-to-right-click-in-microsoft-excel"}},"content":"1. Click **Start**\\n2. Click **Run** and type in: **services.msc**\\n3. **Stop** the **Automatic Update** service\\n4. **Navigate** to: **C:\\\\Windows**\\n5. **Delete** the: SoftwareDistribution folder\\n6. **Start** the **Automatic Update** service\\n7. Double click on the SCCM Update icon in the notification tray and select Install & Install now."},{"id":"/2014/03/28/unable-to-right-click-in-microsoft-excel","metadata":{"permalink":"/2014/03/28/unable-to-right-click-in-microsoft-excel","source":"@site/blog/2014-03-28-unable-to-right-click-in-microsoft-excel.md","title":"Unable to right click in Microsoft Excel","description":"This is fixed by removing the Excel11.xlb excel template file located in the following locations:","date":"2014-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.24,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Unable to right click in Microsoft Excel","tags":["Windows"]},"unlisted":false,"prevItem":{"title":".NET Framework not installing through SCCM","permalink":"/2014/03/28/net-framework-not-installing-through-sccm"},"nextItem":{"title":"Windows Server 2012 - This computer\u2019s hardware may not support booting to this disk.","permalink":"/2014/03/28/windows-server-2012-this-computers-hardware-may-not-support-booting-to-this-disk"}},"content":"This is fixed by removing the Excel11.xlb excel template file located in the following locations:\\n\\nWin7 32bit:\\n\\nC:/Program Files/Microsoft Office/Office14/XLSTART\\n\\nC:/Users/UserName/AppData/Roaming/Microsoft/Excel/XLSTART\\n\\nWin7 64bit:\\n\\nC:/Program Files(x86)/Microsoft Office/Office14/XLSTART\\n\\nC:/Users/UserName/AppData/Roaming/MicrosoftExcel/XLSTART\\n\\nNote: Make sure Excel is closed before attempted to remove this file \u2013 Excel will automatically generate a new Excel11.xlb template."},{"id":"/2014/03/28/windows-server-2012-this-computers-hardware-may-not-support-booting-to-this-disk","metadata":{"permalink":"/2014/03/28/windows-server-2012-this-computers-hardware-may-not-support-booting-to-this-disk","source":"@site/blog/2014-03-28-windows-server-2012-this-computers-hardware-may-not-support-booting-to-this-disk.md","title":"Windows Server 2012 - This computer\u2019s hardware may not support booting to this disk.","description":"This usually occurs when you are using a new SATA or solid-state drive that hasn&#8217;t been initialized yet.","date":"2014-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.315,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Server 2012 - This computer\u2019s hardware may not support booting to this disk.","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Unable to right click in Microsoft Excel","permalink":"/2014/03/28/unable-to-right-click-in-microsoft-excel"},"nextItem":{"title":"Clear Print Queue from Printer Installer","permalink":"/2014/03/27/clear-print-queue-from-printer-installer"}},"content":"This usually occurs when you are using a new SATA or solid-state drive that hasn&#8217;t been initialized yet.\\n\\n  1. To fix this issue &#8211; use the Windows Server 2012 device manager tools during setup to create the partition on the hard drive that you are attempting to install to. You can use this opportunity to layout any extra partitions that you may need."},{"id":"/2014/03/27/clear-print-queue-from-printer-installer","metadata":{"permalink":"/2014/03/27/clear-print-queue-from-printer-installer","source":"@site/blog/2014-03-27-clear-print-queue-from-printer-installer.md","title":"Clear Print Queue from Printer Installer","description":"1. Open the Printer Installer admin\xa0interface","date":"2014-03-27T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.41,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Clear Print Queue from Printer Installer","tags":["Misc","Windows"],"date":"2014-03-27 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows Server 2012 - This computer\u2019s hardware may not support booting to this disk.","permalink":"/2014/03/28/windows-server-2012-this-computers-hardware-may-not-support-booting-to-this-disk"},"nextItem":{"title":"How to remove user accounts from Windows 7 registry after being incorrectly removed","permalink":"/2014/03/25/user-accounts-from-win7-registry-after-being-incorrectly-removed"}},"content":"1. <b style=\\"line-height: 1.5em\\">Open</b> <span style=\\"line-height: 1.5em\\">the </span><b style=\\"line-height: 1.5em\\">Printer Installer </b>admin\xa0interface\\n  2. **Login** with your Printer Installer details.\\n  3. You are now logged into the Printer Installer Admin panel\\n  4. In the **search** field up the top \u2013 type in the Print **Queue** **name** of the printer \u2013 once found select to bring up the printer\u2019s configuration.\\n  5. **Click** on **Queue** (top menu bar)\\n  6. **Select** \u2013 **Delete all** \u2013 to delete all queued jobs. This could take a few minutes."},{"id":"/2014/03/25/user-accounts-from-win7-registry-after-being-incorrectly-removed","metadata":{"permalink":"/2014/03/25/user-accounts-from-win7-registry-after-being-incorrectly-removed","source":"@site/blog/2014-03-25-user-accounts-from-win7-registry-after-being-incorrectly-removed.md","title":"How to remove user accounts from Windows 7 registry after being incorrectly removed","description":"Note: This usually needs to be done when the Windows 7 profile has been manually deleted and not properly removed \u2013 as Windows 7 uses a security identifier attached to the user account in the registry and this needs to be cleared as well_","date":"2014-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to remove user accounts from Windows 7 registry after being incorrectly removed","tags":["Windows"],"date":"2014-03-25 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Clear Print Queue from Printer Installer","permalink":"/2014/03/27/clear-print-queue-from-printer-installer"},"nextItem":{"title":"Filebot is removing media file extensions","permalink":"/2014/03/23/filebot-is-removing-media-file-extensions"}},"content":"Note: This usually needs to be done when the Windows 7 profile has been manually deleted and not properly removed \u2013 as Windows 7 uses a security identifier attached to the user account in the registry and this needs to be cleared as well_\\n\\nNote: It is also recommend making a backup of the registry before this change \u2013 and you may need to restart the PC for the registry entry to be able to be removed.\\n\\n  1. Press Windows Key + R to open the **Run** dialog window.\\n  2. Type in: **regedit** and press Enter or click Ok\\n  3. Press yes to accept UAC prompt.\\n  4. Navigate to: **HKEY\\\\_LOCAL\\\\_MACHINESOFTWAREMicrosoftWindows NTCurrentVersionProfileList******\\n  5. ******This lists the SID (security identifier) that is linked to your account \u2013 scroll down the list and on the right hand side under CentralProfile and ProfileImagePath find the correct user profile you would like to remove.******\\n  6. ******Once you have found the correct one \u2013 right click the SID key you would like to remove and select Delete.******\\n  7. **Exit out of the Registry Editor and restart the workstation \u2013 the user should now be able to login and generate a new profile folder and security identifier.**"},{"id":"/2014/03/23/filebot-is-removing-media-file-extensions","metadata":{"permalink":"/2014/03/23/filebot-is-removing-media-file-extensions","source":"@site/blog/2014-03-23-filebot-is-removing-media-file-extensions.md","title":"Filebot is removing media file extensions","description":"Although frustrating this fix is a simple one!","date":"2014-03-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Filebot is removing media file extensions","tags":["Windows"],"date":"2014-03-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to remove user accounts from Windows 7 registry after being incorrectly removed","permalink":"/2014/03/25/user-accounts-from-win7-registry-after-being-incorrectly-removed"},"nextItem":{"title":"How to mount a VHD in Windows 7","permalink":"/2014/03/06/how-to-mount-a-vhd-in-windows-7"}},"content":"Although frustrating this fix is a simple one!\\n\\n  1. Navigate to Rename Options\\n  2. Change Extensions: Override to: Extensions: Preserve"},{"id":"/2014/03/06/how-to-mount-a-vhd-in-windows-7","metadata":{"permalink":"/2014/03/06/how-to-mount-a-vhd-in-windows-7","source":"@site/blog/2014-03-06-how-to-mount-a-vhd-in-windows-7.md","title":"How to mount a VHD in Windows 7","description":"Mounting a VHD (Virtual Hard Disk) in Windows 7 is extremely useful and this technique can be used to recover files from a Windows system image.","date":"2014-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to mount a VHD in Windows 7","tags":["Windows"],"date":"2014-03-06 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Filebot is removing media file extensions","permalink":"/2014/03/23/filebot-is-removing-media-file-extensions"},"nextItem":{"title":"How to delete Autocomplete entries in Chrome","permalink":"/2014/03/01/how-to-delete-autocomplete-entries-in-chrome"}},"content":"Mounting a VHD _(Virtual Hard Disk)_ in Windows 7 is extremely useful and this technique can be used to recover files from a Windows system image. \\n\\n  1. Click Start\\n  2. Right click Computer\\n  3. Left click Manage\\n  4. Left click on Disk Management\\n  5. Left click on Action\\n  6. Left click on Attach VHD\\n  7. Navigate to location of the VHD file and click Ok"},{"id":"/2014/03/01/how-to-delete-autocomplete-entries-in-chrome","metadata":{"permalink":"/2014/03/01/how-to-delete-autocomplete-entries-in-chrome","source":"@site/blog/2014-03-01-how-to-delete-autocomplete-entries-in-chrome.md","title":"How to delete Autocomplete entries in Chrome","description":"Deleting the Auto Complete entries in Google Chrome is very useful \u2013 especially if you are like me and wasn\'t paying attention and misspelt a username that would have forever been popping up in the login field.","date":"2014-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.615,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to delete Autocomplete entries in Chrome","tags":["Mac OSX","Windows"],"date":"2014-03-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to mount a VHD in Windows 7","permalink":"/2014/03/06/how-to-mount-a-vhd-in-windows-7"},"nextItem":{"title":"How to make iOS 7 easier to read","permalink":"/2014/03/01/how-to-make-ios-7-easier-to-read"}},"content":"Deleting the Auto Complete entries in Google Chrome is very useful \u2013 especially if you are like me and wasn\'t paying attention and misspelt a username that would have forever been popping up in the login field.\\n\\n  1. Open Chrome\\n  2. Navigate to the login webpage of the autocomplete details you would like to remove.\\n  3. Start typing to bring up the name you want to remove\\n  4. Using the Arrow keys navigate to select the entry you would like to remove\\n  5. Press SHIFT+DELETE _(if you are using OSX \u2013 you can press FN+SHIFT+DEL)_ keys at the same time to permanently remove these details from Google Chromes cache.\\n\\nYou have now stopped that autocomplete entry from forever reminding you of that misspell!"},{"id":"/2014/03/01/how-to-make-ios-7-easier-to-read","metadata":{"permalink":"/2014/03/01/how-to-make-ios-7-easier-to-read","source":"@site/blog/2014-03-01-how-to-make-ios-7-easier-to-read.md","title":"How to make iOS 7 easier to read","description":"1. Open Settings","date":"2014-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.22,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to make iOS 7 easier to read","tags":["Mac OSX"],"date":"2014-03-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to delete Autocomplete entries in Chrome","permalink":"/2014/03/01/how-to-delete-autocomplete-entries-in-chrome"},"nextItem":{"title":"How to re enable gadgets in Windows 7","permalink":"/2014/03/01/how-to-re-enable-gadgets-in-windows-7"}},"content":"1. Open **Settings**\\n2. Select **General**\\n3. Select **Accessibility\\n4. Swipe to **Enable Bold Text**\\n5. Select and **swipe** the **Larger**\\n6. **Type** option to adjust text size.\\n7. Select and **swipe** the **Increase**\\n8. **Contrast** option to make the text easier to read."},{"id":"/2014/03/01/how-to-re-enable-gadgets-in-windows-7","metadata":{"permalink":"/2014/03/01/how-to-re-enable-gadgets-in-windows-7","source":"@site/blog/2014-03-01-how-to-re-enable-gadgets-in-windows-7.md","title":"How to re enable gadgets in Windows 7","description":"1. Click Start","date":"2014-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.5,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to re enable gadgets in Windows 7","tags":["Windows"],"date":"2014-03-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to make iOS 7 easier to read","permalink":"/2014/03/01/how-to-make-ios-7-easier-to-read"},"nextItem":{"title":"OneNote 2013 \u2013 Convert Handwriting to Text","permalink":"/2014/02/28/onenote-2013-convert-handwriting-to-text"}},"content":"1. Click Start\\n  2. Click Control Panel\\n  3. Click Program and Features\\n  4. Click Uninstall a Program\\n  5. This will now launch the installed programs list\\n  6. Click &#8220;Turn Windows Features On and Off&#8221; on the left hand menu\\n  7. Check Windows Gadget Platform\\n  8. Press Ok\\n  9. Your PC will restart and gadgets will be re-enabled.\\n\\n_Note: Although originally enabled in Windows 7 \u2013 Microsoft had deactivated the gadgets due to unstable/malicious code that allowed unsavoury individuals remote access to the PC. If re-enabled make you are running the latest Windows Update and the Windows Firewall service is enabled._"},{"id":"/2014/02/28/onenote-2013-convert-handwriting-to-text","metadata":{"permalink":"/2014/02/28/onenote-2013-convert-handwriting-to-text","source":"@site/blog/2014-02-28-onenote-2013-convert-handwriting-to-text.md","title":"OneNote 2013 \u2013 Convert Handwriting to Text","description":"1. Click on Draw","date":"2014-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.43,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OneNote 2013 \u2013 Convert Handwriting to Text","tags":["Windows"],"date":"2014-02-28 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to re enable gadgets in Windows 7","permalink":"/2014/03/01/how-to-re-enable-gadgets-in-windows-7"},"nextItem":{"title":"Guide to using Hyper-V","permalink":"/2014/02/23/guide-to-using-hyper-v"}},"content":"1. Click on Draw\\n  2. Click on Type\\n  3. Drag over your handwritten notes and click Ink to Text _(you can also right your click your selection and choose Ink to Text)_\\n  4. It will then convert your selection to Text\\n\\nNote if your handwriting doesn&#8217;t convert you can follow the guide below:\\n\\n  1. Click on Draw\\n  2. Click on Lasso tool\\n  3. Select your handwritten text\\n  4. Right click\\n  5. Select Treat Select Ink As\\n  6. Click Handwriting\\n  7. Attempt the Ink to Text again."},{"id":"/2014/02/23/guide-to-using-hyper-v","metadata":{"permalink":"/2014/02/23/guide-to-using-hyper-v","source":"@site/blog/2014-02-23-guide-to-using-hyper-v.md","title":"Guide to using Hyper-V","description":"Introduction to Hyper-V","date":"2014-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":4.615,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Guide to using Hyper-V","image":"/wp-content/uploads/2014/02/Hyper-V-logo.png","tags":["Windows"],"date":"2014-02-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"OneNote 2013 \u2013 Convert Handwriting to Text","permalink":"/2014/02/28/onenote-2013-convert-handwriting-to-text"},"nextItem":{"title":"How to WOL an entire IP range","permalink":"/2014/01/26/how-to-wol-an-entire-ip-range"}},"content":"### Introduction to Hyper-V\\n\\nHyper-V is an emulation/hyper-visor Microsoft developed technology \u2013 similar to VMWare Workstation or ESXI that allows you to run Virtual Machines and different workloads simultaneously. Hyper-V is currently available in Windows 8, Windows Server 2008, Windows Server 2008 R2, Windows 8.1 & Windows Server 2012, Windows Server 2012 R2. The Hyper-V hypervisor allows multiple workloads to run on the same physical hardware that in the past would have otherwise only been suitable for one workload \u2013 allowing for power and resource efficiency.\\n\\n### Hyper-V Specifications\\n\\n**Host operating system:**\\n\\nTo install the Hyper-V role, Windows Server 2008, Windows Server 2008 R2 Standard, Enterprise or Datacentre edition, Windows Server 2012 Standard or Datacentre edition, or Windows 8 (or 8.1) Pro or Enterprise edition is required. Hyper-V is only supported on x86-64 variants of Windows. It can be installed regardless of whether the installation is a full or core installation. **Processor:\xa0**An x86-64 processor\\n\\nHardware-assisted virtualization support: This is available in processors that include a virtualization option; specifically, Intel VT or AMD Virtualization (AMD-V, formerly code-named &#8220;Pacifica&#8221;).\\n\\nA NX bit-compatible CPU must be available and Hardware Data Execution Prevention (DEP) must be enabled.\\n\\nAlthough this is not an official requirement, Windows Server 2008 R2 and a CPU with second-level address translation support are recommended for workstations.\\n\\nSecond-level address translation is a mandatory requirement for Hyper-V in Windows 8\\n\\n**Memory**\\n\\nMinimum 2 GB. (Each virtual machine requires its own memory, and so realistically much more.)\\n\\nWindows Server 2008 Standard (x64) Hyper-V full GUI or Core supports up to 31 GB of memory for running VMs, plus 1 GB for the Hyper-V parent OS.]\\n\\nMaximum total memory per system for Windows Server 2008 R2 hosts: 32 GB (Standard) or 2 TB (Enterprise, Datacentre)\\n\\nMaximum total memory per system for Windows Server 2012 hosts: 4 TB\\n\\n### Guest operating systems\\n\\nHyper-V in Windows Server 2008 and 2008 R2 supports virtual machines with up to 4 processors each (1, 2, or 4 processors depending on guest OS-see below)\\n\\nHyper-V in Windows Server 2012 supports virtual machines with up to 64 processors each.\\n\\nHyper-V in Windows Server 2008 and 2008 R2 supports up to 384 VMs per system.\\n\\nHyper-V in Windows Server 2012 supports up to 1024 active virtual machines per system.\\n\\nHyper-V supports both 32-bit (x86) and 64-bit (x64) guest VMs.\\n\\n#### Improvements of Hyper-V in Windows Server 2012\\n\\nHyper-V Extensible Virtual Switch\\n\\nNetwork virtualization\\n\\nMulti-tenancy\\n\\nStorage Resource Pools\\n\\n.vhdx disk format supporting virtual hard disks as large as 64 TB with power failure resiliency\\n\\nVirtual Fibre Channel\\n\\nOffloaded data transfer\\n\\nHyper-V replica\\n\\nCross-premise connectivity\\n\\nCloud backup\\n\\n#### Installing Hyper-V in Windows 8 & Windows 8.1\\n\\n  1. Navigate to **Control Panel** & select **Uninstall a Program** underneath programs\\n  2. Select **Turn Windows Features On or Off**\\n  3. Check **Hyper-V & Hyper-V Platform** and select additional relevant features \u2013 I would HIGHLY recommend Hyper-V GUI Management Tools.\\n  4. Windows will go through and install the Hyper-V modules onto the Windows 8 workstation. Once completed the workstation will need a restart and you will be able to now fully utilise Hyper-V!\\n\\n#### Installing Hyper-V for Windows Server 2012\\n\\n  1. Open **Server Manager**\\n  2. Click **Add Roles and Features** and click **Next**\\n  3. Select **Hyper-V Role** and click **Next**\\n  4. Select the appropriate NIC (network interface card) for live migration and select **Next**\\n  5. Select the default locations for your VHD (virtual hard disks) and VM (virtual machine) configuration files to be held and select **Next**\\n  6. Once the Hyper-V role has been fully installed \u2013 restart the Windows Server 2012 machine. You should now have access to the Hyper-V Manager.\\n\\n### Hyper-V Tips and Tricks\\n\\n#### How to stop and restart the Hyper-V service\\n\\n  1. Open **Hyper-V Manager**\\n  2. Select the relevant Hyper-V server\\n  3. Select Stop Service\\n\\n#### How to create a new Virtual Machine in Hyper-V\\n\\n  1. Open **Hyper-V Manager**\\n  2. Select **New** and **Virtual Machine**\\n  3. The before you Begin Wizard will now appear \u2013 click **Next**\\n  4. Choose a name for your Virtual Machine and verify the storage location is for the virtual machine is appropriate and click **Next**\\n  5. Here you can select the &#8220;generation&#8221; of the Virtual Machine.\\n  6. **Generation 1** \u2013 These are for 32bit legacy based systems usually used for Windows 7 and Windows XP/Linux virtual machines.\\n  7. **Generation 2** \u2013 These are for Windows Server 2012 or 64bit versions of Windows 8\\n  8. <em style=\\"font-family: sans-serif; font-size: medium;\\">NOTE: Once Virtual Machine generation has been selected \u2013 YOU CANNOT change it.</em>\\n  9. <em style=\\"font-family: sans-serif; font-size: medium;\\">NOTE: Using </em>Convert-VMGeneration <em style=\\"font-family: sans-serif; font-size: medium;\\">script you can convert a Generation 1 VM to Generation 2.</em>\\n 10. You can now **select** the **RAM** amount you would like to delegate to your Virtual Machine and select **Next**\\n 11. If you have VLAN setup you can set your Virtual Machine to use the connection \u2013 I don&#8217;t so I am just going to click **Next**\\n 12. Now you can specify the appropriate VHD (Virtual Hard Disk) for the Virtual Machine to use \u2013 you can either create a new one or use an existing VHD.\\n 13. This is where you **specify** the **path** to your installation media for your virtual machine \u2013 such as an OS (operating system) ISO.\\n 14. You have finally setup your new Virtual Machine! Verify all the settings are correct and click **Finish** to configure and generate your new Virtual Machines.\\n 15. You can now **right click** on your **Virtual Machine** under the Hyper-V Manager and **select Connect\u2026** to **start** it."},{"id":"/2014/01/26/how-to-wol-an-entire-ip-range","metadata":{"permalink":"/2014/01/26/how-to-wol-an-entire-ip-range","source":"@site/blog/2014-01-26-how-to-wol-an-entire-ip-range.md","title":"How to WOL an entire IP range","description":"WOL (Wake on Lan) functions \u2013 can be extremely useful especially in remote management and deployment of resources. I have come across a neat little free utility named: SoftPerfect Network Scanner which although being freeware comes with a manner of useful functionality from small to large WANs so I recommend to anyone having a flick through the utility.","date":"2014-01-26T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.46,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to WOL an entire IP range","tags":["Windows"],"date":"2014-01-26 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Guide to using Hyper-V","permalink":"/2014/02/23/guide-to-using-hyper-v"},"nextItem":{"title":"Setup Parental Control on a TP-Link ADSL router","permalink":"/2014/01/26/parental-control-on-a-tp-link"}},"content":"WOL (Wake on Lan) functions \u2013 can be extremely useful especially in remote management and deployment of resources. I have come across a neat little free utility named: <a href=\\"http://www.softperfect.com/products/networkscanner/\\" target=\\"_blank\\">SoftPerfect Network Scanner</a> which although being freeware comes with a manner of useful functionality from small to large WANs so I recommend to anyone having a flick through the utility. \\n\\n  1. First download the utility &#8211; <a href=\\"http://www.softperfect.com/products/networkscanner/\\" target=\\"_blank\\">SoftPerfect Network Scanner</a> and save to an easily locatable location _\u2013 ie Desktop or My Documents/Downloads_. \\n  2. Extract the netscan zip file. There are 2 folders \u2013 one for 32 or one for 64 bit. \\n  3. Open netscan.exe\\n  4. In the Range from fields type the IP range you would like to scan \u2013 ie 192.168.1.1 to 192.168.1.255 and press Start Scanning to the right. \\n  5. Depending on the size of your network \u2013 this may take anywhere from a few seconds to minutes. \\n  6. Once completed press Ctrl+A to select all devices _(You can also go into View, Quick Filter Hosts & select Shared Folders to help distinguish what machines are actually workstations)._ \\n  7. Right click and select Wake-On-LAN, Send Wake-On-LAN signal to send the WOL packet to the workstations in the IP range. \\n\\nNote: What I would recommend doing \u2013 is doing this process while you know the workstations have been started and instead of sending the Wake-On-LAN signal in Step 8 \u2013 select Save MAC to WOL Manager. You can then go to Options on the top menu of the Network Scanner and select Wake-On-LAN Manager \u2013 here you can add more workstations \u2013 clear the workstations by hostname/mac address and subnet that you do not want to include and specify a delay in sending out the Wake-On-LAN packets between workstations."},{"id":"/2014/01/26/parental-control-on-a-tp-link","metadata":{"permalink":"/2014/01/26/parental-control-on-a-tp-link","source":"@site/blog/2014-01-26-parental-control-on-a-tp-link.md","title":"Setup Parental Control on a TP-Link ADSL router","description":"In this example I am using a &#8211; TD-W8960N.","date":"2014-01-26T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.71,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Setup Parental Control on a TP-Link ADSL router","tags":["Misc"],"date":"2014-01-26 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to WOL an entire IP range","permalink":"/2014/01/26/how-to-wol-an-entire-ip-range"},"nextItem":{"title":"Samsung TV \u2013 Blinking Red Lights","permalink":"/2014/01/26/samsung-tv-blinking-red-lights"}},"content":"In this example I am using a &#8211; TD-W8960N<span style=\\"color: black;font-family: Arial;font-size: 9pt;background-color: #fafafa\\">.</span>\\n  \\n_I am only going to give you a brief outline on how to set this up as various people need various settings \u2013 however it is extremely easy to utilise and doesn&#8217;t require a router restart to take effect._<span style=\\"color: black;font-family: Arial;font-size: 9pt;background-color: #fafafa\\"><br /> </span>\\n\\n  1. First navigate to the routers administration page \u2013 open your internet browser and navigate to: 192.168.1.1\\n  2. On the left hand side Menu select Advanced Setup\\n  3. Select Parental Control\\n  4. Here you have 2 options \u2013 you can place a Time Restriction which will allow you to specify by time and day _(by MAC address filtering)_ that will Block or Allow internet connectivity during certain periods. The second is URL filtering allowing you to place a block on certain URLs."},{"id":"/2014/01/26/samsung-tv-blinking-red-lights","metadata":{"permalink":"/2014/01/26/samsung-tv-blinking-red-lights","source":"@site/blog/2014-01-26-samsung-tv-blinking-red-lights.md","title":"Samsung TV \u2013 Blinking Red Lights","description":"1. Unplug your TV from the power for 5 minutes to allow for power capacitors to clear charge.","date":"2014-01-26T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.375,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Samsung TV \u2013 Blinking Red Lights","tags":["Misc"],"date":"2014-01-26 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Setup Parental Control on a TP-Link ADSL router","permalink":"/2014/01/26/parental-control-on-a-tp-link"},"nextItem":{"title":"The differences between Hosted and Distributed Branchcache","permalink":"/2014/01/13/the-differences-between-hosted-distributed-branchcache"}},"content":"1. Unplug your TV from the power for 5 minutes to allow for power capacitors to clear charge.\\n  2. Plug TV back into power and press the on button.\\n\\nNote: If this fails to turn the TV on and blinking red lights still insist \u2013 contact your manufacturer or local television repair store. A few Samsung models _(such as the Samsung Series 6 LCD sets)_ are well known for faulty capacitors which causes this problem."},{"id":"/2014/01/13/the-differences-between-hosted-distributed-branchcache","metadata":{"permalink":"/2014/01/13/the-differences-between-hosted-distributed-branchcache","source":"@site/blog/2014-01-13-the-differences-between-hosted-distributed-branchcache.md","title":"The differences between Hosted and Distributed Branchcache","description":"Branchcache is one of those things \u2013 that can either be a blessing or a curse depending on how your network is setup.","date":"2014-01-13T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":2.17,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"The differences between Hosted and Distributed Branchcache","tags":["Misc"],"date":"2014-01-13 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Samsung TV \u2013 Blinking Red Lights","permalink":"/2014/01/26/samsung-tv-blinking-red-lights"},"nextItem":{"title":"Help Manual How to publish to epub","permalink":"/2014/01/12/help-manual-how-to-publish-to-epub"}},"content":"Branchcache is one of those things \u2013 that can either be a blessing or a curse depending on how your network is setup.\\n\\nSupported by Windows 7/Windows 8 & Server 2008 R2/Server 2012 \u2013 Branchcache is one of those technologies that I believe should be setup as default from the start \u2013 especially in larger organisations or enterprises.\\n\\nFirst off \u2013 Branchcache is a method of &#8220;transparent caching&#8221; popular files from a network share to a more locally centralized \u2013 to the requesting computer \u2013 without increasing network bandwidth for a file that continuously gets pulled from a fileserver.\\n\\n#### Hosted\\n\\nBranchcache hosted is a method of actually having a Branchcache server &#8211; on an actual physical remote site away from the main fileserver.\\n\\nSay you request a file &#8220;Untitled.docx&#8221; from the fileserver (at the main location \u2013 which may be on the opposite side of the country from your location) \u2013 however some of your colleagues also have requested the same file. The hosted Branchcache server that is usually located closer to your physical location then the main fileserver \u2013 holds a &#8220;cached&#8221; copy which your computer will grab first. If no one had requested the &#8220;Untitled.docx&#8221; previously your Windows workstation will then retrieve the file from the main fileserver and the Branchcache server will then host the file for an amount of time \u2013 specified by your systems administrator \u2013 this allows faster read times on the files you are commonly working on and reduces overall network latency.\\n\\n#### Distributed\\n\\nBranchcache Distribution is a method in which there is no local branchcache server available. This method is commonly used for smaller external remote sites \u2013 compared to the hosted method which requires a server setup. Through the use of the Local Group Policy distributed branchcache allows you to retrieve commonly used documents from other branchcache supported Windows workstations without the use of a dedicated server.\\n\\nThe files get &#8220;cached&#8221; onto the local workstations \u2013 allowing any supported Branchcache workstation to pick up the cached files without having to download the files from the network fileserver \u2013 reducing the continuous pull from the network fileserver and allowing for faster start \u2013 this method however is not recommended in a highly secure environment as files are stored on the local workstations.\\n\\n_Note: Both methods do a file check to make sure the file version you are opening \u2013 is the most up to date. If the file you are opening is out of date \u2013 to the one stored on the fileserver then Branchcache will automatically run the latest version and update it&#8217;s cache._"},{"id":"/2014/01/12/help-manual-how-to-publish-to-epub","metadata":{"permalink":"/2014/01/12/help-manual-how-to-publish-to-epub","source":"@site/blog/2014-01-12-help-manual-how-to-publish-to-epub.md","title":"Help Manual How to publish to epub","description":"Help & Manual allows documents to be published to multiple formats \u2013 such as ePub & CHM easily.","date":"2014-01-12T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.29,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Help Manual How to publish to epub","tags":["Windows"],"date":"2014-01-12 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"The differences between Hosted and Distributed Branchcache","permalink":"/2014/01/13/the-differences-between-hosted-distributed-branchcache"},"nextItem":{"title":"VMWare Service Manager How to setup Watched Calls","permalink":"/2014/01/09/vmware-service-manager-how-to-setup-watched-calls"}},"content":"Help & Manual allows documents to be published to multiple formats \u2013 such as ePub & CHM easily.\\n\\n  1. First open the Project you want to publish _(in this guide I am using the Get\\\\_Me\\\\_Started example project).\\n  2. Click on Publish (top menu)\\n  3. Under Publish Format \u2013 select Apple iBooks (ePUB Format)\\n  4. Then select Publish Now."},{"id":"/2014/01/09/vmware-service-manager-how-to-setup-watched-calls","metadata":{"permalink":"/2014/01/09/vmware-service-manager-how-to-setup-watched-calls","source":"@site/blog/2014-01-09-vmware-service-manager-how-to-setup-watched-calls.md","title":"VMWare Service Manager How to setup Watched Calls","description":"In VMWare Service Manager you can add a Watched Call; state to an Incident or Service Request. This function allows you to automatically get emailed (via the email address on your People record) any changes and updates that have happened to the call \u2013 this makes keeping up with what is happening in a call extremely useful especially for Service Desk / Help Desk staff.","date":"2014-01-09T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.895,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"VMWare Service Manager How to setup Watched Calls","tags":["Windows"],"date":"2014-01-09 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Help Manual How to publish to epub","permalink":"/2014/01/12/help-manual-how-to-publish-to-epub"},"nextItem":{"title":"My thoughts on Razers Project Christine","permalink":"/2014/01/08/my-thoughts-on-razers-project-christine"}},"content":"In VMWare Service Manager you can add a Watched Call; state to an Incident or Service Request. This function allows you to automatically get emailed (via the email address on your People record) any changes and updates that have happened to the call \u2013 this makes keeping up with what is happening in a call extremely useful especially for Service Desk / Help Desk staff.\\n\\nNote: Putting this watched call state on will not add any history or adjust any of the lifecycle management of the call.\\n\\n* First open an Incident \u2013 in the top banner click Watched Call\\n* A dialog box will open saying: A watch has been placed on the following call(s): (call number)\\n* Once you receive that dialog box \u2013 you have successfully setup a Watched Call and will be notified via email of any call changes.\\n\\nNote: In Outlook I have setup a rule that will forward any emails containing the following subject line:\\n\\nForum Notification &#8211; VMware Service Manager Call into a folder called \u2013 Call_Watchlist \u2013 I recommend doing this."},{"id":"/2014/01/08/my-thoughts-on-razers-project-christine","metadata":{"permalink":"/2014/01/08/my-thoughts-on-razers-project-christine","source":"@site/blog/2014-01-08-my-thoughts-on-razers-project-christine.md","title":"My thoughts on Razers Project Christine","description":"Razer \u2013 a company well known for its PC gaming peripherals.","date":"2014-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.585,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"My thoughts on Razers Project Christine","tags":["Misc"],"date":"2014-01-08 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"VMWare Service Manager How to setup Watched Calls","permalink":"/2014/01/09/vmware-service-manager-how-to-setup-watched-calls"},"nextItem":{"title":"Surface Pro \u2013 How to install Hyper-V","permalink":"/2014/01/07/surface-pro-how-to-install-hyper-v"}},"content":"Razer \u2013 a company well known for its PC gaming peripherals.\\n\\nWith its sleep black and green designs Razer has captivated many a gamer \u2013 including myself as I type this using my Razer Blackwidow keyboard.\\n\\nNow &#8211; out of nowhere the company has released concepts for two products \u2013 the <a href=\\"http://www.razerzone.com/nabu\\" target=\\"_blank\\">Razer Nabu</a> a digital wristband that connects to your mobile phone offering various options \u2013 and <a href=\\"http://www.razerzone.com/christine\\" target=\\"_blank\\">Project Christine</a> the modular PC design.\\n\\nComputer components themselves are of a modular design \u2013 as soon as they are compatible with each other you can swap and upgrade most parts \u2013 Project Christine takes this modular design further in a fashion that reminds me of lego.\\n\\n\\n## <span id=\\"Simplicity\\">Simplicity</span>\\n\\n  * The design like all of Razers products is simple yet aesthetically pleasing.\\n  * Modular \u2013 pick & part parts \u2013 want another CPU or SSD? Simply plug it in.\\n\\n## <span id=\\"Future_proofish\\">Future proof<em>(ish)<br /> </em></span>\\n\\n  * Razer are claiming that when technology changes so do the modular modules.\\n  * Up to date technology with the latest brand GFX & CPU chipsets.\\n\\n## <span id=\\"Silent_Cool\\">Silent & Cool</span>\\n\\n  * Each modular component will feature inbuilt water cooling.\\n  * The water cooling will allow the components to be cool and quiet.\\n  * As an added note \u2013 the water cooled components will most likely make purchasing the modules already overclocked easier.\\n\\n## <span id=\\"The_Downside\\">The Downside</span>\\n\\n  * It is only a concept and prototype at the moment \u2013 usual Razer product lines seem to take 1 year to hit the shelves. This will probably be a 2015 product.\\n  * Even though the modules will connect via the PCI-E BUS this will depend on manufacturers reselling Project Christine part &#8220;modules&#8221;.\\n  * No pricing has been released yet.\\n\\nAll in all \u2013 I quite enjoy the look of Project Christine and may well see it in my office space next year."},{"id":"/2014/01/07/surface-pro-how-to-install-hyper-v","metadata":{"permalink":"/2014/01/07/surface-pro-how-to-install-hyper-v","source":"@site/blog/2014-01-07-surface-pro-how-to-install-hyper-v.md","title":"Surface Pro \u2013 How to install Hyper-V","description":"1. Bring up the Metro Windows 8 interface","date":"2014-01-07T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.61,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Surface Pro \u2013 How to install Hyper-V","tags":["Windows"],"date":"2014-01-07 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"My thoughts on Razers Project Christine","permalink":"/2014/01/08/my-thoughts-on-razers-project-christine"},"nextItem":{"title":"Outlook 2010 \u2013 How to find your Exchange address","permalink":"/2014/01/02/outlook-2010-how-to-find-your-exchange-address"}},"content":"1. Bring up the Metro Windows 8 interface\\n  2. Type in: Programs & Features\\n  3. Select Turn Windows Features on or off on the left hand side menu\\n  4. Check Hyper-V _(make sure \u2013 Hyper-V Management Tools & Hyper-V platforms are also selected inside the Hyper-V section)_.\\n  5. Click Ok\\n  6. The Hyper-V services will get installed. Once completed restart your Surface.\\n  7. Launch the Hyper-V Manager and you are good to go to start playing with your Virtual machines.\\n\\n>Note: The same method allows Hyper-V to be installed on Windows 8.1 if you are running a 64bit OS & your Motherboard & CPU supports virtualization.\\n\\nNote: Surface RT does not support Hyper-V \u2013 this is only suitable for the Surface Pros."},{"id":"/2014/01/02/outlook-2010-how-to-find-your-exchange-address","metadata":{"permalink":"/2014/01/02/outlook-2010-how-to-find-your-exchange-address","source":"@site/blog/2014-01-02-outlook-2010-how-to-find-your-exchange-address.md","title":"Outlook 2010 \u2013 How to find your Exchange address","description":"1. Open Outlook","date":"2014-01-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Outlook 2010 \u2013 How to find your Exchange address","tags":["Windows"],"date":"2014-01-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Surface Pro \u2013 How to install Hyper-V","permalink":"/2014/01/07/surface-pro-how-to-install-hyper-v"},"nextItem":{"title":"VMWare Workstation \u2013 How to share a folder with a virtual machine","permalink":"/2014/01/02/vmware-workstation-how-to-share-a-folder-with-a-virtual-machine"}},"content":"1. Open **Outlook**\\n  2. Click on **File**\\n  3. Click on **Account Settings**\\n  4. Click **Account Settings**\\n  5. Select the mailbox you would like to retrieve the Exchange address of and select Change\\n  6. Your Exchange address is titled under the Server field"},{"id":"/2014/01/02/vmware-workstation-how-to-share-a-folder-with-a-virtual-machine","metadata":{"permalink":"/2014/01/02/vmware-workstation-how-to-share-a-folder-with-a-virtual-machine","source":"@site/blog/2014-01-02-vmware-workstation-how-to-share-a-folder-with-a-virtual-machine.md","title":"VMWare Workstation \u2013 How to share a folder with a virtual machine","description":"1. First up \u2013 you need to create a folder on the host machine. For this example we are using a folder on the Desktop.","date":"2014-01-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.315,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"VMWare Workstation \u2013 How to share a folder with a virtual machine","tags":["Windows"],"date":"2014-01-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Outlook 2010 \u2013 How to find your Exchange address","permalink":"/2014/01/02/outlook-2010-how-to-find-your-exchange-address"},"nextItem":{"title":"Ubuntu \u2013 How to do a Distribution Upgrade","permalink":"/2014/01/01/ubuntu-how-to-do-a-distribution-upgrade"}},"content":"1. First up \u2013 you need to create a folder on the host machine. For this example we are using a folder on the Desktop.\\n2. Right click a blank space on the Desktop and select \u2013 New, Folder.\\n3. Name it: vmshared and press Enter\\n4. This will be our base folder \u2013 anything we put in here will be shared with the virtual machines.\\n5. Open VMWare Workstation\\n6. Right click the Virtual Machine you are using and select Settings\\n7. Click on the Options tab and navigate to \u2013 Shared Folders\\n8. Here you can choose to have it Always Enabled or Enabled until next power off or suspend \u2013 I will select Always Enabled.\\n9. Check Map as a network drive in Windows Guests\\n10. Down the bottom select Add\\n11. This Wizard will guide you through to navigating to the hosts path \u2013 the location of the folder you created earlier \u2013 ie Deskop/vmshared\\n12. You can specify an alternative name if you want or just select Next\\n13. You can specify to have the share Read Only here if you want, make sure Enable this Share is selected and select Finish\\n14. Done \u2013 as soon you as press Ok your virtual machine would have mapped a network share to your workstation. You can find this under My Computer or Computer!\\n\\nNote: You can map the same network share between multiple Virtual Machines.\\nNote: The Virtual machine must be on to allow Shared Folder configuration \u2013 if it is off the option will simply not be enabled."},{"id":"/2014/01/01/ubuntu-how-to-do-a-distribution-upgrade","metadata":{"permalink":"/2014/01/01/ubuntu-how-to-do-a-distribution-upgrade","source":"@site/blog/2014-01-01-ubuntu-how-to-do-a-distribution-upgrade.md","title":"Ubuntu \u2013 How to do a Distribution Upgrade","description":"1. Launch a Terminal","date":"2014-01-01T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.575,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Ubuntu \u2013 How to do a Distribution Upgrade","tags":["Linux"],"date":"2014-01-01 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"VMWare Workstation \u2013 How to share a folder with a virtual machine","permalink":"/2014/01/02/vmware-workstation-how-to-share-a-folder-with-a-virtual-machine"},"nextItem":{"title":"Flex+ &#8211; Using Self-Support to restore user\u2019s preferences","permalink":"/2013/12/23/flex-using-self-support-to-restore-users-preferences"}},"content":"1. Launch a Terminal\\n  2. Type in: **_sudo apt-get update_**  \\n   (this will ping all the upgrade mirrors to determine if they are live and what upgrade packages are available)\\n  3. Once completed \u2013 type in: sudo apt-get dist-upgrade _(this will start pooling down the various packages to do a distribution upgrade. Keep in mind \u2013 apt-get update will just do an update within the same distribution)._\\n\\nNote: Ubuntu now has an Update Manager that is available by launching the Unity search function and typing in \u2013 Update Manager. This is a GUI method of upgrading. I have had failure with this method personally so recommend using the command line for distribution upgrades for more visibility."},{"id":"/2013/12/23/flex-using-self-support-to-restore-users-preferences","metadata":{"permalink":"/2013/12/23/flex-using-self-support-to-restore-users-preferences","source":"@site/blog/2013-12-23-flex-using-self-support-to-restore-users-preferences.md","title":"Flex+ &#8211; Using Self-Support to restore user\u2019s preferences","description":"1. Click Start","date":"2013-12-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.29,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Flex+ &#8211; Using Self-Support to restore user\u2019s preferences","tags":["Windows"],"date":"2013-12-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Ubuntu \u2013 How to do a Distribution Upgrade","permalink":"/2014/01/01/ubuntu-how-to-do-a-distribution-upgrade"},"nextItem":{"title":"Lync 2013 \u2013 Turn on event logging for Windows event viewer","permalink":"/2013/12/23/lync-2013-turn-on-event-logging-for-windows-event-viewer"}},"content":"1. Click Start\\n2. Select All Programs\\n3. Select Immidio Flex+\\n5. Select Flex+ Self-Support\\n6. Now for example \u2013you are having outlook issues \u2013 select Microsoft Outlook 2010\\n7. Click Reset (to reset Outlook 2010 back to Factory Defaults \u2013 or Restore to restore from backup. You can restore a Flex+ backup for the last 2 weeks."},{"id":"/2013/12/23/lync-2013-turn-on-event-logging-for-windows-event-viewer","metadata":{"permalink":"/2013/12/23/lync-2013-turn-on-event-logging-for-windows-event-viewer","source":"@site/blog/2013-12-23-lync-2013-turn-on-event-logging-for-windows-event-viewer.md","title":"Lync 2013 \u2013 Turn on event logging for Windows event viewer","description":"1. Open Lync 2013","date":"2013-12-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Lync 2013 \u2013 Turn on event logging for Windows event viewer","tags":["Windows"],"date":"2013-12-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Flex+ &#8211; Using Self-Support to restore user\u2019s preferences","permalink":"/2013/12/23/flex-using-self-support-to-restore-users-preferences"},"nextItem":{"title":"Windows 7 \u2013 Authentication prompt for cached credentials in a domain environment","permalink":"/2013/12/23/windows-7-authentication-prompt-for-cached-credentials-in-a-domain-environment"}},"content":"1. Open Lync 2013\\n  2. Click Options\\n  3. Change Logging in Lync to: Full\\n  4. Check \u2013 Also turn on Windows Event Logging for Lync to collect troubleshooting info\\n  5. Click Ok"},{"id":"/2013/12/23/windows-7-authentication-prompt-for-cached-credentials-in-a-domain-environment","metadata":{"permalink":"/2013/12/23/windows-7-authentication-prompt-for-cached-credentials-in-a-domain-environment","source":"@site/blog/2013-12-23-windows-7-authentication-prompt-for-cached-credentials-in-a-domain-environment.md","title":"Windows 7 \u2013 Authentication prompt for cached credentials in a domain environment","description":"1. In order to fix this \u2013 you need to close all Internet Explorer windows.","date":"2013-12-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.635,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows 7 \u2013 Authentication prompt for cached credentials in a domain environment","tags":["Windows"],"date":"2013-12-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Lync 2013 \u2013 Turn on event logging for Windows event viewer","permalink":"/2013/12/23/lync-2013-turn-on-event-logging-for-windows-event-viewer"},"nextItem":{"title":"Windows 7 \u2013 How to delete users profiles","permalink":"/2013/12/23/windows-7-how-to-delete-users-profiles"}},"content":"1. In order to fix this \u2013 you need to close all Internet Explorer windows.\\n  2. Open Credential Manager (start type in: Credential Manager)\\n  3. Under Windows Credentials \u2013 select & remove all associated Username & Passwords\\n  4. Once removed log the user out and get the user to login again.\\n  5. They should now be able to access\xa0the Intranet & Internet.\\n\\n<em style=\\"font-family: sans-serif; font-size: medium;\\">Note: I have come across this when the users \u2013 password has reached expiry so when attempting to login again the user is usually asked to reset his or her password.</em>\\n  \\n<em style=\\"font-family: sans-serif; font-size: medium;\\">Note: Also make sure the proxy details in Internet Explorer are set correctly for your environment.</em>\\n  \\n<em style=\\"font-family: sans-serif; font-size: medium;\\">Note: Also \u2013 Windows 8 compatible.</em>"},{"id":"/2013/12/23/windows-7-how-to-delete-users-profiles","metadata":{"permalink":"/2013/12/23/windows-7-how-to-delete-users-profiles","source":"@site/blog/2013-12-23-windows-7-how-to-delete-users-profiles.md","title":"Windows 7 \u2013 How to delete users profiles","description":"1. Click on Start","date":"2013-12-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.175,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows 7 \u2013 How to delete users profiles","tags":["Windows"],"date":"2013-12-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows 7 \u2013 Authentication prompt for cached credentials in a domain environment","permalink":"/2013/12/23/windows-7-authentication-prompt-for-cached-credentials-in-a-domain-environment"},"nextItem":{"title":"Free Internet Explorer \u2013 Development Virtual Machines","permalink":"/2013/12/20/free-internet-explorer-development-virtual-machines"}},"content":"1. Click on Start\\n  2. Right click Computer\\n  3. Select Properties.\\n  4. Click Advanced System Settings.\\n  5. Under \u2013 User Profiles select Settings\\n  6. Choose the profile you would like to Delete\\n  7. Select Delete"},{"id":"/2013/12/20/free-internet-explorer-development-virtual-machines","metadata":{"permalink":"/2013/12/20/free-internet-explorer-development-virtual-machines","source":"@site/blog/2013-12-20-free-internet-explorer-development-virtual-machines.md","title":"Free Internet Explorer \u2013 Development Virtual Machines","description":"I have recently found a good web resource \u2013 90 day restricted Virtual Machines, compatible with Hyper-V/VMWare & Virtual box that I thought I would share.","date":"2013-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Free Internet Explorer \u2013 Development Virtual Machines","tags":["Misc"],"date":"2013-12-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows 7 \u2013 How to delete users profiles","permalink":"/2013/12/23/windows-7-how-to-delete-users-profiles"},"nextItem":{"title":"How to remove Applications from the application list on the Play Store","permalink":"/2013/12/20/how-to-remove-applications-from-the-application-list-on-the-play-store"}},"content":"I have recently found a good web resource \u2013 90 day restricted Virtual Machines, compatible with Hyper-V/VMWare & Virtual box that I thought I would share.\\n\\nThese Virtual Machines are intended for web development testing and include Windows XP machines running Internet Explorer 6 to Windows 8.1 machines running Internet Explorer 11 _(as of this post)_.\\n\\n  1. To access these Virtual Machines \u2013 go to: <a href=\\"https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/\\" target=\\"_blank\\">Modern.ie</a> and select &#8220;Get Free VMs&#8221; to see the download list. \\n\\nNote: This website also has a modern interface \u2013 with a lot of resources for webpage development in Internet Explorer. Useful for any webpage builder and is developed by Microsoft."},{"id":"/2013/12/20/how-to-remove-applications-from-the-application-list-on-the-play-store","metadata":{"permalink":"/2013/12/20/how-to-remove-applications-from-the-application-list-on-the-play-store","source":"@site/blog/2013-12-20-how-to-remove-applications-from-the-application-list-on-the-play-store.md","title":"How to remove Applications from the application list on the Play Store","description":"1. On your Android \u2013 open the Play Store.","date":"2013-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to remove Applications from the application list on the Play Store","layout":"post","tags":["Mobile"],"date":"2013-12-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Free Internet Explorer \u2013 Development Virtual Machines","permalink":"/2013/12/20/free-internet-explorer-development-virtual-machines"},"nextItem":{"title":"Windows Server 2012 \u2013 Convert Core installation to full GUI installation","permalink":"/2013/12/20/windows-server-2012-convert-core-installation-to-full-gui-installation"}},"content":"1. On your Android \u2013 open the Play Store.\\n  2. Click Menu\\n  3. Click My Apps\\n  4. On the left \u2013 select All\\n  5. Select the Application you would like to remove and select \u2013 &#8220;Remove (app name) from my Apps&#8221;\\n  6. Click Ok\\n  7. Now on the Google Play website \u2013 under your Applications the application you removed should be removed from the list."},{"id":"/2013/12/20/windows-server-2012-convert-core-installation-to-full-gui-installation","metadata":{"permalink":"/2013/12/20/windows-server-2012-convert-core-installation-to-full-gui-installation","source":"@site/blog/2013-12-20-windows-server-2012-convert-core-installation-to-full-gui-installation.md","title":"Windows Server 2012 \u2013 Convert Core installation to full GUI installation","description":"1. You have to have administration rights for this \u2013 open an elevated powershell prompt and type: Install-WindowsFeature Server-Gui-Mgmt-Infra, Server-Gui-Shell \u2013Restart press Enter for the cmdlet to install the Server 2012 user interface.","date":"2013-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.165,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Server 2012 \u2013 Convert Core installation to full GUI installation","tags":["Windows"],"date":"2013-12-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to remove Applications from the application list on the Play Store","permalink":"/2013/12/20/how-to-remove-applications-from-the-application-list-on-the-play-store"},"nextItem":{"title":"Windows Vista \u2013 Not loading Control Panel","permalink":"/2013/12/20/windows-vista-not-loading-control-panel"}},"content":"1. You have to have administration rights for this \u2013 open an elevated powershell prompt and type: **Install-WindowsFeature Server-Gui-Mgmt-Infra, Server-Gui-Shell \u2013Restart** press Enter for the cmdlet to install the Server 2012 user interface."},{"id":"/2013/12/20/windows-vista-not-loading-control-panel","metadata":{"permalink":"/2013/12/20/windows-vista-not-loading-control-panel","source":"@site/blog/2013-12-20-windows-vista-not-loading-control-panel.md","title":"Windows Vista \u2013 Not loading Control Panel","description":"Having problems when you try opening the Vista Control panel and all you see is the outline of the window?","date":"2013-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.67,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Vista \u2013 Not loading Control Panel","tags":["Windows"],"date":"2013-12-20 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows Server 2012 \u2013 Convert Core installation to full GUI installation","permalink":"/2013/12/20/windows-server-2012-convert-core-installation-to-full-gui-installation"},"nextItem":{"title":"How to add users to the Remote Computers local Administrators groups","permalink":"/2013/12/18/how-to-add-users-to-remote-computers-local-administrators-groups"}},"content":"Having problems when you try opening the Vista Control panel and all you see is the outline of the window? \\n\\nFollow the instructions below to repair it. \\n\\n1. Left Click the Vista &#8220;Orb&#8221; \\n\\n2. Left Click All Programs \\n\\n3. Left Click Accessories \\n\\n5. Left Click Notepad \\n\\n6. Copy & paste the following into Notepad (To copy, select the text below and click Copy. To paste, right click on where you would like to paste it and select Paste) \\n\\n\\nnet stop readyboost \\n\\nnet stop slsvc \\n\\nnet start slsvc \\n\\nnet start readyboost \\n\\n@cls \\n\\n\\n7. Click File \\n\\n8. click Save \\n\\n9. Save the file as &#8220;ControlPanelFix.bat&#8221; and close Notepad \\n\\n10. Run the ControlPanelFix.bat file. \\n\\nNote: You can also try accessing parts of Control Panel by going to Computer, and in the left hand pane, select Control Panel"},{"id":"/2013/12/18/how-to-add-users-to-remote-computers-local-administrators-groups","metadata":{"permalink":"/2013/12/18/how-to-add-users-to-remote-computers-local-administrators-groups","source":"@site/blog/2013-12-18-how-to-add-users-to-remote-computers-local-administrators-groups.md","title":"How to add users to the Remote Computers local Administrators groups","description":"1. First you need to download &#8211; PSEXEC  &#8211; a command line tool that will allow you to execute commands on remote workstations.","date":"2013-12-18T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to add users to the Remote Computers local Administrators groups","tags":["Windows"],"date":"2013-12-18 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows Vista \u2013 Not loading Control Panel","permalink":"/2013/12/20/windows-vista-not-loading-control-panel"},"nextItem":{"title":"How to setup delegation in VMWare Service Manager","permalink":"/2013/12/18/how-to-setup-delegation-in-vmware-service-manager"}},"content":"1. First you need to download &#8211; <a href=\\"http://technet.microsoft.com/en-us/sysinternals/bb897553.aspx\\" target=\\"_blank\\">PSEXEC </a> &#8211; a command line tool that will allow you to execute commands on remote workstations.\\n  2. Once downloaded open My Computer/Computer and navigate to: C:\\\\Windows\\\\System32 on your local machine.\\n  3. Extract the zip file contents to the System32 folder (this will allow you to run the psexec command from any folder within Command Prompt).\\n  4. Navigate to your Documents or Desktop and create a new txt file called: computernames.txt\\n  5. In the computernames.txt document \u2013 add (one in each new line) the computer names that you will like to add the user to the Administrators group of.\\n  6. Once they have been added \u2013 save the time with the computer names.\\n  7. Now we need to create a batch script \u2013 open a new Notepad document and in the first line type: \\n  \\n      PSEXEC.EXE @computernames.txt NET LOCALGROUP Administrators **DOMAIN\\\\****USERID** /ADD\\n\\n  8. Replace \u2013 DOMAIN & USERID with the user you would like to add.\\n  9. Press File and click Save As and type in: &#8220;addusers_multiple.bat&#8221;\\n 10. This will save the file as a batch script\\n 11. Now \u2013 run the script to start importing the users into the remote computers local administrators group.\\n\\nNote: Proper syntac allows for the group name to be set like &#8221; NET LOCALGROUP &#8220;Administrators&#8221; \u2013 however my test on a Windows XP workstation didn&#8217;t like the &#8220;&#8221; so I got rid of them \u2013 if you encounter an error with unable to find local group \u2013 re add them around the group name in the script.\\n\\nNote: Tested on Windows 7 workstations without an issue."},{"id":"/2013/12/18/how-to-setup-delegation-in-vmware-service-manager","metadata":{"permalink":"/2013/12/18/how-to-setup-delegation-in-vmware-service-manager","source":"@site/blog/2013-12-18-how-to-setup-delegation-in-vmware-service-manager.md","title":"How to setup delegation in VMWare Service Manager","description":"1. Open VMWare Service Manager","date":"2013-12-18T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to setup delegation in VMWare Service Manager","tags":["Windows"],"date":"2013-12-18 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to add users to the Remote Computers local Administrators groups","permalink":"/2013/12/18/how-to-add-users-to-remote-computers-local-administrators-groups"},"nextItem":{"title":"Remove unused user profiles on a remote Windows workstation","permalink":"/2013/12/18/remove-unused-user-profiles-on-a-remote-win-pc"}},"content":"1. Open VMWare Service Manager\\n  2. Open the managers People record\\n  3. On the left hand menu select Delegation\\n  4. Click Add \u2013 and add the user that you want to delegate approval to.\\n  5. Select Notify Delegate to send an email to the delegate so they are aware of the change. Make sure Activate Delegation is ticked. \\n  6. You have now setup delegation!"},{"id":"/2013/12/18/remove-unused-user-profiles-on-a-remote-win-pc","metadata":{"permalink":"/2013/12/18/remove-unused-user-profiles-on-a-remote-win-pc","source":"@site/blog/2013-12-18-remove-unused-user-profiles-on-a-remote-win-pc.md","title":"Remove unused user profiles on a remote Windows workstation","description":"Note: The intended guide for this audience is a Help Desk or Service Desk \u2013 this is intended for a domain setup with the user having local administration rights on the workstation.","date":"2013-12-18T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Remove unused user profiles on a remote Windows workstation","tags":["Windows"],"date":"2013-12-18 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to setup delegation in VMWare Service Manager","permalink":"/2013/12/18/how-to-setup-delegation-in-vmware-service-manager"},"nextItem":{"title":"How to install Google Apps onto CyanogenMod","permalink":"/2013/12/17/how-to-install-google-apps-onto-cyanogenmod"}},"content":"Note: The intended guide for this audience is a Help Desk or Service Desk \u2013 this is intended for a domain setup with the user having local administration rights on the workstation.\\n\\n  1. First, you need to download tool called: <a href=\\"http://helgeklein.com/free-tools/delprof2-user-profile-deletion-tool/\\" target=\\"_blank\\" rel=\\"noopener\\">Delprof2</a>\\n(this is a remake of the Microsoft Delprof utility that Microsoft had dropped support & updates for)\\n  2. Once downloaded open My Computer/Computer and navigate to c:\\\\Windows\\\\System32\xa0on your local machine.\\n  3. Extract the delprof2 zip file & folder and copy DelProf2.exe to the folder you opened earlier \u2013\xa0c:\\\\Windows\\\\System32\\n  4. Now \u2013 open command prompt \u2013 click Start, Run and type in cmd and press Enter (For Windows 7 workstations \u2013 you can type Command in the search field or press the Windows Key + R to open the run dialog box).\\n  5. REMEMBER to make sure the profiles are backed up first \u2013 this will completely remove the local cache \u2013 use the syntax: **delprof2 /u /r /c:computername** and press Enter. This will connect to the remote workstation you specified in the &#8220;computername&#8221; field and remove all locally cached profiles that are not being used. It will also clear up remote registry entries making this a useful & easy tool for clearing up Roaming Profiles in Windows 7-10.\\n\\nYou can also run: delprof2 /u /d:30 /c:computername\\n\\n_Note: What I have done in the past is create a batch script that will clear up remote profiles from workstations & add this to a scheduled task &#8211; in conjunction with SCCM disk reporting this is useful for keeping on top of workstation&#8217;s HDD space._"},{"id":"/2013/12/17/how-to-install-google-apps-onto-cyanogenmod","metadata":{"permalink":"/2013/12/17/how-to-install-google-apps-onto-cyanogenmod","source":"@site/blog/2013-12-17-how-to-install-google-apps-onto-cyanogenmod.md","title":"How to install Google Apps onto CyanogenMod","description":"Note: This is usually \u2013 for the nightly builds. The stable builds seem to have Google integration.","date":"2013-12-17T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":1.2,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install Google Apps onto CyanogenMod","tags":["Mobile"],"date":"2013-12-17 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Remove unused user profiles on a remote Windows workstation","permalink":"/2013/12/18/remove-unused-user-profiles-on-a-remote-win-pc"},"nextItem":{"title":"How to repair an App-V application","permalink":"/2013/12/17/how-to-repair-an-app-v-application"}},"content":"Note: This is usually \u2013 for the nightly builds. The stable builds seem to have Google integration.\\n\\nNote: Please backup your phone beforehand \u2013 I take no responsibility if your phone fails to boot.\\n  \\n  1. First thing is first \u2013 download Gapps (goo.im/gapps/)\\n  2. Plug your Android phone into your computer\\n  3. Open My Computer/Computer & navigate to the Download folder\\n  4. Create a folder in the Downloads folder called: gapps\\n  5. Copy the downloaded Gapps zip file (leave as is \u2013 do not uncompress) into the folder\\n  6. Once copied turn your Android phone off.\\n  7. Now you need to boot into the phones Recovery mode \u2013 for the Samsung Galaxy phones & Clockwork Recovery press Power & Volume Up button at the same time to boot the phone and open Recovery mode.\\n  8. Once in Recovery mode \u2013 navigate to: -install zip\\n  9. Select \u2013 install zip from /sdcard\\n 10. Select the Gapps zip file\\n 11. Select Yes \u2013 Install Gapps\\n 12. It will go then go the copying process and uncompress the zip & install Gapps.\\n 13. Once completed \u2013 it will say: Install from SD Card Complete.\\n 14. Navigate back to the root menu of Recovery Mode & select Reboot System Now.\\n 15. The android device will restart and you will see &#8220;Android is upgrading.&#8221;\\n 16. Congratulations you have now installed GApps onto your Android device and should now have access to Google Play & Gmail."},{"id":"/2013/12/17/how-to-repair-an-app-v-application","metadata":{"permalink":"/2013/12/17/how-to-repair-an-app-v-application","source":"@site/blog/2013-12-17-how-to-repair-an-app-v-application.md","title":"How to repair an App-V application","description":"1. In Windows \u2013 click on the notification tray by the time.","date":"2013-12-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.47,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to repair an App-V application","tags":["Windows"],"date":"2013-12-17 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to install Google Apps onto CyanogenMod","permalink":"/2013/12/17/how-to-install-google-apps-onto-cyanogenmod"},"nextItem":{"title":"Reset Windows 7/8 Firewall to its default state","permalink":"/2013/12/17/reset-windows-78-firewall-to-its-default-state"}},"content":"1. In Windows \u2013 click on the notification tray by the time.\\n  2. Double click on the Microsoft Application Virtualization (App-V) client\\n  3. You will be greeted with the Virtual Application Management dialog \u2013 press VIRTUAL APPS\\n  4. On the right hand side \u2013 under the progress bar select Repair\\n  5. This will start a Repair-AppvClientPackage script to repair & redeploy; the application back to the user.\\n\\nNote: The repair will not work if the application process is running. Make sure it is fully closed in the Windows task manager before attempting the repair."},{"id":"/2013/12/17/reset-windows-78-firewall-to-its-default-state","metadata":{"permalink":"/2013/12/17/reset-windows-78-firewall-to-its-default-state","source":"@site/blog/2013-12-17-reset-windows-78-firewall-to-its-default-state.md","title":"Reset Windows 7/8 Firewall to its default state","description":"Note: You need the rights to be able to run an elevated Command Prompt.","date":"2013-12-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.52,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Reset Windows 7/8 Firewall to its default state","tags":["Windows"],"date":"2013-12-17 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to repair an App-V application","permalink":"/2013/12/17/how-to-repair-an-app-v-application"},"nextItem":{"title":"How to create an SSH tunnel with PuTTY","permalink":"/2013/12/11/how-to-create-an-ssh-tunnel-with-putty"}},"content":"Note: You need the rights to be able to run an elevated Command Prompt.\\n\\n**Windows 7**\\n\\n  1. Click Start\\n  2. Click All Programs\\n  3. Click Accessories\\n  4. Right click Command Prompt\\n  5. Select Run as Administrator\\n  6. In the command prompt type: **netsh advfirewall reset**\\n  7. Press Enter to reset your firewall rules to their default state\\n\\n**Windows 8**\\n\\n  1. Press the Windows key to open the charm interface\\n  2. Type in: command\\n  3. Right click Command Prompt\\n  4. Select Run as Administrator\\n  5. In the command prompt type: **netsh advfirewall reset**\\n  6. Press Enter to reset your firewall rules to their default state"},{"id":"/2013/12/11/how-to-create-an-ssh-tunnel-with-putty","metadata":{"permalink":"/2013/12/11/how-to-create-an-ssh-tunnel-with-putty","source":"@site/blog/2013-12-11-how-to-create-an-ssh-tunnel-with-putty.md","title":"How to create an SSH tunnel with PuTTY","description":"Open PuTTY","date":"2013-12-11T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.8,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to create an SSH tunnel with PuTTY","tags":["Windows"],"date":"2013-12-11 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Reset Windows 7/8 Firewall to its default state","permalink":"/2013/12/17/reset-windows-78-firewall-to-its-default-state"},"nextItem":{"title":"How to setup Virtual Desktops in Windows","permalink":"/2013/12/11/how-to-setup-virtual-desktops-in-windows"}},"content":"Open PuTTY\\n\\nCreate a new PuTTY session to the remote server\\n\\nEnter in the server details\\n\\nSave session\\n\\nClick on Connection\\n\\nClick on SSH and select Tunnels\\n\\nIn the source field enter the port you would like to forward.  In the section below, select Dynamic and Auto.\\n\\nClick Add\\n\\nGo to the main PuTTY session dialog, and save the changes you have made.\\n\\nPress Open to open your Putty session and login if it asks for the details.\\n\\nNow you need to configure the application you are trying to tunnel\\n\\nFor example, in Internet Explorer, click Tools, Internet Options, click Connections, Choose LAN Settings and change the proxy to localhost\\n\\n(SOCKS5 as Proxy type if asked) and enter the port that you were forwarding into the port information.\\n\\nSave and you should now be routing your traffic through the tunnel\\n\\nUnix\\nYou can use the following command to enable an SSH Tunnel on a Linux machine:\\nssh -D localhost:PORT example@example.com"},{"id":"/2013/12/11/how-to-setup-virtual-desktops-in-windows","metadata":{"permalink":"/2013/12/11/how-to-setup-virtual-desktops-in-windows","source":"@site/blog/2013-12-11-how-to-setup-virtual-desktops-in-windows.md","title":"How to setup Virtual Desktops in Windows","description":"First you need a third party utility called \u2013 Dexpot. This utility is free for non-commercial use & can be obtained from: Dexpot.de.","date":"2013-12-11T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.125,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to setup Virtual Desktops in Windows","tags":["Windows"],"date":"2013-12-11 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to create an SSH tunnel with PuTTY","permalink":"/2013/12/11/how-to-create-an-ssh-tunnel-with-putty"},"nextItem":{"title":"How to get an Android mobile device on the Tor network","permalink":"/2013/12/08/how-to-get-an-android-mobile-device-on-the-tor-network"}},"content":"First you need a third party utility called \u2013 Dexpot. This utility is free for non-commercial use & can be obtained from: <a href=\\"http://Dexpot.de\\" target=\\"_blank\\">Dexpot.de</a>.\\n\\n  1. Once downloaded \u2013 run the installation file to install.\\n  2. Click Next & I Agree to accept the license agreement.\\n  3. Select Private Use for home use \u2013 if you are using this commercially you can select Commercial Use (30 day trial) and select next.\\n  4. Make sure the Destination folder is correct (default path is: C:Program Files (x86)Dexpot for 64bit machines) and select Next.\\n  5. Uncheck \u2013 I accept the Terms of the Tune-up Utilities box to prevent grey ware from being installed & select Next to actually start the installation process.\\n  6. Once installation has completed \u2013 Click Finish to launch Dexpot.\\n  7. On your Taskbar you will now have 4 Virtual Desktops \u2013 you can select each one to navigate back & forth between them.\\n  9. You are now running Virtual Desktops!\\n\\n### Dexpot Configuration\\n  \\n  1. In the notification tray \u2013 right click the Dexpot logo.\\n  2. Select Settings\\n  3. From here \u2013 you can adjust the number of concurrent Virtual Desktops running on your workstation from 2 to 20.\\n  4. You can also make it start during Windows start-up, configure applications to open in certain Virtual Desktops & set different Desktop icons for different Desktops."},{"id":"/2013/12/08/how-to-get-an-android-mobile-device-on-the-tor-network","metadata":{"permalink":"/2013/12/08/how-to-get-an-android-mobile-device-on-the-tor-network","source":"@site/blog/2013-12-08-how-to-get-an-android-mobile-device-on-the-tor-network.md","title":"How to get an Android mobile device on the Tor network","description":"Thankfully over the last few years \u2013 connecting securely & joining your workstations to the Tor network have become a lot easier \u2013 for both Linux & Windows clients. Now with a nifty little application called \u2013 Orbit you can easily traffic HTTP (port 80) traffic & other random Android application traffic through the Tor network. Follow the guide below to get your android device & applications running through the Tor network.","date":"2013-12-08T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":1.245,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to get an Android mobile device on the Tor network","tags":["Mobile"],"date":"2013-12-08 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to setup Virtual Desktops in Windows","permalink":"/2013/12/11/how-to-setup-virtual-desktops-in-windows"},"nextItem":{"title":"VMWare Certified Associate (VCA) Exam Code","permalink":"/2013/12/08/vmware-certified-associate-vca-exam-code"}},"content":"Thankfully over the last few years \u2013 connecting securely & joining your workstations to the Tor network have become a lot easier \u2013 for both Linux & Windows clients. Now with a nifty little application called \u2013 [Orbit](https://play.google.com/store/apps/details?id=org.torproject.android&hl=en) you can easily traffic HTTP (port 80) traffic & other random Android application traffic through the Tor network. Follow the guide below to get your android device & applications running through the Tor network.\\n\\nNote: This application needs root access for full functionality and is recommend.\\n\\n  1. Open Play Store/Google Play on your android device.\\n  2. Search for: Orbot.\\n  3. Select \u2013 Orbot: Proxy with Tor _(from The Tor Project)_ & install it to your Android device.\\n  4. Once installed \u2013 open Orbot.\\n  5. Up the top of the application \u2013 click Settings\\n  6. Make sure \u2013 Start Orbot on Boot is checked.\\n  7. Scroll down to Transparent Proxying & make sure Transparent Proxying is enabled \u2013 then check Tor Everything to make sure all your application#8217;s network traffic is directed to the Tor network.\\n  8. Now \u2013 scroll right to the bottom of the settings and make sure in the Debug area \u2013 Network Auto-Sleep is checked to reduce battery consumption.\\n  9. Go back to the main Orbot home screen and press the large silver power button to start proxying your traffic through the Tor network.\\n\\nNote: You can confirm the traffic is being proxyed by monitoring the Download & Upload traffic on the bottom half of the Orbot screen."},{"id":"/2013/12/08/vmware-certified-associate-vca-exam-code","metadata":{"permalink":"/2013/12/08/vmware-certified-associate-vca-exam-code","source":"@site/blog/2013-12-08-vmware-certified-associate-vca-exam-code.md","title":"VMWare Certified Associate (VCA) Exam Code","description":"VMWare have launched a campaign to show off their new 3 VMWare Certified Associate exams.","date":"2013-12-08T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.55,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"VMWare Certified Associate (VCA) Exam Code","tags":["Misc"],"date":"2013-12-08 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to get an Android mobile device on the Tor network","permalink":"/2013/12/08/how-to-get-an-android-mobile-device-on-the-tor-network"},"nextItem":{"title":"Dishonored \u2013 Black screen during game start-up","permalink":"/2013/12/05/dishonored-black-screen-during-game-start-up"}},"content":"VMWare have launched a campaign to show off their new 3 [VMWare Certified Associate](http://mylearn.vmware.com/portals/certification/) exams.\\n\\n* VMWare Certified Associate \u2013 Workforce Mobility\\n* VMWare Certified Associate \u2013 Data Center Virtualization\\n* VMWare Certified Associate \u2013 Cloud\\n\\nThe following is a code that will lower the exam cost from $160.00NZD to $80.00 before the end of the year. The code has limited uses \u2013 get in as soon as you can \u2013 it can be used for any of the 3 VMware Certified Associate exams.\\n\\n_Note: To register to the code sign up for the exam, log into pearsonvue & enter it at the Payment Information wizard screen.\\n\\n>Exam Code: VMRT7A0D9F9B"},{"id":"/2013/12/05/dishonored-black-screen-during-game-start-up","metadata":{"permalink":"/2013/12/05/dishonored-black-screen-during-game-start-up","source":"@site/blog/2013-12-05-dishonored-black-screen-during-game-start-up.md","title":"Dishonored \u2013 Black screen during game start-up","description":"It runs correctly if you put &#8220;-windowed&#8221; in the launch options!","date":"2013-12-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.23,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Dishonored \u2013 Black screen during game start-up","tags":["Windows"],"date":"2013-12-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"VMWare Certified Associate (VCA) Exam Code","permalink":"/2013/12/08/vmware-certified-associate-vca-exam-code"},"nextItem":{"title":"Drobo Detected an Internal Error","permalink":"/2013/12/04/drobo-detected-an-internal-error"}},"content":"It runs correctly if you put &#8220;-windowed&#8221; in the launch options!\\n\\n  1. Right click the Dishonored shortcut\\n  2. Left click Properties\\n  3. Click on the shortcut tab\\n  4. In Target set: C:Program Files (x86)DishonoredBinariesWin32Dishonored.exe&#8221; \u2013windowed\\n  5. Click Ok\\n\\n_Note: Tested on Windows 8/8.1 & Windows 7._"},{"id":"/2013/12/04/drobo-detected-an-internal-error","metadata":{"permalink":"/2013/12/04/drobo-detected-an-internal-error","source":"@site/blog/2013-12-04-drobo-detected-an-internal-error.md","title":"Drobo Detected an Internal Error","description":"If you just had this error and are GASPING like I was at 10PM at night \u2013 never fear there is still hope!","date":"2013-12-04T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.79,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Drobo Detected an Internal Error","tags":["Misc"],"date":"2013-12-04 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Dishonored \u2013 Black screen during game start-up","permalink":"/2013/12/05/dishonored-black-screen-during-game-start-up"},"nextItem":{"title":"Outlook 2010 \u2013 Cannot process RSS Feed","permalink":"/2013/12/03/outlook-2010-cannot-process-rss-feed"}},"content":"If you just had this error and are GASPING like I was at 10PM at night \u2013 never fear there is still hope!\\n\\nNote: Tested with the Drobo 5N.\\n  \\n  1. Go to the [Drobo Support website](http://www.drobo.com/support/updates) & download the correct latest firmware for your Drobo.\\n  2. Take ALL the internal drives out of the Drobo _(this is to make sure your Drobo applications \u2013 even Drobo Port applications & storage are not affected)_.\\n  3. If you haven&#8217;t already got the Drobo Dashboard you can also download that from the Drobo Support website \u2013 Open the Drobo Dashboard.\\n  4. Wait 5 minutes till it scans the network and locates your Drobo.\\n  5. Once located click on your Drobo & select Tools within the Drobo dashboard.\\n  6. Click Manual Update\\n  7. Navigate to the location of the firmware update file and upgrade. This will require a restart.\\n  8. Once the process has completed restart the Drobo & reinsert your drives."},{"id":"/2013/12/03/outlook-2010-cannot-process-rss-feed","metadata":{"permalink":"/2013/12/03/outlook-2010-cannot-process-rss-feed","source":"@site/blog/2013-12-03-outlook-2010-cannot-process-rss-feed.md","title":"Outlook 2010 \u2013 Cannot process RSS Feed","description":"1. Open regedit","date":"2013-12-03T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Outlook 2010 \u2013 Cannot process RSS Feed","tags":["Windows"],"date":"2013-12-03 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Drobo Detected an Internal Error","permalink":"/2013/12/04/drobo-detected-an-internal-error"},"nextItem":{"title":"Configure Ricoh printers to use DHCP","permalink":"/2013/12/02/configure-ricoh-printers-to-use-dhcp"}},"content":"1. Open regedit\\n  1. Navigate to: HKCU\\\\Software\\\\Microsoft\\\\Office\\\\12.0\\\\Outlook\\\\Options\\\\RSS\\n  1. In the: DWORD: Disable\\n  1. Change the value from 1 to 0 to enable RSS functionality."},{"id":"/2013/12/02/configure-ricoh-printers-to-use-dhcp","metadata":{"permalink":"/2013/12/02/configure-ricoh-printers-to-use-dhcp","source":"@site/blog/2013-12-02-configure-ricoh-printers-to-use-dhcp.md","title":"Configure Ricoh printers to use DHCP","description":"1. Go to: User Tools/Counter (top left)","date":"2013-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.245,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Configure Ricoh printers to use DHCP","authors":["Luke"],"date":"2013-12-02 00:00:00 +1300","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Outlook 2010 \u2013 Cannot process RSS Feed","permalink":"/2013/12/03/outlook-2010-cannot-process-rss-feed"},"nextItem":{"title":"How to lock a workstation in a Remote Desktop session","permalink":"/2013/12/02/how-to-lock-a-workstation-in-a-remote-desktop-session"}},"content":"1. <span style=\\"font-family:Times New Roman;font-size:12pt\\">Go to: <strong>User Tools/Counter</strong> (top left)<br /> </span>\\n  2. <span style=\\"font-family:Times New Roman;font-size:12pt\\">Select: <strong>System Settings</strong><br /> </span>\\n  3. <span style=\\"font-family:Times New Roman;font-size:12pt\\">Select: <strong>Interface Settings</strong><br /> </span>\\n  4. <span style=\\"font-family:Times New Roman;font-size:12pt\\">Select: <strong>Machine iPv4 Address</strong><br /> </span>\\n  5. <span style=\\"font-family:Times New Roman;font-size:12pt\\">Select: <strong>Auto-Obtain (DHCP)</strong><br /> </span>"},{"id":"/2013/12/02/how-to-lock-a-workstation-in-a-remote-desktop-session","metadata":{"permalink":"/2013/12/02/how-to-lock-a-workstation-in-a-remote-desktop-session","source":"@site/blog/2013-12-02-how-to-lock-a-workstation-in-a-remote-desktop-session.md","title":"How to lock a workstation in a Remote Desktop session","description":"1. Right click on the remote desktop>","date":"2013-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.135,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to lock a workstation in a Remote Desktop session","authors":["Luke"],"tags":["Windows"],"date":"2013-12-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Configure Ricoh printers to use DHCP","permalink":"/2013/12/02/configure-ricoh-printers-to-use-dhcp"},"nextItem":{"title":"Unable to load client print control error in Windows XP","permalink":"/2013/12/02/unable-to-load-client-print-control-error-in-windows-xp"}},"content":"1. Right click on the remote desktop>\\n  2. Point to New and click Shortcut.\\n  3. In the Create Shortcut dialog box, type in the following:\\n\\n    c:WindowsSystem32rundll32.exe user32.dll,LockWorkStation"},{"id":"/2013/12/02/unable-to-load-client-print-control-error-in-windows-xp","metadata":{"permalink":"/2013/12/02/unable-to-load-client-print-control-error-in-windows-xp","source":"@site/blog/2013-12-02-unable-to-load-client-print-control-error-in-windows-xp.md","title":"Unable to load client print control error in Windows XP","description":"Run the follow as: Administrator","date":"2013-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.04,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Unable to load client print control error in Windows XP","tags":["Windows"],"date":"2013-12-02 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to lock a workstation in a Remote Desktop session","permalink":"/2013/12/02/how-to-lock-a-workstation-in-a-remote-desktop-session"},"nextItem":{"title":"Excel 2010 \u2013 Adjusting Trusted Publisher\u2019s and Macro/Visual Basic Scripting","permalink":"/2013/12/01/excel-2010-adjusting-trusted-publishers-and-macrovisual-basic-scripting"}},"content":"Run the follow as: Administrator\\n\\n    c:regsvr32.exe /s c:winntsystem32rsclientprint.dll"},{"id":"/2013/12/01/excel-2010-adjusting-trusted-publishers-and-macrovisual-basic-scripting","metadata":{"permalink":"/2013/12/01/excel-2010-adjusting-trusted-publishers-and-macrovisual-basic-scripting","source":"@site/blog/2013-12-01-excel-2010-adjusting-trusted-publishers-and-macrovisual-basic-scripting.md","title":"Excel 2010 \u2013 Adjusting Trusted Publisher\u2019s and Macro/Visual Basic Scripting","description":"Open Excel 2010","date":"2013-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.605,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Excel 2010 \u2013 Adjusting Trusted Publisher\u2019s and Macro/Visual Basic Scripting","date":"2013-12-01 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Unable to load client print control error in Windows XP","permalink":"/2013/12/02/unable-to-load-client-print-control-error-in-windows-xp"},"nextItem":{"title":"How to adjust line spacing in Word 2010","permalink":"/2013/12/01/how-to-adjust-line-spacing-in-word-2010"}},"content":"<ol style=\\"margin-left: 39pt\\">\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Open <strong>Excel 2010</strong><br /> </span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Click <strong>File</strong><br /> </span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Click <strong>Options</strong></span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Click on <strong>Trust Center</strong><br /> </span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Click <strong>Trust Center Setting</strong></span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Click <strong>Macro Settings</strong></span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Check \u2013 <strong>Enable All macros (not recommended; potentially dangerous code can run)</strong><br /> </span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Check <strong>Trust Access to the VBA project object model </strong>check box </span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Click Ok </span>\\n  </li>\\n  <li>\\n    <span style=\\"font-family: Trebuchet MS;font-size: 10pt\\">Restart Excel 2010 </span>\\n  </li>\\n</ol>"},{"id":"/2013/12/01/how-to-adjust-line-spacing-in-word-2010","metadata":{"permalink":"/2013/12/01/how-to-adjust-line-spacing-in-word-2010","source":"@site/blog/2013-12-01-how-to-adjust-line-spacing-in-word-2010.md","title":"How to adjust line spacing in Word 2010","description":"* Click on Paragraph","date":"2013-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.215,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to adjust line spacing in Word 2010","date":"2013-12-01 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Excel 2010 \u2013 Adjusting Trusted Publisher\u2019s and Macro/Visual Basic Scripting","permalink":"/2013/12/01/excel-2010-adjusting-trusted-publishers-and-macrovisual-basic-scripting"},"nextItem":{"title":"Enable Swype mode on the Galaxy S4","permalink":"/2013/05/15/swypes4"}},"content":"<span style=\\"font-family:Times New Roman;font-size:12pt\\"><br /> </span>\\n\\n<p style=\\"margin-left: 36pt\\">\\n  <span style=\\"font-family:Times New Roman;font-size:12pt\\"><br /> </span>\\n</p>\\n\\n  * Click on Paragraph \\n  * Click the little &#8220;enlarge icon&#8221; \\n  * Change Line Spacing to: Single and uncheck don&#8217;t add space between paragraphs of the same style."},{"id":"/2013/05/15/swypes4","metadata":{"permalink":"/2013/05/15/swypes4","source":"@site/blog/2013-05-15-swypes4.md","title":"Enable Swype mode on the Galaxy S4","description":"1. Settings","date":"2013-05-15T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Enable Swype mode on the Galaxy S4","date":"2013-05-15 00:00:00 +1300","authors":["Luke"],"tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"How to adjust line spacing in Word 2010","permalink":"/2013/12/01/how-to-adjust-line-spacing-in-word-2010"},"nextItem":{"title":"How to redeem Neverwinter keys","permalink":"/2013/05/04/how-to-redeem-neverwinter-keys"}},"content":"1. Settings\\n  2. My Device\\n  3. Language and Input.\\n  4. Keyboards and input methods\\n  5. Choose the cog next to the &#8220;Samsung keyboard&#8221; option\\n  6. \xa0Check the &#8220;Continuous input&#8221; option"},{"id":"/2013/05/04/how-to-redeem-neverwinter-keys","metadata":{"permalink":"/2013/05/04/how-to-redeem-neverwinter-keys","source":"@site/blog/2013-05-04-how-to-redeem-neverwinter-keys.md","title":"How to redeem Neverwinter keys","description":"Have an item key for Neverwinter? Possibly from a Magazine promotion. Click on the link below to redeem.","date":"2013-05-04T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.11,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to redeem Neverwinter keys","date":"2013-05-04 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Enable Swype mode on the Galaxy S4","permalink":"/2013/05/15/swypes4"},"nextItem":{"title":"Forcefully remove WordPress Posts using PHPMyAdmin","permalink":"/2013/03/29/forcefully-remove-wordpress-posts-using-phpmyadmin"}},"content":"Have an item key for Neverwinter? Possibly from a Magazine promotion. Click on the link below to redeem.\\n\\n<a title=\\"Neverwinter_RedeemKey\\" href=\\"https://my.perfectworld.com/nw/redeemkey\\" target=\\"_blank\\">https://my.perfectworld.com/nw/redeemkey</a>"},{"id":"/2013/03/29/forcefully-remove-wordpress-posts-using-phpmyadmin","metadata":{"permalink":"/2013/03/29/forcefully-remove-wordpress-posts-using-phpmyadmin","source":"@site/blog/2013-03-29-forcefully-remove-wordpress-posts-using-phpmyadmin.md","title":"Forcefully remove WordPress Posts using PHPMyAdmin","description":"&nbsp;","date":"2013-03-29T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.2,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Forcefully remove WordPress Posts using PHPMyAdmin","date":"2013-03-29 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to redeem Neverwinter keys","permalink":"/2013/05/04/how-to-redeem-neverwinter-keys"},"nextItem":{"title":"How to Boot from CD/DVD on an iMac","permalink":"/2013/03/29/how-to-boot-from-cddvd-on-an-imac"}},"content":"&nbsp;\\n\\n  1. Launch PHPMyAdmin _(either directly or through your web hosting control panel &#8211; doesn&#8217;t matter)_.\\n  2. Go into the wordpress database\\n  3. Go into your: **Posts** table\\n  4. Click **Operations**\\n  5. Click **Empty the table (TRUNCATE)**\\n  6. Click Ok"},{"id":"/2013/03/29/how-to-boot-from-cddvd-on-an-imac","metadata":{"permalink":"/2013/03/29/how-to-boot-from-cddvd-on-an-imac","source":"@site/blog/2013-03-29-how-to-boot-from-cddvd-on-an-imac.md","title":"How to Boot from CD/DVD on an iMac","description":"1. Hold C during\xa0boot-up\xa0to load from CD/DVD (this skips the boot menu and loads the CD/DVD automatically).","date":"2013-03-29T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.115,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to Boot from CD/DVD on an iMac","authors":["Luke"],"tags":["Mac OSX"],"date":"2013-03-29 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Forcefully remove WordPress Posts using PHPMyAdmin","permalink":"/2013/03/29/forcefully-remove-wordpress-posts-using-phpmyadmin"},"nextItem":{"title":"Picasa &#8211; Upload a photo using browser uploaded.","permalink":"/2013/03/19/picasa-upload-a-photo-using-browser-uploaded"}},"content":"1. <span style=\\"line-height: 13px;\\">Hold </span><strong style=\\"line-height: 13px;\\">C</strong> <span style=\\"line-height: 13px;\\">during\xa0boot-up\xa0to load from CD/DVD<em> (this skips the boot menu and loads the CD/DVD automatically)</em>.</span>"},{"id":"/2013/03/19/picasa-upload-a-photo-using-browser-uploaded","metadata":{"permalink":"/2013/03/19/picasa-upload-a-photo-using-browser-uploaded","source":"@site/blog/2013-03-19-picasa-upload-a-photo-using-browser-uploaded.md","title":"Picasa &#8211; Upload a photo using browser uploaded.","description":"1. Right click Photo","date":"2013-03-19T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.1,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Picasa &#8211; Upload a photo using browser uploaded.","date":"2013-03-19 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to Boot from CD/DVD on an iMac","permalink":"/2013/03/29/how-to-boot-from-cddvd-on-an-imac"},"nextItem":{"title":"Restore your iPhone/iPad/iPod Touch Firmware","permalink":"/2013/02/26/restore-your-iphoneipadipod-touch-firmware"}},"content":"1. Right click Photo\\n  2. Copy Full Path\\n  3. Right click in Save dialog\\n  4. Select Paste\\n  5. Click Ok"},{"id":"/2013/02/26/restore-your-iphoneipadipod-touch-firmware","metadata":{"permalink":"/2013/02/26/restore-your-iphoneipadipod-touch-firmware","source":"@site/blog/2013-02-26-restore-your-iphoneipadipod-touch-firmware.md","title":"Restore your iPhone/iPad/iPod Touch Firmware","description":"Warning: Following this method will erase all your data including Contacts & Photos","date":"2013-02-26T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.68,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Restore your iPhone/iPad/iPod Touch Firmware","authors":["Luke"],"tags":["Mobile"],"date":"2013-02-26 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Picasa &#8211; Upload a photo using browser uploaded.","permalink":"/2013/03/19/picasa-upload-a-photo-using-browser-uploaded"},"nextItem":{"title":"Using phpmyadmin to remove WordPress posts","permalink":"/2013/02/25/using-phpmyadmin-to-remove-wordpress-posts"}},"content":"Warning: Following this method will erase all your data including Contacts & Photos\\n\\n  1. First you need to download your iPhone/iPod Touch firmware. Click \u201c<a title=\\"iPod/iPhone/iPad Firmware\\" href=\\"http://www.felixbruns.de/iPod/firmware/\\" target=\\"_blank\\">here</a>\u201d to download the firmware.\\n  2. Once the download has been completed \u2013 Open iTunes\\n  3. Click on your iPhone/iPod Touch _(if using older version of ituneswdsMisc; on the left if not it is top right)_\\n  4. Hold down **SHIFT** and left click the **Restore** button.\\n  5. A Browse for dialog will open \u2013 locate and select the downloaded iPhone/iPod firmware.\\n  6. Click Open\\n  7. iTunes will now verify the consistency and authenticity of the firmware file to verify it is a valid Apple firmware. Then start the restore process.\\n  8. Once completed your iPhone/iPod Touch should restart and you should now have a freshly reset iPhone/iPod Touch."},{"id":"/2013/02/25/using-phpmyadmin-to-remove-wordpress-posts","metadata":{"permalink":"/2013/02/25/using-phpmyadmin-to-remove-wordpress-posts","source":"@site/blog/2013-02-25-using-phpmyadmin-to-remove-wordpress-posts.md","title":"Using phpmyadmin to remove WordPress posts","description":"Warning: \xa0This will remove all posts on your wordpress installation. Make sure you have a backup or simply don\u2019t want them anymore&#8230; when they are gone they are gone!","date":"2013-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.4,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Using phpmyadmin to remove WordPress posts","date":"2013-02-25 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Restore your iPhone/iPad/iPod Touch Firmware","permalink":"/2013/02/26/restore-your-iphoneipadipod-touch-firmware"},"nextItem":{"title":"Windows Vista Memory Management Blue Screen of Death","permalink":"/2013/02/25/windows-vista-memory-management-blue-screen-of-death"}},"content":"Warning: \xa0_This will remove all posts on your wordpress installation. Make sure you have a backup or simply don\u2019t want them anymore&#8230; when they are gone they are gone!_\\n\\n  1. Login to PHPMyAdmin\\n  2. On the left hand side \u2013 select your wordpress database\\n  3. Click on the Posts stable\\n  4. Click Operations (top menu bar)\\n  5. Click Empty the table (TRUNCATE)\\n  6. Click Ok\\n\\nNote: Clearing the postsmeta table will delete the references to your images and post tags."},{"id":"/2013/02/25/windows-vista-memory-management-blue-screen-of-death","metadata":{"permalink":"/2013/02/25/windows-vista-memory-management-blue-screen-of-death","source":"@site/blog/2013-02-25-windows-vista-memory-management-blue-screen-of-death.md","title":"Windows Vista Memory Management Blue Screen of Death","description":"The Windows Vista Memory Management BSOD (Blue Screen Of Death) is usually caused by a faulty RAM (Random Access Memory) module or a RAM module that has an incorrect or different speed then the other module which might be in the computer.","date":"2013-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.405,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Vista Memory Management Blue Screen of Death","authors":["Luke"],"tags":["Windows"],"date":"2013-02-25 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Using phpmyadmin to remove WordPress posts","permalink":"/2013/02/25/using-phpmyadmin-to-remove-wordpress-posts"},"nextItem":{"title":"Booting into Recovery Partition instead of Operating System","permalink":"/2013/02/23/booting-into-recovery-partition-instead-of-operating-system"}},"content":"The Windows Vista Memory Management BSOD _(Blue Screen Of Death)_ is usually caused by a faulty RAM _(Random Access Memory)_ module or a RAM module that has an incorrect or different speed then the other module which might be in the computer.\\n\\n* Download <a title=\\"memtestx86\\" href=\\"http://www.memtest86.com/download.htm\\" target=\\"_blank\\">memtestx86 </a>and burn to a CD\\n* Boot from the CD and allow the scan to go \u2013 any red lines indicate bad RAM. I suggest allowing the scan to go for 2 passes."},{"id":"/2013/02/23/booting-into-recovery-partition-instead-of-operating-system","metadata":{"permalink":"/2013/02/23/booting-into-recovery-partition-instead-of-operating-system","source":"@site/blog/2013-02-23-booting-into-recovery-partition-instead-of-operating-system.md","title":"Booting into Recovery Partition instead of Operating System","description":"Having issues with your computer booting into the Recovery Partition instead of the Windows operating system? Try the guide below to repair.","date":"2013-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.755,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Booting into Recovery Partition instead of Operating System","authors":["Luke"],"tags":["Windows"],"date":"2013-02-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows Vista Memory Management Blue Screen of Death","permalink":"/2013/02/25/windows-vista-memory-management-blue-screen-of-death"},"nextItem":{"title":"How to boot into a CD/DVD on a iMac/Macbook","permalink":"/2013/02/23/how-to-boot-into-a-cddvd-on-a-imacmacbook"}},"content":"Having issues with your computer booting into the Recovery Partition instead of the Windows operating system? Try the guide below to repair.\\n\\n\\n  1. First download <a title=\\"GParted (Gnome Partition Manager)\\" href=\\"http://gparted.sourceforge.net/download.php\\" target=\\"_blank\\">Gparted</a> and burn it to a CD\\n  2. Boot from the Gparted CD _(F12 or Esc are common hotkeys to bring up the boot menu during system startup)_\\n  3. Just press Enter through the default options to boot into the user interface.\\n  4. You should see several partitions on your computer \u2013 this usually consists of System, Operating System (C:) and Data.\\n  5. Right click the Operating System partition\\n  6. Left click Flags\\n  7. Left click Boot _(the boot flag will set the OS partition to bootable \u2013 it is probably set to System or the Recovery partition at the moment)_\\n  8. Click ok\\n  9. Now shutdown gparted and restart your computer. It should now boot into the operating system."},{"id":"/2013/02/23/how-to-boot-into-a-cddvd-on-a-imacmacbook","metadata":{"permalink":"/2013/02/23/how-to-boot-into-a-cddvd-on-a-imacmacbook","source":"@site/blog/2013-02-23-how-to-boot-into-a-cddvd-on-a-imacmacbook.md","title":"How to boot into a CD/DVD on a iMac/Macbook","description":"1. Press\xa0C during iMac/Macbook startup","date":"2013-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.025,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to boot into a CD/DVD on a iMac/Macbook","authors":["Luke"],"tags":["Mac OSX"],"date":"2013-02-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Booting into Recovery Partition instead of Operating System","permalink":"/2013/02/23/booting-into-recovery-partition-instead-of-operating-system"},"nextItem":{"title":"Network Map Windows startup Script","permalink":"/2013/02/23/network-map-windows-startup-script"}},"content":"1. Press\xa0**C** during iMac/Macbook startup"},{"id":"/2013/02/23/network-map-windows-startup-script","metadata":{"permalink":"/2013/02/23/network-map-windows-startup-script","source":"@site/blog/2013-02-23-network-map-windows-startup-script.md","title":"Network Map Windows startup Script","description":"`","date":"2013-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.255,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Network Map Windows startup Script","authors":["Luke"],"tags":["Windows"],"date":"2013-02-23 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to boot into a CD/DVD on a iMac/Macbook","permalink":"/2013/02/23/how-to-boot-into-a-cddvd-on-a-imacmacbook"},"nextItem":{"title":"Visual/HTML Editor stuck in WordPress","permalink":"/2013/02/19/visualhtml-editor-stuck-in-wordpress"}},"content":"`<br />\\necho off<br />\\ncls<br />\\nnet use z: /delete /y<br />\\nnet use z: \\"192.168.1.1share\\" /y<br />\\n` \\n\\nNote: \xa0&#8220;Z is the drive letter I have assigned for this example &#8211; you can however change this to what you want _(if it already is not in use of course)_.)"},{"id":"/2013/02/19/visualhtml-editor-stuck-in-wordpress","metadata":{"permalink":"/2013/02/19/visualhtml-editor-stuck-in-wordpress","source":"@site/blog/2013-02-19-visualhtml-editor-stuck-in-wordpress.md","title":"Visual/HTML Editor stuck in WordPress","description":"Having problems with creating or editing a post on WordPress and you cannot change between the Visual Editor and HTML Editor?","date":"2013-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.38,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Visual/HTML Editor stuck in WordPress","authors":["Luke"],"date":"2013-02-19 00:00:00 +1300","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Network Map Windows startup Script","permalink":"/2013/02/23/network-map-windows-startup-script"},"nextItem":{"title":"How to increase posts displayed in WordPress All Post page","permalink":"/2013/02/17/how-to-increase-posts-displayed-in-wordpress-all-post-page"}},"content":"Having problems with creating or editing a post on WordPress and you cannot change between the Visual Editor and HTML Editor?\\n\\n  1. First thing is trying to clearing your browser cache.\\n  2. If that\xa0doesn&#8217;t\xa0work the issue is probably the theme you are using, some themes have shortcodes and therefore have buttons on the editor that might be damaged or corrupt \u2013 the fix for this is update to a newer theme version or change the theme."},{"id":"/2013/02/17/how-to-increase-posts-displayed-in-wordpress-all-post-page","metadata":{"permalink":"/2013/02/17/how-to-increase-posts-displayed-in-wordpress-all-post-page","source":"@site/blog/2013-02-17-how-to-increase-posts-displayed-in-wordpress-all-post-page.md","title":"How to increase posts displayed in WordPress All Post page","description":"Do you want to display more then 20 posts in the All Posts list within WordPress? Follow the guide below to reveal the rest.","date":"2013-02-17T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.535,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to increase posts displayed in WordPress All Post page","date":"2013-02-17 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Visual/HTML Editor stuck in WordPress","permalink":"/2013/02/19/visualhtml-editor-stuck-in-wordpress"},"nextItem":{"title":"Windows Mail not displaying contacts","permalink":"/2013/02/12/windows-mail-not-displaying-contacts"}},"content":"Do you want to display more then 20 posts in the All Posts list within WordPress? Follow the guide below to reveal the rest.\\n\\n  1. Make sure you are logged into your WordPress account\\n  2. Click on **Posts** _(left hand side)_\\n  3. Click **All Posts**\\n  4. Click **Screen Options** _(up the top)_\\n  5. You should now see a an editable number box &#8211; at the moment it should be set to: 20. **Change** this **field** to match how many posts you would like to display _(for example 500)_.\\n  6. Click **Apply**\\n  7. Your All Posts page should now display the amount of posts listed in step 5"},{"id":"/2013/02/12/windows-mail-not-displaying-contacts","metadata":{"permalink":"/2013/02/12/windows-mail-not-displaying-contacts","source":"@site/blog/2013-02-12-windows-mail-not-displaying-contacts.md","title":"Windows Mail not displaying contacts","description":"This happens when Windows Mail is using a different contacts list to the Windows Contacts \u2013 such as a newsgroup.","date":"2013-02-12T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Mail not displaying contacts","date":"2013-02-12 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to increase posts displayed in WordPress All Post page","permalink":"/2013/02/17/how-to-increase-posts-displayed-in-wordpress-all-post-page"},"nextItem":{"title":"Outlook Data File Cannot be accessed","permalink":"/2013/02/11/outlook-data-file-cannot-be-accessed"}},"content":"This happens when Windows Mail is using a different contacts list to the Windows Contacts \u2013 such as a newsgroup.\\n\\n  1. Open **Windows Mail**\\n  2. Click **Contacts** _(left hand side)_\\n  3. Click **Import**\\n  4. Click **From Windows Address Book**\\n\\nWindows Mail will then import all your contacts into the address book restart Windows Mail and you should now be able to display and compose messages to your contacts."},{"id":"/2013/02/11/outlook-data-file-cannot-be-accessed","metadata":{"permalink":"/2013/02/11/outlook-data-file-cannot-be-accessed","source":"@site/blog/2013-02-11-outlook-data-file-cannot-be-accessed.md","title":"Outlook Data File Cannot be accessed","description":"I have found this issue occurs commonly when upgrading from Outlook 2007 to 2010.","date":"2013-02-11T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.315,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Outlook Data File Cannot be accessed","authors":["Luke"],"date":"2013-02-11 00:00:00 +1300","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Windows Mail not displaying contacts","permalink":"/2013/02/12/windows-mail-not-displaying-contacts"},"nextItem":{"title":"The messaging interface has returned an unknown error","permalink":"/2013/02/07/the-messaging-interface-has-returned-an-unknown-error"}},"content":"I have found this issue occurs commonly when upgrading from Outlook 2007 to 2010.\\n\\n&nbsp;\\n\\n  1. Open Outlook 2010\\n  2. Click **File** _(top left)_\\n  3. Click **Account Settings**\\n  4. Click **Account Settings**\\n  5. Click **Change Folder**\\n  6. Click the \u201c**+**\u201d on the left of the folder name to reveal hidden subfolders.\\n  7. Click **Inbox**\\n  8. Click **Ok**\\n  9. Close all open\xa0dialog\xa0and **restart** Outlook."},{"id":"/2013/02/07/the-messaging-interface-has-returned-an-unknown-error","metadata":{"permalink":"/2013/02/07/the-messaging-interface-has-returned-an-unknown-error","source":"@site/blog/2013-02-07-the-messaging-interface-has-returned-an-unknown-error.md","title":"The messaging interface has returned an unknown error","description":"This issue commonly occurs in Microsoft Outlook when Outlook has reached its file size limit. To fix this you need to allow Outlook to use a larger sized table.","date":"2013-02-06T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.455,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"The messaging interface has returned an unknown error","date":"2013-02-06T00:00:00.000Z","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Outlook Data File Cannot be accessed","permalink":"/2013/02/11/outlook-data-file-cannot-be-accessed"},"nextItem":{"title":"How to rip Audio from a DVD","permalink":"/2013/01/31/how-to-rip-audio-from-a-dvd"}},"content":"This issue commonly occurs in Microsoft Outlook when Outlook has reached its file size limit. To fix this you need to allow Outlook to use a larger sized table.\\n\\n  1. Launch Microsoft Outlook\\n  2. Right click the folder list (on the left hand side) and left click Properties.\\n  3. Click Advanced\\n  4. Check _\u201cAllow upgrade to large tables\u201d_.\\n  5. Click Ok\\n  6. Click Ok\\n  7. Restart Outlook\\n\\nNote: You can also force emails to be deleted thereby clearing the Outlook table by selecting emails and select SHIFT-DEL at the same time."},{"id":"/2013/01/31/how-to-rip-audio-from-a-dvd","metadata":{"permalink":"/2013/01/31/how-to-rip-audio-from-a-dvd","source":"@site/blog/2013-01-31-how-to-rip-audio-from-a-dvd.md","title":"How to rip Audio from a DVD","description":"In this guide I am using a media player called: VLC (VideoLAN) to do the conversion \u2013 it can be downloaded free \u201chere\u201d.","date":"2013-01-31T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.645,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to rip Audio from a DVD","authors":["Luke"],"tags":["Windows"],"date":"2013-01-31 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"The messaging interface has returned an unknown error","permalink":"/2013/02/07/the-messaging-interface-has-returned-an-unknown-error"},"nextItem":{"title":"Shutdown Batch Script","permalink":"/2013/01/17/shutdown-batch-script"}},"content":"In this guide I am using a media player called: VLC (VideoLAN) to do the conversion \u2013 it can be downloaded free \u201c<a title=\\"VLC Media Player\\" href=\\"http://www.videolan.org/vlc/download-windows.html\\" target=\\"_blank\\">here</a>\u201d.\\n\\n  1. Once installed \u2013 open VLC media player\\n  2. Put the DVD you want to rip the audio into your computer\\n  3. Click **Media** _(up-the-top)_\\n  4. Click **Convert**/Save\\n  5. Click **Disc** _(up-the-top)_\\n  6. Make sure the DVD is selected\\n  7. Press **Convert/Save** _(down-the-bottom)_\\n  8. Change the destination file field to the location where you would like the audio saved to.\\n  9. In the **profile** drop down box select **Audio \u2013MP3**\\n 10. Click **Start**\\n\\nNote: Make sure you have the right stream selected in step 6 &#8211; or you might just record the Menu sound &#8211; for example: VOB\\\\_1 compared to Vob\\\\_0."},{"id":"/2013/01/17/shutdown-batch-script","metadata":{"permalink":"/2013/01/17/shutdown-batch-script","source":"@site/blog/2013-01-17-shutdown-batch-script.md","title":"Shutdown Batch Script","description":"Save the following into a notepad document and name it with .bat at the end &#8211; for example shutdown.BAT. You can then either set it to the desktop or add it to a scheduled task to turn your computer off at a certain time.","date":"2013-01-17T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.495,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Shutdown Batch Script","date":"2013-01-17 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to rip Audio from a DVD","permalink":"/2013/01/31/how-to-rip-audio-from-a-dvd"},"nextItem":{"title":"Windows Update Error 0x800f081f","permalink":"/2013/01/17/windows-update-error-0x800f081f"}},"content":"Save the following into a notepad document and name it with .bat at the end &#8211; for example shutdown.BAT. You can then either set it to the desktop or add it to a scheduled task to turn your computer off at a certain time.\\n\\n`<br />\\n@echo off<br />\\nshutdown.exe -s -t 00<br />\\nexit<br />\\n` \\n\\nNote: Changing &#8220;-s&#8221; to &#8220;-r&#8221; will restart the computer, instead of turning it off and adjusting the &#8220;-t&#8221; will change the time in seconds till it shuts down. Useful if you are wanting the computer to shutdown after an hour or so."},{"id":"/2013/01/17/windows-update-error-0x800f081f","metadata":{"permalink":"/2013/01/17/windows-update-error-0x800f081f","source":"@site/blog/2013-01-17-windows-update-error-0x800f081f.md","title":"Windows Update Error 0x800f081f","description":"This problem is generally caused by corrupted/damaged DNS Cache. Follow the instructions below to repair.","date":"2013-01-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.01,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Update Error 0x800f081f","date":"2013-01-17 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Shutdown Batch Script","permalink":"/2013/01/17/shutdown-batch-script"},"nextItem":{"title":"Windows XP Startup Batch Script","permalink":"/2013/01/17/windows-xp-startup-batch-script"}},"content":"This problem is generally caused by corrupted/damaged DNS Cache. Follow the instructions below to repair.\\n\\n  1. Click Start\\n  2. Click All Programs/Programs\\n  3. Click Accessories\\n  4. Right click Command Prompt select Run as Administrator\\n  5. Type in:_ **ipconfig /flushdns**_\\n  6. Press Enter\\n  7. Restart your computer and attempt Windows Update.\\n\\n\\nIf the above doesn\u2019t work then you can bypass the problem by assigning your own DNS server , such as Google DNS.\\n\\n  1. Click Start\\n  2. Click Control Panel\\n  3. Click Network/Network & Sharing Center\\n  4. Click Change Adapter Settings (on the left hand side)\\n  5. Right click Local Area Connection _(if you are using wired method of connecting \u2013 if not select Wireless Connection)_\\n  6. Select Properties\\n  7. Click Internet Protocol Version 4 (TCP/IPv4)\\n  8. Click Properties\\n  9. Select Use the following DNS server addresses\\n 10. Type in: 8.8.8.8 and on the second line 8.8.4.4\\n 11. Click Ok\\n 12. You are now routing your computers DNS through google\u2019s servers instead of the service offered by your Internet Service Provider. _Depending on who and where you are \u2013 you might notice a slowdown in internet browsing or a speed up \u2013 this is normal._\\n 13. Restart your computer and attempt Windows Update."},{"id":"/2013/01/17/windows-xp-startup-batch-script","metadata":{"permalink":"/2013/01/17/windows-xp-startup-batch-script","source":"@site/blog/2013-01-17-windows-xp-startup-batch-script.md","title":"Windows XP Startup Batch Script","description":"Save the following into a notepad document and name it with .bat at the end &#8211; for example WinXPStartup.BAT. You can then run the batch script directly from the recovery console on a Windows XP CD.","date":"2013-01-17T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.345,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows XP Startup Batch Script","date":"2013-01-17 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Windows Update Error 0x800f081f","permalink":"/2013/01/17/windows-update-error-0x800f081f"},"nextItem":{"title":"Repair Windows Explorer start-up","permalink":"/2013/01/15/repair-windows-explorer-start-up"}},"content":"Save the following into a notepad document and name it with .bat at the end &#8211; for example WinXPStartup.BAT. You can then run the batch script directly from the recovery console on a Windows XP CD.\\n\\n`<br />\\n@echo off<br />\\nCD ..<br />\\nATTRIB -H C:boot.ini<br />\\nATTRIB -S C:boot.ini<br />\\nATRIB -R C:boot.ini<br />\\ndel boot.ini<br />\\nBOOTCFG /Rebuild<br />\\nCHKDSK /R /F<br />\\nFIXBOOT<br />\\n`"},{"id":"/2013/01/15/repair-windows-explorer-start-up","metadata":{"permalink":"/2013/01/15/repair-windows-explorer-start-up","source":"@site/blog/2013-01-15-repair-windows-explorer-start-up.md","title":"Repair Windows Explorer start-up","description":"This usually occurs if Viruses or Spyware has infected explorer.exe and changed the registry entry to stop it from starting.","date":"2013-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.52,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Repair Windows Explorer start-up","authors":["Luke"],"tags":["Windows"],"date":"2013-01-15 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Windows XP Startup Batch Script","permalink":"/2013/01/17/windows-xp-startup-batch-script"},"nextItem":{"title":"Ubuntu \u2013 Out of range","permalink":"/2013/01/14/ubuntu-out-of-range"}},"content":"This usually occurs if Viruses or Spyware has infected _explorer.exe_ and changed the registry entry to stop it from starting.\\n\\n  1. Click Start (orb)\\n  2. Type in: **regedit**\\n  3. Press Enter\\n  4. Navigate to:\xa0 **HKEY\\\\_LOCAL\\\\_MACHINESoftwareMicrosoftWindows NTCurrentVersionwinlogon**\\n  5. Double click the **Shell** entry\\n  6. Make sure it is set to: **Explorer.exe**\\n  7. Press Ok\\n  8. **Restart** your computer\\n\\n&nbsp;\\n\\nIf the above\xa0doesn&#8217;t\xa0work attempt a system restore by pressing F8 during Windows Vista boot to bring up the Boot Menu and select Startup Repair.\\n\\n&nbsp;\\n\\n_Note: Tested with Windows Vista &#8211; ok_\\n\\n_Note: Also may work on Windows 7 &#8211; though I haven&#8217;t personally tested this._"},{"id":"/2013/01/14/ubuntu-out-of-range","metadata":{"permalink":"/2013/01/14/ubuntu-out-of-range","source":"@site/blog/2013-01-14-ubuntu-out-of-range.md","title":"Ubuntu \u2013 Out of range","description":"This issue is commonly caused by the screen resolution settings in Ubuntu becoming too high for the actual monitor. This guide is written with the Ubuntu Live CD and version 12.10 in mind.","date":"2013-01-14T08:40:11.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.65,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Ubuntu \u2013 Out of range","date":"2013-01-14T08:40:11.000Z","authors":["Luke"],"tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Repair Windows Explorer start-up","permalink":"/2013/01/15/repair-windows-explorer-start-up"},"nextItem":{"title":"How to hide PHP errors on a WordPress installation","permalink":"/2013/01/14/how-to-hide-php-errors-on-a-wordpress-installation"}},"content":"This issue is commonly caused by the screen resolution settings in Ubuntu becoming too high for the actual monitor. This guide is written with the Ubuntu Live CD and version 12.10 in mind.\\n\\n  1. During start-up select your language when prompted.\\n  2. Press **F6** (Other Options)\\n  3. At the end of the command line type: **vga=791**\\n  4. Press Enter\\n\\n_Ubuntu should now boot with a resolution of 1024 by 768 (32bit). Click &#8220;<a title=\\"VESA Video Modes\\" href=\\"http://en.wikipedia.org/wiki/VESA_BIOS_Extensions#Linux_video_mode_numbers\\" target=\\"_blank\\">here</a>&#8221; for a Wikipedia link to other Video display choices._\\n\\nIf that doesn&#8217;t work:\\n\\nStart the Live CD process\\n\\n  1. **Unplug** the **VGA** port for the monitor for about **5 minutes** &#8211; _allowing Ubuntu to properly start._\\n  2. **Plug** the monitor back **in** and Ubuntu should automatically select the right resolution and display."},{"id":"/2013/01/14/how-to-hide-php-errors-on-a-wordpress-installation","metadata":{"permalink":"/2013/01/14/how-to-hide-php-errors-on-a-wordpress-installation","source":"@site/blog/2013-01-14-how-to-hide-php-errors-on-a-wordpress-installation.md","title":"How to hide PHP errors on a WordPress installation","description":"1. Using an FTP client \u2013 such as Filezilla\xa0log in\xa0to your website.","date":"2013-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.315,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to hide PHP errors on a WordPress installation","date":"2013-01-14 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Ubuntu \u2013 Out of range","permalink":"/2013/01/14/ubuntu-out-of-range"},"nextItem":{"title":"How to import a CSV into iMail","permalink":"/2013/01/14/how-to-import-a-csv-into-imail"}},"content":"1. Using an FTP client \u2013 such as <a title=\\"FileZilla\\" href=\\"http://filezilla-project.org/\\" target=\\"_blank\\">Filezilla</a>\xa0log in\xa0to your website.\\n  2. Navigate to your\xa0WordPress\xa0directory\\n  3. Right click **_wp-config.php_**\\n  4. Select **Edit**\\n  5. Add: \xa0**_@ini\\\\_set(&#8216;display\\\\_errors&#8217;, 0);\xa0_**to the wp-config document \u2013 up the top is fine.\\n  6. Now save the document, your FTP client should now upload the changes and any PHP errors you may be displaying will be hidden."},{"id":"/2013/01/14/how-to-import-a-csv-into-imail","metadata":{"permalink":"/2013/01/14/how-to-import-a-csv-into-imail","source":"@site/blog/2013-01-14-how-to-import-a-csv-into-imail.md","title":"How to import a CSV into iMail","description":"First \u2013 make sure iMail is closed.","date":"2013-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.295,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to import a CSV into iMail","date":"2013-01-14 00:00:00 +1300","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to hide PHP errors on a WordPress installation","permalink":"/2013/01/14/how-to-hide-php-errors-on-a-wordpress-installation"},"nextItem":{"title":"How to connect a Kindle to Wifi","permalink":"/2012/12/27/how-to-connect-a-kindle-to-wifi"}},"content":"_First \u2013 make sure iMail is closed._\\n\\n  1. Open **Finder**\\n  2. Go to your OSX install HDD and navigate to: **_Applications_**\\n  3. Open **Address Book**\\n  4. Click on **File**\\n  5. Click **Import**\\n  6. Navigate to the location of the saved CSV _(Comma Separated Values)_ and select it.\\n  7. Select Open\\n\\n_You have now successfully imported your contacts into iMail._"},{"id":"/2012/12/27/how-to-connect-a-kindle-to-wifi","metadata":{"permalink":"/2012/12/27/how-to-connect-a-kindle-to-wifi","source":"@site/blog/2012-12-27-how-to-connect-a-kindle-to-wifi.md","title":"How to connect a Kindle to Wifi","description":"1. Press the Home Button","date":"2012-12-27T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.36,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to connect a Kindle to Wifi","date":"2012-12-27 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to import a CSV into iMail","permalink":"/2013/01/14/how-to-import-a-csv-into-imail"},"nextItem":{"title":"iPhone/iPod \u2013 App Store is in Chinese","permalink":"/2012/12/17/iphoneipod-app-store-is-in-chinese"}},"content":"1. Press the **Home** Button\\n  2. Press the **Menu** Button\\n  3. Select **Experiment**\\n  4. Select **Launch** **Browser**\\n  5. Press **Menu**\\n  6. Select **Bookmarks**\\n  7. Here you can either select a pre-selected **Bookmark** or **enter** the **URL** of an address you want to go to.\\n  8. The Wifi dialog will then launch allowing you to select your Wireless Access Point and enter in your security key (remember the security key is case sensitive)."},{"id":"/2012/12/17/iphoneipod-app-store-is-in-chinese","metadata":{"permalink":"/2012/12/17/iphoneipod-app-store-is-in-chinese","source":"@site/blog/2012-12-17-iphoneipod-app-store-is-in-chinese.md","title":"iPhone/iPod \u2013 App Store is in Chinese","description":"1. Launch App Store","date":"2012-12-17T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.505,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"iPhone/iPod \u2013 App Store is in Chinese","date":"2012-12-17 00:00:00 +1300","authors":["Luke"],"tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"How to connect a Kindle to Wifi","permalink":"/2012/12/27/how-to-connect-a-kindle-to-wifi"},"nextItem":{"title":"How to access the HDD on a HP Mini","permalink":"/2012/12/14/how-to-access-the-hdd-on-a-hp-mini"}},"content":"1. Launch **App Store**\\n  2. Select **Featured** _(left most option \u2013 down bottom)_\\n  3. Scroll to the bottom and select your **account** _(the button is\xa0centered\xa0and long)_\\n  4. Here you can **change** your Account **country**.\\n\\nIf the above doesn&#8217;t\xa0work \u2013 you need to either create a new iTunes account temporarily or borrow someone\u2019s.\\n\\n  1. On the iPhone/iPod go to **Settings**\\n  2. Select **iTunes & App Store**\\n  3. **Log** **in** with an **English** apple **account**.\\n  4. **Open** App **Store**\\n\\nThe App Store will switch to English \u2013 you can now logout of the English account and the iPhone/iPod will retain its language setting."},{"id":"/2012/12/14/how-to-access-the-hdd-on-a-hp-mini","metadata":{"permalink":"/2012/12/14/how-to-access-the-hdd-on-a-hp-mini","source":"@site/blog/2012-12-14-how-to-access-the-hdd-on-a-hp-mini.md","title":"How to access the HDD on a HP Mini","description":"1. Turn the laptop upside down \xa0(make sure it is resting against a soft surface, such as a towel.)","date":"2012-12-14T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.355,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to access the HDD on a HP Mini","date":"2012-12-14 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"iPhone/iPod \u2013 App Store is in Chinese","permalink":"/2012/12/17/iphoneipod-app-store-is-in-chinese"},"nextItem":{"title":"Black screen during Windows 8 installation","permalink":"/2012/12/13/black-screen-during-windows-8-installation"}},"content":"1. Turn the laptop upside down \xa0(_make sure it is resting against a soft surface, such as a towel.)_\\n  2. **Remove** the **battery**\\n  3. Behind the battery is an **orange** **slider**.\\n  4. **Slide** the orange slider **left**.\\n  5. Now **push** the **casing** **forward** _(towards the battery end)_\\n  6. The plastic casing will then slide off revealing the HDD and on-board CMOs battery.\\n\\nNote: The HP Mini uses a 2.5&#8243; SATA HDD"},{"id":"/2012/12/13/black-screen-during-windows-8-installation","metadata":{"permalink":"/2012/12/13/black-screen-during-windows-8-installation","source":"@site/blog/2012-12-13-black-screen-during-windows-8-installation.md","title":"Black screen during Windows 8 installation","description":"This issue occurs when Windows 8 switches to UEFI install mode \u2013 on a non-compatible motherboard. This is commonly caused by multiple HDDs plugged in during boot. Follow the guide below to fix it.","date":"2012-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.405,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Black screen during Windows 8 installation","date":"2012-12-13 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to access the HDD on a HP Mini","permalink":"/2012/12/14/how-to-access-the-hdd-on-a-hp-mini"},"nextItem":{"title":"Change your hostname in Ubuntu","permalink":"/2012/12/13/change-your-hostname-in-ubuntu"}},"content":"This issue occurs when Windows 8 switches to UEFI install mode \u2013 on a non-compatible motherboard. This is commonly caused by multiple HDDs plugged in during boot. Follow the guide below to fix it.\\n\\n  1. **Unplug** any secondary **HDDs** (Hard drives) from your computer and leave the operating system HDD in.\\n  2. **Begin** the **installation** again &#8211; the install should automatically change the install to master boot record.\\n  3. Once installed &#8211; turn computer off and plug your other HDDs in."},{"id":"/2012/12/13/change-your-hostname-in-ubuntu","metadata":{"permalink":"/2012/12/13/change-your-hostname-in-ubuntu","source":"@site/blog/2012-12-13-change-your-hostname-in-ubuntu.md","title":"Change your hostname in Ubuntu","description":"Note: These instructions are for Ubuntu 12.10. Changing\xa0your\xa0host name\xa0in Ubuntu guide.","date":"2012-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.125,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Change your hostname in Ubuntu","date":"2012-12-13 00:00:00 +1300","authors":["Luke"],"tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Black screen during Windows 8 installation","permalink":"/2012/12/13/black-screen-during-windows-8-installation"},"nextItem":{"title":"Metro 2033 This program cannot continue because PhysXloader","permalink":"/2012/12/13/metro-2033-this-program-cant-continue-because-physxloader-dll"}},"content":"_Note: These instructions are for Ubuntu 12.10. Changing\xa0your\xa0host name\xa0in Ubuntu guide._\\n\\n  1. Go to **System Settings**\\n  2. Click **Details**\\n  3. Click: **Give a unique name**"},{"id":"/2012/12/13/metro-2033-this-program-cant-continue-because-physxloader-dll","metadata":{"permalink":"/2012/12/13/metro-2033-this-program-cant-continue-because-physxloader-dll","source":"@site/blog/2012-12-13-metro-2033-this-program-cant-continue-because-physxloader-dll.md","title":"Metro 2033 This program cannot continue because PhysXloader","description":"1. Click Start.","date":"2012-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.395,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Metro 2033 This program cannot continue because PhysXloader","date":"2012-12-13 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Change your hostname in Ubuntu","permalink":"/2012/12/13/change-your-hostname-in-ubuntu"},"nextItem":{"title":"How to do a colour adjustment on a Sharp MX-4101N","permalink":"/2012/11/26/colour-adjust-sharp-mx-4101n"}},"content":"1. Click **Start**.\\n  2. Click **Computer/My Computer.**\\n  3. Navigate to:\xa0 **_C:Program Files (x86)Steamsteamappscommonmetro 2033install_** \xa0(_the path might be different dependant on your machine._\\n  4. Find and **run** the PhysX **application** **installer** _(in my instance it is: PhysX\\\\_10.02.22\\\\_9.10.0222_SystemSoftware.exe)_ it will then **uninstall** the copy of **PhysX** on the machine.\\n  5. Once the install has completed **run** the PhysX **installer** again to\xa0**re-install**.\\n  6. Once completed \u2013 you have to close and **restart** the **Steam** client.\\n  7. Attempt to run Metro 2033"},{"id":"/2012/11/26/colour-adjust-sharp-mx-4101n","metadata":{"permalink":"/2012/11/26/colour-adjust-sharp-mx-4101n","source":"@site/blog/2012-11-26-colour-adjust-sharp-mx-4101n.md","title":"How to do a colour adjustment on a Sharp MX-4101N","description":"How to do a colour adjustment on a Sharp MX-4101N","date":"2012-11-26T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.4,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to do a colour adjustment on a Sharp MX-4101N","date":"2012-11-26 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Metro 2033 This program cannot continue because PhysXloader","permalink":"/2012/12/13/metro-2033-this-program-cant-continue-because-physxloader-dll"},"nextItem":{"title":"Acronis Snap Deploy spwizimg.dll error","permalink":"/2012/11/13/acronis-snap-deploy-spwizimg-dll-error"}},"content":"_How to do a colour adjustment on a Sharp MX-4101N_\\n\\n&nbsp;\\n\\n**For Copier:**\\n\\n  1. **System** Settings (right of the LCD panel)\\n  2. Select on **_Admin_** (type in password &#8211; default is lowercase admin)\\n  3. Select **Copy**\\n  4. Select **Settings**\\n  5. Navigate to: **Colour Adjustments**\\n  6. Select **Auto Colour Calibration**\\n\\n**For Printer:**\\n\\n  1. **System** Settings (right of the LCD panel)\\n  2. Select on **_Admin_** (type in password &#8211; default is lowercase admin)\\n  3. Select **Printer** **Settings**\\n  4. Select **Auto Colour Calibration**"},{"id":"/2012/11/13/acronis-snap-deploy-spwizimg-dll-error","metadata":{"permalink":"/2012/11/13/acronis-snap-deploy-spwizimg-dll-error","source":"@site/blog/2012-11-13-acronis-snap-deploy-spwizimg-dll-error.md","title":"Acronis Snap Deploy spwizimg.dll error","description":"Attempting to install Acronis Snap Deploy and getting the \u201cspwizimg.dll\u201d error? Follow the guide below to repair.","date":"2012-11-13T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.35,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Acronis Snap Deploy spwizimg.dll error","date":"2012-11-13 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to do a colour adjustment on a Sharp MX-4101N","permalink":"/2012/11/26/colour-adjust-sharp-mx-4101n"},"nextItem":{"title":"Ubuntu &#8211; Wireless is disabled by hardware switch","permalink":"/2012/11/13/ubuntu-wireless-is-disabled-by-hardware-switch"}},"content":"Attempting to install Acronis Snap Deploy and getting the \u201cspwizimg.dll\u201d error? Follow the guide below to repair.\\n\\n  1. Click \u201c<a title=\\"spqizimg.dll download\\" href=\\"http://www.dllme.com/dll/files/spwizimg_dll.html\\" target=\\"_blank\\">here</a>\u201d to download the **_spwizimg.dll_**\\n  2. Once downloaded, **extract** to a folder you can easily access.\\n  3. Right click the spwizimg.dll and select **Copy**\\n  4. Click **Start**\\n  5. Click **Computer/My Computer**\\n  6. Navigate to **C:WINDOWSSYSTEM**\\n  7. Right click and select **Paste**\\n  8. Attempt Acronis Snap Deploy\xa0re install."},{"id":"/2012/11/13/ubuntu-wireless-is-disabled-by-hardware-switch","metadata":{"permalink":"/2012/11/13/ubuntu-wireless-is-disabled-by-hardware-switch","source":"@site/blog/2012-11-13-ubuntu-wireless-is-disabled-by-hardware-switch.md","title":"Ubuntu &#8211; Wireless is disabled by hardware switch","description":"This commonly occurs right after an upgrade/update to the operating system.","date":"2012-11-13T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.605,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Ubuntu &#8211; Wireless is disabled by hardware switch","date":"2012-11-13 00:00:00 +1300","authors":["Luke"],"tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Acronis Snap Deploy spwizimg.dll error","permalink":"/2012/11/13/acronis-snap-deploy-spwizimg-dll-error"},"nextItem":{"title":"Excel \u2013 Add a space in a column of data.","permalink":"/2012/11/12/excel-add-a-space-in-a-column-of-data"}},"content":"This commonly occurs right after an upgrade/update to the operating system.\\n\\n  1. First thing is first \u2013 your laptop will probably have a Wireless On & Off feature which is either on the side of the laptop chassis or front usually in the form of a button or switch. If it\u2019s off, turn it on!\\n  2. If the wireless is turned on and you are still getting an issue open a Terminal and type: `sudo rfkill unblock wifi`\\n  3. Press Enter\\n  4. You should now be able to access the wireless.\\n\\nIf neither of the above work \u2013 then the problem most likely is you\u2019re wireless on & off switch and it might need to be rewired to be permanently on."},{"id":"/2012/11/12/excel-add-a-space-in-a-column-of-data","metadata":{"permalink":"/2012/11/12/excel-add-a-space-in-a-column-of-data","source":"@site/blog/2012-11-12-excel-add-a-space-in-a-column-of-data.md","title":"Excel \u2013 Add a space in a column of data.","description":"Ever had a bunch of next and numbers in a column and needed to separate them with a space? Follow this guide.","date":"2012-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.595,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Excel \u2013 Add a space in a column of data.","date":"2012-11-12 00:00:00 +1300","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Ubuntu &#8211; Wireless is disabled by hardware switch","permalink":"/2012/11/13/ubuntu-wireless-is-disabled-by-hardware-switch"},"nextItem":{"title":"The settings saved on this computer do not match the requirements of the network","permalink":"/2012/11/12/the-settings-saved-on-this-computer-do-not-match-the-requirements-of-the-network"}},"content":"Ever had a bunch of next and numbers in a column and needed to separate them with a space? Follow this guide.\\n\\n  1. Find the column you want to add a space to and right click the column heading.\\n  2. Left click Format Cells\\n  3. Click Custom _(bottom)_\\n  4. Now work out how many letters/words you want before a space and type: # for each character, press Space where you want a space to be and then type the rest of the # _(for example for a NZ telephone number it would look like: \xa0### ####)_\\n  5. Click Ok to accept changes.\\n\\n_Note: You can preview the changes you make in the preview dialog above the custom edit field._"},{"id":"/2012/11/12/the-settings-saved-on-this-computer-do-not-match-the-requirements-of-the-network","metadata":{"permalink":"/2012/11/12/the-settings-saved-on-this-computer-do-not-match-the-requirements-of-the-network","source":"@site/blog/2012-11-12-the-settings-saved-on-this-computer-do-not-match-the-requirements-of-the-network.md","title":"The settings saved on this computer do not match the requirements of the network","description":"1. Click Start","date":"2012-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.175,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"The settings saved on this computer do not match the requirements of the network","date":"2012-11-12 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Excel \u2013 Add a space in a column of data.","permalink":"/2012/11/12/excel-add-a-space-in-a-column-of-data"},"nextItem":{"title":"How to access Administrative Tools Windows Server 2012","permalink":"/2012/11/06/how-to-access-administrative-tools-windows-server-2012"}},"content":"1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Open **Network and Sharing Center**\\n  4. Click **Manage Wireless Networks** _(to the left)_\\n  5. Right click and **remove** your wireless **network** from the list\\n  6. **Attempt** re-**connection**"},{"id":"/2012/11/06/how-to-access-administrative-tools-windows-server-2012","metadata":{"permalink":"/2012/11/06/how-to-access-administrative-tools-windows-server-2012","source":"@site/blog/2012-11-06-how-to-access-administrative-tools-windows-server-2012.md","title":"How to access Administrative Tools Windows Server 2012","description":"1. Press the windows key on your keyboard","date":"2012-11-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.155,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to access Administrative Tools Windows Server 2012","date":"2012-11-06 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"The settings saved on this computer do not match the requirements of the network","permalink":"/2012/11/12/the-settings-saved-on-this-computer-do-not-match-the-requirements-of-the-network"},"nextItem":{"title":"How to adjust the Server Manager Refresh rate in Windows Server 2012","permalink":"/2012/11/06/how-to-adjust-the-server-manager-refresh-rate-in-windows-server-2012"}},"content":"1. Press the **windows** **key** on your keyboard\\n  2. On the metro interface click **Administrative** **Tools**\\n\\n_Note: Using Server Manager, you can also select Tools (top right) to select common tasks._"},{"id":"/2012/11/06/how-to-adjust-the-server-manager-refresh-rate-in-windows-server-2012","metadata":{"permalink":"/2012/11/06/how-to-adjust-the-server-manager-refresh-rate-in-windows-server-2012","source":"@site/blog/2012-11-06-how-to-adjust-the-server-manager-refresh-rate-in-windows-server-2012.md","title":"How to adjust the Server Manager Refresh rate in Windows Server 2012","description":"1. Open Server Manager","date":"2012-11-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.225,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to adjust the Server Manager Refresh rate in Windows Server 2012","date":"2012-11-06 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to access Administrative Tools Windows Server 2012","permalink":"/2012/11/06/how-to-access-administrative-tools-windows-server-2012"},"nextItem":{"title":"How to disable password complexity requirements in Windows Server 2012","permalink":"/2012/11/06/how-to-disable-password-complexity-requirements-in-windows-server-2012"}},"content":"1. Open Server **Manager**\\n  2. Click **Manage** _(top right)_\\n  3. Click **Server** **Manager** **Properties**\\n  4. Make sure &#8220;**_Specify the Server Manager Data refresh period (in minutes)_**\u201d is ticked\\n  5. **Type** in the **number** of minutes you want to refresh your server(s) status.\\n  6. Click **Ok**"},{"id":"/2012/11/06/how-to-disable-password-complexity-requirements-in-windows-server-2012","metadata":{"permalink":"/2012/11/06/how-to-disable-password-complexity-requirements-in-windows-server-2012","source":"@site/blog/2012-11-06-how-to-disable-password-complexity-requirements-in-windows-server-2012.md","title":"How to disable password complexity requirements in Windows Server 2012","description":"1. \xa0Open Server Manager","date":"2012-11-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.31,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to disable password complexity requirements in Windows Server 2012","authors":["Luke"],"tags":["Windows"],"date":"2012-11-06 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to adjust the Server Manager Refresh rate in Windows Server 2012","permalink":"/2012/11/06/how-to-adjust-the-server-manager-refresh-rate-in-windows-server-2012"},"nextItem":{"title":"How to stop Server Manager from starting on start-up in Windows Server 2012","permalink":"/2012/11/06/how-to-stop-server-manager-from-starting-on-start-up-in-windows-server-2012"}},"content":"1. \xa0**Open** Server **Manager**\\n  2. Click **Tools** _(top right)_\\n  3. Click **Local Security Policy**\\n  4. **Navigate** to Account Policies/**Password** **Policy**\\n  5. Double click \u201c**_Password must meet complexity requirements_**\u201d\\n  6. Click **Disable**\\n  7. Click **Ok**\\n\\n&nbsp;\\n\\nNote: You can also Group Policy which includes Local security policies by pressing the Windows Key+R at the same type and typing: _gpedit.msc_ into the run dialog."},{"id":"/2012/11/06/how-to-stop-server-manager-from-starting-on-start-up-in-windows-server-2012","metadata":{"permalink":"/2012/11/06/how-to-stop-server-manager-from-starting-on-start-up-in-windows-server-2012","source":"@site/blog/2012-11-06-how-to-stop-server-manager-from-starting-on-start-up-in-windows-server-2012.md","title":"How to stop Server Manager from starting on start-up in Windows Server 2012","description":"1. Open Server Manager","date":"2012-11-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.135,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to stop Server Manager from starting on start-up in Windows Server 2012","date":"2012-11-06 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to disable password complexity requirements in Windows Server 2012","permalink":"/2012/11/06/how-to-disable-password-complexity-requirements-in-windows-server-2012"},"nextItem":{"title":"How to replace the LCD on a Compaq CQ57","permalink":"/2012/11/05/cq57_lcd_replacement"}},"content":"1. **Open** **Server** **Manager**\\n  2. Click **Manage** _(top right)_\\n  3. Click **Server** **Manager** **Properties**\\n  4. Tick &#8220;**_Do not start Server manager automatically at logon_**&#8220;\\n  5. Click **Ok**"},{"id":"/2012/11/05/cq57_lcd_replacement","metadata":{"permalink":"/2012/11/05/cq57_lcd_replacement","source":"@site/blog/2012-11-05-cq57_lcd_replacement.md","title":"How to replace the LCD on a Compaq CQ57","description":"What you need","date":"2012-11-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.575,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to replace the LCD on a Compaq CQ57","date":"2012-11-05 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to stop Server Manager from starting on start-up in Windows Server 2012","permalink":"/2012/11/06/how-to-stop-server-manager-from-starting-on-start-up-in-windows-server-2012"},"nextItem":{"title":"How to access a HP DV7 Hard drive","permalink":"/2012/11/05/how-to-access-a-hp-dv7-hard-drive"}},"content":"What you need\\n\\n* \xa0Flat Head Screw-Driver\\n* Razor Blade\\n\\n  1. Using a flathead screwdriver or razor blade gently lift the little plastic covers on the lower right and left of the panel casing _(covering screws)_.\\n  2. Once removed, unscrew the 2 screws \u2013 one on either side.\\n  3. Using a slim flathead slide the flathead/razor underneath the area where the screws was and slide up slowly work you your way around the edge of the panel undoing clips on all side until you get to the webcam.\\n  4. Then start again from the other side slowly releasing each clip as you go, once you get to the webcam it simply comes off.\\n  5. Now gently slide your finger or the flathead screwdriver on the lower part of the panel casing to release it.\\n  6. Next remove the x2 copper screws on either side on the bottom of the panel &#8220;holder&#8221; to loosen it.\\n  7. Now remove the silver screws on either side _(should be 3 on each side)_ of the &#8220;holder&#8221; which is holding the panel in place.\\n  8. Gently rest the panel down so it is resting on the keyboard\\n  9. Unplug the data/video cable from the LCD panel and swap your new one in.\\n 10. Reverse the process above to put it back together, remember to give the panel a test before you go through all the work to put it all back.\\n\\n_Note: Always best to have the laptop resting on a soft surface (ie towel) to avoid scratches._\\n\\n_Note: Make sure laptop battery has been removed and the laptop is unplugged from AC power to prevent any chance of turning laptop on._\\n\\n_Note: I have found the webcam area has more resistance (due to the fact there is a webcam there) starting from either end and making your way to the webcam decreases the chance of cracking the plastic casing._"},{"id":"/2012/11/05/how-to-access-a-hp-dv7-hard-drive","metadata":{"permalink":"/2012/11/05/how-to-access-a-hp-dv7-hard-drive","source":"@site/blog/2012-11-05-how-to-access-a-hp-dv7-hard-drive.md","title":"How to access a HP DV7 Hard drive","description":"1. Turn the laptop over so you have the bottom of the laptop facing up (I recommend placing on a soft surface such as a towel to avoid any scratches to the chassis.","date":"2012-11-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to access a HP DV7 Hard drive","date":"2012-11-05 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to replace the LCD on a Compaq CQ57","permalink":"/2012/11/05/cq57_lcd_replacement"},"nextItem":{"title":"How to import your account settings into Windows Live Mail","permalink":"/2012/11/05/how-to-import-your-account-settings-into-windows-live-mail"}},"content":"1. Turn the laptop over so you have the bottom of the laptop facing up _(I recommend placing on a soft surface such as a towel to avoid any scratches to the chassis._\\n  2. Move the battery latch right \u2013 to unlock the bottom panel.\\n  3. Now on the top of the panel _(bottom cover)_ gently lift up.\\n  4. You should now have access to the 2.5\u201d HDD."},{"id":"/2012/11/05/how-to-import-your-account-settings-into-windows-live-mail","metadata":{"permalink":"/2012/11/05/how-to-import-your-account-settings-into-windows-live-mail","source":"@site/blog/2012-11-05-how-to-import-your-account-settings-into-windows-live-mail.md","title":"How to import your account settings into Windows Live Mail","description":"1. Open Windows Live Mail","date":"2012-11-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.29,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to import your account settings into Windows Live Mail","date":"2012-11-05 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to access a HP DV7 Hard drive","permalink":"/2012/11/05/how-to-access-a-hp-dv7-hard-drive"},"nextItem":{"title":"How to repair the Windows XP boot loader","permalink":"/2012/11/05/how-to-repair-the-windows-xp-boot-loader"}},"content":"1. Open Windows Live Mail\\n  2. Open Windows Live Mail Options tab _(top left)_\\n  3. Click Options\\n  4. Click Email Accounts\\n  5. Click Import\\n  6. Browse to the location of the backup IAF file and click Open to import your account settings.\\n\\n_Note: To export your mail account settings follow steps 1-4 and click Export instead of Import._"},{"id":"/2012/11/05/how-to-repair-the-windows-xp-boot-loader","metadata":{"permalink":"/2012/11/05/how-to-repair-the-windows-xp-boot-loader","source":"@site/blog/2012-11-05-how-to-repair-the-windows-xp-boot-loader.md","title":"How to repair the Windows XP boot loader","description":"What you need.","date":"2012-11-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.355,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to repair the Windows XP boot loader","date":"2012-11-05 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to import your account settings into Windows Live Mail","permalink":"/2012/11/05/how-to-import-your-account-settings-into-windows-live-mail"},"nextItem":{"title":"How to create a shutdown shortcut","permalink":"/2012/10/31/how-to-create-a-shutdown-shortcut"}},"content":"What you need.\\n\\n* Windows XP CD _(doesn\u2019t matter whether Home or Professional)_\\n\\n  1. \xa0Boot the computer from the Windows XP disk at start-up\\n  2. Once greeted with the Welcome to Windows XP setup screen select Recovery Console.\\n  3. Select your Windows installation _(if prompted)_.\\n  4. Type:\xa0 _fixboot_\\n  5. Press Enter\\n  6. Press Y\\n  7. Type: _fdisk /mbr_\\n  8. Press Enter\\n  9. Restart the computer and remove the Windows XP CD."},{"id":"/2012/10/31/how-to-create-a-shutdown-shortcut","metadata":{"permalink":"/2012/10/31/how-to-create-a-shutdown-shortcut","source":"@site/blog/2012-10-31-how-to-create-a-shutdown-shortcut.md","title":"How to create a shutdown shortcut","description":"This tip is useful if you don\u2019t want to go through the hassle of clicking Start, Shutdown.","date":"2012-10-31T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.625,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to create a shutdown shortcut","date":"2012-10-31 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to repair the Windows XP boot loader","permalink":"/2012/11/05/how-to-repair-the-windows-xp-boot-loader"},"nextItem":{"title":"The network password needs to be 128 bits or 256 bits depending on your network configuration","permalink":"/2012/10/29/the-network-password-needs-to-be-128-bits-or-256-bits-depending-on-your-network-configuration"}},"content":"This tip is useful if you don\u2019t want to go through the hassle of clicking Start, Shutdown.\\n\\n  1. Right click a blank spot on the desktop, select New, Shortcut\\n  2. In the location field type: **_shutdown \u2013s \u2013t 00_**\\n  3. Click Next\\n  4. Type in the name you want it named as, such as Shutdown\\n  5. Click Finish\\n  6. You can now drag the shortcut to the Taskbar or keep it on the desktop.\\n\\n_Note: the \u201c-S\u201d indicates shutdown, changing it to \u201c-R\u201d will mean restart. The \u201c-t\u201d is time adjusting the number from 00 will change the shutdown count from immediate to what you set it to in seconds._\\n\\nTested on Windows XP/Vista/Windows 7 _(inc 64bit)_ _| Should also work with Windows 9 (untested)_"},{"id":"/2012/10/29/the-network-password-needs-to-be-128-bits-or-256-bits-depending-on-your-network-configuration","metadata":{"permalink":"/2012/10/29/the-network-password-needs-to-be-128-bits-or-256-bits-depending-on-your-network-configuration","source":"@site/blog/2012-10-29-the-network-password-needs-to-be-128-bits-or-256-bits-depending-on-your-network-configuration.md","title":"The network password needs to be 128 bits or 256 bits depending on your network configuration","description":"Attempting to enter in your wireless passkey and getting \u201cThe network password needs to be 128bits or 256bits depending on your network configuration \u201cerror? This usually occurs when the networking stack has been damaged/corrupted. Follow the instructions below to repair:","date":"2012-10-29T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.38,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"The network password needs to be 128 bits or 256 bits depending on your network configuration","date":"2012-10-29 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to create a shutdown shortcut","permalink":"/2012/10/31/how-to-create-a-shutdown-shortcut"},"nextItem":{"title":"Data Is Invalid fix for Windows XP","permalink":"/2012/10/25/data-is-invalid-fix-for-windows-xp"}},"content":"Attempting to enter in your wireless passkey and getting \u201cThe network password needs to be 128bits or 256bits depending on your network configuration \u201cerror? This usually occurs when the networking stack has been damaged/corrupted. Follow the instructions below to repair:\\n\\n  1. Click **Start**\\n  2. Click **Run**\\n  3. Type in: _**cmd**_\\n  4. Press **Enter**\\n  5. The windows Command Prompt should launch, type in: **_netsh winsock reset_**\\n  6. Press **Enter**\\n  7. **Restart** your **computer** and attempt wireless connectivity."},{"id":"/2012/10/25/data-is-invalid-fix-for-windows-xp","metadata":{"permalink":"/2012/10/25/data-is-invalid-fix-for-windows-xp","source":"@site/blog/2012-10-25-data-is-invalid-fix-for-windows-xp.md","title":"Data Is Invalid fix for Windows XP","description":"Having issues trying to install device drivers in Windows XP and its stopping half way through and giving you the \u201cData is Invalid\u201d error? Following the guide below to repair.","date":"2012-10-25T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.595,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Data Is Invalid fix for Windows XP","date":"2012-10-25 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"The network password needs to be 128 bits or 256 bits depending on your network configuration","permalink":"/2012/10/29/the-network-password-needs-to-be-128-bits-or-256-bits-depending-on-your-network-configuration"},"nextItem":{"title":"How to remove a Toshiba Tecra M3 Keyboard","permalink":"/2012/10/25/remove-a-toshiba-tecra-m3-keyboard"}},"content":"Having issues trying to install device drivers in Windows XP and its stopping half way through and giving you the \u201cData is Invalid\u201d error? Following the guide below to repair.\\n\\nWhat you need\\n\\n* Windows XP Home/Professional CD\\n\\n  1. Open My **Computer**\\n  2. Navigate to: **C:WINDOWSDRIVERCACHEi386**\\n  3. **Delete** the **_driver.cab_** file.\\n  4. Now **copy** the **_driver.cab_** file from the i386 folder from your Windows XP CD into the **_C:WINDOWSDRIVERCACHEi386_** folder.\\n  5. **Right** click **My Computer**\\n  6. Left click **Properties**\\n  7. Click **Hardware**\\n  8. Click **Device Manager**\\n  9. **Right** **click** the device you were attempting to install _(should have a yellow exclamation mark next to it)_ select **Uninstall** then restart your computer.\\n\\n_You should now be able to\xa0re-install\xa0the device drivers._"},{"id":"/2012/10/25/remove-a-toshiba-tecra-m3-keyboard","metadata":{"permalink":"/2012/10/25/remove-a-toshiba-tecra-m3-keyboard","source":"@site/blog/2012-10-25-remove-a-toshiba-tecra-m3-keyboard.md","title":"How to remove a Toshiba Tecra M3 Keyboard","description":"What you need","date":"2012-10-25T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.505,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to remove a Toshiba Tecra M3 Keyboard","date":"2012-10-25 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Data Is Invalid fix for Windows XP","permalink":"/2012/10/25/data-is-invalid-fix-for-windows-xp"},"nextItem":{"title":"Google Chrome \u2013 Your Profile Could Not Be Opened Correctly","permalink":"/2012/10/24/google-chrome-your-profile-could-not-be-opened-correctly"}},"content":"What you need\\n\\n* Philips Screwdriver\\n* Small Razor Blade/Flat-Head Screwdriver\\n\\n  1. Remove the Laptop Battery (to prevent laptop from getting bumped and turning on).\\n  2. On the casing of the laptop _(just above the keyboard \u2013where the power button is located)_locate a little notch on either side and lift. Gently remove the plastic casing.\\n  3. You should now be able to see 2 screws _(on either side of the top laptop)_ and unscrew them.\\n  4. Put your fingers on either side of the top part of the keyboard and gently lift upwards\\n  5. Lift the keyboard out and unplug it."},{"id":"/2012/10/24/google-chrome-your-profile-could-not-be-opened-correctly","metadata":{"permalink":"/2012/10/24/google-chrome-your-profile-could-not-be-opened-correctly","source":"@site/blog/2012-10-24-google-chrome-your-profile-could-not-be-opened-correctly.md","title":"Google Chrome \u2013 Your Profile Could Not Be Opened Correctly","description":"Every time you open Google Chrome you\u2019re getting the \u201cYour profile could not be opened correctly\u201d error? This is how I fixed it.","date":"2012-10-24T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.44,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Google Chrome \u2013 Your Profile Could Not Be Opened Correctly","date":"2012-10-24 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to remove a Toshiba Tecra M3 Keyboard","permalink":"/2012/10/25/remove-a-toshiba-tecra-m3-keyboard"},"nextItem":{"title":"Aspire One Keyboard Removal","permalink":"/2012/10/23/aspire-one-keyboard-removal"}},"content":"Every time you open Google Chrome you\u2019re getting the \u201cYour profile could not be opened correctly\u201d error? This is how I fixed it.\\n\\n  1. Open **My Computer/Computer**\\n  2. Go into your operating system drive (usually Local Disk C:)\\n  3. Go to _**Users/Documents & Settings**_\\n  4. Go into your user account (for example: C:Usersluke)\\n  5. Navigate to: **AppDataLocalGoogleChromeUser DataDefault**\\n  6. **Delete** **WebData**\\n\\n_If you are running Windows XP navigate to: Application DataLocalGoogleChromeUserDataDefault._\\n\\n_Note: Default is the name of the Google profile; it may be different depending on your setup._"},{"id":"/2012/10/23/aspire-one-keyboard-removal","metadata":{"permalink":"/2012/10/23/aspire-one-keyboard-removal","source":"@site/blog/2012-10-23-aspire-one-keyboard-removal.md","title":"Aspire One Keyboard Removal","description":"Things you will need.","date":"2012-10-23T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.49,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Aspire One Keyboard Removal","date":"2012-10-23 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Google Chrome \u2013 Your Profile Could Not Be Opened Correctly","permalink":"/2012/10/24/google-chrome-your-profile-could-not-be-opened-correctly"},"nextItem":{"title":"ASUS Recovery Partition","permalink":"/2012/10/23/asus-recovery-partition"}},"content":"Things you will need.\\n\\n* Small Razor Blade\\n\\nor\\n\\n* Small Flat-Head Screwdriver\\n\\n  1. Unplug the Aspire One battery _(to avoid any chances of the laptop turning on while you are working on it)._\\n  2. On the right hand side of the Keyboard you should see several notches, get your Razor Blade and gently pry the keyboard up over the notches.\\n  3. Once lifted push the little notch in the top middle of the keyboard in and slide the keyboard out.\\n\\n_Note: It is normal for the Laptop keyboard cable to be \u201cbent\u201d to fix in the slot._"},{"id":"/2012/10/23/asus-recovery-partition","metadata":{"permalink":"/2012/10/23/asus-recovery-partition","source":"@site/blog/2012-10-23-asus-recovery-partition.md","title":"ASUS Recovery Partition","description":"1. Press F9 during POST (computer start) to enter into the ASUS Recovery Partition.","date":"2012-10-23T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.105,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"ASUS Recovery Partition","date":"2012-10-23 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Aspire One Keyboard Removal","permalink":"/2012/10/23/aspire-one-keyboard-removal"},"nextItem":{"title":"How to open a Brother DP-550CJ Printer","permalink":"/2012/10/23/how-to-open-a-brother-dp-550cj-printer"}},"content":"1. Press **F9** during POST (computer start) to enter into the ASUS Recovery Partition.\\n\\n_Note: Tested on an Acer x58 Laptop_"},{"id":"/2012/10/23/how-to-open-a-brother-dp-550cj-printer","metadata":{"permalink":"/2012/10/23/how-to-open-a-brother-dp-550cj-printer","source":"@site/blog/2012-10-23-how-to-open-a-brother-dp-550cj-printer.md","title":"How to open a Brother DP-550CJ Printer","description":"What you will need","date":"2012-10-23T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.575,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to open a Brother DP-550CJ Printer","date":"2012-10-23 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"ASUS Recovery Partition","permalink":"/2012/10/23/asus-recovery-partition"},"nextItem":{"title":"Windows Update Error 66a or 0x8007066A","permalink":"/2012/10/18/windows-update-error-66a-or-0x8007066a"}},"content":"_What you will need_\\n\\n* Small Flat-Head Screwdriver\\n* Philips Screwdriver\\n\\n  1. Undo the clips at the front, one on each side and lift the plastic panel up slightly.\\n  2. Take the top plastic cover off, revealing the Ink Cartridge area using a Flat-Head Screwdriver gently lift the clips on either side up and gently lift the plastic casing up.\\n  3. Remove the two Copper Screws on the top of the back part of the printer.\\n  4. Once removed, gently lift the top chassis off.\\n  5. You should now have unfettered view of the keyboard, undo the latches on each side.\\n  6. Slide the keyboard towards you and gently lift it up out of place."},{"id":"/2012/10/18/windows-update-error-66a-or-0x8007066a","metadata":{"permalink":"/2012/10/18/windows-update-error-66a-or-0x8007066a","source":"@site/blog/2012-10-18-windows-update-error-66a-or-0x8007066a.md","title":"Windows Update Error 66a or 0x8007066A","description":"1. Click Start","date":"2012-10-18T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.33,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Update Error 66a or 0x8007066A","date":"2012-10-18 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to open a Brother DP-550CJ Printer","permalink":"/2012/10/23/how-to-open-a-brother-dp-550cj-printer"},"nextItem":{"title":"Kaspersky Rescue Disk Change Update Server Location","permalink":"/2012/10/17/kaspersky-rescue-disk-change-update-server-location"}},"content":"1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click **Uninstall a Program**/Add and Remove Programs\\n  4. Go down the list and **find** Net **Framework** 4 **Client** **Profile**\\n  5. Right click Net Framework 4 Client Profile and select Change/**Repair**.\\n  6. Select Repair\\n\\n_The Net Framework will then go through and repair its files and registry entries, once completed restart your windows machine and attempt Windows Update again._"},{"id":"/2012/10/17/kaspersky-rescue-disk-change-update-server-location","metadata":{"permalink":"/2012/10/17/kaspersky-rescue-disk-change-update-server-location","source":"@site/blog/2012-10-17-kaspersky-rescue-disk-change-update-server-location.md","title":"Kaspersky Rescue Disk Change Update Server Location","description":"1. Click Settings","date":"2012-10-17T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.195,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Kaspersky Rescue Disk Change Update Server Location","date":"2012-10-17 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Windows Update Error 66a or 0x8007066A","permalink":"/2012/10/18/windows-update-error-66a-or-0x8007066a"},"nextItem":{"title":"AppHangXPro Internet Explorer 9","permalink":"/2012/10/15/apphangxpro-internet-explorer-9"}},"content":"1. Click **Settings**\\n  2. Click **My Update Centre**\\n  3. Click **Settings**\\n  4. Choose **Select** from **List**\\n  5. Select your **country**\\n\\n_Note: You can also add Kaspersky update servers by clicking Add if you want to add a custom one._"},{"id":"/2012/10/15/apphangxpro-internet-explorer-9","metadata":{"permalink":"/2012/10/15/apphangxpro-internet-explorer-9","source":"@site/blog/2012-10-15-apphangxpro-internet-explorer-9.md","title":"AppHangXPro Internet Explorer 9","description":"This issue commonly occurs on websites such as Facebook and Internet Explorer 9","date":"2012-10-15T08:55:11.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.27,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"AppHangXPro Internet Explorer 9","date":"2012-10-15T08:55:11.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Kaspersky Rescue Disk Change Update Server Location","permalink":"/2012/10/17/kaspersky-rescue-disk-change-update-server-location"},"nextItem":{"title":"Error 921 Installing Android Apps","permalink":"/2012/10/15/error-921-installing-android-apps"}},"content":"_This issue commonly occurs on websites such as Facebook and Internet Explorer 9_\\n\\n  1. Open Internet Explorer 9\\n  2. Click Tools\\n  3. Click Internet Options\\n  4. Click on the Security Tab\\n  5. Uncheck Protected Mode\\n  6. Click Ok\\n\\n_Restart your browser and attempt to use the website that was having the HangXPro error before._"},{"id":"/2012/10/15/error-921-installing-android-apps","metadata":{"permalink":"/2012/10/15/error-921-installing-android-apps","source":"@site/blog/2012-10-15-error-921-installing-android-apps.md","title":"Error 921 Installing Android Apps","description":"Attempting to install applications from the Google Store and getting Error 921? I got this after several OS reinstalls on my phone and installing the same apps several time.","date":"2012-10-15T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.335,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Error 921 Installing Android Apps","date":"2012-10-15 00:00:00 +1300","authors":["Luke"],"tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"AppHangXPro Internet Explorer 9","permalink":"/2012/10/15/apphangxpro-internet-explorer-9"},"nextItem":{"title":"Google Apps &#8211; How to add pages to headings","permalink":"/2012/10/08/google-apps-how-to-add-pages-to-top-site-header"}},"content":"_Attempting to install applications from the Google Store and getting Error 921? I got this after several OS reinstalls on my phone and installing the same apps several time._\\n\\n  1. In order to fix this, you need to go into **Settings**\\n  2. **Accounts**\\n  3. **Remove** your **Google** **Account**\\n  4. Then re-**add** your **account**\\n\\n_Now you should be able to download and install the applications you are after._"},{"id":"/2012/10/08/google-apps-how-to-add-pages-to-top-site-header","metadata":{"permalink":"/2012/10/08/google-apps-how-to-add-pages-to-top-site-header","source":"@site/blog/2012-10-08-google-apps-how-to-add-pages-to-top-site-header.md","title":"Google Apps &#8211; How to add pages to headings","description":"To add a page to the top navigation (tabs) following the instructions below:","date":"2012-10-08T09:47:05.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.745,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Google Apps &#8211; How to add pages to headings","date":"2012-10-08T09:47:05.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Error 921 Installing Android Apps","permalink":"/2012/10/15/error-921-installing-android-apps"},"nextItem":{"title":"Using Clonezilla &#8211; Clone to Image","permalink":"/2012/10/04/using-clonezilla-clone-to-image"}},"content":"To add a page to the top navigation (tabs) following the instructions below:\\n\\nFirst you must have already created the page you want to add.\\n\\n  1. Once you have logged into a Site click **_More_** _(up the top)_\\n  2. Click **_Edit Site Layout_**\\n  3. Click **_Edit Horizontal Navigation_** _(by hovering your mouse over the top tabs)_\\n  4. Here you can Add/Remove pages to and from the top navigation panel, to Add a page click **_Add Page_**\\n  5. **Select** the **page** you would like to add, click **Ok**\\n  6. Click again to **confirm**.\\n  7. Then **close** the Edit Horizontal Navigation by clicking Close _(upper right)_.\\n  8. _To remove pages, go back to the Horizontal Navigation, select the page you want to remove and click the \u201cX\u201d on the right hand side._\\n  9. Click Ok\\n\\n&nbsp;\\n\\nNote: Remember in the Add/Remove Page section you can use the indents to create sub-menus."},{"id":"/2012/10/04/using-clonezilla-clone-to-image","metadata":{"permalink":"/2012/10/04/using-clonezilla-clone-to-image","source":"@site/blog/2012-10-04-using-clonezilla-clone-to-image.md","title":"Using Clonezilla &#8211; Clone to Image","description":"1. Download Clonezilla","date":"2012-10-04T15:19:09.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.325,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Using Clonezilla &#8211; Clone to Image","date":"2012-10-04T15:19:09.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Google Apps &#8211; How to add pages to headings","permalink":"/2012/10/08/google-apps-how-to-add-pages-to-top-site-header"},"nextItem":{"title":"How to repair Minda Home/Professional","permalink":"/2012/10/04/how-to-repair-minda-homeprofessional"}},"content":"1. Download <a title=\\"Clonezilla\\" href=\\"http://clonezilla.org/downloads.php\\" target=\\"_blank\\">Clonezilla</a>\\n  2. Burn to CD\\n  3. Boot from CD\\n  4. Select Clonezilla VGA mode _(top option)_\\n  5. Wait for Clonezilla to extract the files from the CD into the computers RAM\\n  6. Select your language ie English\\n  7. If using a different keyboard, choose Select Keymap from Arch List, otherwise leave Don&#8217;t touch key map as default, press Enter\\n  8. Select Start Clonezilla If you are cloning directly onto another drive choose device-device in this case we are cloning to an image so select device-image\\n  9. This is where you specify where the device will save to select: _local_dev_\\n 10. Insert the USB device when prompted, press Enter\\n 11. Select the device to mount\\n 12. Select the directory; I just leave it as top level\\n 13. Leave beginner\\n 14. Select SaveDisk (Save local disk as an image)\\n 15. Type in the name of the image, it is safe to leave it the default time/date\\n 16. Select the disk or drive you would like to clone, press Enter\\n 17. Select Skip Checking, unless you have reason to believe the drive might be failing.\\n 18. To avoid mistakes restoring images make sure you select Yes, Check the Save image (this will take a bit of extra time but worth it in case of problems restoring)\\n 19. Press Enter to continue\\n 20. It will then do a last scan of the hardware and ask you for confirmation (make sure that the drives you are trying to clone are the right one, this is your last chance) press Y and enter to continue\\n\\n&nbsp;"},{"id":"/2012/10/04/how-to-repair-minda-homeprofessional","metadata":{"permalink":"/2012/10/04/how-to-repair-minda-homeprofessional","source":"@site/blog/2012-10-04-how-to-repair-minda-homeprofessional.md","title":"How to repair Minda Home/Professional","description":"Basically you need to download the latest version of Minda from LIC (lic.co.nz) and in order to do this you need your PART (participant) code and your Minda PIN to login.","date":"2012-10-04T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.51,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to repair Minda Home/Professional","date":"2012-10-04 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Using Clonezilla &#8211; Clone to Image","permalink":"/2012/10/04/using-clonezilla-clone-to-image"},"nextItem":{"title":"Setup commonly visited webpage Tab in Internet Explorer","permalink":"/2012/10/03/setup-commonly-visited-webpage-tab-in-internet-explorer"}},"content":"Basically you need to download the latest version of Minda from LIC (lic.co.nz) and in order to do this you need your PART _(participant)_ code and your Minda PIN to login.\\n\\n  1. Once you have logged in to the LIC website and greeted with \\"\u201c\\"Welcome insertnamehere\\" click \u201cGet MINDA Software\u201d \u2013 on the left hand side.\\n  2. Then click the blue download button titled \u201cClick here to download the latest version of MINDA\u201d.\\n  3. It will then download minda.exe and all you need to do is run it, say Yes to any prompts and it will install/repair Minda and the Firebird server."},{"id":"/2012/10/03/setup-commonly-visited-webpage-tab-in-internet-explorer","metadata":{"permalink":"/2012/10/03/setup-commonly-visited-webpage-tab-in-internet-explorer","source":"@site/blog/2012-10-03-setup-commonly-visited-webpage-tab-in-internet-explorer.md","title":"Setup commonly visited webpage Tab in Internet Explorer","description":"&nbsp;","date":"2012-10-03T11:40:48.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.44,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Setup commonly visited webpage Tab in Internet Explorer","date":"2012-10-03T11:40:48.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to repair Minda Home/Professional","permalink":"/2012/10/04/how-to-repair-minda-homeprofessional"},"nextItem":{"title":"x800703F0 installing Vista SP1","permalink":"/2012/10/02/x800703f0-installing-vista-sp1"}},"content":"&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click on <strong>Tools</strong> (small gear top right)\\n  </li>\\n  <li>\\n    Click <strong>Internet Options</strong>\\n  </li>\\n  <li>\\n    In the <strong>General</strong> <strong>Tab</strong>, click Tabbed\\n  </li>\\n  <li>\\n    Click <strong>Settings</strong> in the Tabbed area\\n  </li>\\n  <li>\\n    Go to the middle of the of the dialog to the \u201c<strong><em>When a new tab is opened, open</em></strong>\u201d\\n  </li>\\n  <li>\\n    <strong>Change</strong> the drop-down box <strong>to</strong> select: \u201c<strong><em>The new tab page</em></strong>\u201d\\n  </li>\\n  <li>\\n    Press <strong>Ok</strong>\\n  </li>\\n  <li>\\n    Press Ok\\n  </li>\\n</ol>\\n\\nNow when you open a new Tab it will display your commonly visited webpages."},{"id":"/2012/10/02/x800703f0-installing-vista-sp1","metadata":{"permalink":"/2012/10/02/x800703f0-installing-vista-sp1","source":"@site/blog/2012-10-02-x800703f0-installing-vista-sp1.md","title":"x800703F0 installing Vista SP1","description":"Trying to install Service Pack 1 for Windows Vista and getting \u201cx800703F0\u201d error? Follow the guides below to fix it.","date":"2012-10-02T13:06:32.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.8,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"x800703F0 installing Vista SP1","date":"2012-10-02T13:06:32.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Setup commonly visited webpage Tab in Internet Explorer","permalink":"/2012/10/03/setup-commonly-visited-webpage-tab-in-internet-explorer"},"nextItem":{"title":"Google Apps \u2013 Share documents outside domain","permalink":"/2012/10/02/google-apps-share-documents-outside-domain"}},"content":"_Trying to install Service Pack 1 for Windows Vista and getting \u201cx800703F0\u201d error? Follow the guides below to fix it._\\n\\n_Enable the Windows Module Installer Service_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click on the Vista Orb (<strong>Start</strong>)\\n  </li>\\n  <li>\\n    In the <strong>search</strong> field type: <strong><em>services.msc</em></strong> > Press Enter\\n  </li>\\n  <li>\\n    A list of Windows Services will appear, scroll down the list and <strong>find</strong> <strong><em>Windows Module Installer</em></strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> <strong>Service</strong> and select <strong>Properties</strong>\\n  </li>\\n  <li>\\n    Click on <strong>Start</strong><em> (also change the startup to Automatic)</em>\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    Attempt SP1 <strong>setup</strong> again.\\n  </li>\\n</ol>\\n\\n_Set registry entries_\\n\\n_\xa0_Click on the Vista Orb (**Start**)\\n\\n<ol start=\\"1\\">\\n  <li>\\n    In the <strong>search</strong> field <strong>type</strong>: <strong><em>regsvr32 wintrust.dll</em></strong> > Press Enter\\n  </li>\\n  <li>\\n    In the search field type: <strong><em>regsvr32 rsaenh.dll</em></strong> \xa0> Press Enter\\n  </li>\\n  <li>\\n    In the search field type:<strong><em> regsvr32 cryptdlg.dll</em></strong> > Press Enter\\n  </li>\\n  <li>\\n    In the search field type: <strong><em>regsvr32 softpub.dll</em></strong>> Press Enter\\n  </li>\\n  <li>\\n    Attempt SP1 <strong>setup</strong> again.\\n  </li>\\n</ol>"},{"id":"/2012/10/02/google-apps-share-documents-outside-domain","metadata":{"permalink":"/2012/10/02/google-apps-share-documents-outside-domain","source":"@site/blog/2012-10-02-google-apps-share-documents-outside-domain.md","title":"Google Apps \u2013 Share documents outside domain","description":"1. Login to your Google Apps admin","date":"2012-10-02T12:49:32.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.695,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Google Apps \u2013 Share documents outside domain","date":"2012-10-02T12:49:32.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"x800703F0 installing Vista SP1","permalink":"/2012/10/02/x800703f0-installing-vista-sp1"},"nextItem":{"title":"Install Ubuntu netbook edition","permalink":"/2012/10/02/ubuntu-netbook"}},"content":"1. Login to your Google Apps admin\\n  2. Click on Manage This Domain\\n  3. Click Service Settings\\n  4. Click Documents _(or Drive if you have Google Drive enabled on the domain)_\\n  5. Check \u201cUsers can publish document to the public internet\u201d\\n  6. Click Save\\n\\n&nbsp;\\n\\n_Note: If you are having issues with this not working, make sure that the when the document, ie a Powerpoint document is set to be shared with the public when adding it to your site article._\\n\\n_Note: It has also been reported by various sources that this was a bug and if you were running the free version you could activate a trial to the enterprise/paid version of Google Apps, check the box as above and then remove the trial. If you have the paid version, contact Google Support if you are having issues._"},{"id":"/2012/10/02/ubuntu-netbook","metadata":{"permalink":"/2012/10/02/ubuntu-netbook","source":"@site/blog/2012-10-02-ubuntu-netbook.md","title":"Install Ubuntu netbook edition","description":"Note: Be best to be plugged into the Internet (through Ethernet) to make sure latest updates are installed and downloaded while running the install.","date":"2012-10-02T12:29:14.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":1.15,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Install Ubuntu netbook edition","date":"2012-10-02T12:29:14.000Z","authors":["Luke"],"tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Google Apps \u2013 Share documents outside domain","permalink":"/2012/10/02/google-apps-share-documents-outside-domain"},"nextItem":{"title":"Borderlands 2 XINPUT1_3.dll","permalink":"/2012/10/02/borderlands-2-xinput1_3-dll"}},"content":"_Note: Be best to be plugged into the Internet (through Ethernet) to make sure latest updates are installed and downloaded while running the install._\\n\\n_Note: Early 2011/As of 11.04 Ubuntu &#8220;folded&#8221; the Netbook Edition into the normal installation instead of having it separate.\xa0_\\n\\n&nbsp;\\n\\n  1. Download Ubuntu (tested with 12.04)\\n  2. Burn\\n  3. Boot\\n  4. Install Ubuntu\\n  5. Wait till it loads the\xa0set-up\xa0into PC&#8217;s RAM\\n  6. Click next\\n  7. Select your partition information (warning this could and will erase your data if something goes wrong)\\n  8. Click next\\n  9. Double check settings and click Install now\\n 10. Select your region\\n 11. Click Next\\n 12. Select your keyboard layout\\n 13. Click Next\\n 14. You will be noticing the progress bar below, it is installing at the same time as you are setting up the installation to your own preferences\\n 15. Enter in your Username and password (you must enter in a password, this will also be your root password)\\n 16. Click Continue\\n 17. Select your User Account profile picture and select Continue\\n 18. Wait for Ubuntu to finish installing (10-20 minutes to install.)\\n 19. Click Restart Now\\n 20. When prompted remove the Ubuntu CD/DVD from the drive and press Enter\\n 21. The PC then restarts and will load the Ubuntu OS\\n\\n&nbsp;\\n\\n_Away you go! Ubuntu should have automatically changed its resolution depending on your screen size, whether small\xa0net-book\xa0or large monitor._"},{"id":"/2012/10/02/borderlands-2-xinput1_3-dll","metadata":{"permalink":"/2012/10/02/borderlands-2-xinput1_3-dll","source":"@site/blog/2012-10-02-borderlands-2-xinput1_3-dll.md","title":"Borderlands 2 XINPUT1_3.dll","description":"\u201cMissing file XINPUT13.dll\u201d error while trying to load Borderlands 2 on PC?_","date":"2012-10-02T11:24:24.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.18,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Borderlands 2 XINPUT1_3.dll","date":"2012-10-02T11:24:24.000Z","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Install Ubuntu netbook edition","permalink":"/2012/10/02/ubuntu-netbook"},"nextItem":{"title":"Vodafone New Zealand Phone Settings","permalink":"/2012/10/02/vodafone-new-zealand-phone-settings-2"}},"content":"_\u201cMissing file XINPUT1_3.dll\u201d error while trying to load Borderlands 2 on PC?_\\n\\n&nbsp;\\n\\n  1. Update your <a title=\\"DirectX\\" href=\\"http://www.microsoft.com/en-us/download/details.aspx?id=35\\" target=\\"_blank\\">DirectX</a> to the latest version\\n  2. Install Steam _(if not installed, it will install XINPUT1 and dependencies)_"},{"id":"/2012/10/02/vodafone-new-zealand-phone-settings-2","metadata":{"permalink":"/2012/10/02/vodafone-new-zealand-phone-settings-2","source":"@site/blog/2012-10-02-vodafone-new-zealand-phone-settings-2.md","title":"Vodafone New Zealand Phone Settings","description":"These settings are used for setting up Chinese/foreign phones onto the Vodafone NZ network.","date":"2012-10-02T10:52:04.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.485,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Vodafone New Zealand Phone Settings","date":"2012-10-02T10:52:04.000Z","authors":["Luke"],"tags":["Misc","Mobile"]},"unlisted":false,"prevItem":{"title":"Borderlands 2 XINPUT1_3.dll","permalink":"/2012/10/02/borderlands-2-xinput1_3-dll"},"nextItem":{"title":"How to disable automatic restarts in Windows 7 due to Windows Updates","permalink":"/2012/10/02/how-to-disable-automatic-restarts-in-windows-7-due-to-win-updates"}},"content":"These settings are used for setting up Chinese/foreign phones onto the Vodafone NZ network.\\n\\nAPN (Access Point Name)\\n\\nlive.vodafone.com\xa0\xa0 &#8211; WAP/PXT\\n\\nwww.vodafone.net.nz -Email/DATA\\n\\n&nbsp;\\n\\nIP/Internet Protocol Address:\\n\\n172.030.038.003 (Not needed for 3G phone)\\n\\n172.030.038.003:9201 &#8211; Only to be used if above doesn&#8217;t work.\\n\\n&nbsp;\\n\\nPort/Port Number:\\n\\n9201, 09201\\n\\n&nbsp;\\n\\nHome/Start Page:\\n\\nlive.vodafone.com\\n\\n&nbsp;\\n\\nConnection Type/Access Method:\\n\\nGPRS\\n\\n&nbsp;\\n\\nUsername/Accessname:\\n\\nLeave field blank (Not needed)\\n\\n&nbsp;\\n\\nPassword:\\n\\nLeave field blank (Not needed)\\n\\n&nbsp;\\n\\nMMS Server/MMS Relay:\\n\\npxt.vodafone.net.nz/pxtsend\\n\\npxt.vodafone.net.nz/pxtsend:9201 &#8211; Only to be used if above doesn&#8217;t work.\\n\\n&nbsp;\\n\\n&nbsp;\\n\\nGet mobile specific setup for supported phones at http://www.vodafone.co.nz/help/configure-your-mobile/index.jsp"},{"id":"/2012/10/02/how-to-disable-automatic-restarts-in-windows-7-due-to-win-updates","metadata":{"permalink":"/2012/10/02/how-to-disable-automatic-restarts-in-windows-7-due-to-win-updates","source":"@site/blog/2012-10-02-how-to-disable-automatic-restarts-in-windows-7-due-to-win-updates.md","title":"How to disable automatic restarts in Windows 7 due to Windows Updates","description":"&nbsp;","date":"2012-10-02T03:47:38.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.73,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to disable automatic restarts in Windows 7 due to Windows Updates","date":"2012-10-02T03:47:38.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Vodafone New Zealand Phone Settings","permalink":"/2012/10/02/vodafone-new-zealand-phone-settings-2"},"nextItem":{"title":"Public Alternative DNS Resolvers","permalink":"/2012/10/01/dns_resolver"}},"content":"&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    In the search field <strong>type</strong>: <strong><em>regedit</em></strong>\\n  </li>\\n  <li>\\n    Click the <strong><em>regedit.exe</em></strong> result to open Registry Editor\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to: <strong><em>HKEY_LOCAL_MACHINE/SOFTWARE/Polices/Microsoft/Windows</em></strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> an empty space in the right area and select <strong>New</strong>\\n  </li>\\n  <li>\\n    Click <strong>Key</strong>\\n  </li>\\n  <li>\\n    <strong>Name</strong> the new key: <strong><em>WindowsUpdate</em></strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> the new WindowsUpdate <strong>key</strong>\\n  </li>\\n  <li>\\n    Select <strong>New</strong>\\n  </li>\\n  <li>\\n    Click <strong>Key</strong>\\n  </li>\\n  <li>\\n    <strong>Name</strong> the new <strong>key</strong>: <strong><em>AU</em></strong>\\n  </li>\\n  <li>\\n    <strong>Select</strong> the key key and <strong>right</strong> <strong>click</strong> and empty area\\n  </li>\\n  <li>\\n    Select <strong>New</strong>\\n  </li>\\n  <li>\\n    Click <strong>DWORD</strong> (32-bit) value\\n  </li>\\n  <li>\\n    <strong>Name</strong> the <strong>DWORD</strong>: <strong><em>NoAutoRebootWithLoggedOnUsers</em></strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> <strong><em>NoAutoRebootWithLoggedOnUsers</em></strong>\\n  </li>\\n  <li>\\n    Select <strong>Modify</strong>\\n  </li>\\n  <li>\\n    Enter \u201c<strong>1</strong>\u201d in the value box\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    <strong>Close</strong> regedit\\n  </li>\\n</ol>\\n\\n_Restart your computer and Windows should no longer restart due to automatic Windows Updates_"},{"id":"/2012/10/01/dns_resolver","metadata":{"permalink":"/2012/10/01/dns_resolver","source":"@site/blog/2012-10-01-dns_resolver.md","title":"Public Alternative DNS Resolvers","description":"Public Alternative DNS Resolvers","date":"2012-10-01T19:30:29.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Public Alternative DNS Resolvers","date":"2012-10-01T19:30:29.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to disable automatic restarts in Windows 7 due to Windows Updates","permalink":"/2012/10/02/how-to-disable-automatic-restarts-in-windows-7-due-to-win-updates"},"nextItem":{"title":"Brother DCP 195C Unable to Print Error 36","permalink":"/2012/10/01/brother-dcp-195c-unable-to-print-error-36"}},"content":"_Public Alternative DNS Resolvers_\\n\\n**Google**: \xa08.8.8.8 8.8.4.4\\n\\n**OpenDNS**:208.67.222.222 208.67.220.220 Y\\n\\n**Sunbelt**: \xa074.118.212.1 74.118.212.2\\n\\n**Symantec**: \xa0198.153.192.1 198.153.194.\\n\\n**UltraDNS**: \xa0156.154.70.1 156.154.71.1\\n\\n**Verio / NTT**: -none- 129.250.35.250 129.250.35.251\\n\\n**ScrubIT: \xa0**67.138.54.100,\xa0207.225.209.66 _(includes porn blocking)_"},{"id":"/2012/10/01/brother-dcp-195c-unable-to-print-error-36","metadata":{"permalink":"/2012/10/01/brother-dcp-195c-unable-to-print-error-36","source":"@site/blog/2012-10-01-brother-dcp-195c-unable-to-print-error-36.md","title":"Brother DCP 195C Unable to Print Error 36","description":"In order to fix this issue you need to reset the Ink Waste counter, which is an ink counter that monitors how many prints that it has available on that one Ink Cartridge. Follow the methods below to repair this:","date":"2012-10-01T10:05:01.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.86,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Brother DCP 195C Unable to Print Error 36","date":"2012-10-01T10:05:01.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Public Alternative DNS Resolvers","permalink":"/2012/10/01/dns_resolver"},"nextItem":{"title":"How to rename multiple files in Windows XP","permalink":"/2012/10/01/how-to-rename-multiple-files-in-windows-xp"}},"content":"_In order to fix this issue you need to reset the Ink Waste counter, which is an ink counter that monitors how many prints that it has available on that one Ink Cartridge. Follow the methods below to repair this:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    On the Brother printer press the <strong>MENU</strong> and <strong>MONO</strong> buttons\\n  </li>\\n  <li>\\n    While <strong>holding</strong> the <strong>MENU</strong> and <strong>MONO</strong> buttons down press the <strong>UP</strong> <strong>ARROW</strong> <strong><em>4 times</em></strong>.\\n  </li>\\n  <li>\\n    Now the computer should be in maintenance mode and you can stop pressing the buttons.\\n  </li>\\n  <li>\\n    <strong>Using</strong> the <strong>Arrow</strong> keys to select and the <strong>Set</strong> button to select go to: <strong>8</strong>\\n  </li>\\n  <li>\\n    Then select <strong></strong>\\n  </li>\\n  <li>\\n    <strong>Select</strong> through modes <strong>using</strong> the <strong>Mono</strong> button until a <strong>Purge</strong> <strong>Counter</strong> appears.\\n  </li>\\n  <li>\\n    Now <strong>enter</strong> <strong><em>2,7,8,3</em></strong> by <strong>using</strong> the <strong>Arrow</strong> Keys and <strong>Set</strong> to select to <strong>reset</strong> the <strong>counter</strong> to <strong><em></em></strong>.\\n  </li>\\n  <li>\\n    Now tap the <strong>Stop</strong> key\\n  </li>\\n  <li>\\n    Using the arrow keys again <strong>enter</strong> <strong>9</strong> and press <strong>set</strong> to <strong>select</strong>.\\n  </li>\\n</ol>\\n\\n_Now you should be able to print._"},{"id":"/2012/10/01/how-to-rename-multiple-files-in-windows-xp","metadata":{"permalink":"/2012/10/01/how-to-rename-multiple-files-in-windows-xp","source":"@site/blog/2012-10-01-how-to-rename-multiple-files-in-windows-xp.md","title":"How to rename multiple files in Windows XP","description":"First make sure all the files are in the same folder","date":"2012-10-01T09:44:02.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.455,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to rename multiple files in Windows XP","date":"2012-10-01T09:44:02.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Brother DCP 195C Unable to Print Error 36","permalink":"/2012/10/01/brother-dcp-195c-unable-to-print-error-36"},"nextItem":{"title":"Nvlddmkm.sys/dxgkrnl.sys/dxgmms1.sys Issues","permalink":"/2012/10/01/nvlddmkm-sysdxgkrnl-sysdxgmms1-issues"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    First <strong>make</strong> <strong>sure</strong> all the <strong>files</strong> are in the <strong>same</strong> <strong>folder</strong>\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to that folder\\n  </li>\\n  <li>\\n    Now using your mouse <strong>drag</strong> over the <strong>files</strong> to <strong>rename</strong> <strong>them</strong><em>\xa0(or you can use the CTRL key and left click on the mouse to select the ones you want to rename individually)</em>\\n  </li>\\n  <li>\\n    Press <strong>File</strong>\\n  </li>\\n  <li>\\n    Click <strong>Rename</strong>\\n  </li>\\n  <li>\\n    Now <strong>type</strong> in your <strong>new</strong> <strong>name</strong><em>\xa0(for example:\xa0 tech)</em>\\n  </li>\\n  <li>\\n    Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    Now all the selected files will be renamed\xa0<em>(for example: tech(1), tech(2))</em>\\n  </li>\\n</ol>"},{"id":"/2012/10/01/nvlddmkm-sysdxgkrnl-sysdxgmms1-issues","metadata":{"permalink":"/2012/10/01/nvlddmkm-sysdxgkrnl-sysdxgmms1-issues","source":"@site/blog/2012-10-01-nvlddmkm-sysdxgkrnl-sysdxgmms1-issues.md","title":"Nvlddmkm.sys/dxgkrnl.sys/dxgmms1.sys Issues","description":"These issues are usually caused by damage and/or faulty nVidia drivers and DirectX components within Windows. Follow the instructions below to repair:","date":"2012-10-01T09:35:59.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.985,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Nvlddmkm.sys/dxgkrnl.sys/dxgmms1.sys Issues","date":"2012-10-01T09:35:59.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to rename multiple files in Windows XP","permalink":"/2012/10/01/how-to-rename-multiple-files-in-windows-xp"},"nextItem":{"title":"Plug & Play Software Enumerator No Sound Win XP","permalink":"/2012/10/01/pnp-enum-no-sound"}},"content":"_These issues are usually caused by damage and/or faulty nVidia drivers and DirectX components within Windows. Follow the instructions below to repair:_\\n\\n&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Firstly you need to <strong>download</strong> a tool called \u201c<strong>Driver Sweeper</strong>\u201d\\n  </li>\\n  <li>\\n    Download and <strong>install</strong> Driver Sweeper\\n  </li>\\n  <li>\\n    <strong>Download</strong> the latest version of the\xa0<a title=\\"nVidia English Drivers\\" href=\\"http://www.nvidia.com/Download/index.aspx?lang=en-us\\" target=\\"_blank\\">nVidia driver</a>\xa0<em>(but do not install yet)</em>\xa0make sure you have the correct one for your OS and architecture\xa0<em>(ie 32Bit or 64Bit).</em>\\n  </li>\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Control Panel</strong>\\n  </li>\\n  <li>\\n    Click <strong>Add/Remove Programs</strong>\xa0<em>(or Uninstall a program)</em>\\n  </li>\\n  <li>\\n    <strong>Select</strong> <strong>nVidia</strong> and <strong>Uninstall</strong>.\\n  </li>\\n  <li>\\n    Once uninstalled, reboot your computer into <strong>Safe Mode</strong>\xa0<em>(Press\xa0<strong>F8</strong>\xa0at Windows\xa0start-up\xa0to get to the Safe Mode)</em>\\n  </li>\\n  <li>\\n    Run <strong>run</strong> <strong>Driver Sweeper</strong>\xa0<em>(which should be located Start, Programs, Driver Sweeper)</em>\\n  </li>\\n  <li>\\n    You should have a list with checkboxes in front of you ignore the rest and <strong>check</strong> <strong>NVIDIA</strong> <strong>Display</strong>.\\n  </li>\\n  <li>\\n    Click <strong>Clean</strong>\\n  </li>\\n  <li>\\n    <strong>Restart</strong> your computer\xa0<em>(in Normal Mode)</em>\\n  </li>\\n  <li>\\n    Now <strong>install</strong> the nVidia <strong>drivers</strong>\\n  </li>\\n  <li>\\n    Restart and you should be good-to-go!\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n_If the above does not work, then you need to update\xa0<a title=\\"DirectX \\" href=\\"http://www.microsoft.com/en-us/download/details.aspx?id=35\\" target=\\"_blank\\">DirectX</a>\xa0and/or run a\xa0<a title=\\"Memtest x86\\" href=\\"http://www.memtest.org/#downiso\\" target=\\"_blank\\">RAM</a>\xa0scan._"},{"id":"/2012/10/01/pnp-enum-no-sound","metadata":{"permalink":"/2012/10/01/pnp-enum-no-sound","source":"@site/blog/2012-10-01-pnp-enum-no-sound.md","title":"Plug & Play Software Enumerator No Sound Win XP","description":"Click Start","date":"2012-10-01T01:52:56.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.94,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Plug & Play Software Enumerator No Sound Win XP","date":"2012-10-01T01:52:56.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Nvlddmkm.sys/dxgkrnl.sys/dxgmms1.sys Issues","permalink":"/2012/10/01/nvlddmkm-sysdxgkrnl-sysdxgmms1-issues"},"nextItem":{"title":"Network Cat5 Wiring Guide","permalink":"/2012/09/30/network-cat5-wiring-guide"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Programs</strong>\\n  </li>\\n  <li>\\n    Click <strong>Accessories</strong>\\n  </li>\\n  <li>\\n    Click <strong>Command Prompt</strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong>: \xa0<strong><em>md temp</em></strong> | press Enter\\n  </li>\\n  <li>\\n    Now we need to change to it type: <strong>Type</strong>: \xa0<strong><em>cd temp</em></strong> | press Enter\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0\xa0<strong><em>copy %windir%infmachine.inf tempallmachine.inf</em></strong>\xa0<em>(That will copy the \u201cmachine.inf to our newly created folder\u201d)</em>\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0\xa0<strong><em>copy %windir%system32driversswenum.sys temp</em></strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0<strong><em>copy %windir%system32streamci.dll temp</em></strong>\\n  </li>\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>My Computer/Computer</strong>\\n  </li>\\n  <li>\\n    Click on <strong>C</strong>:\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> into <strong>Temp</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> \u201c<strong><em>allmachine.inf</em></strong>\u201d\\n  </li>\\n  <li>\\n    Left click <strong>Open</strong> With, <strong>Notepad</strong>\\n  </li>\\n  <li>\\n    Click <strong>Edit</strong>\\n  </li>\\n  <li>\\n    Click <strong>Go</strong> To..\\n  </li>\\n  <li>\\n    Type in \u201c<strong><em>22</em></strong>\u201d > press Enter\\n  </li>\\n  <li>\\n    Select and <strong>delete</strong>: <strong><em>ExcludeFromSelect=*</em></strong>\\n  </li>\\n  <li>\\n    Click File, <strong>Save</strong>.\\n  </li>\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> My Computer/<strong>Computer</strong>\\n  </li>\\n  <li>\\n    Left click <strong>Properties</strong>\\n  </li>\\n  <li>\\n    Click <strong>Hardware</strong>\\n  </li>\\n  <li>\\n    Click <strong>Device</strong> <strong>Manager</strong>\\n  </li>\\n  <li>\\n    Click <strong>Add</strong> <strong>Hardware</strong>\xa0<em>(may be under ActionsAdd Hardware)</em>\\n  </li>\\n  <li>\\n    Click <strong>Have</strong> <strong>Disk</strong>\\n  </li>\\n  <li>\\n    <strong>Browse</strong> to c:/<strong>Temp</strong>\\n  </li>\\n  <li>\\n    A list of devices should now appear select Plug and Play Software Enumerator and click <strong>Install</strong>\\n  </li>\\n</ol>"},{"id":"/2012/09/30/network-cat5-wiring-guide","metadata":{"permalink":"/2012/09/30/network-cat5-wiring-guide","source":"@site/blog/2012-09-30-network-cat5-wiring-guide.md","title":"Network Cat5 Wiring Guide","description":"Straight Through/Patch Cable","date":"2012-09-30T20:16:28.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Network Cat5 Wiring Guide","date":"2012-09-30T20:16:28.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Plug & Play Software Enumerator No Sound Win XP","permalink":"/2012/10/01/pnp-enum-no-sound"},"nextItem":{"title":"Navigation to the webpage has been cancelled","permalink":"/2012/09/30/nav-to-the-webpage-has-been-cancelled"}},"content":"**Straight Through/Patch Cable**\\n\\n_Both ends are the same._\\n\\n  * Orange-White\\n  * Orange\\n  * Green-White\\n  * Blue\\n  * Blue-White\\n  * Green\\n  * Brown-White\\n  * Brown\\n\\n**Crossover Cable**\\n\\n  * Green-White\\n  * \xa0Green\\n  * Orange-White\\n  * Blue\\n  * Blue-White\\n  * Orange\\n  * Brown-White\\n  * Brown"},{"id":"/2012/09/30/nav-to-the-webpage-has-been-cancelled","metadata":{"permalink":"/2012/09/30/nav-to-the-webpage-has-been-cancelled","source":"@site/blog/2012-09-30-nav-to-the-webpage-has-been-cancelled.md","title":"Navigation to the webpage has been cancelled","description":"Click Start","date":"2012-09-30T17:33:17.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.5,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Navigation to the webpage has been cancelled","date":"2012-09-30T17:33:17.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Network Cat5 Wiring Guide","permalink":"/2012/09/30/network-cat5-wiring-guide"},"nextItem":{"title":"\u201cOut of Memory\u201d Fix when Upgrading to Windows 2000 from Windows 98SE","permalink":"/2012/09/30/out-of-memory-fix-when-upgrading-to-win2000-from-win98se"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> <strong>Internet</strong> <strong>Explorer</strong>\\n  </li>\\n  <li>\\n    Left click <strong>Properties</strong>\\n  </li>\\n  <li>\\n    Click on the <strong>Advanced</strong> Tab\\n  </li>\\n  <li>\\n    Left click <strong>Restore Defaults</strong>\\n  </li>\\n  <li>\\n    Close Internet Explorer\\n  </li>\\n  <li>\\n    <strong>Reopen Internet Explorer</strong> and attempt to load the webpage again.\\n  </li>\\n</ol>\\n\\n_If that didn\u2019t work make sure that you don\u2019t have InternetServiceOffers as a scheduled task._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Control Panel</strong>\\n  </li>\\n  <li>\\n    Click <strong>Administrative</strong> <strong>Tools</strong>\\n  </li>\\n  <li>\\n    Click <strong>Task Manager</strong>\\n  </li>\\n  <li>\\n    Look for: <em>InternetServiceOffers/Registration</em>\\n  </li>\\n  <li>\\n    If you can find them then <strong>delete</strong> them.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/out-of-memory-fix-when-upgrading-to-win2000-from-win98se","metadata":{"permalink":"/2012/09/30/out-of-memory-fix-when-upgrading-to-win2000-from-win98se","source":"@site/blog/2012-09-30-out-of-memory-fix-when-upgrading-to-win2000-from-win98se.md","title":"\u201cOut of Memory\u201d Fix when Upgrading to Windows 2000 from Windows 98SE","description":"Click\xa0Start","date":"2012-09-30T17:19:54.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.335,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"\u201cOut of Memory\u201d Fix when Upgrading to Windows 2000 from Windows 98SE","date":"2012-09-30T17:19:54.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Navigation to the webpage has been cancelled","permalink":"/2012/09/30/nav-to-the-webpage-has-been-cancelled"},"nextItem":{"title":"Cannot delete Slingshot Webmail Quota filled","permalink":"/2012/09/30/cannot-delete-slingshot-webmail-quota-filled"}},"content":"<div>\\n  <ol start=\\"1\\">\\n    <li>\\n      Click\xa0<strong>Start</strong>\\n    </li>\\n    <li>\\n      Click\xa0<strong>Run</strong>\\n    </li>\\n    <li>\\n      <strong>Type</strong>:\xa0<strong><em>msconfig</em></strong>\xa0(If for some reason you do have have this<a title=\\"The Tech Guide MSConfig download link\\" href=\\"http://www.thetechguide.com/downloads/msconfig.zip\\">\xa0download</a>\xa0it and copy to c:WindowsSystem)\\n    </li>\\n    <li>\\n      Press\xa0<strong>Enter</strong>\\n    </li>\\n    <li>\\n      On the General tab press\xa0<strong>Advanced</strong>\\n    </li>\\n    <li>\\n      <strong>Check</strong>\xa0<strong><em>Limit Memory</em></strong>\\n    </li>\\n    <li>\\n      In the amount\xa0<strong>type</strong>:\xa0<strong><em>512(MB)</em></strong>\\n    </li>\\n    <li>\\n      Press ok<strong></strong>\\n    </li>\\n    <li>\\n      Click\xa0<strong>Ok</strong>\\n    </li>\\n    <li>\\n      <strong>Restart</strong>\xa0your computer and attempt upgrade again.\\n    </li>\\n  </ol>\\n</div>"},{"id":"/2012/09/30/cannot-delete-slingshot-webmail-quota-filled","metadata":{"permalink":"/2012/09/30/cannot-delete-slingshot-webmail-quota-filled","source":"@site/blog/2012-09-30-cannot-delete-slingshot-webmail-quota-filled.md","title":"Cannot delete Slingshot Webmail Quota filled","description":"Log into Slingshots\xa0webmail","date":"2012-09-30T16:54:44.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.46,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Cannot delete Slingshot Webmail Quota filled","date":"2012-09-30T16:54:44.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"\u201cOut of Memory\u201d Fix when Upgrading to Windows 2000 from Windows 98SE","permalink":"/2012/09/30/out-of-memory-fix-when-upgrading-to-win2000-from-win98se"},"nextItem":{"title":"How to view Print count on a Sharp MX-4101N Printer","permalink":"/2012/09/30/how-to-view-print-count-on-a-sharp-mx-4101n-printer"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    <strong>Log</strong> <strong>into</strong> Slingshots\xa0<a title=\\"Slingshot Webmail\\" href=\\"http://webmail.slingshot.co.nz/\\" target=\\"_blank\\">webmail</a>\\n  </li>\\n  <li>\\n    Click on <strong>Options</strong> (top menu)\\n  </li>\\n  <li>\\n    Click on <strong>Deleting and Moving Messages</strong>\\n  </li>\\n  <li>\\n    <strong>Uncheck</strong> &#8220;<strong><em>When deleting messages, move them to your Deleted Items folder instead of marking them as deleted?&#8221;</em></strong>\\n  </li>\\n  <li>\\n    Click <strong>Save</strong> Options\\n  </li>\\n  <li>\\n    Now you can delete messages and they will appear with a line through them. Click Purge Deleted Options (right hand top menu) to erase.\\n  </li>\\n</ol>\\n\\n_Warning using this method will delete any emails you delete without away to recover them._"},{"id":"/2012/09/30/how-to-view-print-count-on-a-sharp-mx-4101n-printer","metadata":{"permalink":"/2012/09/30/how-to-view-print-count-on-a-sharp-mx-4101n-printer","source":"@site/blog/2012-09-30-how-to-view-print-count-on-a-sharp-mx-4101n-printer.md","title":"How to view Print count on a Sharp MX-4101N Printer","description":"To view the print count on a Sharp MX-4101N printer, hold down the Copy button on the printer&#8217;s Touch Pad to view the Total Count which includes Black & White and Color prints.","date":"2012-09-30T16:38:42.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.165,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to view Print count on a Sharp MX-4101N Printer","date":"2012-09-30T16:38:42.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Cannot delete Slingshot Webmail Quota filled","permalink":"/2012/09/30/cannot-delete-slingshot-webmail-quota-filled"},"nextItem":{"title":"One of the files containing the systems registry data had to be recovered","permalink":"/2012/09/30/files-cont-sysms-registry-data-recovered"}},"content":"To view the print count on a Sharp MX-4101N printer, **hold** down the **Copy** button on the printer&#8217;s Touch Pad to **view** the Total **Count** which includes Black & White and Color prints."},{"id":"/2012/09/30/files-cont-sysms-registry-data-recovered","metadata":{"permalink":"/2012/09/30/files-cont-sysms-registry-data-recovered","source":"@site/blog/2012-09-30-files-cont-sysms-registry-data-recovered.md","title":"One of the files containing the systems registry data had to be recovered","description":"This can someone be the result of bad RAM or Video RAM. Try to change the video memory in the BIOs from the lower number (usually 32MB/64MB) to the higher number (64/128MB) if this does not work try the following below to repair the \u201cNTUser.DAT\u201d file that has become corrupted/damaged on the User Account.","date":"2012-09-30T15:40:36.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":2.045,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"One of the files containing the systems registry data had to be recovered","date":"2012-09-30T15:40:36.000Z","authors":["Luke"],"layout":"post","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to view Print count on a Sharp MX-4101N Printer","permalink":"/2012/09/30/how-to-view-print-count-on-a-sharp-mx-4101n-printer"},"nextItem":{"title":"Google Apps Migration &#8211; Exchange to Google Apps","permalink":"/2012/09/30/google-apps-mig"}},"content":"_This can someone be the result of bad RAM or Video RAM. Try to change the video memory in the BIOs from the lower number (usually 32MB/64MB) to the higher number (64/128MB) if this does not work try the following below to repair the \u201cNTUser.DAT\u201d file that has become corrupted/damaged on the User Account._\\n\\n_First we have to reveal Hidden files & folders:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click Start\\n  </li>\\n  <li>\\n    Click Documents/My Documents\\n  </li>\\n  <li>\\n    Click Tools\\n  </li>\\n  <li>\\n    Click Folder Options\\n  </li>\\n  <li>\\n    Click View\\n  </li>\\n  <li>\\n    Click Show Hidden Files, Folders, and drives.\\n  </li>\\n  <li>\\n    Click Ok\\n  </li>\\n</ol>\\n\\n_Now we have to create a new administrator user account:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click Start\\n  </li>\\n  <li>\\n    Click Control Panel\\n  </li>\\n  <li>\\n    Click User Accounts\\n  </li>\\n  <li>\\n    Select New Account\\n  </li>\\n  <li>\\n    Set privileges to Administrator\\n  </li>\\n  <li>\\n    Now log out of the damaged account and login to your newly created User Account.\\n  </li>\\n  <li>\\n    Click Start\\n  </li>\\n  <li>\\n    Click My Computer/Computer\\n  </li>\\n  <li>\\n    Click on C:\\n  </li>\\n  <li>\\n    Navigate to Documents & Settings\\n  </li>\\n  <li>\\n    You should have a list of User Accounts on your computer shown; go into the \u201cdamaged\u201d User profile folder.\\n  </li>\\n  <li>\\n    Right click ntuser.dat and select rename to: ntuser.bak (bak=backup)\\n  </li>\\n  <li>\\n    Open up a new explorer Window by clicking Start and choose My Computer/Computer\\n  </li>\\n  <li>\\n    Navigate to c:WINDOWSREPAIR\\n  </li>\\n  <li>\\n    Right click ntuser.dat and select Copy\\n  </li>\\n  <li>\\n    Go back to the damaged profile folder right click a blank space and select Paste\\n  </li>\\n  <li>\\n    Now log out\\n  </li>\\n  <li>\\n    Now we need to log back into the damaged User Profile again.\\n  </li>\\n  <li>\\n    Windows will now use the NTUser.dat file we copied across earlier to regenerate a new set of undamaged files, do not worry if your theme has changed or things don\u2019t look quite right.\\n  </li>\\n  <li>\\n    Once the User Account has finished loading we need to log-out and switch back to the newly created account again.\\n  </li>\\n  <li>\\n    Click Start\\n  </li>\\n  <li>\\n    Click My Computer/Computer\\n  </li>\\n  <li>\\n    Click on C:\\n  </li>\\n  <li>\\n    Navigate to Documents & Settings\\n  </li>\\n  <li>\\n    Click on the damaged User Profile account\\n  </li>\\n  <li>\\n    Now right click the NTuser.dat file to:ntuser.old\\n  </li>\\n  <li>\\n    Right click the ntuser.bak file we renamed earlier and select rename and rename it back to NTUser.DAT.\\n  </li>\\n  <li>\\n    Log out of the User account and log back into the damaged User Profile, it should now be fixed.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/google-apps-mig","metadata":{"permalink":"/2012/09/30/google-apps-mig","source":"@site/blog/2012-09-30-google-apps-mig.md","title":"Google Apps Migration &#8211; Exchange to Google Apps","description":"Sample CSV (Comma\xa0Separated\xa0Values File) for migration to Google Apps using the Apps\xa0Migration tool\xa0from Exchange. The bottom is a sample of the heading needed for Google to recognise it.","date":"2012-09-30T15:36:33.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.18,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Google Apps Migration &#8211; Exchange to Google Apps","date":"2012-09-30T15:36:33.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"One of the files containing the systems registry data had to be recovered","permalink":"/2012/09/30/files-cont-sysms-registry-data-recovered"},"nextItem":{"title":"How to Merge Facebook Info into Google Contacts","permalink":"/2012/09/30/merge-facebook-info-into-google-contacts"}},"content":"Sample CSV (Comma\xa0Separated\xa0Values File) for migration to Google Apps using the Apps\xa0<a title=\\"Exchange_to_Google_Tool\\" href=\\"https://tools.google.com/dlpage/exchangemigration/\\" target=\\"_blank\\">Migration tool</a>\xa0from Exchange. The bottom is a sample of the heading needed for Google to recognise it.\\n\\n&nbsp;\\n\\noffice@domain.com#password,office@newdomain.com\\n  \\nadmin@domain.com#password,admin@newdomain.com\\n  \\nteacher@domain.com#password,teacher@newdomain.com\\n\\n&nbsp;"},{"id":"/2012/09/30/merge-facebook-info-into-google-contacts","metadata":{"permalink":"/2012/09/30/merge-facebook-info-into-google-contacts","source":"@site/blog/2012-09-30-merge-facebook-info-into-google-contacts.md","title":"How to Merge Facebook Info into Google Contacts","description":"First we use a web-based tool called \u201cFriends To Gmail\u201d which collates the data from your Facebook friends such as Name, Birthday and Biography information (currently this does not support Phone Numbers).","date":"2012-09-30T15:16:50.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":2.18,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to Merge Facebook Info into Google Contacts","date":"2012-09-30T15:16:50.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Google Apps Migration &#8211; Exchange to Google Apps","permalink":"/2012/09/30/google-apps-mig"},"nextItem":{"title":"Reset Windows 7 Security Permissions","permalink":"/2012/09/30/reset-windows-7-security-permissions"}},"content":"First we use a web-based tool called \u201cFriends To Gmail\u201d which collates the data from your Facebook friends such as Name, Birthday and Biography information (currently this does not support Phone Numbers).\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Open</strong>\xa0<a title=\\"Friends To Gmail\\" href=\\"http://www.friendstogmail.com/\\" target=\\"_blank\\">Friends to Gmail</a>\\n  </li>\\n  <li>\\n    Click on \u201c<strong><em>Connect With Facebook</em></strong>\u201d\\n  </li>\\n  <li>\\n    <strong>Login</strong>/Authenticate using your <strong>Facebooks</strong> data, you might want to select the security settings for this transactions to allow only you to see any information posted.\\n  </li>\\n  <li>\\n    Click <strong>Next</strong>\\n  </li>\\n  <li>\\n    Friends to Gmail will then go through and <strong>collect</strong> your friend\u2019s <strong>data</strong>; it will then list all the data in a huge frame.\\n  </li>\\n  <li>\\n    Using your mouse left <strong>click</strong> the first part of your friends data and <strong>drag</strong> the mouse down to the <strong>bottom</strong> select the data as you go once everything is selected right click a highlighted part and left click Copy.\\n  </li>\\n  <li>\\n    Now we need to <strong>paste</strong> it as a file Google Contacts will be able to read, the easiest way to do this is to open up <strong>Notepad</strong>. Click Start\\n  </li>\\n  <li>\\n    Click Programs\\n  </li>\\n  <li>\\n    Click Accessories\\n  </li>\\n  <li>\\n    Click Notepad\\n  </li>\\n  <li>\\n    Right click in your brand new Notepad document and left click Paste\\n  </li>\\n  <li>\\n    Once your data has been pasted into the Notepad document click File, <strong>Save</strong>\\n  </li>\\n  <li>\\n    Save your document in an easily accessible replace and name it \u201cfacebook.csv\u201d\\n  </li>\\n  <li>\\n    Now we <strong>head</strong> to\xa0<a title=\\"Google Contacts\\" href=\\"http://www.google.com/contacts/\\" target=\\"_blank\\">Google Contacts</a>\\n  </li>\\n  <li>\\n    Click More\\n  </li>\\n  <li>\\n    Click <strong>Import</strong>\\n  </li>\\n  <li>\\n    On the Import <strong>Contacts</strong> dialog select \u201cChoose File\u201d\\n  </li>\\n  <li>\\n    Locate and select your facebook.csv file and click Save\\n  </li>\\n  <li>\\n    Click Import\\n  </li>\\n  <li>\\n    <strong>Google</strong> will now import the data from Facebook into its database and create a new contact group.\\n  </li>\\n  <li>\\n    Click on the new contact group on the left (which should be named depending on the date/time) and click Find & Merge Contacts up the top.\\n  </li>\\n  <li>\\n    Google will then display a list of changes to contacts you already have and allow you to merge the changes into the new contacts.\\n  </li>\\n  <li>\\n    Once done you can remove the Group and you have now imported your contact information.\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n_If you have an Android phone and want to import your friends profile pictures from Facebook to Google Contacts, good news there is a free utility you can use._\\n\\nFirst you need to head to Google Play and download a tool called <a title=\\"Contact Picture Sync\\" href=\\"https://play.google.com/store/apps/details?id=heinrisch.contact.picture.sync&hl=en\\" target=\\"_blank\\">Contact Picture Sync</a>\xa0to your Android device.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    On your Android device, open <strong>Contact Picture Sync</strong>\\n  </li>\\n</ol>"},{"id":"/2012/09/30/reset-windows-7-security-permissions","metadata":{"permalink":"/2012/09/30/reset-windows-7-security-permissions","source":"@site/blog/2012-09-30-reset-windows-7-security-permissions.md","title":"Reset Windows 7 Security Permissions","description":"Click Start","date":"2012-09-30T15:12:42.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.405,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Reset Windows 7 Security Permissions","date":"2012-09-30T15:12:42.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to Merge Facebook Info into Google Contacts","permalink":"/2012/09/30/merge-facebook-info-into-google-contacts"},"nextItem":{"title":"How to fix error 0x8004FF01 while installing Microsoft Security Essentals","permalink":"/2012/09/30/0x8004ff01"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> <strong>to Programs/Accessories</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> Command <strong>Prompt</strong> and <strong>select</strong> <strong><em>Run As Administrator</em></strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0<strong><em>secedit /configure /cfg %windir%infdefltbase.inf /db defltbase.sdb /verbose</em></strong>\\n  </li>\\n  <li>\\n    Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    <strong>Wait</strong> for it to complete the operation and the <strong>permissions</strong> should be <strong>reset</strong> back to <strong>Factory</strong> <strong>Settings</strong>.\\n  </li>\\n</ol>\\n\\nIf the above does not work, try this\xa0<a title=\\"Security_Restore\\" href=\\"http://www.softpedia.com/get/Security/Security-Related/Security-Restore.shtml\\" target=\\"_blank\\">Security Restore Utility</a>.\\n\\n_This has also been reported as the fix for Windows Update Error #8007005_"},{"id":"/2012/09/30/0x8004ff01","metadata":{"permalink":"/2012/09/30/0x8004ff01","source":"@site/blog/2012-09-30-0x8004ff01.md","title":"How to fix error 0x8004FF01 while installing Microsoft Security Essentals","description":"Problems getting Security Essentials installed? Try these tips below.","date":"2012-09-30T14:55:48.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.755,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to fix error 0x8004FF01 while installing Microsoft Security Essentals","date":"2012-09-30T14:55:48.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Reset Windows 7 Security Permissions","permalink":"/2012/09/30/reset-windows-7-security-permissions"},"nextItem":{"title":"Service Pack did not install. Reverting changes","permalink":"/2012/09/30/srv-pack-not-install-revrt-changes"}},"content":"_Problems getting Security Essentials installed? Try these tips below._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Remove</strong> any old <strong>Antivirus</strong> software that could be causing incompatibility with Security Essentials\\n  </li>\\n  <li>\\n    It is likely that some parts of the old antivirus are remaining, such as Norton&#8217;s Symantec services. Attempt running the antivirus removal tools from the developer\u2019s websites to remove leftover traces.\\n  </li>\\n  <li>\\n    <strong>Download</strong> & <strong>install</strong> the latest Microsoft\xa0<a href=\\"http://www.microsoft.com/download/en/details.aspx?id=8483\\" target=\\"_blank\\">Windows Installer</a>\xa0& attempt Security Essentials install.\\n  </li>\\n</ol>\\n\\n_Still not working? Then try the following._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Run a System File Checker (Start, Run type: cmd then press Enter to open Command Prompt, then type sfc /scannow then press Enter) &#8211; You may need your operating system CD at this point if it needs to grab files off it.\\n  </li>\\n  <li>\\n    Open Command Prompt, following the instructions above & type: &#8220;<strong><em>reg delete HKLM/SOFTWARE/Microsoft/SQMClient/Windows/DisabledSessions /va /f</em></strong>&#8221; press <strong>Enter</strong>.\\n  </li>\\n  <li>\\n    Attempt reinstall.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/srv-pack-not-install-revrt-changes","metadata":{"permalink":"/2012/09/30/srv-pack-not-install-revrt-changes","source":"@site/blog/2012-09-30-srv-pack-not-install-revrt-changes.md","title":"Service Pack did not install. Reverting changes","description":"This usually happens when a service that is required, the Trusted Installer is not running. Follow the instructions below to repair this.","date":"2012-09-30T14:55:20.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.39,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Service Pack did not install. Reverting changes","date":"2012-09-30T14:55:20.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to fix error 0x8004FF01 while installing Microsoft Security Essentals","permalink":"/2012/09/30/0x8004ff01"},"nextItem":{"title":"How to use the KWorld DVD Maker 2","permalink":"/2012/09/30/how-to-use-kworld-dvd-maker-2"}},"content":"_This usually happens when a service that is required, the Trusted Installer is not running. Follow the instructions below to repair this._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Run</strong>\\n  </li>\\n  <li>\\n    Type:\xa0<strong><em>services.msc</em></strong><em>\xa0</em>> Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    You should be greeted with a Services dialog list look for TrustedInstaller.\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> <strong><em>TrustedInstaller</em></strong> and click <strong>Properties</strong>\\n  </li>\\n  <li>\\n    <strong>Set</strong> the TrustedInstaller service to <strong>Automatic</strong> and click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Attempt Service Pack installation again.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/how-to-use-kworld-dvd-maker-2","metadata":{"permalink":"/2012/09/30/how-to-use-kworld-dvd-maker-2","source":"@site/blog/2012-09-30-how-to-use-kworld-dvd-maker-2.md","title":"How to use the KWorld DVD Maker 2","description":"To operate this device, you need to have to have installed Cyberlink DVD Producer that came on a CD with your device.","date":"2012-09-30T14:53:44.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.745,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to use the KWorld DVD Maker 2","date":"2012-09-30T14:53:44.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Service Pack did not install. Reverting changes","permalink":"/2012/09/30/srv-pack-not-install-revrt-changes"},"nextItem":{"title":"Uninstall CS6 Master Collection","permalink":"/2012/09/30/uninstall-cs6-master-collection"}},"content":"_To operate this device, you need to have to have installed Cyberlink DVD Producer that came on a CD with your device._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First <strong>plug</strong> <strong>in</strong> the DVD Maker 2 (USB) device into the supplied External USB Cable and plug that into the computer, then connect the USB device into the component outputs on the device, for example VHS machine you are trying to copy from.\\n  </li>\\n  <li>\\n    <strong>Open</strong> DVD <strong>Producer</strong>\\n  </li>\\n  <li>\\n    <strong>Press</strong> the Direct to DVD <strong>button</strong> (at the top)\\n  </li>\\n  <li>\\n    Put in a <strong>Blank</strong> <strong>DVD</strong> <strong>into</strong> your DVD R/W <strong>drive</strong>\\n  </li>\\n  <li>\\n    Click <strong>Next</strong>\\n  </li>\\n  <li>\\n    Make sure the right <strong>Recorder</strong> <strong>device</strong> is <strong>select</strong> and press <strong>Record</strong>.\\n  </li>\\n  <li>\\n    Press <strong>End Record</strong> to <strong>finalise</strong> and <strong>finish</strong> the recording.\xa0<em>Make sure that if you are recording from a VHS machine to make sure that the VHS is rewind and press Play when recording.</em>\\n  </li>\\n</ol>"},{"id":"/2012/09/30/uninstall-cs6-master-collection","metadata":{"permalink":"/2012/09/30/uninstall-cs6-master-collection","source":"@site/blog/2012-09-30-uninstall-cs6-master-collection.md","title":"Uninstall CS6 Master Collection","description":"Click\xa0Start","date":"2012-09-30T14:50:53.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.26,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Uninstall CS6 Master Collection","date":"2012-09-30T14:50:53.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to use the KWorld DVD Maker 2","permalink":"/2012/09/30/how-to-use-kworld-dvd-maker-2"},"nextItem":{"title":"Sony Tablet S 1080 Handbrake Settings","permalink":"/2012/09/30/sony-tablet-s-1080-handbrake-settings"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    Click\xa0<strong>Start</strong>\\n  </li>\\n  <li>\\n    Click\xa0<strong>Control</strong>\xa0<strong>Panel</strong>\\n  </li>\\n  <li>\\n    Click\xa0<strong>Add/Remove Programs/Uninstall a Program</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong>\xa0click\xa0<strong>Adobe CS6 Master Collection</strong>\\n  </li>\\n  <li>\\n    <strong>Left</strong>\xa0<strong>click</strong>\xa0<strong>Uninstall</strong>\\n  </li>\\n  <li>\\n    Follow the prompts to uninstall.\\n  </li>\\n</ol>\\n\\n_In order to remove the registry information for CS6 including activation information delete the folders below:_\\n\\n**_C:Program Files (x86)Common FilesAdobeSLCache\\n  \\nC:ProgramDataAdobeSLStore_**"},{"id":"/2012/09/30/sony-tablet-s-1080-handbrake-settings","metadata":{"permalink":"/2012/09/30/sony-tablet-s-1080-handbrake-settings","source":"@site/blog/2012-09-30-sony-tablet-s-1080-handbrake-settings.md","title":"Sony Tablet S 1080 Handbrake Settings","description":"Open Handbrake","date":"2012-09-30T14:35:20.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.29,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Sony Tablet S 1080 Handbrake Settings","date":"2012-09-30T14:35:20.000Z","authors":["Luke"],"tags":["Mobile","Windows"]},"unlisted":false,"prevItem":{"title":"Uninstall CS6 Master Collection","permalink":"/2012/09/30/uninstall-cs6-master-collection"},"nextItem":{"title":"How to Backup/Import Vivotech Camera Settings","permalink":"/2012/09/30/backupimport-vivotech-cam-setting"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    Open <strong>Handbrake</strong>\\n  </li>\\n  <li>\\n    Make sure the <strong>video</strong> <strong>file</strong> you would like to convert is <strong>selected</strong>.\\n  </li>\\n  <li>\\n    On the right hand side <strong>click iPhone 4</strong>\\n  </li>\\n  <li>\\n    Leave everything as default except you need to <strong>change</strong> the <strong>resolution</strong> to: <strong><em>1280&#215;800</em></strong>\\n  </li>\\n  <li>\\n    <strong>Convert</strong>, copy to your Tablet S and enjoy your video!\\n  </li>\\n</ol>"},{"id":"/2012/09/30/backupimport-vivotech-cam-setting","metadata":{"permalink":"/2012/09/30/backupimport-vivotech-cam-setting","source":"@site/blog/2012-09-30-backupimport-vivotech-cam-setting.md","title":"How to Backup/Import Vivotech Camera Settings","description":"Wishing to backup or import your configuration settings for your Vivotech cameras, follow the instructions below:","date":"2012-09-30T14:27:43.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.7,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to Backup/Import Vivotech Camera Settings","date":"2012-09-30T14:27:43.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Sony Tablet S 1080 Handbrake Settings","permalink":"/2012/09/30/sony-tablet-s-1080-handbrake-settings"},"nextItem":{"title":"No Sound in Chrome","permalink":"/2012/09/30/no-sound-in-chrome"}},"content":"_Wishing to backup or import your configuration settings for your Vivotech cameras, follow the instructions below:_\\n\\n**\xa0**\\n\\n**How to Backup your Vivotech settings**\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Programs/All Programs</strong>\\n  </li>\\n  <li>\\n    Click <strong>VIVOTEK Inc</strong>\\n  </li>\\n  <li>\\n    Click ST***Tools\\n  </li>\\n  <li>\\n    Open <strong>Import-Export Utility</strong>\\n  </li>\\n  <li>\\n    Click <strong>Export</strong>\\n  </li>\\n  <li>\\n    <strong>Select</strong> where you would like to <strong>save</strong> the configuration data to and click <strong>Export</strong>/<strong>Save</strong>.\\n  </li>\\n  <li>\\n    Your <strong>Camera</strong> settings are now <strong>saved</strong> as a <strong>BIN</strong> file in the folder you chose above.\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n**How to Import your Vivotech Settings**\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Programs/All Programs</strong>\\n  </li>\\n  <li>\\n    Click <strong>VIVOTEK Inc</strong>\\n  </li>\\n  <li>\\n    Click ST***Tools\\n  </li>\\n  <li>\\n    Open <strong>Import-Export Utility</strong>\\n  </li>\\n  <li>\\n    Click <strong>Import</strong>\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to the folder you have <strong>saved</strong> the <strong>export</strong> <strong>file</strong> to and click <strong>Import/Open</strong>.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/no-sound-in-chrome","metadata":{"permalink":"/2012/09/30/no-sound-in-chrome","source":"@site/blog/2012-09-30-no-sound-in-chrome.md","title":"No Sound in Chrome","description":"1. Open\xa0Chrome","date":"2012-09-30T14:25:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"No Sound in Chrome","date":"2012-09-30T14:25:00.000Z","authors":["Luke"],"tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to Backup/Import Vivotech Camera Settings","permalink":"/2012/09/30/backupimport-vivotech-cam-setting"},"nextItem":{"title":"How to move an inserted image anywhere in Word","permalink":"/win/mv-img-anywhere-in-word"}},"content":"1. **Open\xa0****Chrome**\\n  2. In the address bar (the area you put a website address in) **type** in: **chrome://plugins/**\\n  3. **Under** the **Plug-Ins**, underneath Flash click Disable\\n  4. Navigate to the webpage you were having issues with **sound** on, you should **now****hear** it.\\n\\n_Note: Can cause some Flash animations not to display. Install the latest <a title=\\"Adobe Flash Player\\" href=\\"http://get.adobe.com/flashplayer/\\" target=\\"_blank\\">Flash Player</a>._"},{"id":"win/mv-img-anywhere-in-word","metadata":{"permalink":"/win/mv-img-anywhere-in-word","source":"@site/blog/2012-09-30-mv-img-anywhere-in-word.md","title":"How to move an inserted image anywhere in Word","description":"Right\xa0click\xa0the\xa0Picture\xa0you would like to move","date":"2012-09-30T14:21:44.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to move an inserted image anywhere in Word","date":"2012-09-30T14:21:44.000Z","authors":["Luke"],"tags":["Windows"],"slug":"win/mv-img-anywhere-in-word"},"unlisted":false,"prevItem":{"title":"No Sound in Chrome","permalink":"/2012/09/30/no-sound-in-chrome"},"nextItem":{"title":"Google Bulk Account Update &#8220;read error&#8221;","permalink":"/2012/09/30/google-bulk-account-update-read-error"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    <strong>Right</strong>\xa0<strong>click</strong>\xa0the\xa0<strong>Picture</strong>\xa0you would like to move\\n  </li>\\n  <li>\\n    <strong>Left</strong>\xa0click\xa0<strong>Wrap</strong>\xa0<strong>Text</strong>\\n  </li>\\n  <li>\\n    <strong>Left</strong>\xa0click\xa0<strong>In Front of Text</strong>\\n  </li>\\n  <li>\\n    Now you should be\xa0<strong>able</strong>\xa0<strong>to move the image</strong>\xa0anywhere you want.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/google-bulk-account-update-read-error","metadata":{"permalink":"/2012/09/30/google-bulk-account-update-read-error","source":"@site/blog/2012-09-30-google-bulk-account-update-read-error.md","title":"Google Bulk Account Update &#8220;read error&#8221;","description":"Trying to upload a CSV file to get Google to create multiple users with the email address, first name, and last name & password fields and getting the &#8220;read error&#8221; trying to upload. This is how I fixed it:","date":"2012-09-30T14:01:08.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.445,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Google Bulk Account Update &#8220;read error&#8221;","date":"2012-09-30T14:01:08.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to move an inserted image anywhere in Word","permalink":"/win/mv-img-anywhere-in-word"},"nextItem":{"title":"ntdll.dll Windows Explorer","permalink":"/2012/09/30/ntdll-dll-windows-explorer"}},"content":"Trying to upload a CSV file to get Google to create multiple users with the email address, first name, and last name & password fields and getting the &#8220;read error&#8221; trying to upload. This is how I fixed it:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Locate the file you are uploading and <strong>remove</strong> the &#8220;.csv&#8221; file <strong>extension</strong>. Upload the file without the extension.\\n  </li>\\n</ol>\\n\\n_To get rid of the CSV file extension, you might have to go to a folder, click on Tools, Options, Folder Options and uncheck Show File Extensions._"},{"id":"/2012/09/30/ntdll-dll-windows-explorer","metadata":{"permalink":"/2012/09/30/ntdll-dll-windows-explorer","source":"@site/blog/2012-09-30-ntdll-dll-windows-explorer.md","title":"ntdll.dll Windows Explorer","description":"Having problems with Windows Explorer with the \u201cntdll.dll\u201d error? This usually occurs when you are running an AMD system.","date":"2012-09-30T13:42:29.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.685,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"ntdll.dll Windows Explorer","date":"2012-09-30T13:42:29.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Google Bulk Account Update &#8220;read error&#8221;","permalink":"/2012/09/30/google-bulk-account-update-read-error"},"nextItem":{"title":"Mounting failed with the error: Protocol error","permalink":"/2012/09/30/mnting-failed-with-the-error-protocol-error"}},"content":"Having problems with Windows Explorer with the \u201cntdll.dll\u201d error? This usually occurs when you are running an AMD system.\\n\\n&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> My <strong>Computer</strong>\\n  </li>\\n  <li>\\n    Click <strong>Properties</strong>\\n  </li>\\n  <li>\\n    Click on <strong>Advanced</strong>\\n  </li>\\n  <li>\\n    Click <strong>Environment Variables</strong>\\n  </li>\\n  <li>\\n    Look for \u201c<strong><em>NLSPATH</em></strong>\u201d and <strong>delete</strong>/rename it\\n  </li>\\n  <li>\\n    Press <strong>Ok</strong>\\n  </li>\\n</ol>\\n\\n_Another issue is with Data Execution Prevention, follow the prompts below_:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Control Panel</strong>\\n  </li>\\n  <li>\\n    Click <strong>System</strong>\\n  </li>\\n  <li>\\n    Click the <strong>Advanced</strong> Tab\\n  </li>\\n  <li>\\n    Click <strong>Settings</strong>\\n  </li>\\n  <li>\\n    <strong>Click</strong> on <strong>Data Execution Prevention</strong>\\n  </li>\\n  <li>\\n    Click on \u201c<strong><em>Turn on DEP for all Programs and Services Except Those I Select</em></strong>\u201d\\n  </li>\\n  <li>\\n    Click <strong>Add</strong>\\n  </li>\\n  <li>\\n    Browse to <strong><em>c:Windowsexplorer.exe</em></strong>\\n  </li>\\n  <li>\\n    Click <strong>Add</strong>\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    <strong>Restart</strong> your machine.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/mnting-failed-with-the-error-protocol-error","metadata":{"permalink":"/2012/09/30/mnting-failed-with-the-error-protocol-error","source":"@site/blog/2012-09-30-mnting-failed-with-the-error-protocol-error.md","title":"Mounting failed with the error: Protocol error","description":"Getting &#8220;/sbin/mount.vboxsf Protocol error&#8221; error with Virtualbox?","date":"2012-09-30T13:26:58.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.125,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Mounting failed with the error: Protocol error","date":"2012-09-30T13:26:58.000Z","authors":["Luke"],"tags":["Linux","Misc","Windows"]},"unlisted":false,"prevItem":{"title":"ntdll.dll Windows Explorer","permalink":"/2012/09/30/ntdll-dll-windows-explorer"},"nextItem":{"title":"WWII Battle Tanks T-34 Vs Tiger Crashing","permalink":"/2012/09/30/wwii-battle-tanks-t-34-vs-tiger-crashing"}},"content":"Getting &#8220;/sbin/mount.vboxsf: mounting failed with the error: Protocol error&#8221; error with Virtualbox?\\n\\n&nbsp;\\n\\n  1. The fix,\xa0re-install\xa0[Guest Additions](http://luke.geek.nz//unix/rm-virtualbox-guest-additions/ \\"How to Remove VirtualBox Guest Additions\\").\\n\\n&nbsp;\\n\\n&nbsp;"},{"id":"/2012/09/30/wwii-battle-tanks-t-34-vs-tiger-crashing","metadata":{"permalink":"/2012/09/30/wwii-battle-tanks-t-34-vs-tiger-crashing","source":"@site/blog/2012-09-30-wwii-battle-tanks-t-34-vs-tiger-crashing.md","title":"WWII Battle Tanks T-34 Vs Tiger Crashing","description":"You need to download 3 DLLs click the links below to download and save them in a place you will remember and have easy access to for example your Desktop or Documents folder.","date":"2012-09-30T13:17:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.585,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"WWII Battle Tanks T-34 Vs Tiger Crashing","date":"2012-09-30T13:17:00.000Z","authors":["Luke"],"permalink":"/win/wwii-battle-tanks-t-34-vs-tiger-crashing/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Mounting failed with the error: Protocol error","permalink":"/2012/09/30/mnting-failed-with-the-error-protocol-error"},"nextItem":{"title":"Green Line top of Youtube Video","permalink":"/2012/09/30/green-youtubevid"}},"content":"_You need to download 3 DLLs click the links below to download and save them in a place you will remember and have easy access to for example your Desktop or Documents folder._\\n\\n<a title=\\"MSVCI70.DLL\\" href=\\"http://www.dll-files.com/dllindex/dll-files.shtml?msvci70\\" target=\\"_blank\\">MSVCI70.dll</a>\\n\\n<a title=\\"MSVCP71.DLL\\" href=\\"http://www.dll-files.com/dllindex/dll-files.shtml?msvcp71\\" target=\\"_blank\\">msvcp71.dll</a>\\n\\n<a title=\\"MSVCR71.DLL\\" href=\\"http://www.dll-files.com/dllindex/dll-files.shtml?msvcr71\\" target=\\"_blank\\">msvcr71.dll</a>\\n\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Once the files have been downloaded <strong>extract</strong> them.\\n  </li>\\n  <li>\\n    Right click the files and select <strong>Copy</strong>\\n  </li>\\n  <li>\\n    <strong>Open</strong> My <strong>Computer</strong>\\n  </li>\\n  <li>\\n    Click on <strong>C</strong>:\\n  </li>\\n  <li>\\n    Click on <strong>Windows</strong>\\n  </li>\\n  <li>\\n    <strong>Go</strong> into the <strong><em>System32</em></strong> folder\\n  </li>\\n  <li>\\n    Right click and left click <strong>Paste</strong>\\n  </li>\\n  <li>\\n    Select Yes to <strong>overwrite</strong> any files.\\n  </li>\\n  <li>\\n    <strong>Launch</strong> Battle Tanks\\n  </li>\\n</ol>\\n\\nNote: Tested on Windows 7, Windows Vista"},{"id":"/2012/09/30/green-youtubevid","metadata":{"permalink":"/2012/09/30/green-youtubevid","source":"@site/blog/2012-09-30-green-youtubevid.md","title":"Green Line top of Youtube Video","description":"Getting a green line at the top of each Youtube video? It has recently be caused by a recent Adobe Update, follow the guide below to repair:","date":"2012-09-30T11:39:58.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"},{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.3,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Green Line top of Youtube Video","date":"2012-09-30T11:39:58.000Z","authors":["Luke"],"tags":["Linux","Mac OSX","Windows"]},"unlisted":false,"prevItem":{"title":"WWII Battle Tanks T-34 Vs Tiger Crashing","permalink":"/2012/09/30/wwii-battle-tanks-t-34-vs-tiger-crashing"},"nextItem":{"title":"How to disable Slingshots Proxy","permalink":"/2012/09/30/how-to-disable-slingshots-proxy"}},"content":"Getting a green line at the top of each Youtube video? It has recently be caused by a recent Adobe Update, follow the guide below to repair:\\n\\n  1. **Right** **click** the Youtube **video**\\n  2. **Click** Flash Player **Settings**\\n  3. Make sure you are in the **General** settings **tab** _(left most tab)_\\n  4. **Uncheck** \u201c**_Use Hardware Acceleration_**\u201d\\n  5. **Restart** your browser"},{"id":"/2012/09/30/how-to-disable-slingshots-proxy","metadata":{"permalink":"/2012/09/30/how-to-disable-slingshots-proxy","source":"@site/blog/2012-09-30-how-to-disable-slingshots-proxy.md","title":"How to disable Slingshots Proxy","description":"1. First you need to head to &#8220;Slingshot&#8220;","date":"2012-09-30T11:34:48.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.37,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to disable Slingshots Proxy","date":"2012-09-30T11:34:48.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Green Line top of Youtube Video","permalink":"/2012/09/30/green-youtubevid"},"nextItem":{"title":"Blank webpage with Windows Genuine Advantage","permalink":"/2012/09/30/blank-webpage-with-win-gen-advantage"}},"content":"1. First you need to head to &#8220;<a title=\\"Slingshot ISP NZ\\" href=\\"http://slingshot.co.nz/\\" target=\\"_blank\\">Slingshot</a>&#8220;\\n  2. On the right hand side, **Login** to Your **Account**.\\n  3. Click **My Internet**\\n  4. Click **Internet** **Cache** **Settings**\\n  5. You will see _&#8220;You currently have caching ON&#8221;_ and below that you will &#8220;_If you&#8217;d like to change your settings,\xa0Click here&#8221;_\\n  6. Click it to **turn** Slingshot&#8217;s Internet **Caching** **off**\\n  7. Give it about one minute, then **restart** your Broadband **modem**."},{"id":"/2012/09/30/blank-webpage-with-win-gen-advantage","metadata":{"permalink":"/2012/09/30/blank-webpage-with-win-gen-advantage","source":"@site/blog/2012-09-30-blank-webpage-with-win-gen-advantage.md","title":"Blank webpage with Windows Genuine Advantage","description":"Trying to install Microsoft Security Essentials and it is complaining about not having genuine Windows? Attempting to run the Windows Genuine Advantage validation and you are ending up with a blank screen? This is usually caused when your Time/Date is out.","date":"2012-09-30T11:13:05.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.415,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Blank webpage with Windows Genuine Advantage","date":"2012-09-30T11:13:05.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to disable Slingshots Proxy","permalink":"/2012/09/30/how-to-disable-slingshots-proxy"},"nextItem":{"title":"How to remove OSX User Account Password","permalink":"/2012/09/30/rm-osx-user-account-password"}},"content":"_Trying to install Microsoft Security Essentials and it is complaining about not having genuine Windows? Attempting to run the Windows Genuine Advantage validation and you are ending up with a blank screen? This is usually caused when your Time/Date is out._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Double left click on the <strong>Time/Date</strong>\xa0<em>(Lower right of your screen)</em>\\n  </li>\\n  <li>\\n    Make \xa0any adjustments, make sure that Year/Month and Day <strong>is</strong> <strong>correct</strong>\\n  </li>\\n  <li>\\n    Click Ok\\n  </li>\\n  <li>\\n    <strong>Run</strong> the WGA (Windows Genuine Advantage) <strong>validation</strong> <strong>again</strong>.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/rm-osx-user-account-password","metadata":{"permalink":"/2012/09/30/rm-osx-user-account-password","source":"@site/blog/2012-09-30-rm-osx-user-account-password.md","title":"How to remove OSX User Account Password","description":"With CD","date":"2012-09-30T10:42:21.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":1.985,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to remove OSX User Account Password","date":"2012-09-30T10:42:21.000Z","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Blank webpage with Windows Genuine Advantage","permalink":"/2012/09/30/blank-webpage-with-win-gen-advantage"},"nextItem":{"title":"How to install Windows Media Centre in Windows 8 Preview","permalink":"/2012/09/30/how-to-install-windows-media-centre-in-win8-preview"}},"content":"**With CD**\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Put your OSX CD/DVD into your optical drive.\\n  </li>\\n  <li>\\n    Restart the Mac and hold down the C key while the Mac is starting\\n  </li>\\n  <li>\\n    Now you should be greeted with a start-up menu select the Install (DVD/CD)\\n  </li>\\n  <li>\\n    Once it has booted click Utilities in the upper Menu\\n  </li>\\n  <li>\\n    Select Password Reset/Reset Password\\n  </li>\\n  <li>\\n    Now select your OSX HDD\\n  </li>\\n  <li>\\n    Select the User Account you want to remove/change the password of and enter in a new password.\\n  </li>\\n  <li>\\n    Click Save\\n  </li>\\n  <li>\\n    Now restart your Mac and remove the OSX CD/DVD from the drive.\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n**Without CD Method**\\n\\nNote: You may lose all your Data, documents etc. Please backup first!\\n\\n_Tested on 10.5.6_\\n\\nFollowing the steps below you will create a new User Account which will override your old User Account and take\xa0precedence over logging in you will then remove the password from your old User Account and set it to\xa0log in.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Restart OSX\\n  </li>\\n  <li>\\n    Hold down the Apple Key+S\xa0<em>(or Command+S on newer Macs)</em>\xa0key as the computer first turns on.\\n  </li>\\n  <li>\\n    Once the Mac starts showing lines of text and a terminal you can release the keys.\\n  </li>\\n  <li>\\n    Once you see a blinking cursor type: \u201cmount -uw /\u201d press Enter\\n  </li>\\n  <li>\\n    Type: \u201crm /var/db/.AppleSetupDone\u201d press Enter\\n  </li>\\n  <li>\\n    Type: \u201cshutdown -h now\u201d press Enter\\n  </li>\\n  <li>\\n    Turn the computer back on\\n  </li>\\n  <li>\\n    You will now be greeted with for a new user creation, follow the prompts to create a new User\\n  </li>\\n  <li>\\n    You would have noticed that there is no option to login to your old account, this is normal.\\n  </li>\\n  <li>\\n    Once you are within OSX\\n  </li>\\n  <li>\\n    Click the Apple Logo<em>\xa0(top left)</em>\\n  </li>\\n  <li>\\n    Click System Preferences\\n  </li>\\n  <li>\\n    Click Users\\n  </li>\\n  <li>\\n    Click Unlock\xa0<em>(little lock, down the bottom of the dialog)</em>\\n  </li>\\n  <li>\\n    Select your old User Account\\n  </li>\\n  <li>\\n    Click Change Password\xa0<em>(here you can remove and/or change it depending on what you want to do)</em>\\n  </li>\\n  <li>\\n    Click Login Options\xa0<em>(down the bottom)</em>\\n  </li>\\n  <li>\\n    Click Change Automatic Options to Disabled\\n  </li>\\n  <li>\\n    Restart OSX\\n  </li>\\n  <li>\\n    Now you will be greeted to a login dialog, select your old User Account\\n  </li>\\n  <li>\\n    Login and you can go back to System Preferences/Users to delete the account you had created.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/how-to-install-windows-media-centre-in-win8-preview","metadata":{"permalink":"/2012/09/30/how-to-install-windows-media-centre-in-win8-preview","source":"@site/blog/2012-09-30-how-to-install-windows-media-centre-in-win8-preview.md","title":"How to install Windows Media Centre in Windows 8 Preview","description":"Windows 8 Consumer preview does not have the Windows Media Centre installed by\xa0default (most likely because this will be an Add-on for purchase on release)","date":"2012-09-30T10:21:03.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.725,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to install Windows Media Centre in Windows 8 Preview","date":"2012-09-30T10:21:03.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to remove OSX User Account Password","permalink":"/2012/09/30/rm-osx-user-account-password"},"nextItem":{"title":"Countdown Shopping App List Share","permalink":"/2012/09/30/countdown-app-list-share"}},"content":"Windows 8 Consumer preview does not have the Windows Media Centre installed by\xa0_default (most likely because this will be an Add-on for purchase on release)_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    To install Windows Media Centre go to the Windows 8 <strong>Metro</strong> <strong>Panel</strong>\\n  </li>\\n  <li>\\n    Swipe your Mouse/Finger to the <strong>upper</strong> <strong>right</strong> <strong>corner</strong> of the screen.\\n  </li>\\n  <li>\\n    Click <strong>Search</strong>\\n  </li>\\n  <li>\\n    Type: <strong><em>add</em></strong><em> <strong>features</strong></em>\\n  </li>\\n  <li>\\n    Windows will now <strong>search</strong> live and\xa0<em>\u201cAdd Features to Windows 8\u201d</em>\xa0should appear.\\n  </li>\\n  <li>\\n    <strong>Click</strong> (tap) <strong><em>Add Features to Windows 8</em></strong>\\n  </li>\\n  <li>\\n    <strong>Select</strong> <strong><em>I Already Have a Product Key</em></strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong> in the following <strong>Product</strong> <strong>Key</strong>: <strong><em>MBFBV-W3DP2-2MVKN-PJCQD-KKTF7</em></strong>\\n  </li>\\n  <li>\\n    Select <strong>Next</strong>\\n  </li>\\n  <li>\\n    <strong>Check</strong> the License Terms <strong>checkbox</strong> and <strong>click</strong> <strong><em>Add Features</em></strong>\\n  </li>\\n  <li>\\n    Windows 8 will then <strong>restart</strong> your computer and Windows Media Centre will not be installed\xa0<em>(You can find it pinned to the Metro Start Panel)</em>\\n  </li>\\n</ol>"},{"id":"/2012/09/30/countdown-app-list-share","metadata":{"permalink":"/2012/09/30/countdown-app-list-share","source":"@site/blog/2012-09-30-countdown-app-list-share.md","title":"Countdown Shopping App List Share","description":"&nbsp;","date":"2012-09-30T08:49:51.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":1.225,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Countdown Shopping App List Share","date":"2012-09-30T08:49:51.000Z","authors":["Luke"],"tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"How to install Windows Media Centre in Windows 8 Preview","permalink":"/2012/09/30/how-to-install-windows-media-centre-in-win8-preview"},"nextItem":{"title":"How to create a password protected directory in Plesk","permalink":"/2012/09/30/how-to-create-a-password-protected-directory-in-plesk"}},"content":"&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    To send other people your Countdown Shopping list, do the following:\\n  </li>\\n  <li>\\n    First\xa0<em>(You have to have either a Bluetooth pair with the device or Internet access for email, going to run this guide through the email way but similar to other methods)</em>\\n  </li>\\n  <li>\\n    Go to the Countdown <strong>application</strong> on your phone.\\n  </li>\\n  <li>\\n    <strong>Select</strong> my <strong>lists</strong>\\n  </li>\\n  <li>\\n    <strong>Select</strong> the <strong>list</strong> you want to <strong>send</strong> for someone\xa0<em>(for example Master List)</em>\\n  </li>\\n  <li>\\n    Now you need to <strong>open</strong> up the Countdown <strong>menu</strong>\xa0<em>(so on your phone click the Menu button, for example on the Galaxy Ace it is the button with Lines going down on the left hand side of the phone)</em>\\n  </li>\\n  <li>\\n    Select <strong>Share</strong>\\n  </li>\\n  <li>\\n    Now you need to <strong>select</strong> your <strong>sharing</strong> <strong>method</strong>, in this case we are going to use Email so select \u201cEmail to a friend<em>\u201d\xa0 (Other methods Bluetooth/Dropbox/Gmail etc are located under Share with another app user)</em>\\n  </li>\\n  <li>\\n    Select Gmail or what email application you use on your phone.\\n  </li>\\n  <li>\\n    Now enter in the email address of the person you are going to send it to and send.\\n  </li>\\n  <li>\\n    The other person simply needs to open the email on their phone and click the link in the email\xa0<em>(to the attached TXT document)</em>\\n  </li>\\n  <li>\\n    When it selects what program to open it with, select your Web Browser.\\n  </li>\\n  <li>\\n    The Countdown Shopping app will then launch and <strong>import</strong> the list\\n  </li>\\n</ol>"},{"id":"/2012/09/30/how-to-create-a-password-protected-directory-in-plesk","metadata":{"permalink":"/2012/09/30/how-to-create-a-password-protected-directory-in-plesk","source":"@site/blog/2012-09-30-how-to-create-a-password-protected-directory-in-plesk.md","title":"How to create a password protected directory in Plesk","description":"First off log into Plesk","date":"2012-09-30T08:37:23.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.46,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to create a password protected directory in Plesk","date":"2012-09-30T08:37:23.000Z","authors":["Luke"],"layout":"post","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Countdown Shopping App List Share","permalink":"/2012/09/30/countdown-app-list-share"},"nextItem":{"title":"How to add Album Art to iTunes","permalink":"/2012/09/30/how-to-add-album-art-to-itunes"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    First off <strong>log</strong> <strong>in</strong>to Plesk\\n  </li>\\n  <li>\\n    Click on <strong>Websites and Domains</strong>\\n  </li>\\n  <li>\\n    <strong>Click</strong> Hide <strong>Advanced</strong> <strong>Options</strong><em>\xa0(down the bottom)</em>\\n  </li>\\n  <li>\\n    Click on <strong>Password</strong>&#8211;<strong>protected</strong> <strong>Directories</strong>\\n  </li>\\n  <li>\\n    Click <strong>Add</strong> Protected Directory\\n  </li>\\n  <li>\\n    <strong>Select</strong> the <strong>folder</strong> you want to password protect and click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    Next select your newly made protected directory and click <strong>Add</strong> <strong>User</strong>\\n  </li>\\n  <li>\\n    <strong>Add</strong> the <strong>user</strong>(s) that you want to have access to the directory including their password and click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    You have now Pass worded a directory.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/how-to-add-album-art-to-itunes","metadata":{"permalink":"/2012/09/30/how-to-add-album-art-to-itunes","source":"@site/blog/2012-09-30-how-to-add-album-art-to-itunes.md","title":"How to add Album Art to iTunes","description":"Sometimes Album art may not be avaliable or you would like to add your own here is how you do it.","date":"2012-09-30T00:57:23.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.755,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to add Album Art to iTunes","date":"2012-09-30T00:57:23.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to create a password protected directory in Plesk","permalink":"/2012/09/30/how-to-create-a-password-protected-directory-in-plesk"},"nextItem":{"title":"Google Calendar Sync Error 1011","permalink":"/2012/09/30/google-cal-sync-error-1011"}},"content":"_Sometimes Album art may not be avaliable or you would like to add your own here is how you do it._\\n\\n  1. **Open** iTunes and go to your Music Library. **View** it in **Grid** View so that all of your albums are displayed.\\n  2. **Right**&#8211;**click** the **album** whose album art you wish to change. **Select** **Get** **Info**.\\n  3. When asked if you are sure you want to edit information for multiple items, click the Yes button.\\n  4. Go to the **Artwork** section and double-click on it.\\n  5. **Browse** to the desired album **art** and click the **Open** button.\\n  6. The songs will be updated with the new album art and it will then be displayed in your Music Library.\\n\\n_You can also Copy & Paste Album art directly into it by right clicking the image you want and select copy. Then right click the Artwork screen in iTunes and select paste._"},{"id":"/2012/09/30/google-cal-sync-error-1011","metadata":{"permalink":"/2012/09/30/google-cal-sync-error-1011","source":"@site/blog/2012-09-30-google-cal-sync-error-1011.md","title":"Google Calendar Sync Error 1011","description":"Open Outlook","date":"2012-09-30T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.295,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Google Calendar Sync Error 1011","permalink":"/misc/google-cal-sync-error-1011/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to add Album Art to iTunes","permalink":"/2012/09/30/how-to-add-album-art-to-itunes"},"nextItem":{"title":"Sims Social Practice Hammering 7 Times","permalink":"/2012/09/30/sims-social-hammering7"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    Open <strong>Outlook</strong>\\n  </li>\\n  <li>\\n    Click <strong>File</strong>\\n  </li>\\n  <li>\\n    Click <strong>Help</strong>\\n  </li>\\n  <li>\\n    Click <strong>Options</strong>\\n  </li>\\n  <li>\\n    Click <strong>Add-ins</strong>\\n  </li>\\n  <li>\\n    Click the <strong>Manage</strong> dropdown list\\n  </li>\\n  <li>\\n    Click <strong>Disabled</strong> <strong>Items</strong>\\n  </li>\\n  <li>\\n    Click <strong>Go</strong>\\n  </li>\\n  <li>\\n    Select <strong><em>Addin:google</em></strong>..\\n  </li>\\n  <li>\\n    Click <strong>Enable</strong>\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    Now <strong>do</strong> a Google Calender <strong>Sync</strong>\\n  </li>\\n</ol>"},{"id":"/2012/09/30/sims-social-hammering7","metadata":{"permalink":"/2012/09/30/sims-social-hammering7","source":"@site/blog/2012-09-30-sims-social-hammering7.md","title":"Sims Social Practice Hammering 7 Times","description":"Having problems with the Canvas Stretcher promotion and needing to Practice Hammering 7 Times? The fix for this is simple.","date":"2012-09-30T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.375,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Sims Social Practice Hammering 7 Times","date":"2012-09-30 00:00:00 +1300","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Google Calendar Sync Error 1011","permalink":"/2012/09/30/google-cal-sync-error-1011"},"nextItem":{"title":"How to stop SysPrep from starting","permalink":"/2012/09/30/stop-sysprep-starting"}},"content":"_Having problems with the Canvas Stretcher promotion and needing to Practice Hammering 7 Times? The fix for this is simple._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    In the\xa0<a title=\\"Sims Social\\" href=\\"https://apps.facebook.com/thesimssocial/\\" target=\\"_blank\\">Sims Social</a>\xa0click on <strong>Career</strong> <strong>Quest</strong>\xa0<em>(to the lower right of your avatar)</em>\\n  </li>\\n  <li>\\n    Click <strong>Jobs</strong>\\n  </li>\\n  <li>\\n    Then click <strong>Practice</strong> <strong>Hammering</strong>\xa0<em>(the 10 minute job to top left)</em>\\n  </li>\\n  <li>\\n    <strong>Go</strong> <strong>back</strong> every <strong>10</strong> <strong>Minutes</strong> to finish the job, until you have <strong>completed</strong> it <strong>7</strong> <strong>times</strong>.\\n  </li>\\n</ol>"},{"id":"/2012/09/30/stop-sysprep-starting","metadata":{"permalink":"/2012/09/30/stop-sysprep-starting","source":"@site/blog/2012-09-30-stop-sysprep-starting.md","title":"How to stop SysPrep from starting","description":"Having issues with sysprep popping up all the time? Follow the instructions below to stop that annoyance from happening.","date":"2012-09-30T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.725,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to stop SysPrep from starting","layout":"post","permalink":"/win/stop-sysprep-starting/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Sims Social Practice Hammering 7 Times","permalink":"/2012/09/30/sims-social-hammering7"},"nextItem":{"title":"How to repair Windows Boot Loader","permalink":"/2012/09/29/repair-win-boot-loader"}},"content":"_Having issues with sysprep popping up all the time? Follow the instructions below to stop that annoyance from happening._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Run</strong>\\n  </li>\\n  <li>\\n    Type: <strong><span style=\\"text-decoration: underline;\\">regedit</span></strong>\\n  </li>\\n  <li>\\n    Once the <strong>Registry</strong> Editor loads, navigate to: <strong><em>HKEY_LOCAL_MACHINESYSTEMSetup</em></strong>\\n  </li>\\n  <li>\\n    Now to the right you should see some registry entries which you need to edit.\\n  </li>\\n  <li>\\n    <strong>Change AuditInProgress to \u201c0\u201d</strong>\\n  </li>\\n  <li>\\n    <strong>Change cmdLine to \u201csetup \u2013newsetup\u201d</strong>\\n  </li>\\n  <li>\\n    <strong>Change FactoryPreInstallInProgress to \u201c0\u201d</strong>\\n  </li>\\n  <li>\\n    <strong>Change MiniSetupInProgress to \u201c0\u201d</strong>\\n  </li>\\n  <li>\\n    <strong>Change SetupType to \u201c0\u201d</strong>\\n  </li>\\n  <li>\\n    <strong>Change SystemSetupInProgress to \u201c0\u201d</strong>\\n  </li>\\n</ol>\\n\\n_You are pretty much changing Yes I do want to run SysPrep to No I don\u2019t want to._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Finally, <strong>navigate</strong> to: <strong><em>HKEY_LOCAL_MACHINEMicrosoftFactory</em></strong> and <strong>delete</strong> <strong>Factory</strong>.\\n  </li>\\n</ol>\\n\\n_You are now safe to delete the SYSPREP folder where it may be (ie C:SYSPREP)_"},{"id":"/2012/09/29/repair-win-boot-loader","metadata":{"permalink":"/2012/09/29/repair-win-boot-loader","source":"@site/blog/2012-09-29-repair-win-boot-loader.md","title":"How to repair Windows Boot Loader","description":"First things first! In order to repair the Windows boot loaders you need the\xa0appropriate\xa0Windows CD/DVD for example:\xa0Windows XP Home/Professional CD for the Windows XP installation & Windows 7 DVD for Windows 7 install.","date":"2012-09-29T09:03:55.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.375,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to repair Windows Boot Loader","date":"2012-09-29T09:03:55.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to stop SysPrep from starting","permalink":"/2012/09/30/stop-sysprep-starting"},"nextItem":{"title":"How to reset Ventrillo settings back to default","permalink":"/2012/09/28/how-to-reset-ventrillo-settings-back-to-default"}},"content":"First things first! In order to repair the Windows boot loaders you need the\xa0appropriate\xa0Windows CD/DVD for example:\xa0_Windows XP Home/Professional CD for the Windows XP installation & Windows 7 DVD for Windows 7 install._\\n\\n**Windows XP**\\n\\nThe instructions on repairing this are similar to the\xa0[DDL Kernel fix](http://http://luke.geek.nz//load-needed-dlls-for-kernel/ \\"Load needed DLLs for Kernel\\").\\n\\nTo access the recover console follow the prompts below:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First thing you need to have a Windows XP <strong>CD</strong>\\n  </li>\\n  <li>\\n    Put CD in drive and <strong>boot</strong> from it, you may have to press ESC or F8 to open Boot Prompt to select CD Drive.\\n  </li>\\n  <li>\\n    Once it has loaded choose <strong>Recovery Console</strong>\\n  </li>\\n  <li>\\n    Type:\xa0<strong><em>bootcfg /rebuild\xa0</em></strong>(This will generate a new Boot.ini file)\\n  </li>\\n  <li>\\n    Press Enter twice at the prompts that follow.\\n  </li>\\n  <li>\\n    Type:\xa0<strong><em>fixboot</em></strong>\\n  </li>\\n  <li>\\n    Press Enter\\n  </li>\\n  <li>\\n    Type:\xa0<strong><em>fixmbr</em></strong>\\n  </li>\\n  <li>\\n    Press Enter\\n  </li>\\n  <li>\\n    Type:\xa0<em>exit\xa0</em>(This will restart your computer, eject your CD)\\n  </li>\\n</ol>\\n\\n**Windows Vista/Windows 7**\\n\\nThe bootloader fix for both Windows Vista and Windows 7 is luckily similar.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First thing you need to do is get your Windows Vista/Windows 7 <strong>DVD</strong>\\n  </li>\\n  <li>\\n    Put DVD in drive and <strong>boot</strong> from it, once prompted select Repair This Computer (lower left hand side of the Windows dialog)\\n  </li>\\n  <li>\\n    Select your Windows install you want to repair and select Next\\n  </li>\\n  <li>\\n    Now you should be greeted with the <strong>Advanced System Recovery</strong> dialog box, select <strong>Command Prompt</strong>\\n  </li>\\n  <li>\\n    Type:\xa0<strong><em>bootrec.exe /FixMbr</em></strong>\\n  </li>\\n  <li>\\n    Press Enter\\n  </li>\\n  <li>\\n    Type:\xa0<strong><em>bootrec.exe /FixBoot</em></strong>\\n  </li>\\n  <li>\\n    Press Enter\\n  </li>\\n  <li>\\n    Type:\xa0<em>exit\xa0</em>(to close the Command Prompt)\\n  </li>\\n  <li>\\n    Restart your machine & eject the CD/DVD ROM.\\n  </li>\\n</ol>"},{"id":"/2012/09/28/how-to-reset-ventrillo-settings-back-to-default","metadata":{"permalink":"/2012/09/28/how-to-reset-ventrillo-settings-back-to-default","source":"@site/blog/2012-09-28-how-to-reset-ventrillo-settings-back-to-default.md","title":"How to reset Ventrillo settings back to default","description":"1. Click Start","date":"2012-09-28T10:41:34.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to reset Ventrillo settings back to default","date":"2012-09-28T10:41:34.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to repair Windows Boot Loader","permalink":"/2012/09/29/repair-win-boot-loader"},"nextItem":{"title":"Monitor rotation wrong","permalink":"/2012/09/28/monitor-rotation-wrong"}},"content":"1. Click Start\\n  2. Click Run\\n  3. Type in: **_%appdata%ventrilo_**\\n  4. Right click ventrillo2.ini\\n  5. Select Rename\\n  6. Name it something different like ventlol\\n\\n_Make sure ventrilo is closed._\\n\\n&nbsp;"},{"id":"/2012/09/28/monitor-rotation-wrong","metadata":{"permalink":"/2012/09/28/monitor-rotation-wrong","source":"@site/blog/2012-09-28-monitor-rotation-wrong.md","title":"Monitor rotation wrong","description":"Having problems where your monitor rotation is wrong, such as the start menu ending up upside down at the top of the screen? Follow the instructions below to repair.","date":"2012-09-28T09:49:26.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.94,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Monitor rotation wrong","date":"2012-09-28T09:49:26.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to reset Ventrillo settings back to default","permalink":"/2012/09/28/how-to-reset-ventrillo-settings-back-to-default"},"nextItem":{"title":"Sony Tablet S Not Starting Reset","permalink":"/2012/09/28/sony-tablet-s-not-starting-reset"}},"content":"_Having problems where your monitor rotation is wrong, such as the start menu ending up upside down at the top of the screen? Follow the instructions below to repair._\\n\\nCommon fix for those using an Intel graphics driver, try the following Key combination:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    On the keyboard, press and hold the: <strong>CTRL+ALT</strong> key then <strong>hold</strong> down the <strong>UP-ARROW</strong> button, to <strong>switch</strong> the <strong>screen</strong> <strong>rotation</strong>, keep <strong>pressing</strong> the UP-<strong>ARROW</strong> till you get your desired <strong>rotation</strong>.\\n  </li>\\n</ol>\\n\\n_If the step above didn\u2019t work, try this:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Look for the Time on the Start Bar\\n  </li>\\n  <li>\\n    Around the time will be small little icons, now position your mouse you might have to put it sideways and click to reveal any hidden icons on the small arrow.\\n  </li>\\n  <li>\\n    Look for the Graphics driver tray icon, it might be nVidia<em>\xa0(which is a green icon)</em>\xa0or ATI\xa0<em>(which is a red icon)</em>\xa0and right click it.\\n  </li>\\n  <li>\\n    Left click Rotation\\n  </li>\\n  <li>\\n    Choose Normal\\n  </li>\\n  <li>\\n    Press Ok<em>\xa0(within 15 seconds)</em>\xa0on the dialog Window that opens to accept the changes if your rotation is now the right way.\\n  </li>\\n</ol>"},{"id":"/2012/09/28/sony-tablet-s-not-starting-reset","metadata":{"permalink":"/2012/09/28/sony-tablet-s-not-starting-reset","source":"@site/blog/2012-09-28-sony-tablet-s-not-starting-reset.md","title":"Sony Tablet S Not Starting Reset","description":"Having problems where your Sony S Tablet is getting power from the adapter but actually not turning on? I had this issue and this is how I got it running again.","date":"2012-09-28T09:25:50.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.55,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Sony Tablet S Not Starting Reset","date":"2012-09-28T09:25:50.000Z","authors":["Luke"],"tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"Monitor rotation wrong","permalink":"/2012/09/28/monitor-rotation-wrong"},"nextItem":{"title":"Missing Facebook ticker","permalink":"/2012/09/25/missing-facebook-ticker"}},"content":"_Having problems where your Sony S Tablet is getting power from the adapter but actually not turning on? I had this issue and this is how I got it running again._\\n\\n  1. On the side panel of your Tablet you will see small letters, stating Reset.\\n  2. Get a **Pin** or Paper Clip and put it in the small **hole** next to the Reset word.\\n  3. **Hold** for **10 seconds**\\n  4. Your Tablet should now **restart** on its own.\\n\\n_Note: Pressing the Reset button will not set the tablet back to Factory settings, but pressing the Reset button and turning your Tablet off will reset it back to Factory settings._"},{"id":"/2012/09/25/missing-facebook-ticker","metadata":{"permalink":"/2012/09/25/missing-facebook-ticker","source":"@site/blog/2012-09-25-missing-facebook-ticker.md","title":"Missing Facebook ticker","description":"Missing the Facebook Ticker?","date":"2012-09-25T21:33:02.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.185,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Missing Facebook ticker","date":"2012-09-25T21:33:02.000Z","authors":["Luke"],"layout":"post","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Sony Tablet S Not Starting Reset","permalink":"/2012/09/28/sony-tablet-s-not-starting-reset"},"nextItem":{"title":"NTLDR Windows XP","permalink":"/2012/09/25/ntldr-windows-xp"}},"content":"Missing the Facebook Ticker?\\n\\nThe ticker gets activated once you have reached a certain amount of friends on your Facebook profile who are active. The fix is simply, invite more friends! General count is about 100 Friends."},{"id":"/2012/09/25/ntldr-windows-xp","metadata":{"permalink":"/2012/09/25/ntldr-windows-xp","source":"@site/blog/2012-09-25-ntldr-windows-xp.md","title":"NTLDR Windows XP","description":"To repair this issue you need to have a Windows XP CD.","date":"2012-09-25T18:47:01.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.655,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"NTLDR Windows XP","date":"2012-09-25T18:47:01.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Missing Facebook ticker","permalink":"/2012/09/25/missing-facebook-ticker"},"nextItem":{"title":"You do not have enough permission to read eeepc Xandros Install","permalink":"/2012/09/18/you-do-not-have-enough-permission-to-read-eeepc-xandros-install"}},"content":"To repair this issue you need to have a Windows XP CD.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First you need to <strong>boot</strong> from the <strong>Windows</strong> <strong>CD</strong>\\n  </li>\\n  <li>\\n    Once at the first \u201cwelcome\u201d screen, press <strong>R</strong> to enter the Recovery Console\\n  </li>\\n  <li>\\n    Press the number of the Windows install you would like to repair (ie 1)\\n  </li>\\n  <li>\\n    Type in your Administrator password if there is one, if not press Enter.\\n  </li>\\n  <li>\\n    Now we need to <strong>copy</strong> two <strong>files</strong> from the Windows <strong>CD</strong> to your Windows install folders, type in the commands below:\\n  </li>\\n  <li>\\n    <strong><em>copy d:I386ntldr C</em></strong>: (Press Enter)\\n  </li>\\n  <li>\\n    <strong><em>copy d:I386ntdetect.com C</em></strong>: (Press Enter)\\n  </li>\\n  <li>\\n    Type: <strong>exit</strong>\\n  </li>\\n</ol>\\n\\nYour computer will now restart, remove the Windows CD and you should now have a bootable Windows installation."},{"id":"/2012/09/18/you-do-not-have-enough-permission-to-read-eeepc-xandros-install","metadata":{"permalink":"/2012/09/18/you-do-not-have-enough-permission-to-read-eeepc-xandros-install","source":"@site/blog/2012-09-18-you-do-not-have-enough-permission-to-read-eeepc-xandros-install.md","title":"You do not have enough permission to read eeepc Xandros Install","description":"You do not have enough permission - eeePC Xandros","date":"2012-09-19T09:49:26.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.2,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"You do not have enough permission to read eeepc Xandros Install","date":"2012-09-19T09:49:26.000Z","authors":["Luke"],"tags":["Linux"]},"unlisted":false,"prevItem":{"title":"NTLDR Windows XP","permalink":"/2012/09/25/ntldr-windows-xp"},"nextItem":{"title":"Remove Sidebar ads on Handycafe Client","permalink":"/2012/09/13/rm-sidebar-ads-on-handycafe-client"}},"content":"You do not have enough permission - eeePC Xandros\\n\\nThis usually occurs when the /media folder does not exist and usually occurs after the Full Desktop has been enabled.\\n\\n  1. Open a **Terminal**\\n  2. Type:_**\xa0sudo mkdir /media**_\\n  3. Press Enter"},{"id":"/2012/09/13/rm-sidebar-ads-on-handycafe-client","metadata":{"permalink":"/2012/09/13/rm-sidebar-ads-on-handycafe-client","source":"@site/blog/2012-09-13-rm-sidebar-ads-on-handycafe-client.md","title":"Remove Sidebar ads on Handycafe Client","description":"In order to remove the Ads on the sidebar, we need to block them using an inbuilt list called the HOST file.","date":"2012-09-13T10:39:52.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.82,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Remove Sidebar ads on Handycafe Client","date":"2012-09-13T10:39:52.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"You do not have enough permission to read eeepc Xandros Install","permalink":"/2012/09/18/you-do-not-have-enough-permission-to-read-eeepc-xandros-install"},"nextItem":{"title":"Apple Pages File Location","permalink":"/2012/09/13/apple-pages-store"}},"content":"In order to remove the Ads on the sidebar, we need to block them using an inbuilt list called the HOST file.\\n\\n&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Go to a machine with Handycafe installed\\n  </li>\\n  <li>\\n    Click on My Computer/Computer\\n  </li>\\n  <li>\\n    Goto: C:WindowsSystem32drivers\\n  </li>\\n  <li>\\n    Find the \u201c<strong>hosts</strong>\u201d file\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> the <strong>host</strong> file and left click <strong>Properties</strong>\\n  </li>\\n  <li>\\n    <strong>Uncheck</strong> Use <strong>Read</strong> <strong>Only</strong> and left click <strong>Apply</strong>\\n  </li>\\n  <li>\\n    Right click the hosts file and left click <strong>Open</strong> <strong>With</strong>\\n  </li>\\n  <li>\\n    Select <strong>Notepad</strong>\\n  </li>\\n  <li>\\n    Ignoring everything listed in the file, go to the bottom of the file and enter in the following:\\n  </li>\\n  <li>\\n    <strong><em>127.0.0.1 ad.handycafe.com</em></strong><strong></strong>\\n  </li>\\n  <li>\\n    <strong><em>127.0.0.1 search.handycafe.com</em></strong><strong></strong>\\n  </li>\\n  <li>\\n    Click File\\n  </li>\\n  <li>\\n    Click <strong>Save</strong>\\n  </li>\\n  <li>\\n    <strong>Restart</strong> the machine and the Ads should now vanish, do the same procedure on all your Caf\xe9 machines.\\n  </li>\\n</ol>\\n\\nNote: Using a local firewall on the machine you could also block the Domains as another method of blocking them."},{"id":"/2012/09/13/apple-pages-store","metadata":{"permalink":"/2012/09/13/apple-pages-store","source":"@site/blog/2012-09-13-apple-pages-store.md","title":"Apple Pages File Location","description":"Apple Pages, stores it&#8217;s files at the location below:","date":"2012-09-13T08:49:10.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.055,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Apple Pages File Location","date":"2012-09-13T08:49:10.000Z","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Remove Sidebar ads on Handycafe Client","permalink":"/2012/09/13/rm-sidebar-ads-on-handycafe-client"},"nextItem":{"title":"How to install Fonts in Windows","permalink":"/2012/09/13/howto-install-fonts-in-windows"}},"content":"Apple Pages, stores it&#8217;s files at the location below:\\n\\n**//var/mobile/applications/Pages/Library/Application Support/Documents**"},{"id":"/2012/09/13/howto-install-fonts-in-windows","metadata":{"permalink":"/2012/09/13/howto-install-fonts-in-windows","source":"@site/blog/2012-09-13-howto-install-fonts-in-windows.md","title":"How to install Fonts in Windows","description":"1. Click Start","date":"2012-09-13T08:44:59.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to install Fonts in Windows","date":"2012-09-13T08:44:59.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Apple Pages File Location","permalink":"/2012/09/13/apple-pages-store"},"nextItem":{"title":"How to check Vodafone NZ Account Balance","permalink":"/2012/09/13/vodafonenz_accountbal"}},"content":"1. Click **Start**\\n  2. Click **Run**\\n  3. **Type**: **fonts**\\n  4. Press **Enter**\\n  5. The font dialog box will open. All you need to do now is just **drag** the **font** **files** you want to install **into** the **folder**.\\n\\n_Alternatively you can doing the following:_\\n\\n  1. Click **File**\\n  2. **Select** **Install** **New Font**\\n  3. Browse and **locate** the **font**\\n  4. Press **ok**.\\n\\n_The fonts directory is located at C:WINDOWSFonts_"},{"id":"/2012/09/13/vodafonenz_accountbal","metadata":{"permalink":"/2012/09/13/vodafonenz_accountbal","source":"@site/blog/2012-09-13-vodafonenz_accountbal.md","title":"How to check Vodafone NZ Account Balance","description":"Mobile Phone","date":"2012-09-13T08:27:21.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.81,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to check Vodafone NZ Account Balance","date":"2012-09-13T08:27:21.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to install Fonts in Windows","permalink":"/2012/09/13/howto-install-fonts-in-windows"},"nextItem":{"title":"ffmpeg Linux commands","permalink":"/2012/09/13/ffmpeg-linux-commands"}},"content":"**Mobile Phone**\\n\\n_These instructions differ slightly depending on your phones make/model. Please adjust accordingly._\\n\\n  1. On the phone, **go** **to** the **Main** **Menu**\\n  2. Go to **Messaging**\\n  3. Choose **Create** **New**\\n  4. You are now going to **make** a **TXT** **message** **to**: **777** (which is Vodafone)\\n  5. In the text **message** **type**: **bal**\\n  6. **Send** (_Vodafone will then txt you back with your balance_)\\n\\n**How to top-up or check your balance using a Vodem**\\n\\n  1. Click **Start**\\n  2. click **Programs** (or All Programs)\\n  3. Click **Vodafone**\\n  4. Click Vodafone **SMS**\\n  5. Click **New** **SMS** to create a new SMS\\n  6. For the **send**-to number type: **777** (Type: 887 for topping up)\\n  7. In the **message** type: **bal** (For topping up, type in your top-up code)\\n  8. **Send**\xa0(_Vodafone will then txt you back with your balance or top your Vodem up_)\\n\\n_You can also use the <a title=\\"Vodafone\\" href=\\"http://www.vodafone.co.nz/\\" target=\\"_blank\\">Vodafone</a>\xa0homepage to check your balance and top-up, even when there is no credit\xa0available._\\n\\n&nbsp;"},{"id":"/2012/09/13/ffmpeg-linux-commands","metadata":{"permalink":"/2012/09/13/ffmpeg-linux-commands","source":"@site/blog/2012-09-13-ffmpeg-linux-commands.md","title":"ffmpeg Linux commands","description":"Getting infos from a video file","date":"2012-09-13T08:17:24.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":2.405,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"ffmpeg Linux commands","date":"2012-09-13T08:17:24.000Z","authors":["Luke"],"layout":"post","tags":["Linux"]},"unlisted":false,"prevItem":{"title":"How to check Vodafone NZ Account Balance","permalink":"/2012/09/13/vodafonenz_accountbal"},"nextItem":{"title":"Faulting application name: SimCitySocieties.exe","permalink":"/2012/09/07/faulting-app-simcitysocieties-exe"}},"content":"**Getting infos from a video file**\\n\\nffmpeg -i video.avi\\n\\n**Turn X images to a video sequence**\\n\\nffmpeg -f image2 -i image%d.jpg video.mpg\\n\\n_This command will transform all the images from the current directory (named_\\n  \\n_image1.jpg, image2.jpg, etc\u2026) to a video file named video.mpg._\\n\\n**Turn a video to X images**\\n\\nffmpeg -i video.mpg image%d.jpg\\n\\n_This command will generate the files named image1.jpg, image2.jpg, \u2026_\\n\\n_The following image formats are also availables : PGM, PPM, PAM, PGMYUV, JPEG,_\\n  \\n_GIF, PNG, TIFF, SGI._\\n\\n**Encode a video sequence for the iPod/iPhone**\\n\\nffmpeg -i source_video.avi input -acodec aac -ab 128kb -vcodec mpeg4 -b 1200kb\\n  \\n-mbd 2 -flags +4mv+trell -aic 2 -cmp 2 -subcmp 2 -s 320&#215;180 -title X\\n  \\nfinal_video.mp4\\n\\n_Explanations:_\\n\\n_* Source: source_video.avi_\\n  \\n_* Audio codec: aac_\\n  \\n_* Audio bitrate: 128kb/s_\\n  \\n_* Video codec: mpeg4_\\n  \\n_* Video bitrate: 1200kb/s_\\n  \\n_* Video size: 320px by 180px_\\n  \\n_* Generated video: final_video.mp4_\\n\\n**Encode video for the PSP**\\n\\nffmpeg -i source_video.avi -b 300 -s 320&#215;240 -vcodec xvid -ab 32 -ar 24000\\n  \\n-acodec aac final_video.mp4\\n\\n_Explanations:_\\n\\n_* Source: source_video.avi_\\n  \\n_* Audio codec: aac_\\n  \\n_* Audio bitrate: 32kb/s_\\n  \\n_* Video codec: xvid_\\n  \\n_* Video bitrate: 1200kb/s_\\n  \\n_* Video size: 320px by 180px_\\n  \\n_* Generated video: final_video.mp4_\\n\\n**Extracting sound from a video, and save it as Mp3**\\n\\nffmpeg -i source_video.avi -vn -ar 44100 -ac 2 -ab 192 -f mp3 sound.mp3\\n\\n_Explanations:_\\n\\n_* Source video: source_video.avi_\\n  \\n_* Audio bitrate: 192kb/s_\\n  \\n_* Output format: mp3_\\n  \\n_* Generated sound: sound.mp3_\\n\\n**Convert a wav file to Mp3**\\n\\nffmpeg -i son\\\\_origine.avi -vn -ar 44100 -ac 2 -ab 192 -f mp3 son\\\\_final.mp3\\n\\n**Convert .avi video to .mpg**\\n\\nffmpeg -i video\\\\_origine.avi video\\\\_finale.mpg\\n\\n**Convert .mpg to .avi**\\n\\nffmpeg -i video\\\\_origine.mpg video\\\\_finale.avi\\n\\n**Convert .avi to animated gif(uncompressed)**\\n\\nffmpeg -i video\\\\_origine.avi gif\\\\_anime.gif\\n\\n**Mix a video with a sound file**\\n\\nffmpeg -i son.wav -i video\\\\_origine.avi video\\\\_finale.mpg\\n\\n**Convert .avi to .flv**\\n\\nffmpeg -i video_origine.avi -ab 56 -ar 44100 -b 200 -r 15 -s 320&#215;240 -f flv\\n  \\nvideo_finale.flv\\n\\n**Convert .avi to dv**\\n\\nffmpeg -i video_origine.avi -s pal -r pal -aspect 4:3 -ar 48000 -ac 2\\n  \\nvideo_finale.dv\\n\\n_or_\\n\\nffmpeg -i video\\\\_origine.avi -target pal-dv video\\\\_finale.dv\\n\\n**Convert .avi to mpeg for dvd players**\\n\\nffmpeg -i source_video.avi -target pal-dvd -ps 2000000000 -aspect 16:9\\n  \\nfinale_video.mpeg\\n\\n_Explanations:_\\n\\n_* target pal-dvd: Output format_\\n  \\n_* ps 2000000000 maximum size for the output file, in bits (here, 2 Gb)_\\n  \\n_* aspect 16:9 : Widescreen_\\n\\n**Compress .avi to divx**\\n\\nffmpeg -i video\\\\_origine.avi -s 320&#215;240 -vcodec msmpeg4v2 video\\\\_finale.avi\\n\\n**Compress Ogg Theora to Mpeg dvd**\\n\\nffmpeg -i film\\\\_sortie\\\\_cinelerra.ogm -s 720&#215;576 -vcodec mpeg2video -acodec mp3\\n  \\nfilm_termin\xe9e.mpg\\n\\n**Compress .avi to SVCD mpeg2**\\n\\n_**NTSC format:**_\\n\\nffmpeg -i video\\\\_origine.avi -target ntsc-svcd video\\\\_finale.mpg\\n\\n_**PAL format:**_\\n\\nffmpeg -i video\\\\_origine.avi -target pal-svcd video\\\\_finale.mpg\\n\\n**Compress .avi to VCD mpeg2**\\n\\n_Convert file to Matroska container without re-encoding_\\n\\nffmpeg -i input_file.wmv -vcodec copy -sameq -acodec copy -f matroska\\n  \\noutput_file.mkv\\n\\n_**NTSC format:**_\\n\\nffmpeg -i video\\\\_origine.avi -target ntsc-vcd video\\\\_finale.mpg\\n\\n_**PAL format:**_\\n\\nffmpeg -i video\\\\_origine.avi -target pal-vcd video\\\\_finale.mpg\\n\\n**\xa0Multi-pass encoding with ffmpeg**\\n\\nffmpeg -i fichierentree -pass 2 -passlogfile ffmpeg2pass fichiersortie-2\\n\\n&nbsp;"},{"id":"/2012/09/07/faulting-app-simcitysocieties-exe","metadata":{"permalink":"/2012/09/07/faulting-app-simcitysocieties-exe","source":"@site/blog/2012-09-07-faulting-app-simcitysocieties-exe.md","title":"Faulting application name: SimCitySocieties.exe","description":"Getting the &#8220;Faulting application name 1.0.4.243&#8221; and cannot play your game? Try the fix below.","date":"2012-09-07T06:26:44.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.385,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Faulting application name: SimCitySocieties.exe","date":"2012-09-07T06:26:44.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"ffmpeg Linux commands","permalink":"/2012/09/13/ffmpeg-linux-commands"},"nextItem":{"title":"Enable Javascript in Internet Explorer","permalink":"/2012/09/06/enable-javascript-ie"}},"content":"Getting the &#8220;Faulting application name: SimCitySocieties.exe, version: 1.0.4.243&#8221; and cannot play your game? Try the fix below.\\n\\n_Usually caused by the Tradmark characters in a folder name &#8220;\u2122&#8221; which Windows does not support._\\n\\n  1. **Uninstall\xa0SimCity** Societies\\n  2. **Reinstall** the game and make sure the **install** **path** is\xa0**C:Program FilesElectronic ArtsSimCity Societies**\\n\\n_Note: If the above does not work, then you need to edit the registry to change the games install path: &#8220;Install Dir&#8221; in HKEY\\\\_LOCAL\\\\_MACHINESOFTWAREElectronic ArtsSimCity Societies&#8221;_\\n\\n&nbsp;"},{"id":"/2012/09/06/enable-javascript-ie","metadata":{"permalink":"/2012/09/06/enable-javascript-ie","source":"@site/blog/2012-09-06-enable-javascript-ie.md","title":"Enable Javascript in Internet Explorer","description":"Problems running Javascript and want to enable it? This is how you do it.","date":"2012-09-06T08:42:19.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.275,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Enable Javascript in Internet Explorer","date":"2012-09-06T08:42:19.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Faulting application name: SimCitySocieties.exe","permalink":"/2012/09/07/faulting-app-simcitysocieties-exe"},"nextItem":{"title":"Eliminate period after each word in letter","permalink":"/2012/09/06/eliminate-period"}},"content":"<div>\\n  Problems running Javascript and want to enable it? This is how you do it.\\n</div>\\n\\n<div>\\n</div>\\n\\n  1. Open **Internet** **Explorer\xa0**\\n  2. Click on the &#8220;**Tools**&#8221; menu.\\n  3. Click **Internet** **Options**\\n  4. Click &#8220;**Security**&#8221; tab at the top\\n  5. Click on &#8220;**Custom** **Level**,&#8221; then click on the &#8220;**Enable** **Active** **Scripting**&#8221; option.\\n  6. Click &#8220;OK&#8221;"},{"id":"/2012/09/06/eliminate-period","metadata":{"permalink":"/2012/09/06/eliminate-period","source":"@site/blog/2012-09-06-eliminate-period.md","title":"Eliminate period after each word in letter","description":"Getting a period after each word in Microsoft Office? That is due to formatting marks, being enabled. To disable them follow the instructions below:","date":"2012-09-06T06:36:34.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.215,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Eliminate period after each word in letter","date":"2012-09-06T06:36:34.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Enable Javascript in Internet Explorer","permalink":"/2012/09/06/enable-javascript-ie"},"nextItem":{"title":"Increase Internet Explorers Download Limit","permalink":"/2012/09/05/increase-internet-explorers-download-limit"}},"content":"_Getting a period after each word in Microsoft Office? That is due to formatting marks, being enabled. To disable them follow the instructions below:_\\n\\n  1. Open **Word**\\n  2. Click **Tools**\\n  3. Click **Options**\\n  4. View and **untick** the **SPACES** box\\n  5. Click **ok**"},{"id":"/2012/09/05/increase-internet-explorers-download-limit","metadata":{"permalink":"/2012/09/05/increase-internet-explorers-download-limit","source":"@site/blog/2012-09-05-increase-internet-explorers-download-limit.md","title":"Increase Internet Explorers Download Limit","description":"Internet Explorer limits the amount of concurrent downloads. This is how you increase it","date":"2012-09-05T21:39:27.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.475,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Increase Internet Explorers Download Limit","date":"2012-09-05T21:39:27.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Eliminate period after each word in letter","permalink":"/2012/09/06/eliminate-period"},"nextItem":{"title":"Disable the Windows Vista/XP Hotkeys","permalink":"/2012/09/05/disable-the-windows-vistaxp-hotkeys"}},"content":"Internet Explorer limits the amount of concurrent downloads. This is how you increase it\\n\\n  1. \xa0Click **Start**\\n  2. Click **Run**\\n  3. Type in &#8220;**notepad**&#8221; and press enter\\n  4. **Copy** the following and **paste** the HKey Local machine code underneath these instructions.\\n  5. Save the document as **&#8220;IEDownloads.reg**&#8220;\\n  6. **Go** to where you **saved** the **document** and double **click** it to **merge** it into the **registry**. If a confirmation prompt appears then **accept** it.\\n  7. **Restart** the computer and you should be able to download more then 8 files at a time.\\n\\n&nbsp;\\n\\n[HKEY\\\\_LOCAL\\\\_MACHINESOFTWAREMicrosoftInternet ExplorerMAINFeatureControlFEATURE_MAXCONNECTIONSPERSERVER]\\n\\n&#8220;iexplore.exe&#8221;=dword:0000000a"},{"id":"/2012/09/05/disable-the-windows-vistaxp-hotkeys","metadata":{"permalink":"/2012/09/05/disable-the-windows-vistaxp-hotkeys","source":"@site/blog/2012-09-05-disable-the-windows-vistaxp-hotkeys.md","title":"Disable the Windows Vista/XP Hotkeys","description":"1. Click Start","date":"2012-09-05T18:31:58.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.245,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Disable the Windows Vista/XP Hotkeys","date":"2012-09-05T18:31:58.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Increase Internet Explorers Download Limit","permalink":"/2012/09/05/increase-internet-explorers-download-limit"},"nextItem":{"title":"Disable Windows 7 Password 30 Timeout","permalink":"/2012/09/05/disable-win7"}},"content":"1. Click **Start**\\n  2. In the Search box type **_regedit.exe_**\\n  3. Press Enter\\n  4. In the registry editor locate to:\xa0_**HKEY\\\\_CURRENT\\\\_USERSoftwareMicrosoftWindowsCurrentVersionPoliciesExplorer**_\\n  5. **Create** a 32-bit DWORD **value** called **_NoWinKeys_** and set the value to **1**.\\n  6. You\u2019ll need to **log** on and back **off** for the **changes** to take effect."},{"id":"/2012/09/05/disable-win7","metadata":{"permalink":"/2012/09/05/disable-win7","source":"@site/blog/2012-09-05-disable-win7.md","title":"Disable Windows 7 Password 30 Timeout","description":"1. Click Start","date":"2012-09-05T18:25:05.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.105,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Disable Windows 7 Password 30 Timeout","date":"2012-09-05T18:25:05.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Disable the Windows Vista/XP Hotkeys","permalink":"/2012/09/05/disable-the-windows-vistaxp-hotkeys"},"nextItem":{"title":"A program on your computer has corrupted IE8 default search provider settings fix","permalink":"/2012/09/05/search-provider-settings-fix"}},"content":"1. Click **Start**\\n  2. Click **Programs**\\n  3. Click **Accessories**\\n  4. Click **Command** **Prompt**\\n  5. **Type** in:\xa0**_net accounts /maxpwage:unlimited_**\\n  6. Press **enter**"},{"id":"/2012/09/05/search-provider-settings-fix","metadata":{"permalink":"/2012/09/05/search-provider-settings-fix","source":"@site/blog/2012-09-05-search-provider-settings-fix.md","title":"A program on your computer has corrupted IE8 default search provider settings fix","description":"Nothing causes more frustration, then when your\xa0favorite\xa0Internet Explorer search\xa0engine\xa0keeps resetting. This is how you fix it.","date":"2012-09-05T18:09:37.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.735,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"A program on your computer has corrupted IE8 default search provider settings fix","date":"2012-09-05T18:09:37.000Z","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Disable Windows 7 Password 30 Timeout","permalink":"/2012/09/05/disable-win7"},"nextItem":{"title":"DCS A10 C Warthog Crashing Windows Vista/Windows 7","permalink":"/2012/09/05/warthogcrash"}},"content":"Nothing causes more frustration, then when your\xa0favorite\xa0Internet Explorer search\xa0engine\xa0keeps resetting. This is how you fix it.\\n\\n&#8221; IE8 will reset the search provider setting to default setting of xxx search. IE8 will open the search provider dialog where you can change your search provider&#8221;.\\n\\n  1. Close any instances of Internet Explorer you have open\\n  2. Click **Start**.\\n  3. Click **Run**.\\n  4. Type in **regedit** and press enter\\n  5. In the editor, expand and navigate to the following registry key: _**HKEY\\\\_CURRENT\\\\_USERSoftwareMicrosoftWindowsCurrentVersionExplorer**_<wbr>_**User**_</wbr>\\n  6. **Right** **click** User **Shell** Folders\\n  7. Select **New** and then **Expandable** **String** value.\\n  8. A new value is added in right pane\\n  9. Type in **AppData** and press enter\\n 10. Double click AppData\\n 11. Under value **data** type in_**\xa0%USERPROFILE%Application Dat**_**a**. \xa0_Note: If there is already AppData in the right pane, double click and correct it to %USERPROFILE%Application Data_\\n 12. **Exit** Registry Editor\\n 13. **Open** **Internet** Explorer"},{"id":"/2012/09/05/warthogcrash","metadata":{"permalink":"/2012/09/05/warthogcrash","source":"@site/blog/2012-09-05-warthogcrash.md","title":"DCS A10 C Warthog Crashing Windows Vista/Windows 7","description":"Having random crashes on DCS A10 C Warthog? With Texture mismatches and Runtime errors? This is usually due to security permissions inside Windows.","date":"2012-09-05T17:58:26.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.365,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"DCS A10 C Warthog Crashing Windows Vista/Windows 7","date":"2012-09-05T17:58:26.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"A program on your computer has corrupted IE8 default search provider settings fix","permalink":"/2012/09/05/search-provider-settings-fix"},"nextItem":{"title":"Retrieve Windows/Office Keys from Remote PC or External HDD","permalink":"/2012/09/05/retrieve-windoffice-keys-from-remote-pc-or-external-hdd"}},"content":"Having random crashes on DCS A10 C Warthog? With Texture mismatches and Runtime errors? This is usually due to security permissions inside Windows.\\n\\n  1. Click Start\\n  2. In the Search field, type: CMD. Then press enter.\\n  3. Type** _bcdedit /set increaseuserva 3072_** and press enter\\n\\n_You may need to temporarily disable UAC/run as administrator for this to work_\\n\\n_If you wish to reverse this command back to default, simply use &#8220;bcdedit /deletevalue increaseuserva&#8221;_"},{"id":"/2012/09/05/retrieve-windoffice-keys-from-remote-pc-or-external-hdd","metadata":{"permalink":"/2012/09/05/retrieve-windoffice-keys-from-remote-pc-or-external-hdd","source":"@site/blog/2012-09-05-retrieve-windoffice-keys-from-remote-pc-or-external-hdd.md","title":"Retrieve Windows/Office Keys from Remote PC or External HDD","description":"Using a Windows utility called ProduKey you can retrieve the product keys for Microsoft Windows and Office.","date":"2012-09-05T11:00:38.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.945,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Retrieve Windows/Office Keys from Remote PC or External HDD","date":"2012-09-05T11:00:38.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"DCS A10 C Warthog Crashing Windows Vista/Windows 7","permalink":"/2012/09/05/warthogcrash"},"nextItem":{"title":"Deposit all Collectibles in Guild Wars 2 at the same time","permalink":"/2012/09/03/deposit_gw2_collect"}},"content":"Using a Windows utility called ProduKey you can retrieve the product keys for Microsoft Windows and Office.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First click \u201c<a title=\\"Produ Key\\" href=\\"http://www.nirsoft.net/utils/product_cd_key_viewer.html\\" target=\\"_blank\\">here</a>\u201d to <strong>download</strong> (the link is down the bottom, you want the ZIP file)\\n  </li>\\n  <li>\\n    Open the saved file and <strong>extract</strong> <strong>to</strong> a <strong>folder</strong> that is easily accessible\\n  </li>\\n  <li>\\n    <strong>Open</strong> <strong>ProduKey</strong>.exe and it will automatically retrieve the product keys for the local machine.\\n  </li>\\n  <li>\\n    To <strong>recover</strong> the keys <strong>for</strong> another <strong>Windows</strong> installation, <strong>click</strong> <strong>File</strong>, Select <strong>Source</strong>\\n  </li>\\n  <li>\\n    Select <strong>Load</strong> the Product Keys from <strong>External</strong> <strong>Windows</strong> <strong>Directory</strong> and <strong>click</strong> <strong>Browse</strong> to <strong>locate</strong> the <strong>Windows</strong> directory you want to <strong>retrieve</strong> the <strong>keys</strong> for.\\n  </li>\\n</ol>\\n\\nNote: Using this utility you can also recover the keys using a software hive and remote computers on the network\xa0_(you have admin access to)._\\n\\nNote: Below are the command prompts commands to run the tool from the windows Command Prompt.\\n\\n_produkey.exe /remote Server01_\\n\\n_produkey.exe /remotefile &#8220;c:tempcomputers.txt&#8221;_\\n\\n_produkey.exe /regfile &#8220;F:WINNTsystem32configsoftware&#8221;_\\n\\n_produkey.exe /windir &#8220;c:winnt&#8221; /shtml &#8220;c:temppk.html&#8221;_\\n\\n_produkey.exe /remoteall_\\n\\n_produkey.exe /remotealldomain MyDomain_\\n\\n_produkey.exe /iprange 192.168.1.10 192.168.1.50_\\n\\n_produkey.exe /stab &#8220;&#8221; >> c:tempprd.txt_\\n\\n_produkey.exe /OfficeKeys 0 /WindowsKeys 1 /shtml f:tempkeys.html_\\n\\n&nbsp;"},{"id":"/2012/09/03/deposit_gw2_collect","metadata":{"permalink":"/2012/09/03/deposit_gw2_collect","source":"@site/blog/2012-09-03-deposit_gw2_collect.md","title":"Deposit all Collectibles in Guild Wars 2 at the same time","description":"Your inventory full of crafting materials & sick of right clicking sending to collection? There is an easy way of doing this.","date":"2012-09-03T08:30:34.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.24,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Deposit all Collectibles in Guild Wars 2 at the same time","date":"2012-09-03T08:30:34.000Z","authors":["Luke"],"tags":["Mac OSX","Windows"]},"unlisted":false,"prevItem":{"title":"Retrieve Windows/Office Keys from Remote PC or External HDD","permalink":"/2012/09/05/retrieve-windoffice-keys-from-remote-pc-or-external-hdd"},"nextItem":{"title":"Cannot open applications downloaded from the Internet","permalink":"/2012/08/31/cannot-open-apps-dwnload-from-the-internet"}},"content":"_Your inventory full of crafting materials & sick of right clicking sending to collection? There is an easy way of doing this._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Go</strong> <strong>to</strong> your <strong>inventory</strong>\xa0<em>(bags)</em>\\n  </li>\\n  <li>\\n    <strong>Click</strong> on the <strong>options</strong>\xa0<em>(top right of the inventory dialog)</em>\\n  </li>\\n  <li>\\n    <strong>Click</strong> <strong>Deposit</strong> All <strong>Collectibles</strong>\\n  </li>\\n</ol>"},{"id":"/2012/08/31/cannot-open-apps-dwnload-from-the-internet","metadata":{"permalink":"/2012/08/31/cannot-open-apps-dwnload-from-the-internet","source":"@site/blog/2012-08-31-cannot-open-apps-dwnload-from-the-internet.md","title":"Cannot open applications downloaded from the Internet","description":"Click Apple logo\xa0(top left hand side)","date":"2012-08-31T20:52:49.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Cannot open applications downloaded from the Internet","date":"2012-08-31T20:52:49.000Z","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Deposit all Collectibles in Guild Wars 2 at the same time","permalink":"/2012/09/03/deposit_gw2_collect"},"nextItem":{"title":"Scrivener Crashing","permalink":"/2012/08/31/scrivener-crashing"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    <strong>Click</strong> <strong>Apple</strong> logo<em>\xa0(top left hand side)</em>\\n  </li>\\n  <li>\\n    Click <strong>System</strong> <strong>Preferences</strong>\\n  </li>\\n  <li>\\n    Click <strong>Security</strong> <strong>&</strong> <strong>Privacy</strong>\\n  </li>\\n  <li>\\n    <strong>Click</strong> the <strong>lock</strong> to allow changes\\n  </li>\\n  <li>\\n    Under the Allow Applications to be downloaded from, <strong>select</strong> <strong>Anywhere</strong>\\n  </li>\\n</ol>"},{"id":"/2012/08/31/scrivener-crashing","metadata":{"permalink":"/2012/08/31/scrivener-crashing","source":"@site/blog/2012-08-31-scrivener-crashing.md","title":"Scrivener Crashing","description":"Attempting to open Scrivener and having the application closing on you, the common way to fix this is adjust the file permissions follow the instructions below to repair.","date":"2012-08-31T10:58:32.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Scrivener Crashing","date":"2012-08-31T10:58:32.000Z","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Cannot open applications downloaded from the Internet","permalink":"/2012/08/31/cannot-open-apps-dwnload-from-the-internet"},"nextItem":{"title":"How to link your Guild Wars 1 account to Guild Wars 2","permalink":"/2012/08/30/link-your-gw1-account-to-gw2"}},"content":"Attempting to open Scrivener and having the application closing on you, the common way to fix this is adjust the file permissions follow the instructions below to repair.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Open</strong> a <strong>Terminal</strong><em>\xa0(Applications/Utilities)\xa0</em>\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0<em>sudo chmod -R 0755 /Applications/Scrivener.app</em>\\n  </li>\\n  <li>\\n    Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    <strong>Open</strong> <strong>Scrivener</strong>\xa0again.\\n  </li>\\n</ol>\\n\\nNote: _This fix has also been able to work on other OSX Applications that have problems launching._"},{"id":"/2012/08/30/link-your-gw1-account-to-gw2","metadata":{"permalink":"/2012/08/30/link-your-gw1-account-to-gw2","source":"@site/blog/2012-08-30-link-your-gw1-account-to-gw2.md","title":"How to link your Guild Wars 1 account to Guild Wars 2","description":"Follow the instructions below to link your Guild Wars 1 account to Guild Wars 2","date":"2012-08-30T19:11:28.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.46,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to link your Guild Wars 1 account to Guild Wars 2","date":"2012-08-30T19:11:28.000Z","authors":["Luke"],"tags":["Mac OSX","Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Scrivener Crashing","permalink":"/2012/08/31/scrivener-crashing"},"nextItem":{"title":"How to leave the Microsoft Security Essentials Customer Experience Program","permalink":"/2012/08/30/leave-msse-exp-prog"}},"content":"Follow the instructions below to link your Guild Wars 1 account to Guild Wars 2\\n\\n&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Go</strong> to Guild Wars 2\xa0<a title=\\"Guild Wars 2 Account\\" href=\\"https://account.guildwars2.com/login\\" target=\\"_blank\\">account page</a>\\n  </li>\\n  <li>\\n    <strong>Login</strong> to your Guild Wars 2 account\\n  </li>\\n  <li>\\n    On the <strong>left</strong> <strong>hand</strong> side click <strong>Link</strong> <strong>Accounts</strong>\\n  </li>\\n  <li>\\n    <strong>Enter</strong> your Guild Wars 1 <strong>details</strong> and click <strong>Ok</strong>\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n_Note: Once\xa0 you have completed the link you should then get an item in your inventory (in Guild Wars 2) that will teleport you to Hall of Monuments._"},{"id":"/2012/08/30/leave-msse-exp-prog","metadata":{"permalink":"/2012/08/30/leave-msse-exp-prog","source":"@site/blog/2012-08-30-leave-msse-exp-prog.md","title":"How to leave the Microsoft Security Essentials Customer Experience Program","description":"Follow the instructions below to leave the Customer\xa0Experience\xa0program for Security Essentials","date":"2012-08-30T16:47:20.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.28,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to leave the Microsoft Security Essentials Customer Experience Program","date":"2012-08-30T16:47:20.000Z","authors":["Luke"],"layout":"post","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to link your Guild Wars 1 account to Guild Wars 2","permalink":"/2012/08/30/link-your-gw1-account-to-gw2"},"nextItem":{"title":"Windows 7 RC to Retail Upgrade","permalink":"/2012/08/30/win7-rc-to-retail-upgrade"}},"content":"Follow the instructions below to leave the Customer\xa0Experience\xa0program for Security Essentials\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Open</strong> Microsoft <strong>Security</strong> <strong>Essentials</strong>\\n  </li>\\n  <li>\\n    <strong>Click</strong> on the low <strong>arrow</strong> (next to the Help on the right hand side)\\n  </li>\\n  <li>\\n    <strong>Click</strong> <strong>Customer</strong>\xa0<strong>Experience</strong>\xa0<strong>Improvement</strong> <strong>Program</strong>\\n  </li>\\n  <li>\\n    <strong>Check</strong> I <strong>don&#8217;t</strong> <strong>want</strong> to <strong>join</strong> the Customer\xa0Experience\xa0Improvement Program\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n</ol>"},{"id":"/2012/08/30/win7-rc-to-retail-upgrade","metadata":{"permalink":"/2012/08/30/win7-rc-to-retail-upgrade","source":"@site/blog/2012-08-30-win7-rc-to-retail-upgrade.md","title":"Windows 7 RC to Retail Upgrade","description":"&nbsp;","date":"2012-08-30T14:59:55.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.385,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows 7 RC to Retail Upgrade","date":"2012-08-30T14:59:55.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to leave the Microsoft Security Essentials Customer Experience Program","permalink":"/2012/08/30/leave-msse-exp-prog"},"nextItem":{"title":"The application failed to initialize properly (0x0000135)","permalink":"/2012/08/30/the-application-failed-to-initialize-properly-0x0000135"}},"content":"&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Extract</strong> the <strong>contents</strong> of your <strong>Windows</strong> <strong>DVD</strong> onto a folder you can easily get to onto the release candidate machine.\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to: <strong>sources</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> click on <strong>cversion.ini</strong> and select <strong>Open</strong> With: <strong>Notepad</strong>.\\n  </li>\\n  <li>\\n    <strong>Search</strong> for: <strong>MinClient</strong> build number and <strong>change</strong> it so it is a <strong>digit</strong> <strong>less</strong> than it <strong>was</strong> <strong>before</strong>.\\n  </li>\\n  <li>\\n    <strong>Save</strong> and <strong>run</strong> the windows <strong>setup</strong> from the folder to do the upgrade.\\n  </li>\\n</ol>"},{"id":"/2012/08/30/the-application-failed-to-initialize-properly-0x0000135","metadata":{"permalink":"/2012/08/30/the-application-failed-to-initialize-properly-0x0000135","source":"@site/blog/2012-08-30-the-application-failed-to-initialize-properly-0x0000135.md","title":"The application failed to initialize properly (0x0000135)","description":"&nbsp;","date":"2012-08-30T14:45:11.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.145,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"The application failed to initialize properly (0x0000135)","date":"2012-08-30T14:45:11.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Windows 7 RC to Retail Upgrade","permalink":"/2012/08/30/win7-rc-to-retail-upgrade"},"nextItem":{"title":"How to change Windows Live Mail Message Store Location","permalink":"/2012/08/30/how-to-change-windows-live-mail-message-store-location"}},"content":"&nbsp;\\n\\nHaving issues opening an application and getting \u201c_0x0000135_\u201d error?\\n\\n  1. The common fix for this is to download the latest <a title=\\"Dot Net Framework\\" href=\\"http://www.microsoft.com/net/download/version-4\\" target=\\"_blank\\">Dot Net Framework</a>"},{"id":"/2012/08/30/how-to-change-windows-live-mail-message-store-location","metadata":{"permalink":"/2012/08/30/how-to-change-windows-live-mail-message-store-location","source":"@site/blog/2012-08-30-how-to-change-windows-live-mail-message-store-location.md","title":"How to change Windows Live Mail Message Store Location","description":"Follow the instructions below to change the Windows Live Mail message store.","date":"2012-08-30T14:20:37.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.465,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to change Windows Live Mail Message Store Location","date":"2012-08-30T14:20:37.000Z","authors":["Luke"],"layout":"post","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"The application failed to initialize properly (0x0000135)","permalink":"/2012/08/30/the-application-failed-to-initialize-properly-0x0000135"},"nextItem":{"title":"Spooler SubSystem App Stopped Working","permalink":"/2012/08/30/spooler-subsystem-app-stopped-working"}},"content":"Follow the instructions below to change the Windows Live Mail message store.\\n\\n&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Open</strong> <strong>Windows</strong> <strong>Live</strong> <strong>Mail</strong>\\n  </li>\\n  <li>\\n    <strong>Click</strong> on the top left button and click <strong>Options</strong>\\n  </li>\\n  <li>\\n    Click <strong>Mail</strong>\\n  </li>\\n  <li>\\n    Click on the <strong>advanced</strong> tab\xa0<em>(to the right of the Options dialog)</em>\\n  </li>\\n  <li>\\n    Click <strong>Maintenance</strong>\\n  </li>\\n  <li>\\n    Click <strong>Store</strong> <strong>Folder</strong>\\n  </li>\\n  <li>\\n    Click <strong>Change</strong>\\n  </li>\\n  <li>\\n    <strong>Browse</strong> for the <strong>location</strong> of your mail store or the place where you want to <strong>change</strong> it to\\n  </li>\\n  <li>\\n    Click Ok\\n  </li>\\n  <li>\\n    <strong>Restart</strong> Windows Live Mail\\n  </li>\\n</ol>"},{"id":"/2012/08/30/spooler-subsystem-app-stopped-working","metadata":{"permalink":"/2012/08/30/spooler-subsystem-app-stopped-working","source":"@site/blog/2012-08-30-spooler-subsystem-app-stopped-working.md","title":"Spooler SubSystem App Stopped Working","description":"Follow the guides below to fix the spooler issues:","date":"2012-08-30T11:57:54.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.765,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Spooler SubSystem App Stopped Working","date":"2012-08-30T11:57:54.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to change Windows Live Mail Message Store Location","permalink":"/2012/08/30/how-to-change-windows-live-mail-message-store-location"},"nextItem":{"title":"SmartSound Quicktracks Plugin Issues","permalink":"/2012/08/28/smartsound-quicktracks-plugin"}},"content":"Follow the guides below to fix the spooler issues:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click<strong> My Computer/Computer</strong>\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to: C:WindowsSystem32spool<strong>Printers</strong>\\n  </li>\\n  <li>\\n    <strong>Delete</strong> <strong>everything</strong> in this folder.\\n  </li>\\n  <li>\\n    <strong>Restart</strong> the computer and attempt printing again.\\n  </li>\\n</ol>\\n\\n_If that does not work, attempt the following:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Run</strong>\\n  </li>\\n  <li>\\n    Type: <strong>regedit</strong> > Press Enter\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to: <strong>HKEY_LOCAL_MACHINESYSTEMCONTROLSET001CONTROLPRINTENVIRONMENTSWINDOWSNTx86DRIVERSVERSION-3</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> click and <strong>delete</strong> the printer <strong>registry</strong> <strong>keys</strong>\\n  </li>\\n  <li>\\n    <strong>Restart</strong> the computer.\\n  </li>\\n</ol>\\n\\n_If that does not work, attempt the following:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Control</strong> <strong>Panel</strong>\\n  </li>\\n  <li>\\n    Click <strong>Add/Remove Programs</strong> or Uninstall a Program\\n  </li>\\n  <li>\\n    Click Turn<strong> Windows Features On/Off</strong>\xa0<em>(left hand side menu)</em>\\n  </li>\\n  <li>\\n    Navigate down to <strong>Print Services</strong>\\n  </li>\\n  <li>\\n    Make sure<strong> LPD</strong> and <strong>LPR</strong> are <strong>ticked</strong>, if not tick them.\\n  </li>\\n  <li>\\n    <strong>Restart</strong> your computer.\\n  </li>\\n</ol>\\n\\n&nbsp;"},{"id":"/2012/08/28/smartsound-quicktracks-plugin","metadata":{"permalink":"/2012/08/28/smartsound-quicktracks-plugin","source":"@site/blog/2012-08-28-smartsound-quicktracks-plugin.md","title":"SmartSound Quicktracks Plugin Issues","description":"This issue commonly occurs on DVD Flick installations but also known for other problems that deal with video codecs. Follow the steps below to repair:","date":"2012-08-28T12:06:34.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.52,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"SmartSound Quicktracks Plugin Issues","date":"2012-08-28T12:06:34.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Spooler SubSystem App Stopped Working","permalink":"/2012/08/30/spooler-subsystem-app-stopped-working"},"nextItem":{"title":"How to set-up your own Action bar in Diablo 3","permalink":"/2012/08/26/how-to-set-up-your-own-action-bar-in-diablo-3"}},"content":"This issue commonly occurs on DVD Flick installations but also known for other problems that deal with video codecs. Follow the steps below to repair:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Download</strong> the \u201c<a title=\\"Windows Installer Cleanup Utility\\" href=\\"http://www.softpedia.com/get/Security/Secure-cleaning/Windows-Installer-CleanUp-Utility.shtml\\" target=\\"_blank\\">Windows Installer Cleanup Utility</a>\u201d\\n  </li>\\n  <li>\\n    <strong>Install</strong> the Windows Installer Cleanup Utility\\n  </li>\\n  <li>\\n    Once, installed <strong>run</strong> the cleanup utility and <strong>select</strong> the <strong>Smartsound</strong> <strong>Quicktracks</strong> plugin listed.\\n  </li>\\n  <li>\\n    <strong>Remove</strong> the plugin and click <strong>ok</strong>\\n  </li>\\n  <li>\\n    <strong>Restart</strong> your computer and attempt your installation again.\\n  </li>\\n</ol>\\n\\n_Note: In some cases you can cancel or skip the error that is complaining about Quicktracks plugin and continue the installation._"},{"id":"/2012/08/26/how-to-set-up-your-own-action-bar-in-diablo-3","metadata":{"permalink":"/2012/08/26/how-to-set-up-your-own-action-bar-in-diablo-3","source":"@site/blog/2012-08-26-how-to-set-up-your-own-action-bar-in-diablo-3.md","title":"How to set-up your own Action bar in Diablo 3","description":"Playing Diablo 3 and want to use abilities that are on the same tree and change the ability locations? Here is how you can do it.","date":"2012-08-26T19:20:18.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.355,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to set-up your own Action bar in Diablo 3","date":"2012-08-26T19:20:18.000Z","authors":["Luke"],"tags":["Mac OSX","Misc","Windows"]},"unlisted":false,"prevItem":{"title":"SmartSound Quicktracks Plugin Issues","permalink":"/2012/08/28/smartsound-quicktracks-plugin"},"nextItem":{"title":"Spotify Crashes Randomly","permalink":"/2012/08/24/spotify-crashes-randomly"}},"content":"_Playing Diablo 3 and want to use abilities that are on the same tree and change the ability locations? Here is how you can do it._\\n\\n  1. Click on **Options**\\n  2. Click **Gameplay**\\n  3. Check **Elective Mode**\\n  4. Click **Ok**\\n  5. Now right click the skill that you would like to change and using the side arrows you can select the skill you would like to add to that slow.\\n\\n&nbsp;"},{"id":"/2012/08/24/spotify-crashes-randomly","metadata":{"permalink":"/2012/08/24/spotify-crashes-randomly","source":"@site/blog/2012-08-24-spotify-crashes-randomly.md","title":"Spotify Crashes Randomly","description":"Having issues with Spotify? Few minutes or seconds after opening it crashes, follow the prompts below to repair:","date":"2012-08-24T19:53:23.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.43,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Spotify Crashes Randomly","date":"2012-08-24T19:53:23.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to set-up your own Action bar in Diablo 3","permalink":"/2012/08/26/how-to-set-up-your-own-action-bar-in-diablo-3"},"nextItem":{"title":"Internet Explorer & Firefox Webpage display issues","permalink":"/2012/08/24/internet-explorer-firefox-webpage-display-issues"}},"content":"_Having issues with Spotify? Few minutes or seconds after opening it crashes, follow the prompts below to repair:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Make sure <strong>Spotify</strong> is <strong>closed</strong><em>\xa0(there is no icon in the notification tray by the time)</em>.\\n  </li>\\n  <li>\\n    Click &#8220;<a title=\\"Facebook Applications\\" href=\\"https://www.facebook.com/appcenter/my\\" target=\\"_blank\\">here</a>&#8221; to go to Facebook&#8217;s Application\\n  </li>\\n  <li>\\n    <strong>Remove</strong> <strong>Spotify</strong>, by clicking the small <strong>x</strong><em>\xa0(you might have to do it twice).</em>\\n  </li>\\n  <li>\\n    <strong>Relaunch</strong> Spotify<em>\xa0(it should now work)</em>\\n  </li>\\n</ol>\\n\\n_Note: If Spotify doesn&#8217;t work reinstall\xa0<a title=\\"Adobe Flash Player\\" href=\\"http://get.adobe.com/flashplayer/\\" target=\\"_blank\\">Adobe Flash Player</a>_"},{"id":"/2012/08/24/internet-explorer-firefox-webpage-display-issues","metadata":{"permalink":"/2012/08/24/internet-explorer-firefox-webpage-display-issues","source":"@site/blog/2012-08-24-internet-explorer-firefox-webpage-display-issues.md","title":"Internet Explorer & Firefox Webpage display issues","description":"Missing buttons and getting weird colours in both Internet Explorer & Firefox? Try the following:","date":"2012-08-24T13:59:53.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.24,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Internet Explorer & Firefox Webpage display issues","date":"2012-08-24T13:59:53.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Spotify Crashes Randomly","permalink":"/2012/08/24/spotify-crashes-randomly"},"nextItem":{"title":"Blank Device Manager in Windows XP","permalink":"/2012/08/24/blank-device-manager-in-windows-xp"}},"content":"Missing buttons and getting weird colours in both Internet Explorer & Firefox? Try the following:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>All Programs/Programs</strong>\\n  </li>\\n  <li>\\n    Click <strong>Accessibility</strong>\\n  </li>\\n  <li>\\n    Click on the <strong>Display</strong> <strong>Settings</strong>\\n  </li>\\n  <li>\\n    Look for <strong>High Contrast</strong> settings and <strong>disabled</strong> them.\\n  </li>\\n</ol>"},{"id":"/2012/08/24/blank-device-manager-in-windows-xp","metadata":{"permalink":"/2012/08/24/blank-device-manager-in-windows-xp","source":"@site/blog/2012-08-24-blank-device-manager-in-windows-xp.md","title":"Blank Device Manager in Windows XP","description":"The issue is caused by security permissions in the registry. This is how you fix it.","date":"2012-08-24T13:41:18.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.835,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Blank Device Manager in Windows XP","date":"2012-08-24T13:41:18.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Internet Explorer & Firefox Webpage display issues","permalink":"/2012/08/24/internet-explorer-firefox-webpage-display-issues"},"nextItem":{"title":"How to fix Telecom T-Stick/Vodafone Vodem Initializing issues","permalink":"/2012/08/22/telecom-t-stickvodafone-vodem-initializing-issues"}},"content":"The issue is caused by security permissions in the registry. This is how you fix it.\\n\\n_You must be logged on as a local administrator to perform this task:_\\n\\n  1. Click &#8220;**Start**&#8220;, &#8220;**Run**&#8220;, and enter &#8220;**regedt32**&#8220;\\n  2. **Maximize**\xa0the &#8220;**HKEY\\\\_LOCAL\\\\_MACHINE**&#8221; window.\\n  3. Scroll down to\xa0**&#8220;SYSTEMCurrentControlSetEnum**&#8220;\\n  4. With &#8220;**Enum**&#8221; selected click the &#8220;**Security**&#8220;, &#8220;**Permissions**&#8221; menu.\\n  5. Click &#8220;**Add**&#8220;.\\n  6. Add the group &#8220;**Everyone**&#8221; and the user &#8220;**SYSTEM**&#8220;.\\n  7. Select &#8220;**Everyone**&#8221; and check &#8220;**Read \xa0ONLY**&#8220;\\n  8. Select &#8220;**SYSTEM**&#8221; and check\xa0**&#8220;Full Control**&#8220;\\n  9. Click the &#8220;**Advanced**&#8221; button at the bottom of the window.\\n 10. \xa0On the Advanced window check &#8220;**Reset permissions on all child objects&#8230;**&#8220;\\n 11. Click &#8220;**OK**&#8220;\\n 12. On the warning dialog click &#8220;**Yes**&#8220;\\n 13. Close the registry editor\\n\\nThis problem can also be caused when the Plug & Play service is not running. Follow these options below.\\n\\n1. Click\xa0**Start**\xa0then click\xa0**Run**\xa0type\xa0**services.msc\xa0**then click Ok.\\n  \\n2.\xa0**Double-click Plug and Play**.\\n\\n_If you receive a Configuration Manager message then click Ok_\\n  \\n3. In the\xa0**Startup Type**\xa0list, click**\xa0Automatic**\xa0& then click\xa0**OK.**\\n  \\n4. Close Services.\\n  \\n5.\xa0**Restart the computer**."},{"id":"/2012/08/22/telecom-t-stickvodafone-vodem-initializing-issues","metadata":{"permalink":"/2012/08/22/telecom-t-stickvodafone-vodem-initializing-issues","source":"@site/blog/2012-08-22-telecom-t-stickvodafone-vodem-initializing-issues.md","title":"How to fix Telecom T-Stick/Vodafone Vodem Initializing issues","description":"Make sure you have credit on your Vodem/T-Stick. This is the common cause of why the modems will not load webpages.","date":"2012-08-22T10:29:18.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.065,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to fix Telecom T-Stick/Vodafone Vodem Initializing issues","date":"2012-08-22T10:29:18.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Blank Device Manager in Windows XP","permalink":"/2012/08/24/blank-device-manager-in-windows-xp"},"nextItem":{"title":"How to run Diablo 2 in Windows 7","permalink":"/2012/08/22/how-to-run-diablo-2-in-windows-7"}},"content":"_Make sure you have credit on your Vodem/T-Stick. This is the common cause of why the modems will not load webpages._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Disconnect</strong> the <strong>T-Stick/Vodem</strong>\\n  </li>\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Programs/All Programs</strong>\\n  </li>\\n  <li>\\n    Click <strong>Accessories</strong>\\n  </li>\\n  <li>\\n    Click <strong>Command Prompt</strong>\\n  </li>\\n  <li>\\n    Type:\xa0 <strong><em>set devmgr_show_nonpresent_devices=1</em></strong>\xa0<em>(this will reveal hidden device drivers in device manager)\xa0</em>> Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    Now we need to load Device Manager, click <strong>Start</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> <strong>Computer/My Computer</strong>\\n  </li>\\n  <li>\\n    Click the <strong>hardware</strong> tab\\n  </li>\\n  <li>\\n    Click <strong>Device Manager</strong>\\n  </li>\\n  <li>\\n    Click <strong>View</strong>\\n  </li>\\n  <li>\\n    Click <strong>Show Hidden Devices</strong>\\n  </li>\\n  <li>\\n    Device Manager will now display a list of devices that look transparent\\n  </li>\\n  <li>\\n    Go through the list, clicking \u201c<strong>+</strong>\u201d next to each one, mainly in the <strong>Network Adapters</strong> and <strong>Universal Serial Bus controllers</strong> sub-section.\\n  </li>\\n  <li>\\n    <strong>Look</strong> for anything <strong>related</strong> to <strong>ZTE/Vodafone/Telecom T-Stick</strong>\\n  </li>\\n  <li>\\n    Now <strong>right</strong> <strong>click</strong> the <strong>devices</strong> that indicate they are for the device and select <strong>Uninstall</strong>, click Ok to <strong>confirm</strong>\\n  </li>\\n  <li>\\n    Once you have removed the devices relating to the mobile broadband device, <strong>close</strong> <strong>Device</strong> <strong>Manager</strong>.\\n  </li>\\n  <li>\\n    <strong>Reconnect</strong> the <strong>T-Stick/Vodem</strong> and your computer should recognise it as a new device and install. It should now be up and running. \xa0If not repeat step 14.\\n  </li>\\n</ol>"},{"id":"/2012/08/22/how-to-run-diablo-2-in-windows-7","metadata":{"permalink":"/2012/08/22/how-to-run-diablo-2-in-windows-7","source":"@site/blog/2012-08-22-how-to-run-diablo-2-in-windows-7.md","title":"How to run Diablo 2 in Windows 7","description":"&nbsp;","date":"2012-08-22T09:54:48.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.615,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to run Diablo 2 in Windows 7","date":"2012-08-22T09:54:48.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to fix Telecom T-Stick/Vodafone Vodem Initializing issues","permalink":"/2012/08/22/telecom-t-stickvodafone-vodem-initializing-issues"},"nextItem":{"title":"Set the Macbook Keyboard Back-light to be on permanently","permalink":"/2012/08/21/macbook-backlight-to-be-on"}},"content":"&nbsp;\\n\\n&nbsp;\\n\\n_First things first, make sure you have Diablo 2 installed._\\n\\n_\xa0_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> the Diablo 2 <strong>icon</strong>\\n  </li>\\n  <li>\\n    Left click <strong>Properties</strong>\\n  </li>\\n  <li>\\n    Click <strong>Compatibility</strong>\\n  </li>\\n  <li>\\n    <strong>Check</strong> the checkbox \u201c<strong><em>Run this program in compatibility for</em></strong>\u201d and <strong>select</strong>: <strong><em>Windows XP (Service Pack 3)</em></strong>\\n  </li>\\n  <li>\\n    <strong>Check</strong> run in <strong><em>256 colors</em></strong>\\n  </li>\\n  <li>\\n    <strong>Check</strong> <strong><em>Disable Desktop Composition</em></strong>\\n  </li>\\n  <li>\\n    <strong>Check</strong> <strong><em>Run this Program as Administrator</em></strong>\xa0<em>(this will by-pass any User Account Control issues)</em>\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    Now <strong>open</strong> <strong>Diablo</strong> 2\xa0<em>(it should now run)</em>\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n_Note: It has also been reported that setting up and adding your Diablo 2 keys to Battle.net will allow you to download a client from Blizzard that will run fine._"},{"id":"/2012/08/21/macbook-backlight-to-be-on","metadata":{"permalink":"/2012/08/21/macbook-backlight-to-be-on","source":"@site/blog/2012-08-21-macbook-backlight-to-be-on.md","title":"Set the Macbook Keyboard Back-light to be on permanently","description":"Open Finder","date":"2012-08-21T11:56:34.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.4,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Set the Macbook Keyboard Back-light to be on permanently","date":"2012-08-21T11:56:34.000Z","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to run Diablo 2 in Windows 7","permalink":"/2012/08/22/how-to-run-diablo-2-in-windows-7"},"nextItem":{"title":"Reset PRAM/NVRAM","permalink":"/2012/08/21/reset-pram-nvram"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    <strong>Open</strong> <strong>Finder</strong>\\n  </li>\\n  <li>\\n    <strong>Navigate</strong> to: <strong><em>Users/YourUser/NameHere/Library/Preferences</em></strong>\\n  </li>\\n  <li>\\n    <em>Warning: Be mindful of changing the wrong file or touching incorrect settings in this folder. This could go wrong!</em>\\n  </li>\\n  <li>\\n    <strong>Open<em>: com.apple.bezelservice.plist</em></strong>\\n  </li>\\n  <li>\\n    <strong>Change</strong> <strong><em>kHWMin</em></strong> to have a value of: <strong>1</strong>\\n  </li>\\n  <li>\\n    Close and <strong>save</strong>\\n  </li>\\n</ol>\\n\\n_You have now adjusted the minimum light the light sensor needs to activate the\xa0back-light\xa0on the keyboard.\xa0_\\n\\n_Note: Only valid for Macbook Pros, normal Macbooks do not contain keyboard\xa0back-lights._"},{"id":"/2012/08/21/reset-pram-nvram","metadata":{"permalink":"/2012/08/21/reset-pram-nvram","source":"@site/blog/2012-08-21-reset-pram-nvram.md","title":"Reset PRAM/NVRAM","description":"Warning: Resetting the PRAM/NVRAM will cause the Mac to lose operating system settings such as Time and Volume settings.","date":"2012-08-21T11:30:28.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.53,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Reset PRAM/NVRAM","date":"2012-08-21T11:30:28.000Z","authors":["Luke"],"tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Set the Macbook Keyboard Back-light to be on permanently","permalink":"/2012/08/21/macbook-backlight-to-be-on"},"nextItem":{"title":"How to get emailed hard drive status reports","permalink":"/2012/08/20/how-to-get-emailed-hard-drive-status-reports"}},"content":"_Warning: Resetting the PRAM/NVRAM will cause the Mac to lose operating system settings such as Time and Volume settings._\\n\\n_Note: Used successfully to fix Touchpad not \u201cclicking\u201d issue on a Macbook Pro_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Make sure the <strong>Mac</strong> is turned <strong>off</strong>\\n  </li>\\n  <li>\\n    <strong>Press</strong> <strong><em>Command+Option+P+R</em></strong> and <strong>hold</strong> <strong>down</strong> the keys\\n  </li>\\n  <li>\\n    <strong>Press</strong> the <strong>Power</strong> Button on the Mac\xa0<strong><em>(while still pressing the keys down)</em></strong>\\n  </li>\\n  <li>\\n    The Mac will then flash and <strong>restart</strong>, <strong>release</strong> the keys once you have reached the <strong>second</strong> <strong>start-up noise</strong>.\\n  </li>\\n  <li>\\n    You have now reset the settings and your Mac should <strong>continue</strong> <strong>booting</strong>.\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n&nbsp;\\n\\n**\xa0**\\n\\n**\xa0**\\n\\n&nbsp;"},{"id":"/2012/08/20/how-to-get-emailed-hard-drive-status-reports","metadata":{"permalink":"/2012/08/20/how-to-get-emailed-hard-drive-status-reports","source":"@site/blog/2012-08-20-how-to-get-emailed-hard-drive-status-reports.md","title":"How to get emailed hard drive status reports","description":"Want to get emailed disk status reports of your drives/hard drives and you are using a Windows PC? Using a great tool developed by Acronis you can. Acronis Drive Monitor runs down in the notification tray (by the time) and monitors your hard drive(s) temperature/spin time and sector failure rate giving you a good indication of when/if they are going to fail before they do. If you are away from the computer or simply would rather get status reports emailed to you, this is how you do it.","date":"2012-08-20T16:16:36.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.905,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to get emailed hard drive status reports","date":"2012-08-20T16:16:36.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Reset PRAM/NVRAM","permalink":"/2012/08/21/reset-pram-nvram"},"nextItem":{"title":"Raspberry Pi Media Centre","permalink":"/2012/08/20/raspberry-pi-media-centre"}},"content":"_Want to get emailed disk status reports of your drives/hard drives and you are using a Windows PC? Using a great tool developed by Acronis you can. Acronis Drive Monitor runs down in the notification tray (by the time) and monitors your hard drive(s) temperature/spin time and sector failure rate giving you a good indication of when/if they are going to fail before they do. If you are away from the computer or simply would rather get status reports emailed to you, this is how you do it._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First things first download <a title=\\"Acronis Drive Monitor\\" href=\\"http://www.acronis.com/homecomputing/products/drive-monitor/#overview\\" target=\\"_blank\\">Acronis Drive Monitor</a>\\n  </li>\\n  <li>\\n    <strong>Install</strong> <strong><em>Acronis Drive Monitor</em></strong>\\n  </li>\\n  <li>\\n    Down by the <strong>notification</strong> <strong>tray</strong> you will see a little <strong>icon</strong> of a hard drive, <strong>double click it</strong>.\\n  </li>\\n  <li>\\n    To view the status of your disks click \u201cShow Disks\u201d you will get an overview on the drive(s) health, clicking on S.M.A.R.T parameters gives you more of an advanced look.\\n  </li>\\n  <li>\\n    Click <strong>Options</strong>\xa0<em>(up the top of Acronis Disk Monitor)</em>\\n  </li>\\n  <li>\\n    On the <strong>Alerts</strong> tab, click \u201c<strong><em>change settings</em></strong>\u201d\\n  </li>\\n  <li>\\n    <strong>Type</strong> in the email <strong>address</strong> you would like them <strong>sent</strong> <strong>to</strong> you can have multiple addresses by separated them with a semicolon.\\n  </li>\\n  <li>\\n    <strong>Type</strong> in the <strong>from</strong> field who you want them <strong>sent from </strong>(<em>ie Home Computer)</em>\\n  </li>\\n  <li>\\n    For the <strong>outgoing</strong> <strong>mail</strong> <strong>server</strong> <strong>settings</strong> you have to type the outgoing mail settings for the email account you wish to use to send from, the easiest way is to check your mail client for the ones used there and copy them in.\\n  </li>\\n  <li>\\n    Click Sent <strong>Test</strong> <strong>Message</strong> to email a test email to the email address you specified above. Once it has been setup click <strong>Ok</strong>.\\n  </li>\\n  <li>\\n    Now you can select to be either emailed about Critical Events\xa0<em>(which stops email spamming in your inbox)</em>\xa0or you can either have regular disk status reports sent\xa0<em>(for a Windows server it is probably better for this to be selected so then you can go back and have a look at drive status if needed.</em>\\n  </li>\\n  <li>\\n    Congratulations you have just set up your computer to email hard drive alerts.\\n  </li>\\n</ol>\\n\\n_Note: One of the adjustments I make for my home computer, is to disable Backup alerts (under Backup Monitoring)_"},{"id":"/2012/08/20/raspberry-pi-media-centre","metadata":{"permalink":"/2012/08/20/raspberry-pi-media-centre","source":"@site/blog/2012-08-20-raspberry-pi-media-centre.md","title":"Raspberry Pi Media Centre","description":"First off, you need a Raspberry Pi and an SD Card/Reader","date":"2012-08-20T15:48:34.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.26,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Raspberry Pi Media Centre","date":"2012-08-20T15:48:34.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to get emailed hard drive status reports","permalink":"/2012/08/20/how-to-get-emailed-hard-drive-status-reports"},"nextItem":{"title":"Windows Vista continuous restarts installing updates","permalink":"/2012/08/20/win-vista-restarts-installing-updates"}},"content":"_First off, you need a Raspberry Pi and an SD Card/Reader_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Once obtained <strong>download</strong>\xa0<a title=\\"Xbian Raspberry Pi\\" href=\\"http://xbian.org/\\" target=\\"_blank\\">Xbian</a>\\n  </li>\\n  <li>\\n    Once downloaded, make sure that the SD Card is blank or is safe to format and plugged in.\\n  </li>\\n  <li>\\n    <strong>Extract</strong> <strong>Xbian</strong> to a folder on your computer\\n  </li>\\n  <li>\\n    <strong>Run</strong> the <strong><em>Win32 Extractor</em></strong> and make sure you <strong>select</strong> the <strong>SD-Card</strong> as the correct <strong>destination</strong>.\\n  </li>\\n  <li>\\n    Once selected press <strong>Run</strong>\\n  </li>\\n  <li>\\n    The tool will then <strong>format</strong>\xa0<em>(erase)</em>\xa0everything on the <strong>SD-Card</strong> and <strong>overlay</strong> the <strong>Xbian</strong> software onto the card and make it bootable, it generally takes <em>1-2 minutes</em> to complete.\\n  </li>\\n  <li>\\n    Once completed make sure that nothing is writing to the SD-Card or has it open and take the card out.\\n  </li>\\n  <li>\\n    <strong>Put</strong> the <strong>SD-Card</strong> into the <strong>Raspberry</strong> <strong>Pi</strong>\xa0<em>(the slot is underneath the Pi)</em>\\n  </li>\\n  <li>\\n    Plug the Pi into power and <strong>attach</strong> it up to <strong><em>a TV/Monitor</em></strong> using HDMI and let it <strong>boot</strong> up <strong>Xbian</strong>.\\n  </li>\\n  <li>\\n    As easy as that, you have now a fully functionally media centre.\\n  </li>\\n</ol>\\n\\n&nbsp;\\n\\n_Note: Xbian automatically boots NTFS/FAT32 External HDDs, however make sure that the external HDD has its own power source the Pi doesn\u2019t have enough juice to pump through to drives that only have one USB cable for both power and data._\\n\\n_Note: To add Videos, click Add Videos, Add Files, Add Source, select the folder that contains the video files it is entirely normal for no videos to appear in this dialog. \xa0_"},{"id":"/2012/08/20/win-vista-restarts-installing-updates","metadata":{"permalink":"/2012/08/20/win-vista-restarts-installing-updates","source":"@site/blog/2012-08-20-win-vista-restarts-installing-updates.md","title":"Windows Vista continuous restarts installing updates","description":"One of the common causes of Windows Vista problems is due to Windows Updates; especially when it forces the computer to continuously restart follow the prompts below to repair.","date":"2012-08-20T15:27:36.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.96,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Vista continuous restarts installing updates","date":"2012-08-20T15:27:36.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Raspberry Pi Media Centre","permalink":"/2012/08/20/raspberry-pi-media-centre"},"nextItem":{"title":"Characters deleting in front while typing","permalink":"/2012/08/20/characters-deleting-in-front-while-typing"}},"content":"_One of the common causes of Windows Vista problems is due to Windows Updates; especially when it forces the computer to continuously restart follow the prompts below to repair._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    First off, you need a Windows Vista DVD or related CD/DVD you can access the files on the Windows installed partition.\\n  </li>\\n  <li>\\n    Using the Windows Vista DVD as an example you need to <strong>Boot</strong> from the <strong>DVD</strong>\\n  </li>\\n  <li>\\n    Once the Windows Vista DVD displays the first dialog Window giving you the options to install click <strong>Repair</strong> <strong>My</strong> <strong>Computer</strong>\xa0<em>(below the Install).</em>\\n  </li>\\n  <li>\\n    <strong>Select</strong> the Windows Vista <strong>installation</strong> you would <strong>like</strong> <strong>to</strong> <strong>edit</strong>\xa0<em>(you would usually only have one).</em>\\n  </li>\\n  <li>\\n    You should now have the Windows System Recovery dialog window, click <strong>Command Prompt</strong>.\\n  </li>\\n  <li>\\n    Once you are greeted with a blinking cursor <strong>type<em>:\xa0del c:WindowsSoftwareDistribution</em></strong><em>\xa0</em>> Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    This will delete the folder that contains all the temp files for Windows Updates\xa0<em>(it will regenerate itself, so safe to delete)</em>.\\n  </li>\\n  <li>\\n    Now <strong>type</strong>:\xa0<strong>cd Windows</strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0<strong><em>cd winsxs</em></strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong>:\xa0<strong><em>del pending.xml</em>\xa0</strong>> Press <strong>Enter</strong>\\n  </li>\\n  <li>\\n    Now <strong>restart</strong> your computer, it should now load Windows properly.\\n  </li>\\n</ol>"},{"id":"/2012/08/20/characters-deleting-in-front-while-typing","metadata":{"permalink":"/2012/08/20/characters-deleting-in-front-while-typing","source":"@site/blog/2012-08-20-characters-deleting-in-front-while-typing.md","title":"Characters deleting in front while typing","description":"Trying to type up a document or statement and the characters keep writing over ones in front? The fix for this is simple:","date":"2012-08-20T14:52:05.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.225,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Characters deleting in front while typing","date":"2012-08-20T14:52:05.000Z","authors":["Luke"],"tags":["Mac OSX","Windows"]},"unlisted":false,"prevItem":{"title":"Windows Vista continuous restarts installing updates","permalink":"/2012/08/20/win-vista-restarts-installing-updates"},"nextItem":{"title":"Vodafone NZ APN Settings","permalink":"/2012/08/20/vodafone-nz-apn-settings"}},"content":"_Trying to type up a document or statement and the characters keep writing over ones in front? The fix for this is simple:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    On your keyboard, press the <strong>Insert</strong> <strong>Key</strong>\\n  </li>\\n</ol>\\n\\n_Now start typing again, you should be good to go!_"},{"id":"/2012/08/20/vodafone-nz-apn-settings","metadata":{"permalink":"/2012/08/20/vodafone-nz-apn-settings","source":"@site/blog/2012-08-20-vodafone-nz-apn-settings.md","title":"Vodafone NZ APN Settings","description":"For use setting up Phones, inc iPhones to the Vodafone network","date":"2012-08-20T13:24:47.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.085,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Vodafone NZ APN Settings","date":"2012-08-20T13:24:47.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Characters deleting in front while typing","permalink":"/2012/08/20/characters-deleting-in-front-while-typing"},"nextItem":{"title":"Turning On/Off Windows 7 Accessibility Options","permalink":"/2012/08/15/turning-onoff-windows-7-accessibility-options"}},"content":"_For use setting up Phones, inc iPhones to the Vodafone network_\\n\\n**APN**: live.vodafone.com\\n\\n**MMSC**: \\n\\n**MMS Proxy**: 172.030.038.003:8080"},{"id":"/2012/08/15/turning-onoff-windows-7-accessibility-options","metadata":{"permalink":"/2012/08/15/turning-onoff-windows-7-accessibility-options","source":"@site/blog/2012-08-15-turning-onoff-windows-7-accessibility-options.md","title":"Turning On/Off Windows 7 Accessibility Options","description":"If your like me and get annoyed with the virtual keyboard popping up out of nowhere follow the instructions below, or maybe you just want to enable them for a better computer\xa0experience.","date":"2012-08-15T08:20:22.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.635,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Turning On/Off Windows 7 Accessibility Options","date":"2012-08-15T08:20:22.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Vodafone NZ APN Settings","permalink":"/2012/08/20/vodafone-nz-apn-settings"},"nextItem":{"title":"Windows Live Mail/Windows Calendar 0x8E5E0247","permalink":"/2012/08/14/windows-live-mailwindows-calendar-0x8e5e0247"}},"content":"If your like me and get annoyed with the virtual keyboard popping up out of nowhere follow the instructions below, or maybe you just want to enable them for a better computer\xa0experience.\\n\\n**Digital Keyboard**\\n\\n  1. Click **Start**\\n  2. Click **Control** **Panel**\\n  3. Click **Ease** **of** **Access**\\n  4. Go into the **Ease of Access\xa0Centre**\\n  5. Choose Use the computer without a mouse or keyboard.\\n  6. **Check** or **uncheck** the box for **Use On-Screen Keyboard**.\\n  7. Click **Save**\\n\\n**How to turn off Narator Windows 7**\\n\\n  1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click **Ease of Access**\\n  4. Go into the **Ease of Access Center**\\n  5. Select **Explore all Settings**\\n  6. Choose **Use the computer without a display**\\n  7. **Uncheck** the checkbox by **Turn on Narrator** and click **Save**"},{"id":"/2012/08/14/windows-live-mailwindows-calendar-0x8e5e0247","metadata":{"permalink":"/2012/08/14/windows-live-mailwindows-calendar-0x8e5e0247","source":"@site/blog/2012-08-14-windows-live-mailwindows-calendar-0x8e5e0247.md","title":"Windows Live Mail/Windows Calendar 0x8E5E0247","description":"Click this \u201cIntel Rapid Storage Technology Driver\u201d","date":"2012-08-14T12:18:06.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.265,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Live Mail/Windows Calendar 0x8E5E0247","date":"2012-08-14T12:18:06.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Turning On/Off Windows 7 Accessibility Options","permalink":"/2012/08/15/turning-onoff-windows-7-accessibility-options"},"nextItem":{"title":"Limit the bandwidth speed of Dropbox","permalink":"/2012/08/14/bandwidth-speed-of-dropbox"}},"content":"<ol start=\\"1\\">\\n  <li>\\n    <strong>Click</strong> this \u201c<a title=\\"Intel Storage\\" href=\\"http://downloadcenter.intel.com/SearchResult.aspx?lang=eng&ProductFamily=Software+Products&ProductLine=Chipset+Software&ProductProduct=Intel%C2%AE+Rapid+Storage+Technology+(Intel%C2%AE+RST)\\" target=\\"_blank\\">Intel Rapid Storage Technology Driver</a>\u201d\\n  </li>\\n  <li>\\n    <strong>Locate</strong> the <strong><em>Intel Matrix Storage Manager</em></strong> for your operating system\\n  </li>\\n  <li>\\n    Download the file to an easily accessible location\\n  </li>\\n  <li>\\n    <strong>Open</strong> and <strong>install</strong> the downloaded file.\\n  </li>\\n  <li>\\n    Once installed <strong>restart</strong> your computer\\n  </li>\\n</ol>"},{"id":"/2012/08/14/bandwidth-speed-of-dropbox","metadata":{"permalink":"/2012/08/14/bandwidth-speed-of-dropbox","source":"@site/blog/2012-08-14-bandwidth-speed-of-dropbox.md","title":"Limit the bandwidth speed of Dropbox","description":"This is how to limit the bandwidth speed of both your Uploads & Downloads for Dropbox","date":"2012-08-14T11:47:20.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.365,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Limit the bandwidth speed of Dropbox","date":"2012-08-14T11:47:20.000Z","authors":["Luke"],"layout":"post","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Windows Live Mail/Windows Calendar 0x8E5E0247","permalink":"/2012/08/14/windows-live-mailwindows-calendar-0x8e5e0247"},"nextItem":{"title":"Openoffice \u2013 There is nothing to print.","permalink":"/2012/08/14/openoffice-there-is-nothing-to-print"}},"content":"_This is how to limit the bandwidth speed of both your Uploads & Downloads for Dropbox_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> the <strong>Dropbox</strong> icon\xa0<em>(down by your time)</em>\\n  </li>\\n  <li>\\n    Click <strong>Preferences</strong>\\n  </li>\\n  <li>\\n    Click on the <strong>Bandwidth</strong> <strong>Tab</strong>\xa0<em>(has a picture of a speedometer)</em>\\n  </li>\\n  <li>\\n    Here you can <strong>adjust</strong> or disable the <strong>bandwidth</strong> limiting of Dropbox\u2019s Uploads & Downloads\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong> when changes are made to confirm your settings.\\n  </li>\\n</ol>"},{"id":"/2012/08/14/openoffice-there-is-nothing-to-print","metadata":{"permalink":"/2012/08/14/openoffice-there-is-nothing-to-print","source":"@site/blog/2012-08-14-openoffice-there-is-nothing-to-print.md","title":"Openoffice \u2013 There is nothing to print.","description":"This issue occurs when the print range for the document has inadvertently changed. Follow the guides below to fix:","date":"2012-08-14T11:39:48.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.295,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Openoffice \u2013 There is nothing to print.","date":"2012-08-14T11:39:48.000Z","authors":["Luke"],"tags":["Mac OSX","Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Limit the bandwidth speed of Dropbox","permalink":"/2012/08/14/bandwidth-speed-of-dropbox"},"nextItem":{"title":"How to remove the Clock in Windows 7","permalink":"/2012/08/06/rmclock-in-win7"}},"content":"This issue occurs when the print range for the document has inadvertently changed. Follow the guides below to fix:\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Format</strong>\xa0<em>(menu at the top)</em>\\n  </li>\\n  <li>\\n    Click <strong>Print</strong> <strong>Ranges</strong>\\n  </li>\\n  <li>\\n    Click <strong>Edit</strong>\\n  </li>\\n  <li>\\n    <strong>Change</strong> the Print <strong>Ranges</strong> to \u201c<strong><em>None</em></strong>\u201d\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n  <li>\\n    You should now be able to <strong>print</strong>.\\n  </li>\\n</ol>"},{"id":"/2012/08/06/rmclock-in-win7","metadata":{"permalink":"/2012/08/06/rmclock-in-win7","source":"@site/blog/2012-08-06-rmclock-in-win7.md","title":"How to remove the Clock in Windows 7","description":"&nbsp;","date":"2012-08-06T09:08:41.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.245,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to remove the Clock in Windows 7","date":"2012-08-06T09:08:41.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Openoffice \u2013 There is nothing to print.","permalink":"/2012/08/14/openoffice-there-is-nothing-to-print"},"nextItem":{"title":"Windows Update 0x8007043B","permalink":"/2012/08/06/winupdate-0x8007043b"}},"content":"&nbsp;\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Down by the time click the little <strong>notification</strong> arrow.\\n  </li>\\n  <li>\\n    Click <strong>Customize</strong>\\n  </li>\\n  <li>\\n    You will be greeted with the <strong>Notification</strong> <strong>Area</strong> <strong>Icons</strong>\xa0dialog\xa0box, <strong>click</strong> <strong><em>Turn System Icons</em></strong> On or off\\n  </li>\\n  <li>\\n    <strong>Change</strong> the <strong>Clock</strong> behaviour <strong>to</strong> <strong>off</strong>\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n</ol>"},{"id":"/2012/08/06/winupdate-0x8007043b","metadata":{"permalink":"/2012/08/06/winupdate-0x8007043b","source":"@site/blog/2012-08-06-winupdate-0x8007043b.md","title":"Windows Update 0x8007043B","description":"Windows Update 0x8007043B Fix","date":"2012-08-06T08:37:33.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.42,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Windows Update 0x8007043B","date":"2012-08-06T08:37:33.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to remove the Clock in Windows 7","permalink":"/2012/08/06/rmclock-in-win7"},"nextItem":{"title":"Brother MFC-210C Reset Page count","permalink":"/2012/08/05/brothers-mfc-210c"}},"content":"Windows Update 0x8007043B Fix\\n\\n&nbsp;\\n\\n  1. Click\xa0**Start**\\n  2. Click\xa0**Run**\\n  3. Type:\xa0**_net.exe stop wuaserv_**\xa0> Press Ok\\n  4. Click\xa0**Start**\\n  5. Click\xa0**Run**\\n  6. Type:\xa0**_regsvr32 wuapi.dll_**\xa0> Press Ok\\n  7. Click\xa0**Start**\\n  8. Click\xa0**Run**\\n  9. Type:\xa0**_regsvr32 wups.dll_**\xa0\xa0> Press Ok\\n 10. Click\xa0**Start**\\n 11. Click\xa0**Run**\\n 12. Type:_\xa0**regsvr32 wuaueng.dll**_\xa0> Press Ok\\n 13. Click\xa0**Start**\\n 14. Click\xa0**Run**\\n 15. Type:**_\xa0regsvr32 msxml3.dll_**\xa0> Press Ok\\n 16. Click\xa0**Start**\\n 17. Click\xa0**Run**\\n 18. Type:_\xa0**regsvr32 atl.dll**_\xa0> Press Ok\\n 19. Click\xa0**Start**\\n 20. Click\xa0**Run**\\n 21. Type:\xa0**_regsvr32 wucltui.dll_**\xa0> Press Ok\\n 22. Click\xa0**Start**\\n 23. Click\xa0**Run**\\n 24. **_Type:\xa0net.exe start wuauserv_**\xa0> Press Ok\\n 25. _Attempt Windows Update again._"},{"id":"/2012/08/05/brothers-mfc-210c","metadata":{"permalink":"/2012/08/05/brothers-mfc-210c","source":"@site/blog/2012-08-05-brothers-mfc-210c.md","title":"Brother MFC-210C Reset Page count","description":"Follow the instructions below to reset your page counter on the Brother MFC-210C","date":"2012-08-05T18:05:35.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Brother MFC-210C Reset Page count","date":"2012-08-05T18:05:35.000Z","authors":["Luke"],"tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Windows Update 0x8007043B","permalink":"/2012/08/06/winupdate-0x8007043b"},"nextItem":{"title":"Getting \u201c Instead of @ When Typing","permalink":"/2012/08/05/getting-instead-of-when-typing"}},"content":"Follow the instructions below to reset your page counter on the Brother MFC-210C\\n\\n_**You need to go into\xa0Maintenance\xa0mode**_\\n\\n  1. To enter\xa0maintenance\xa0mode press the **Menu * 2 8 6 4** within **2 seconds of each other** (All at once)\\n  2. Press **80** and **scroll down to purge count**\\n  3. If the figure is **more than 6000 press 2 7 8 3** to reset the count.\\n  4. **Restart the printer**"},{"id":"/2012/08/05/getting-instead-of-when-typing","metadata":{"permalink":"/2012/08/05/getting-instead-of-when-typing","source":"@site/blog/2012-08-05-getting-instead-of-when-typing.md","title":"Getting \u201c Instead of @ When Typing","description":"This happens when the keyboard region has changed to UK instead of the usual standard US layout.","date":"2012-08-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.43,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"Getting \u201c Instead of @ When Typing","date":"2012-08-05 00:00:00 +1300","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Brother MFC-210C Reset Page count","permalink":"/2012/08/05/brothers-mfc-210c"},"nextItem":{"title":"How to completely remove Microsoft Groove","permalink":"/2012/08/04/rm-microsoft-groove"}},"content":"_This happens when the keyboard region has changed to UK instead of the usual standard US layout._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong> (windows orb)\\n  </li>\\n  <li>\\n    Click <strong>Control Panel</strong>\\n  </li>\\n  <li>\\n    Click <strong>Regional and Language</strong>\\n  </li>\\n  <li>\\n    Click <strong>Options</strong>\\n  </li>\\n  <li>\\n    Click <strong>Languages</strong>\\n  </li>\\n  <li>\\n    Click <strong>Details</strong>\\n  </li>\\n  <li>\\n    Click <strong>Settings</strong>\\n  </li>\\n  <li>\\n    In the drop down list <strong>select</strong> <strong><em>US Keyboard International</em></strong>\\n  </li>\\n  <li>\\n    Click <strong>Ok</strong>\\n  </li>\\n</ol>\\n\\n_Restart the application you were having problems typing in and you should now be able to use @_"},{"id":"/2012/08/04/rm-microsoft-groove","metadata":{"permalink":"/2012/08/04/rm-microsoft-groove","source":"@site/blog/2012-08-04-rm-microsoft-groove.md","title":"How to completely remove Microsoft Groove","description":"Follow the guide below to remove Microsoft Groove from your PC.","date":"2012-08-04T22:58:44.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.25,"hasTruncateMarker":false,"authors":[{"name":"Luke Murray","title":"Author","url":"https://luke.geek.nz","imageURL":"https://luke.geek.nz/img/logo.png","key":"Luke","page":null}],"frontMatter":{"title":"How to completely remove Microsoft Groove","date":"2012-08-04T22:58:44.000Z","authors":["Luke"],"tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Getting \u201c Instead of @ When Typing","permalink":"/2012/08/05/getting-instead-of-when-typing"},"nextItem":{"title":"RAS Entry Creation/Modification Error","permalink":"/2012/07/24/ras-entry-creationmodification-error"}},"content":"Follow the guide below to remove Microsoft Groove from your PC.\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\xa0<em>(windows Orb)</em>\\n  </li>\\n  <li>\\n    Click <strong>Control Panel</strong>\\n  </li>\\n  <li>\\n    Click <strong>Add/Remove Programs/Uninstall a Program</strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> Microsoft <strong>Office</strong>\\n  </li>\\n  <li>\\n    Select <strong>Change</strong>\\n  </li>\\n  <li>\\n    <strong>Deselect</strong> the <strong>Groove</strong> component and next <strong>Next</strong>\\n  </li>\\n</ol>"},{"id":"/2012/07/24/ras-entry-creationmodification-error","metadata":{"permalink":"/2012/07/24/ras-entry-creationmodification-error","source":"@site/blog/2012-07-24-ras-entry-creationmodification-error.md","title":"RAS Entry Creation/Modification Error","description":"This usually occurs when you are attempting to connecting to the Internet using a cell phone.","date":"2012-07-24T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.585,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"RAS Entry Creation/Modification Error","layout":"post","permalink":"/mob/ras-entry-creationmodification-error/","tags":["Mobile","Windows"]},"unlisted":false,"prevItem":{"title":"How to completely remove Microsoft Groove","permalink":"/2012/08/04/rm-microsoft-groove"},"nextItem":{"title":"Enable Predictive Text on a Nokia N85","permalink":"/2012/07/20/predictive-text-on-a-n85"}},"content":"_This usually occurs when you are attempting to connecting to the Internet using a cell phone._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Run</strong>\\n  </li>\\n  <li>\\n    <strong>Type</strong>: <strong><em>services.msc</em></strong><em></em>\\n  </li>\\n  <li>\\n    On the services list scroll down to \u201c<strong><em>Remote Access Connection Manager</em></strong>\u201d\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> and select <strong>Start</strong>\\n  </li>\\n  <li>\\n    <strong>Look</strong> for <strong><em>Remote Access Auto Connection Manager</em></strong>\\n  </li>\\n  <li>\\n    <strong>Right</strong> <strong>click</strong> and select <strong>Start</strong>\\n  </li>\\n  <li>\\n    Attempt to <strong>connect</strong> <strong>now</strong>\\n  </li>\\n</ol>\\n\\n_This is also caused by an issue with Motorola Phone tools you can fix this by running it as Administrator._\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Right click the Motorola Phone Tools icon (usually located on the Desktop)\\n  </li>\\n  <li>\\n    Left click \u201cRun as Administrator\u201d\\n  </li>\\n</ol>"},{"id":"/2012/07/20/predictive-text-on-a-n85","metadata":{"permalink":"/2012/07/20/predictive-text-on-a-n85","source":"@site/blog/2012-07-20-predictive-text-on-a-n85.md","title":"Enable Predictive Text on a Nokia N85","description":"1. Press the green Pencil button","date":"2012-07-20T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.04,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Enable Predictive Text on a Nokia N85","layout":"post","permalink":"/mob/predictive-text-on-a-n85/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"RAS Entry Creation/Modification Error","permalink":"/2012/07/24/ras-entry-creationmodification-error"},"nextItem":{"title":"Cybera \u2013 Unlimited time on client and deactivated on host","permalink":"/2012/07/13/cybera-unlmted-time-on-client-deactivated-on-host"}},"content":"1. Press the green **Pencil** button\\n  2. **Select\xa0Predictive\xa0**Text"},{"id":"/2012/07/13/cybera-unlmted-time-on-client-deactivated-on-host","metadata":{"permalink":"/2012/07/13/cybera-unlmted-time-on-client-deactivated-on-host","source":"@site/blog/2012-07-13-cybera-unlmted-time-on-client-deactivated-on-host.md","title":"Cybera \u2013 Unlimited time on client and deactivated on host","description":"This occurs when the client computer is out-of-sync with the Time/Date that the host computer has setup. In order to fix this problem go to the Client PC and set the correct _Date/Time_.","date":"2012-07-13T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.165,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Cybera \u2013 Unlimited time on client and deactivated on host","layout":"post","permalink":"/win/cybera-unlmted-time-on-client-deactivated-on-host/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Enable Predictive Text on a Nokia N85","permalink":"/2012/07/20/predictive-text-on-a-n85"},"nextItem":{"title":"Loading pbr for descriptor 2","permalink":"/2012/06/25/loading-pbr-for-descriptor-2"}},"content":"This occurs when the client computer is out-of-sync with the Time/Date that the host computer has setup. In order to fix this problem go to the Client PC and set the **correct** **_Date/Time_**."},{"id":"/2012/06/25/loading-pbr-for-descriptor-2","metadata":{"permalink":"/2012/06/25/loading-pbr-for-descriptor-2","source":"@site/blog/2012-06-25-loading-pbr-for-descriptor-2.md","title":"Loading pbr for descriptor 2","description":"This issue usually occurs on Dell machines and it mostly caused by the Windows Boot Loader becoming infected or corrupted.","date":"2012-06-25T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.2,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Loading pbr for descriptor 2","layout":"post","permalink":"/win/loading-pbr-for-descriptor-2/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Cybera \u2013 Unlimited time on client and deactivated on host","permalink":"/2012/07/13/cybera-unlmted-time-on-client-deactivated-on-host"},"nextItem":{"title":"Power Calibration Error when attempting to burn CD/DVD","permalink":"/2012/06/15/power-calibration-burn"}},"content":"_This issue usually occurs on Dell machines and it mostly caused by the Windows Boot Loader becoming infected or corrupted._ \\n\\n  1. Repair Boot Loader\\n  2. \xa0If the Windows XP machine doesn\u2019t boot then you need to do a Windows Repair."},{"id":"/2012/06/15/power-calibration-burn","metadata":{"permalink":"/2012/06/15/power-calibration-burn","source":"@site/blog/2012-06-15-power-calibration-burn.md","title":"Power Calibration Error when attempting to burn CD/DVD","description":"1. Click Start","date":"2012-06-15T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.36,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Power Calibration Error when attempting to burn CD/DVD","layout":"post","permalink":"/misc/power-calibration-burn/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Loading pbr for descriptor 2","permalink":"/2012/06/25/loading-pbr-for-descriptor-2"},"nextItem":{"title":"Using Pastebin Services","permalink":"/2012/06/13/using-pastebin-services"}},"content":"1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click **Administrative Tools**\\n  4. Click **Services\xa0**\\n  5. Look for the &#8220;**IMAPI CD-Burning COM Service**&#8220;\\n  6. **Right** **click** on it and click **properties**.\\n  7. **Change** **Startup** Type to **Disabled**.\\n  8. Click **Apply\xa0**\\n  9. **Restart** the **PC** to apply changes.\\n\\n_You will have to switch to another Burning application, such as Nero or CD Burner XP, and not rely on Windows XPs built in Burning function._"},{"id":"/2012/06/13/using-pastebin-services","metadata":{"permalink":"/2012/06/13/using-pastebin-services","source":"@site/blog/2012-06-13-using-pastebin-services.md","title":"Using Pastebin Services","description":"Pastebin service allows people to upload text, code for public viewing. Very handy when you want\xa0someone&#8217;s\xa0opinion on something you have written or having problems with certain code and require help.","date":"2012-06-13T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":1.065,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using Pastebin Services","layout":"post","permalink":"/misc/using-pastebin-services/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Power Calibration Error when attempting to burn CD/DVD","permalink":"/2012/06/15/power-calibration-burn"},"nextItem":{"title":"How to delete your Last.FM account","permalink":"/2012/06/09/how-to-del-your-fm-account"}},"content":"_Pastebin service allows people to upload text, code for public viewing. Very handy when you want\xa0someone&#8217;s\xa0opinion on something you have written or having problems with certain code and require help._\\n\\n_Below are several recommended Pastebin services_\\n\\n<a title=\\"slexy pastebin\\" href=\\"http://www.slexy.org\\" target=\\"_blank\\"><strong>Slexy</strong></a>\\n\\nSlexy.org is a powerful, slick, and sexy pastebin designed with the user in mind.\\n\\n_Use Slexy.org to paste errors for debugging, show off your code or your tech specs, or share anything else with other Internet users worldwide._\\n\\n_Top features: Tabbing within pastes, pasting through CLI/terminal using PastebinIt, saving preferences_\\n\\n_More features: Paste expiration, paste versioning, line numbers/highlighting, code syntax highlighting, private pastes_\\n\\n_Don&#8217;t worry about those slow pastebins with long URLs, ugly syntax highlighting, messy layouts, and lack of features. With Slexy.org, speed, efficiency, and usefulness are the goals._\\n\\n__\\n  \\n<a title=\\"Pastebin\\" href=\\"http://www.pastebin.com\\" target=\\"_blank\\"><strong>Pastebin</strong></a>\\n\\n_pastebin is here to help you collaborate on debugging code snippets. If you&#8217;re not familiar with the idea, most people use it like this:_\\n\\n_* submit a code fragment to pastebin, getting a url like_ \\n  \\n _* paste the url into an IRC or IM conversation_\\n  \\n _* someone responds by reading and perhaps submitting a modification of your code_\\n  \\n _* you then view the modification, maybe using the built in diff tool to help locate the changes_\\n\\n&nbsp;"},{"id":"/2012/06/09/how-to-del-your-fm-account","metadata":{"permalink":"/2012/06/09/how-to-del-your-fm-account","source":"@site/blog/2012-06-09-how-to-del-your-fm-account.md","title":"How to delete your Last.FM account","description":"Go to your\xa0Last.FM\xa0Account Settings","date":"2012-06-09T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.14,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to delete your Last.FM account","layout":"post","permalink":"/misc/how-to-del-your-fm-account/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Using Pastebin Services","permalink":"/2012/06/13/using-pastebin-services"},"nextItem":{"title":"Enable Full KDE Desktop ASUS eeePC","permalink":"/2012/06/06/enable-full-kde-desktop-asus-eeepc"}},"content":"Go to your\xa0<a title=\\"Last.FM\\" href=\\"https://www.last.fm/settings/account\\" target=\\"_blank\\">Last.FM</a>\xa0Account <strong>Settings</strong>\\n\\nClick on the <strong>Data</strong> tab to the right of the top menu\\n\\nClick <strong>Delete</strong> entire <strong>account</strong> \\n\\nClick <strong>yes</strong> to confirm deletion."},{"id":"/2012/06/06/enable-full-kde-desktop-asus-eeepc","metadata":{"permalink":"/2012/06/06/enable-full-kde-desktop-asus-eeepc","source":"@site/blog/2012-06-06-enable-full-kde-desktop-asus-eeepc.md","title":"Enable Full KDE Desktop ASUS eeePC","description":"1. Open terminal: Tap Ctrl+Alt+T to open the terminal command prompt.","date":"2012-06-06T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Enable Full KDE Desktop ASUS eeePC","layout":"post","permalink":"/linux/enable-full-kde-desktop-asus-eeepc/","tags":["Linux"]},"unlisted":false,"prevItem":{"title":"How to delete your Last.FM account","permalink":"/2012/06/09/how-to-del-your-fm-account"},"nextItem":{"title":"Enable Windows Automatic Updates","permalink":"/2012/06/03/enable-windows-automatic-updates"}},"content":"1. Open **terminal**: **Tap** **Ctrl+Alt+T** to open the terminal command prompt.\\n  2. Give yourself **root** access: Type &#8220;**sudo -i**&#8221; in terminal, and then hit Enter.\\n  3. **Update** **Aptitude**: Type &#8220;a**pt-get update**&#8221; to update your software.\\n  4. **Install** **KSMServer** and **Kicker**: Type &#8220;**apt-get install ksmserver kicker**.&#8221; This will download both of the needed applications to enable the KDE desktop environment.\\n  5. Restart."},{"id":"/2012/06/03/enable-windows-automatic-updates","metadata":{"permalink":"/2012/06/03/enable-windows-automatic-updates","source":"@site/blog/2012-06-03-enable-windows-automatic-updates.md","title":"Enable Windows Automatic Updates","description":"1. Click Start","date":"2012-06-03T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Enable Windows Automatic Updates","layout":"post","permalink":"/win/enable-windows-automatic-updates/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Enable Full KDE Desktop ASUS eeePC","permalink":"/2012/06/06/enable-full-kde-desktop-asus-eeepc"},"nextItem":{"title":"How to fix the issue when the PS3 DVD Drive will not eject","permalink":"/2012/05/31/how-to-fix-the-issue-when-the-ps3-dvd-drive-will-not-eject"}},"content":"1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click **Security**\\n  4. Click **Windows Update**\\n  5. Click **Change Settings**\\n  6. Click _&#8220;**Check for updates but let me choose whether to download and install them**&#8221;\xa0_\\n  7. Click **Ok**"},{"id":"/2012/05/31/how-to-fix-the-issue-when-the-ps3-dvd-drive-will-not-eject","metadata":{"permalink":"/2012/05/31/how-to-fix-the-issue-when-the-ps3-dvd-drive-will-not-eject","source":"@site/blog/2012-05-31-how-to-fix-the-issue-when-the-ps3-dvd-drive-will-not-eject.md","title":"How to fix the issue when the PS3 DVD Drive will not eject","description":"1. At the back of the PS3, turn the Power switch OFF","date":"2012-05-31T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.335,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to fix the issue when the PS3 DVD Drive will not eject","layout":"post","permalink":"/misc/how-to-fix-the-issue-when-the-ps3-dvd-drive-will-not-eject/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Enable Windows Automatic Updates","permalink":"/2012/06/03/enable-windows-automatic-updates"},"nextItem":{"title":"Windows was unable to find a certificate to log you on to the network","permalink":"/2012/05/31/win-unable-to-find-cert-to-log-you-on-to-the-network"}},"content":"1. At the back of the PS3, **turn** the **Power** switch **OFF**\\n  2. **Hold** down the **Eject** Power and **Power** button at the same time.\\n  3. While **holding** down both the eject and power buttons, **switch** the PS3 **on** (Switch at the back)\\n  4. **Hold** the **buttons** down for **_8 seconds_**; it should now force the DVD to eject.\\n\\nAlso refer to the Playstation Help Article: http://playstation.custhelp.com/app/answers/detail/a_id/1783/~/remove-stuck-disc"},{"id":"/2012/05/31/win-unable-to-find-cert-to-log-you-on-to-the-network","metadata":{"permalink":"/2012/05/31/win-unable-to-find-cert-to-log-you-on-to-the-network","source":"@site/blog/2012-05-31-win-unable-to-find-cert-to-log-you-on-to-the-network.md","title":"Windows was unable to find a certificate to log you on to the network","description":"1. Click\xa0Start","date":"2012-05-31T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.37,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows was unable to find a certificate to log you on to the network","layout":"post","permalink":"/win/win-unable-to-find-cert-to-log-you-on-to-the-network/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to fix the issue when the PS3 DVD Drive will not eject","permalink":"/2012/05/31/how-to-fix-the-issue-when-the-ps3-dvd-drive-will-not-eject"},"nextItem":{"title":"How to check your android phone uptime","permalink":"/2012/05/30/how-to-check-your-android-phone-uptime"}},"content":"1. Click\xa0**Start**\\n  2. Click **Run**\\n  3. Type: **_ncpa.cpl_** and press Enter\\n  4. **Right** **click** on the Wireless **network** **connections** icon\\n  5. Left click **Properties**\\n  6. Click the **Wireless** network tab\\n  7. Click **Properties**\\n  8. Click the **Authentication** tab\\n  9. **Uncheck\xa0_Enable IEEE 802.1x authentication_** for this network\\n 10. Click **Ok**\\n\\n_If the above does not work, you will have to\xa0re-install\xa0your Wireless drivers, so head to the manufacturers website and download the latest Wireless drivers and\xa0re-install._"},{"id":"/2012/05/30/how-to-check-your-android-phone-uptime","metadata":{"permalink":"/2012/05/30/how-to-check-your-android-phone-uptime","source":"@site/blog/2012-05-30-how-to-check-your-android-phone-uptime.md","title":"How to check your android phone uptime","description":"Want to check how long your phone has been on for? Follow the instructions below.","date":"2012-05-30T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.19,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to check your android phone uptime","layout":"post","permalink":"/mob/how-to-check-your-android-phone-uptime/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"Windows was unable to find a certificate to log you on to the network","permalink":"/2012/05/31/win-unable-to-find-cert-to-log-you-on-to-the-network"},"nextItem":{"title":"Can\u2019t establish a reliable data connection to the server","permalink":"/2012/05/29/cant-establish-a-reliable-data-connection-to-the-server"}},"content":"Want to check how long your phone has been on for? Follow the instructions below.\\n\\n  1. Go into **Settings**\\n  2. Go down and select **About phone**\\n  3. Select **Status**\\n  4. **Scroll** to the **bottom** to view your phones\xa0Up-time."},{"id":"/2012/05/29/cant-establish-a-reliable-data-connection-to-the-server","metadata":{"permalink":"/2012/05/29/cant-establish-a-reliable-data-connection-to-the-server","source":"@site/blog/2012-05-29-cant-establish-a-reliable-data-connection-to-the-server.md","title":"Can\u2019t establish a reliable data connection to the server","description":"Trying to add your Google Account to your Android phone and it&#8217;s coming up with the error &#8220;Can\u2019t establish a reliable data connection to the server&#8221; try the tips below:","date":"2012-05-29T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.915,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Can\u2019t establish a reliable data connection to the server","layout":"post","permalink":"/mob/cant-establish-a-reliable-data-connection-to-the-server/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"How to check your android phone uptime","permalink":"/2012/05/30/how-to-check-your-android-phone-uptime"},"nextItem":{"title":"How to install Appsync on a Jailbroken iOS Device","permalink":"/2012/05/28/appsync-on-a-jailbroken-ios-device"}},"content":"Trying to add your Google Account to your Android phone and it&#8217;s coming up with the error &#8220;Can\u2019t establish a reliable data connection to the server&#8221; try the tips below:\\n\\n  1. Go into Applications and select the Youtube Application\\n  2. Login using your Google Account\\n  3. Wait for about one minute, while the Youtube application syncs your data.\\n  4. Close the Youtube application\\n  5. Open Settings\\n  6. Go to Accounts\\n  7. Now your Google Account should be listed, if it is not, attempt to add it _(you should now be able to)_\\n  8. . Add YouTube app and login with gmail account\\n\\nIf that does not work, try the following:\\n\\n  1. Turn your mobile phone off\\n  2. Take out your Sim Card\\n  3. Start your phone _(without Sim card)_\\n  4. Click on Settings\\n  5. Click on Accounts\\n  6. Now attempt to add your Google Account\\n  7. Once the account has been added power off your phone and put your Sim card back in.\\n\\nIf that does not work it has been reported a Factory Reset _(which you will lose all your data/contacts)\xa0_will work."},{"id":"/2012/05/28/appsync-on-a-jailbroken-ios-device","metadata":{"permalink":"/2012/05/28/appsync-on-a-jailbroken-ios-device","source":"@site/blog/2012-05-28-appsync-on-a-jailbroken-ios-device.md","title":"How to install Appsync on a Jailbroken iOS Device","description":"1. Open Cydia","date":"2012-05-28T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install Appsync on a Jailbroken iOS Device","layout":"post","permalink":"/mob/appsync-on-a-jailbroken-ios-device/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"Can\u2019t establish a reliable data connection to the server","permalink":"/2012/05/29/cant-establish-a-reliable-data-connection-to-the-server"},"nextItem":{"title":"Black Screen on Windows 7 on an iMac","permalink":"/2012/05/28/bsod-win7-imac"}},"content":"1. Open **Cydia**\\n  2. Select **Manage**\\n  3. Select **Sources**\\n  4. Select **Edit**\\n  5. Select **Add**\\n  6. Now enter the source:\xa0_http://repocydia.com_\\n  7. Select **Add Source**\\n  8. Cydia will now run a repository update and check for latest packages etc\\n  9. Once update has completed **search** for **AppSync** in Cydia\\n 10. **Install** the latest version\\n 11. It will then patch the necessary files to be able to\xa0utilize\xa0iTunes compatibility."},{"id":"/2012/05/28/bsod-win7-imac","metadata":{"permalink":"/2012/05/28/bsod-win7-imac","source":"@site/blog/2012-05-28-bsod-win7-imac.md","title":"Black Screen on Windows 7 on an iMac","description":"This usually occurs due to problems with the nVidia drivers. Follow the prompts below to repair this.","date":"2012-05-28T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.79,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Black Screen on Windows 7 on an iMac","layout":"post","permalink":"/osx/bsod-win7-imac/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to install Appsync on a Jailbroken iOS Device","permalink":"/2012/05/28/appsync-on-a-jailbroken-ios-device"},"nextItem":{"title":"HP Pavilion Slimline Ram/HDD Replacement","permalink":"/2012/05/28/hp-pavilion-slimline-ramhdd-replacement"}},"content":"_This usually occurs due to problems with the nVidia drivers. Follow the prompts below to repair this._\\n\\n  1. Put the Windows 7 DVD into your drive and restart and boot from the DVD\\n  2. When the Windows 7 prompt comes up, select Repair Your Computer (on the lower left)\\n  3. Select your Windows installation and press Next\\n  4. Now you should be at the System Recovery Options, select Command Prompt\\n  5. Once the command prompt opens type: DEL C:WINDOWSSYSTEM32DRIVERSNVLDDMKM.SYS\\n  6. If you have problems with sound type: \xa0DEL C:WINDOWSSYSTEM32DRIVERSati*.sys\\n  7. Now reboot\\n  8. You should now be in Windows 7 (however it will look a bit pale and large, this is normal)\\n  9. Go to the \u201cnvidia\u201d website and download the GT7 Series Driver for Windows 7\\n 10. Now install and reboot.\\n 11. Having done this go to the Nivida web site and download the GT 7 series drivers for Windows 7!\\n 12. Install the drivers and reboot."},{"id":"/2012/05/28/hp-pavilion-slimline-ramhdd-replacement","metadata":{"permalink":"/2012/05/28/hp-pavilion-slimline-ramhdd-replacement","source":"@site/blog/2012-05-28-hp-pavilion-slimline-ramhdd-replacement.md","title":"HP Pavilion Slimline Ram/HDD Replacement","description":"1. Remove the casing by removing 3 screws, then slide off the side panel.","date":"2012-05-28T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.51,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"HP Pavilion Slimline Ram/HDD Replacement","layout":"post","permalink":"/misc/hp-pavilion-slimline-ramhdd-replacement/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Black Screen on Windows 7 on an iMac","permalink":"/2012/05/28/bsod-win7-imac"},"nextItem":{"title":"Google Cheatsheet","permalink":"/2012/05/23/google-cheatsheet"}},"content":"1. **Remove** the **casing** by **removing** **3** screws, then **slide** off the **side** **panel**.\\n  2. Go to the HP Expand Bay and pull the **tab** **up** and **slide** the **bay** **left** at the same time.\\n  3. Now **remove** the **CD-Rom** by **unscrewing** the Black **Screw** on the **lower** **part** of the drive.\\n  4. **Push** the **tab** **out** and **push** the **back** **of** the **CD-Rom** out to create room to move above the RAM slots.\\n  5. To take the HDD out, go to the left hand corner and **unscrew** the **screw**, then **take** **off** the HD **Lock**.\\n  6. **Unplug** **HD** and **push** **out**."},{"id":"/2012/05/23/google-cheatsheet","metadata":{"permalink":"/2012/05/23/google-cheatsheet","source":"@site/blog/2012-05-23-google-cheatsheet.md","title":"Google Cheatsheet","description":"Google Search queries","date":"2012-05-23T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":2.78,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Google Cheatsheet","layout":"post","permalink":"/misc/google-cheatsheet/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"HP Pavilion Slimline Ram/HDD Replacement","permalink":"/2012/05/28/hp-pavilion-slimline-ramhdd-replacement"},"nextItem":{"title":"How to setup your web browser to run through a proxy","permalink":"/2012/05/22/web-browser-to-run-proxy"}},"content":"**Google Search queries**\\n\\n**define:<span style=\\"text-decoration: underline;\\">phrase</span>**\\n\\nShow a list of definitions for phrase\\n  \\n**cache:www.example.com**\\n  \\n_Google&#8217;s cache of example.com_\\n  \\n**link:www.example.com**\\n  \\n_List of websites that link to_\\n  \\n_example.com_\\n  \\n**related:www.example.com**\\n  \\n_List webpages similar to_\\n  \\n_www.example.com_\\n  \\n**info:www.example.com**\\n  \\n_Show information that Google has_\\n  \\n_about www.example.com_\\n  \\n**site:www.example.com**\\n  \\n_List all webpages hosted at_\\n  \\n_www.example.com_\\n  \\n**allintitle:query**\\n  \\n_Restrict the results to those with all_\\n  \\n_of the query words in the title._\\n  \\n**intitle:query**\\n  \\n_Restrict the results to documents_\\n  \\n_containing that word in the title._\\n  \\n**allinurl:query**\\n  \\n_Restrict the results to those with all_\\n  \\n_of the query words in the URL._\\n  \\n**safesearch: sex education**\\n\\n_Search for sex education material_\\n  \\n**weather: hamilton nz**\\n  \\n_Displays the temperature and weather of a city/town_\\n\\n**Use Google as a calculator**\\n\\n_You can use google as a calculator to add/subtract and divide numbers._\\n\\n_+ addition_\\n  \\n_&#8211; subtraction_\\n  \\n_* multiplication_\\n  \\n_/ division_\\n  \\n_% modulus_\\n  \\n_% of percentage of_\\n  \\n_^ raise to a power_\\n\\n_For example: 10000*700_\\n\\n**Use Google to search for Music & Movies**\\n\\n_Music -inurl:htm -inurl:html intitle:\u201dindex of\u201d mp3 \u201cArtist Name\u201d_\\n  \\n_Movies -inurl:htm -inurl:html -inurl:php intitle:\u201dindex of\u201d (mpg|mov|avi|wmv) \u201cMovie Name\u201d_\\n  \\n_Where Artist Name and Movie Name are replaced with the artist or movie you wish to find file directories for._\\n\\n**Use Google to search Rapidshare**\\n\\n_+inurl:avi|mpg|wmv site:rapidshare.de_\\n\\n_To see all archives and programs available\u2026_\\n\\n_+inurl:exe|rar|zip site:rapidshare.de_\\n\\n_To see all music files available\u2026_\\n\\n_+inurl:wma|mp3|ogg site:rapidshare.de_\\n\\n_This should show all files available\u2026_\\n\\n_.* site:rapidshare.de_\\n\\n**Google Services**\\n\\nGoogle AdSense https://www.google.com/adsense/\\n  \\nGoogle AdWords https://adwords.google.com/select/\\n  \\nGoogle Analytics http://google.com/analytics/\\n  \\nGoogle Answers http://answers.google.com/\\n  \\nGoogle Base http://base.google.com/\\n  \\nGoogle Blog Search http://blogsearch.google.com/\\n  \\nGoogle Bookmarks http://www.google.com/bookmarks/\\n  \\nGoogle Books Search http://books.google.com/\\n  \\nGoogle Calendar http://google.com/calendar/\\n  \\nGoogle Catalogs http://catalogs.google.com/\\n  \\nGoogle Code http://code.google.com/\\n  \\nGoogle Deskbar\\n  \\nGoogle Desktop http://desktop.google.com/\\n  \\nGoogle Directory\\n  \\nGoogle Earth http://earth.google.com/\\n  \\nGoogle Finance http://finance.google.com/\\n  \\nGoogle Groups http://groups.google.com/\\n  \\nGoogle Images http://images.google.com/\\n  \\nGoogle Labs http://labs.google.com/\\n  \\nGoogle Local http://local.google.com/\\n  \\nGoogle Maps http://maps.google.com/\\n  \\nGoogle Mars http://www.google.com/mars/\\n  \\nGoogle Mobile http://mobile.google.com/\\n  \\nGoogle Moon http://moon.google.com/\\n  \\nGoogle Movies http://www.google.com/movies\\n  \\nGoogle Music http://www.google.com/musicsearch\\n  \\nGoogle News http://news.google.com/\\n  \\nGoogle Pack http://pack.google.com/\\n  \\nGoogle Page Creator\\n  \\nGoogle Personalized Home\u2026 http://www.google.com/ig\\n  \\nGoogle Personalized Search\\n  \\nGoogle Reader http://www.google.com/reader\\n  \\nGoogle Scholar http://scholar.google.com/\\n  \\nGoogle Search History http://www.google.com/searchhistory\\n  \\nGoogle SMS http://www.google.com/sms/\\n  \\nGoogle Suggest http://www.google.com/webhp?complete=1\\n  \\nGoogle Talk http://talk.google.com/\\n  \\nGoogle Toolbar http://toolbar.google.com/\\n  \\nGoogle Transit Trip Planner http://www.google.com/transit\\n  \\nGoogle Translate http://www.google.com/translate_t\\n  \\nGoogle Video http://video.google.com/\\n  \\nGoogle Web Accelerator\\n  \\nGoogle Web API http://www.google.com/apis/\\n  \\nGoogle Web Search http://www.google.com\\n  \\nGoogle Holiday Logos\\n  \\nGoogle Zeitgeist http://www.google.com/press/intl-zeitgeist.html\\n  \\nGoogle Jobs http://www.google.com/intl/en/jobs/\\n  \\nGoogle University Se\u2026 http://www.google.com/options/universities.html\\n  \\nGoogle Sitemaps https://www.google.com/webmasters/sitemaps/\\n  \\nGoogle Base http://googlebase.blogspot.com/\\n  \\nGoogle Code http://code.google.com/\\n  \\nGoogle Enterprise http://googleenterprise.blogspot.com/\\n  \\nGoogle Maps API http://googlemapsapi.blogspot.com/\\n  \\nGoogle Reader http://googlereader.blogspot.com/\\n  \\nGoogle Research http://googleresearch.blogspot.com/\\n  \\nGoogle Talk http://googletalk.blogspot.com/\\n  \\nGoogle Video http://googlevideo.blogspot.com/\\n  \\nInside AdSense http://adsense.blogspot.com/\\n  \\nInside AdWords http://adwords.blogspot.com/\\n  \\nInside Desktop http://googledesktop.blogspot.com/\\n  \\nInside Sitemap http://sitemaps.blogspot.com/\\n  \\nOfficial Google Blog http://googleblog.blogspot.com/\\n  \\nAdwords API http://adwordsapi.blogspot.com/\\n  \\nBlogger Buzz http://buzz.blogger.com/\\n  \\nBlogger http://www.blogger.com\\n  \\nFroogle http://www.froogle.com/\\n  \\nGmail http://www.gmail.com/\\n  \\nHello\\n  \\nOrkut http://www.orkut.com/\\n  \\nPicasa http://picasa.google.com/\\n  \\nSketchUp http://www.sketchup.com/\\n  \\nWritely http://www.writely.com/\\n\\n**Googlebot Information**\\n\\nGooglebot/2.1 IP Addresses\\n  \\n64.68.80.#\\n  \\n64.68.81.#\\n  \\n64.68.82.#\\n  \\n64.68.84.#\\n  \\n64.68.88.#\\n  \\n216.239.46.#\\n  \\n216.239.38.#\\n  \\n216.239.36.#\\n  \\nGoogle PageRank Formula\\n  \\nPR(U) = (1-d) + d *\\n  \\nsumV(PR(V)/N(V))\\n  \\nSites with PageRank 10\\n  \\nAdobe.com\\n  \\nApple.com\\n  \\nBlogger.com\\n  \\nEnergy.gov\\n  \\nFirstGov.gov\\n  \\nGoogle.com\\n  \\nMacromedia.com\\n  \\nNASA.gov\\n  \\nNSF.gov\\n  \\nNYTimes.com\\n  \\nReal.com\\n  \\nStatCounter.com\\n  \\nW3.org\\n  \\nWebStandards.org\\n  \\nWhitehouse.gov\\n\\n**Contact Google**\\n\\nGoogle Company Information\\n  \\nPublic (NASDAQ: GOOG) and\\n  \\n(LSE: GGEA)\\n  \\nFounded\\n  \\nMenlo Park, California (1998)\\n  \\nLocation\\n  \\nMountain View, California, USA\\n  \\nKey people\\n  \\nEric E. Schmidt\\n  \\nSergey Brin\\n  \\nLarry E. Page\\n  \\nGeorge Reyes\\n  \\nRevenue\\n  \\n$6.138 Billion USD (2005)\\n  \\nNet Income\\n  \\n$1.465 Billion USD (2005)\\n  \\nEmployees\\n  \\n5,680 (2005)\\n  \\nContact Address\\n  \\n2400 E. Bayshore Parkway\\n  \\nMountain View, CA 94043\\n  \\nContact Phone\\n  \\n+1 650 318 0200 (US)\\n  \\nContact Fax\\n  \\n+1 650 618 1499 (US)\\n  \\ndns-admin@google.com"},{"id":"/2012/05/22/web-browser-to-run-proxy","metadata":{"permalink":"/2012/05/22/web-browser-to-run-proxy","source":"@site/blog/2012-05-22-web-browser-to-run-proxy.md","title":"How to setup your web browser to run through a proxy","description":"Internet Explorer","date":"2012-05-22T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.47,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to setup your web browser to run through a proxy","layout":"post","permalink":"/osx/web-browser-to-run-proxy/","tags":["Mac OSX","Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Google Cheatsheet","permalink":"/2012/05/23/google-cheatsheet"},"nextItem":{"title":"Alureon.E removal","permalink":"/2012/05/21/alureon-e-removal"}},"content":"**Internet Explorer**\\n\\n  1. Click Tools\\n  2. Click Internet Options\\n  3. Click Connections\\n  4. Click LAN Settings\\n  5. Click Use a Proxy Server\\n  6. Click Advanced\\n  7. Click HTTP\\n\\n**Mozilla Firefox**\\n\\n  1. Click Tools\\n  2. Click Options\\n  3. Click Advanced\\n  4. Click Settings\\n  5. Click Manual Proxy Configuration\\n\\n**Google Chrome**\\n\\n  1. Click Options\\n  2. Click Settings\\n  3. Click Show Advanced Settings\\n  4. Click Change Proxy Settings\\n  5. Cluck LAN Settings\\n  6. Click Use a Proxy Server\\n  7. Click Advanced\\n  8. Click HTTP\\n\\n**Opera**\\n\\n  1. Click Tools\\n  2. Click Preferences\\n  3. Click Advanced\\n  4. Click Network"},{"id":"/2012/05/21/alureon-e-removal","metadata":{"permalink":"/2012/05/21/alureon-e-removal","source":"@site/blog/2012-05-21-alureon-e-removal.md","title":"Alureon.E removal","description":"1. First update and scan your computer doing a Full scan with your Antivirus to remove all traces of all infections.","date":"2012-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.665,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Alureon.E removal","layout":"post","permalink":"/win/alureon-e-removal/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to setup your web browser to run through a proxy","permalink":"/2012/05/22/web-browser-to-run-proxy"},"nextItem":{"title":"How to disable World of Warcrafts Mature Language Filter","permalink":"/2012/05/21/disable-wow-langfilter"}},"content":"1. First update and scan your computer doing a Full scan with your Antivirus to remove all traces of all infections.\\n  2. Now we need to remove a partition created by the rootkit to keep itself safe from removal.\\n  3. Click on Start\\n  4. Right click My Computer\\n  5. Click Manage\\n  6. Click Disk Management\\n  7. Now you should see a list of partitions on your drive, there should be a very small partition that will be roughly about a few megabytes and should not have a drive letter assigned to it. IGNORE C: AND EVERY OTHER PARITION/DRIVE TO AVOID LOSS OF DATA.\\n  8. Right click on the few megabytes unlabelled partition and left click Delete Volume.\\n  9. Now restart your computer and do another virus scan and you should be all clean."},{"id":"/2012/05/21/disable-wow-langfilter","metadata":{"permalink":"/2012/05/21/disable-wow-langfilter","source":"@site/blog/2012-05-21-disable-wow-langfilter.md","title":"How to disable World of Warcrafts Mature Language Filter","description":"1. Press Esc","date":"2012-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.075,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to disable World of Warcrafts Mature Language Filter","layout":"post","permalink":"/misc/disable-wow-langfilter/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Alureon.E removal","permalink":"/2012/05/21/alureon-e-removal"},"nextItem":{"title":"Setup Drive Spanning in Windows","permalink":"/2012/05/21/drive-spanning-in-windows"}},"content":"1. Press **Esc**\\n  2. Click **Interface**\\n  3. Click **Social**\\n  4. **Uncheck** Mature\xa0**Language\xa0Filter**\\n  5. Press **Ok**"},{"id":"/2012/05/21/drive-spanning-in-windows","metadata":{"permalink":"/2012/05/21/drive-spanning-in-windows","source":"@site/blog/2012-05-21-drive-spanning-in-windows.md","title":"Setup Drive Spanning in Windows","description":"1. (Make sure you have no data available on the partitions, as this will erase everything)","date":"2012-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.485,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Setup Drive Spanning in Windows","layout":"post","permalink":"/win/drive-spanning-in-windows/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to disable World of Warcrafts Mature Language Filter","permalink":"/2012/05/21/disable-wow-langfilter"},"nextItem":{"title":"How to import Sketchup Model into ZBrush using 3DS Max","permalink":"/2012/05/21/how-to-import-sketchup-model-into-zbrush-using-3ds-max"}},"content":"1. (Make sure you have no data available on the partitions, as this will erase everything)\\n  2. First you need to open Disk Management\\n  3. Right click on unallocated space\\n  4. Click Create New Volume\\n  5. Click Next\\n  6. Select Spanned Volume and select Next\\n  7. Select the disks you would like to add to the spanned volume and click Add\\n  8. Adjust any size settings you need to, then click Next\\n  9. Give the drive a drive letter (example G:)\\n 10. Adjust any formatting options, ie making sure the partition will be NTFS.\\n 11. Click Finish"},{"id":"/2012/05/21/how-to-import-sketchup-model-into-zbrush-using-3ds-max","metadata":{"permalink":"/2012/05/21/how-to-import-sketchup-model-into-zbrush-using-3ds-max","source":"@site/blog/2012-05-21-how-to-import-sketchup-model-into-zbrush-using-3ds-max.md","title":"How to import Sketchup Model into ZBrush using 3DS Max","description":"1. Open Sketchup","date":"2012-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.315,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to import Sketchup Model into ZBrush using 3DS Max","layout":"post","permalink":"/misc/how-to-import-sketchup-model-into-zbrush-using-3ds-max/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Setup Drive Spanning in Windows","permalink":"/2012/05/21/drive-spanning-in-windows"},"nextItem":{"title":"Skyrim Oculory Focusing Lens Bug","permalink":"/2012/05/21/skyrim-oculory-focusing-lens-bug"}},"content":"1. Open Sketchup\\n  2. Click File\\n  3. Click Export\\n  4. Export the model to 3DS\\n  5. Open 3DS Max\\n  6. Open the 3DS model that you saved from Sketchup\\n  7. Delete the Cameras\\n  8. Delete the Lights\\n  9. Click File\\n 10. Click Export\\n 11. Export as \u201cobj using ZBrush Presets\u201d\\n 12. Open ZBrush\\n 13. Open the obj file you saved from 3DS Max"},{"id":"/2012/05/21/skyrim-oculory-focusing-lens-bug","metadata":{"permalink":"/2012/05/21/skyrim-oculory-focusing-lens-bug","source":"@site/blog/2012-05-21-skyrim-oculory-focusing-lens-bug.md","title":"Skyrim Oculory Focusing Lens Bug","description":"People have reported that using close range Fire and Ice spells while standing in the middle of the Oculory.","date":"2012-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.195,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Skyrim Oculory Focusing Lens Bug","layout":"post","permalink":"/misc/skyrim-oculory-focusing-lens-bug/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to import Sketchup Model into ZBrush using 3DS Max","permalink":"/2012/05/21/how-to-import-sketchup-model-into-zbrush-using-3ds-max"},"nextItem":{"title":"Windows Vista Modem Connecting","permalink":"/2012/05/21/windows-vista-modem-connecting"}},"content":"_People have reported that using close range Fire and Ice spells while standing in the middle of the Oculory._\\n\\n  1. \xa0First you need to **open** up the **game** console press: **\\\\`**\\n  2. Type: **_player.setstage MG06 55_**\\n  3. Press Enter"},{"id":"/2012/05/21/windows-vista-modem-connecting","metadata":{"permalink":"/2012/05/21/windows-vista-modem-connecting","source":"@site/blog/2012-05-21-windows-vista-modem-connecting.md","title":"Windows Vista Modem Connecting","description":"This usually occurs Operator Assisted Dialing is enabled for the Dialup connection, to disable follow the instructions below:","date":"2012-05-21T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.235,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Vista Modem Connecting","layout":"post","permalink":"/win/windows-vista-modem-connecting/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Skyrim Oculory Focusing Lens Bug","permalink":"/2012/05/21/skyrim-oculory-focusing-lens-bug"},"nextItem":{"title":"Load needed DLLs for Kernel","permalink":"/2012/05/18/load-needed-dlls-for-kernel"}},"content":"_This usually occurs Operator Assisted Dialing is enabled for the Dialup connection, to disable follow the instructions below:_\\n\\n  1. Click **Start**\\n  2. Click **Network Connections**\\n  3. Click your\xa0Dial-up\xa0connection and press the ALT key on your keyboard.\\n  4. Click **Advanced** (up the top)\\n  5. **Untick** Operator **Assisted** **Dialing**"},{"id":"/2012/05/18/load-needed-dlls-for-kernel","metadata":{"permalink":"/2012/05/18/load-needed-dlls-for-kernel","source":"@site/blog/2012-05-18-load-needed-dlls-for-kernel.md","title":"Load needed DLLs for Kernel","description":"1. First thing you need to have a Windows XP CD","date":"2012-05-18T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.485,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Load needed DLLs for Kernel","layout":"post","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Windows Vista Modem Connecting","permalink":"/2012/05/21/windows-vista-modem-connecting"},"nextItem":{"title":"How to remove the HP TouchSmart IQ780a case","permalink":"/2012/05/18/remove-the-hp-touchsmart-iq780a-case"}},"content":"1. First thing you need to have a Windows XP CD\\n  2. Put CD in drive and **boot** from it, you may have to press ESC or F8 to open Boot Prompt to select CD Drive.\\n  3. Once it has loaded choose Recovery Console\\n  4. Type: **_chkdsk /r /f c:_**\\n  5. Press Y\\n  6. This may take an hour or two to complete, once it has completed type: BOOTCFG /rebuild (Press Enter for prompt)\\n  7. Type: **_fixboot_** and press Enter\\n  8. Type: **_fixmbr_** and press Enter\\n  9. Restart your computer which should now be loading Windows properly."},{"id":"/2012/05/18/remove-the-hp-touchsmart-iq780a-case","metadata":{"permalink":"/2012/05/18/remove-the-hp-touchsmart-iq780a-case","source":"@site/blog/2012-05-18-remove-the-hp-touchsmart-iq780a-case.md","title":"How to remove the HP TouchSmart IQ780a case","description":"1. Face the back of the computer towards you.","date":"2012-05-18T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.275,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to remove the HP TouchSmart IQ780a case","layout":"post","permalink":"/misc/remove-the-hp-touchsmart-iq780a-case/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Load needed DLLs for Kernel","permalink":"/2012/05/18/load-needed-dlls-for-kernel"},"nextItem":{"title":"Reset nVidia Graphics Settings","permalink":"/2012/05/18/reset-nvidias-gfx-setting"}},"content":"1. Face the back of the computer towards you.\\n  2. **Removed** the **screw** above the power port\\n  3. **Removed** the **screw** right next to the IR Out\\n  4. **Hold** the side **casing** of the computer and **slide** it towards **you**\\n  5. You have now removed the casing and can access the Power Supply/DVD Drive etc."},{"id":"/2012/05/18/reset-nvidias-gfx-setting","metadata":{"permalink":"/2012/05/18/reset-nvidias-gfx-setting","source":"@site/blog/2012-05-18-reset-nvidias-gfx-setting.md","title":"Reset nVidia Graphics Settings","description":"1. Click Start","date":"2012-05-18T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.16,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Reset nVidia Graphics Settings","layout":"post","permalink":"/win/reset-nvidias-gfx-setting/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to remove the HP TouchSmart IQ780a case","permalink":"/2012/05/18/remove-the-hp-touchsmart-iq780a-case"},"nextItem":{"title":"Make your Windows XP Desktop Icons Background transparent.","permalink":"/2012/05/17/make-your-winxp-desktop-icons-transparent"}},"content":"1. Click **Start**\\n  2. Click **Programs**/All Programs\\n  3. Click **NVidia**\\n  4. Click Nvidia **Control** **Panel**\\n  5. Select **Manage** **3D** settings\\n  6. Click the **Global Settings** tab\\n  7. Click **Restore**\\n  8. Click Ok"},{"id":"/2012/05/17/make-your-winxp-desktop-icons-transparent","metadata":{"permalink":"/2012/05/17/make-your-winxp-desktop-icons-transparent","source":"@site/blog/2012-05-17-make-your-winxp-desktop-icons-transparent.md","title":"Make your Windows XP Desktop Icons Background transparent.","description":"1. Right Click an empty space on the Desktop","date":"2012-05-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.18,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Make your Windows XP Desktop Icons Background transparent.","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Reset nVidia Graphics Settings","permalink":"/2012/05/18/reset-nvidias-gfx-setting"},"nextItem":{"title":"Windows Media Player burning Audio as MP3 instead of Audio CD","permalink":"/2012/05/17/win-media-player-mp3-audio-cd"}},"content":"1. Right Click an empty space on the Desktop\\n  2. Click Properties\\n  3. Click the Desktop tab\\n  4. Click Customize Desktop\\n  5. Click on Desktop\\n  6. Uncheck Lock Desktop Items\\n  7. Click Ok\\n  8. Click Apply"},{"id":"/2012/05/17/win-media-player-mp3-audio-cd","metadata":{"permalink":"/2012/05/17/win-media-player-mp3-audio-cd","source":"@site/blog/2012-05-17-win-media-player-mp3-audio-cd.md","title":"Windows Media Player burning Audio as MP3 instead of Audio CD","description":"Trying to burn an Audio CD but Windows Media Player keeps burning them as Data MP3 disks? This is how you fix it.","date":"2012-05-17T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Media Player burning Audio as MP3 instead of Audio CD","layout":"post","permalink":"/win/win-media-player-mp3-audio-cd/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Make your Windows XP Desktop Icons Background transparent.","permalink":"/2012/05/17/make-your-winxp-desktop-icons-transparent"},"nextItem":{"title":"Diablo 3 Corrupted Update","permalink":"/2012/05/16/diablo-3-corrupted-update"}},"content":"Trying to burn an Audio CD but Windows Media Player keeps burning them as Data MP3 disks? This is how you fix it.\\n\\n  1. Open Windows Media Player\\n  2. Click the small little tab below Burn up the top and select Audio CD\\n  3. That&#8217;s it! Now drag the files onto the burn list on the right, burn and you are good to go!"},{"id":"/2012/05/16/diablo-3-corrupted-update","metadata":{"permalink":"/2012/05/16/diablo-3-corrupted-update","source":"@site/blog/2012-05-16-diablo-3-corrupted-update.md","title":"Diablo 3 Corrupted Update","description":"Trying to either install Diablo 3 or update it and you are getting an error &#8220;appears to be corrupt&#8221; that is preventing the game from installing or updating? \xa0This issue is usually caused by a speed decrease in the download speed of the file (usually\xa0occurred\xa0by either Blizzards servers or your own internet connection)_ try the following below to fix it.","date":"2012-05-16T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.67,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Diablo 3 Corrupted Update","permalink":"/misc/diablo-3-corrupted-update/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Windows Media Player burning Audio as MP3 instead of Audio CD","permalink":"/2012/05/17/win-media-player-mp3-audio-cd"},"nextItem":{"title":"Picasa 3 Not Burning CDs","permalink":"/2012/05/16/picasa-3-not-burning-cds"}},"content":"Trying to either install Diablo 3 or update it and you are getting an error &#8220;appears to be corrupt&#8221; that is preventing the game from installing or updating? \xa0This issue is usually caused by a speed decrease in the download speed of the file (usually\xa0occurred\xa0by either Blizzards servers or your own internet connection)_ try the following below to fix it.\\n\\n  1. First thing, **delete** the **corrupted** **update** file and resume your Diablo install/update (C:Program Files (x86)Diablo IIIUpdates)\\n\\n  1. Another option is to **install** the **game** in a\xa0**separate\xa0folder** or if possible\xa0hard drive\xa0, so instead of installing it in\xa0C:Program Files (x86) install it to C:Diablo III\\n\\nIf you still get the same issue it has been reported that removing the update file and trying again till you get a better server connection has worked but not\xa0guaranteed._"},{"id":"/2012/05/16/picasa-3-not-burning-cds","metadata":{"permalink":"/2012/05/16/picasa-3-not-burning-cds","source":"@site/blog/2012-05-16-picasa-3-not-burning-cds.md","title":"Picasa 3 Not Burning CDs","description":"First \xa0you need to uninstall Picasa, follow the guide below to do this:","date":"2012-05-16T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.585,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Picasa 3 Not Burning CDs","permalink":"/win/picasa-3-not-burning-cds/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Diablo 3 Corrupted Update","permalink":"/2012/05/16/diablo-3-corrupted-update"},"nextItem":{"title":"Set VLC to disable Subtitles by default","permalink":"/2012/05/16/set-vlc-to-disable-subtitles-by-default"}},"content":"First \xa0you need to uninstall Picasa, follow the guide below to do this:\\n\\n  1. Click Start\\n  2. Click Control Panel\\n  3. Click Add or Remove Programs/Uninstall a Program\\n  4. Click Picasa and select Uninstall and when prompted select No to delete the database to avoid loss of your picture file locations.\\n\\nOnce the uninstall has completed you need to delete a DLL file so follow the steps below:\\n\\n  1. Click Start\\n  2. Click My Computer/Computer\\n  3. Navigate to: C:WindowsSystem32drivers\\n  4. Delete the &#8220;Pxhelp20.sys&#8221; file _(Make sure not to touch anything else)_.\\n  5. Now reinstall <a title=\\"Picasa\\" href=\\"http://picasa.google.com/\\" target=\\"_blank\\">Picasa</a>\\n  6. Restart your computer once Picasa has finished installing and you now should be able to burn a CD"},{"id":"/2012/05/16/set-vlc-to-disable-subtitles-by-default","metadata":{"permalink":"/2012/05/16/set-vlc-to-disable-subtitles-by-default","source":"@site/blog/2012-05-16-set-vlc-to-disable-subtitles-by-default.md","title":"Set VLC to disable Subtitles by default","description":"1. Open VideoLAN (VLC)","date":"2012-05-16T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.155,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Set VLC to disable Subtitles by default","permalink":"/misc/set-vlc-to-disable-subtitles-by-default/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Picasa 3 Not Burning CDs","permalink":"/2012/05/16/picasa-3-not-burning-cds"},"nextItem":{"title":"Diablo 3 Error 37","permalink":"/2012/05/15/diablo-3-error-37"}},"content":"1. Open VideoLAN (VLC)\\n  2. Click **Tools**\\n  3. Click **Preferences**\\n  4. Click **All**\\n  5. Click on **Input/Codecs**\\n  6. Look for **_Subtitle Track ID_**\\n  7. Set **ID** to \u201c****\u201d\\n  8. Press **Ok**"},{"id":"/2012/05/15/diablo-3-error-37","metadata":{"permalink":"/2012/05/15/diablo-3-error-37","source":"@site/blog/2012-05-15-diablo-3-error-37.md","title":"Diablo 3 Error 37","description":"Error 37_ occurs when the login servers for Diablo are at full capacity (occurring mainly on the Diablo 3 Launch day). To fix this issue, simply Try again until a spot has opened in the queue.","date":"2012-05-15T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.375,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Diablo 3 Error 37","permalink":"/osx/diablo-3-error-37/","tags":["Mac OSX","Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Set VLC to disable Subtitles by default","permalink":"/2012/05/16/set-vlc-to-disable-subtitles-by-default"},"nextItem":{"title":"Brother Drum Light Flashing","permalink":"/2012/05/14/brother-drum-light-flashing"}},"content":"**Error 37**_ occurs when the login servers for Diablo are at full capacity (occurring mainly on the Diablo 3 Launch day). To fix this issue, simply Try again until a spot has opened in the queue.\\n\\n**Error 317002**_ occurs when the game servers for Diablo 3 are down, either for maintenance\xa0or simply have crashed, the fix for this issue is Try again!! (Also check out Diablo 3 <a title=\\"Diablo 3 Server Status\\" href=\\"http://us.battle.net/d3/en/status\\" target=\\"_blank\\">Server Status</a>)"},{"id":"/2012/05/14/brother-drum-light-flashing","metadata":{"permalink":"/2012/05/14/brother-drum-light-flashing","source":"@site/blog/2012-05-14-brother-drum-light-flashing.md","title":"Brother Drum Light Flashing","description":"To repair this follow the guide below:","date":"2012-05-14T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.37,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Brother Drum Light Flashing","permalink":"/misc/brother-drum-light-flashing/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Diablo 3 Error 37","permalink":"/2012/05/15/diablo-3-error-37"},"nextItem":{"title":"Black Screen of Death Windows Vista/7 NETIO.SYS","permalink":"/2012/05/14/bsod-win-vista7-netio-sys"}},"content":"To repair this follow the guide below:\\n\\n  1. Open the Brother tray door(where you insert the Toner)\\n  2. Press and hold the Go button, until all the Lights are on.\\n  3. Release Go button and close the tray door\\n\\n_Some people have also reported that with some models you actually have to remove the Toner unit before the reset will work, so if the above doesn&#8217;t work try again with the Toner unit out._"},{"id":"/2012/05/14/bsod-win-vista7-netio-sys","metadata":{"permalink":"/2012/05/14/bsod-win-vista7-netio-sys","source":"@site/blog/2012-05-14-bsod-win-vista7-netio-sys.md","title":"Black Screen of Death Windows Vista/7 NETIO.SYS","description":"Caused when your network driver has become corrupted or changed for NetBIOs with an incompatible version from either Windows Updates or a filter from your Firewall (most commonly Zone Alarm).","date":"2012-05-14T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":2.045,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Black Screen of Death Windows Vista/7 NETIO.SYS","permalink":"/win/bsod-win-vista7-netio-sys/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Brother Drum Light Flashing","permalink":"/2012/05/14/brother-drum-light-flashing"},"nextItem":{"title":"How to create a mapped Network drive in Windows 7","permalink":"/2012/05/11/create-a-mapped-network-drive-in-win-7"}},"content":"Caused when your network driver has become corrupted or changed for NetBIOs with an incompatible version from either Windows Updates or a filter from your Firewall (most commonly Zone Alarm).\\n\\nYou can \u201crepair\u201d this by using System Restore to go back to a previous driver, follow the guide below to do this:\\n\\n  1. Restart your computer\\n  2. While it is starting up and just before the computer starts to load Windows continuously tap F8.\\n  3. A text-based menu will appear and using the Arrow Keys and the Enter key select Repair My Computer (up the top of the menu)\\n  4. Now it will prompt with your install selection, choose your Windows install and User Account and press Ok\\n  5. It will now come up with a dialog Window displaying a few options, click System Restore (it will take a while to actually load)\\n  6. Select Next\\n  7. Choose a valid System Restore point (the most recent while the computer was going is the best bet)\\n  8. Select Next and Ok\\n  9. Wait for the restore to finish and restart your computer. It should now be going (If it does not work, you might have to go to a System Restore point more previous then the one you used).\\n\\n\\n<div>\\n  If System Restore cannot find the Windows operating system, you need to run a file system check called a CHKDSK. \xa0Follow the same prompts as above till Step 5, but instead of pressing System Restore, press Command Prompt. Type in: chkdsk /r /f c: and press Enter.\\n</div>\\n\\n<div>\\n  This is also caused by the Quality of Service network protocol in Windows.\\n</div>\\n\\n<div>\\n  Assuming you have a bootable Windows install follow the instructions below:\\n</div>\\n\\nClick Start\\n\\n  1. Click Control Panel\\n  2. Click Network & Sharing Center\\n  3. Click Manage Network Connection (on the left hand side menu)\\n  4. Right click on the Local Area Connection\\n  5. Left click Properties\\n  6. You will now have a list of protocols and services related to your networking device, leave the rest alone and uncheck QoS\xa0 Packet Scheduler\\n  7. Click Ok\\n\\nNote: If you have any valid Firewalls such as Zone Alarm installed, your best bet is once you have regained access to Windows is to uninstall it, the Windows Firewall does a good enough job and you do not need a third party one.\\n\\nNote: Also make sure you are running the latest Windows service pack which helps reduce this issue."},{"id":"/2012/05/11/create-a-mapped-network-drive-in-win-7","metadata":{"permalink":"/2012/05/11/create-a-mapped-network-drive-in-win-7","source":"@site/blog/2012-05-11-create-a-mapped-network-drive-in-win-7.md","title":"How to create a mapped Network drive in Windows 7","description":"Follow the instructions to create a mapped network drive in Windows 7 without windows indexing like a local folder?","date":"2012-05-11T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.825,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to create a mapped Network drive in Windows 7","permalink":"/win/create-a-mapped-network-drive-in-win-7/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Black Screen of Death Windows Vista/7 NETIO.SYS","permalink":"/2012/05/14/bsod-win-vista7-netio-sys"},"nextItem":{"title":"You have exceeded your profile space in Windows XP","permalink":"/2012/05/11/exceeded-your-profile-space-in-win-xp"}},"content":"Follow the instructions to create a mapped network drive in Windows 7 without windows indexing like a local folder?\\n\\n  1. Create a new folder for your network shares (ie c:Shared)\\n  2. Go into the folder you created and make a new folder\xa0\xa0(ie c:SharedMovies)\\n  3. Now you need to link the library into the folder you create in the second step\xa0\xa0(ie c:SharedMovies) by going into Documents and up the top click &#8220;Includes: 2 locations&#8221; and click Add and add the folder.\\n  4. Now go back to your original folder you created\xa0\xa0(ie c:Shared) and delete the folder you made in the second step\xa0(ie c:SharedMovies)\\n  5. Now we make a link to link the folder to the remote folder and to do that we need the Command Prompt, click Start, Run type: cmd and press Enter\\n  6. Type in:\xa0mklink /d c:Shared\xa0remotemachinefolder and press Enter\\n  7. Now your Documents folder should include your remote folder mapped through the c:Shared folder you created earlier like it is a normal folder."},{"id":"/2012/05/11/exceeded-your-profile-space-in-win-xp","metadata":{"permalink":"/2012/05/11/exceeded-your-profile-space-in-win-xp","source":"@site/blog/2012-05-11-exceeded-your-profile-space-in-win-xp.md","title":"You have exceeded your profile space in Windows XP","description":"You need to make a registry file and load it to fix the issues causing this problem, follow the instructions below:","date":"2012-05-11T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"You have exceeded your profile space in Windows XP","permalink":"/win/exceeded-your-profile-space-in-win-xp/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to create a mapped Network drive in Windows 7","permalink":"/2012/05/11/create-a-mapped-network-drive-in-win-7"},"nextItem":{"title":"Microsoft Security Essentials won&#8217;t install","permalink":"/2012/05/11/mse-wont-install"}},"content":"You need to make a registry file and load it to fix the issues causing this problem, follow the instructions below:\\n\\n  1. Click Start\\n  2. Click Programs\\n  3. Click Accessories and click Notepad\\n  4. Copy and paste the following:\\n\\n    Windows Registry Editor Version 5.00\\n\\n    [HKEY_CURRENT_USERSoftwareMicrosoftWindowsCurrentVersionPoliciesSystem] &#8220;EnableProfileQuota&#8221;=-      &#8220;ProfileQuotaMessage&#8221;=- &#8220;MaxProfileSize&#8221;=-&#8220;IncludeRegInProQuota&#8221;=- &#8220;WarnUser&#8221;=- &#8220;WarnUserTimeout&#8221;=-\\n\\n\\n  1. Click File\\n  2. Click Save\\n  3. Save as: fix.reg\\n  4. Run the fix.reg\\n  5. Restart your computer"},{"id":"/2012/05/11/mse-wont-install","metadata":{"permalink":"/2012/05/11/mse-wont-install","source":"@site/blog/2012-05-11-mse-wont-install.md","title":"Microsoft Security Essentials won&#8217;t install","description":"Having problems and your trying to install Microsoft Security Essentials and it just won&#8217;t install? I have ran into this issue and this is how I repaired it.","date":"2012-05-11T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Microsoft Security Essentials won&#8217;t install","permalink":"/win/mse-wont-install/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"You have exceeded your profile space in Windows XP","permalink":"/2012/05/11/exceeded-your-profile-space-in-win-xp"},"nextItem":{"title":"No Sound in Adobe Flash Videos","permalink":"/2012/05/11/no-sound-in-adobe-flash-vids"}},"content":"Having problems and your trying to install Microsoft Security Essentials and it just won&#8217;t install? I have ran into this issue and this is how I repaired it.\\n\\n  1. First, you need to download the <a title=\\"Windows Installer Cleanup Utility\\" href=\\"http://majorgeeks.com/download.php?det=4459\\" target=\\"_blank\\">Windows Installer Cleanup Utility</a>\\n  2. Run the utility and select Microsoft Antimalware\\n  3. Click Remove\\n  4. Click Ok then restart the computer\\n  5. Attempt Security Essentials install."},{"id":"/2012/05/11/no-sound-in-adobe-flash-vids","metadata":{"permalink":"/2012/05/11/no-sound-in-adobe-flash-vids","source":"@site/blog/2012-05-11-no-sound-in-adobe-flash-vids.md","title":"No Sound in Adobe Flash Videos","description":"1. Click Start","date":"2012-05-11T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.115,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"No Sound in Adobe Flash Videos","permalink":"/win/no-sound-in-adobe-flash-vids/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Microsoft Security Essentials won&#8217;t install","permalink":"/2012/05/11/mse-wont-install"},"nextItem":{"title":"Speedometer for your Android Mobile","permalink":"/2012/05/10/android-speedo-app"}},"content":"1. Click Start\\n  2. Click Run\\n  3. Type in:\xa0 cmd.exe\\n  4. Press Enter\\n  5. Type: sfc /scannow\\n\\n_Wait for scan to complete, you may need your Windows CD/DVD for this if it needs to copy data from it._\\n\\nIf the above did not work, try the following below:\\n\\n  1. \xa0Open Notepad (Start, Programs, Accessories, Notepad)\\n  2. Select the text below and right click it and choose Copy.\\n\\n_Windows Registry Editor Version 5.00_\\n\\n_[HKEY\\\\LOCAL\\\\MACHINE\\\\SOFTWARE\\\\Microsof\\\\tWindows NT\\\\CurrentVersion\\\\Drivers32]_\\n\\n_&#8220;midimapper&#8221;=&#8221;midimap.dll&#8221;_\\n\\n_&#8220;msacm.imaadpcm&#8221;=&#8221;imaadp32.acm&#8221;_\\n\\n_&#8220;msacm.msadpcm&#8221;=&#8221;msadp32.acm&#8221;_\\n\\n_&#8220;msacm.msg711&#8243;=&#8221;msg711.acm&#8221;_\\n\\n_&#8220;msacm.msgsm610&#8243;=&#8221;msgsm32.acm&#8221;_\\n\\n_&#8220;msacm.trspch&#8221;=&#8221;tssoft32.acm&#8221;_\\n\\n_&#8220;vidc.cvid&#8221;=&#8221;iccvid.dll&#8221;_\\n\\n_&#8220;VIDC.I420&#8243;=&#8221;i420vfw.dll&#8221;_\\n\\n_&#8220;vidc.iv31&#8243;=&#8221;ir32_32.dll&#8221;_\\n\\n_&#8220;vidc.iv32&#8243;=&#8221;ir32_32.dll&#8221;_\\n\\n_&#8220;vidc.iv41&#8243;=&#8221;ir41_32.ax&#8221;_\\n\\n_&#8220;VIDC.IYUV&#8221;=&#8221;iyuv_32.dll&#8221;_\\n\\n_&#8220;vidc.mrle&#8221;=&#8221;msrle32.dll&#8221;_\\n\\n_&#8220;vidc.msvc&#8221;=&#8221;msvidc32.dll&#8221;_\\n\\n_&#8220;VIDC.YVYU&#8221;=&#8221;msyuv.dll&#8221;_\\n\\n_&#8220;wavemapper&#8221;=&#8221;msacm32.drv&#8221;_\\n\\n_&#8220;msacm.msg723&#8243;=&#8221;msg723.acm&#8221;_\\n\\n_&#8220;vidc.M263&#8243;=&#8221;msh263.drv&#8221;_\\n\\n_&#8220;vidc.M261&#8243;=&#8221;msh261.drv&#8221;_\\n\\n_&#8220;msacm.msaudio1&#8243;=&#8221;msaud32.acm&#8221;_\\n\\n_&#8220;msacm.sl\\\\_anet&#8221;=&#8221;sl\\\\_anet.acm&#8221;_\\n\\n_&#8220;msacm.iac2&#8243;=&#8221;C:WINDOWSsystem32iac25_32.ax&#8221;_\\n\\n_&#8220;vidc.iv50&#8243;=&#8221;ir50_32.dll&#8221;_\\n\\n_&#8220;wave&#8221;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;midi&#8221;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;mixer&#8221;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;VIDC.WMV3&#8243;=&#8221;wmv9vcm.dll&#8221;_\\n\\n_&#8220;VIDC.VP40&#8243;=&#8221;vp4vfw.dll&#8221;_\\n\\n_&#8220;msacm.voxacm160&#8243;=&#8221;vct3216.acm&#8221;_\\n\\n_&#8220;MSVideo&#8221;=&#8221;vfwwdm32.dll&#8221;_\\n\\n_&#8220;MSVideo8&#8243;=&#8221;VfWWDM32.dll&#8221;_\\n\\n_&#8220;wave1&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;midi1&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;mixer1&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;aux&#8221;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;vidc.VP70&#8243;=&#8221;vp7vfw.dll&#8221;_\\n\\n_&#8220;vidc.X264&#8243;=&#8221;x264vfw.dll&#8221;_\\n\\n_&#8220;VIDC.FPS1&#8243;=&#8221;frapsvid.dll&#8221;_\\n\\n_&#8220;vidc.VP60&#8243;=&#8221;vp6vfw.dll&#8221;_\\n\\n_&#8220;vidc.VP61&#8243;=&#8221;vp6vfw.dll&#8221;_\\n\\n_&#8220;vidc.VP62&#8243;=&#8221;vp6vfw.dll&#8221;_\\n\\n_&#8220;vidc.DIVX&#8221;=&#8221;DivX.dll&#8221;_\\n\\n_&#8220;VIDC.UYVY&#8221;=&#8221;msyuv.dll&#8221;_\\n\\n_&#8220;VIDC.YUY2&#8243;=&#8221;msyuv.dll&#8221;_\\n\\n_&#8220;VIDC.YVU9&#8243;=&#8221;tsbyuv.dll&#8221;_\\n\\n_&#8220;VIDC.DRAW&#8221;=&#8221;DVIDEO.DLL&#8221;_\\n\\n_&#8220;VIDC.YV12&#8243;=&#8221;yv12vfw.dll&#8221;_\\n\\n_&#8220;wave2&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;midi2&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;mixer2&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;aux1&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;wave3&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;midi3&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;mixer3&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;aux2&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;VIDC.MSUD&#8221;=&#8221;msulvc05.dll&#8221;_\\n\\n_&#8220;wave4&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;midi4&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;mixer4&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\n_&#8220;aux3&#8243;=&#8221;wdmaud.drv&#8221;_\\n\\nClick File and Save As\\n3. Save as: registry.reg\\n4. Save it as missing.reg\\n5. Run: missing.reg\\n\\nIf the above did not work, try the following below:\\n\\nAnother issue is due to some Wavemapper windows registry files. You need to open Registry Editor.\\n\\n  1. Click Start\\n  2. Click Run\\n  3. Type: regedit\\n  4. Press Enter\\n  5. Navigate to:\xa0 HKEY\\\\LOCAL\\\\MACHINE\\\\SOFTWAR\\\\EMicrosof\\\\tWindows NT\\\\CurrentVersion\\\\Drivers32\\n  6. Click on My Computer and on the right hand side click New/String Value and type: wavemapper then press Enter and type: msacm32.drv\\n  7. Click Ok and restart machine."},{"id":"/2012/05/10/android-speedo-app","metadata":{"permalink":"/2012/05/10/android-speedo-app","source":"@site/blog/2012-05-10-android-speedo-app.md","title":"Speedometer for your Android Mobile","description":"Need a Speedometer with\xa0Acceleration and GPS info? The\xa0Sub Speedo app by AutoRad Industries is what you are after. This is how you can\xa0acquire\xa0this little nifty app.","date":"2012-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.625,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Speedometer for your Android Mobile","permalink":"/mob/android-speedo-app/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"No Sound in Adobe Flash Videos","permalink":"/2012/05/11/no-sound-in-adobe-flash-vids"},"nextItem":{"title":"iTunes could not connect to the iPod Because It is locked with a Passcode.","permalink":"/2012/05/10/itunes-could-not-connect-to-the-ipod-passcode"}},"content":"Need a Speedometer with\xa0Acceleration and GPS info? The\xa0Sub Speedo app by AutoRad Industries is what you are after. This is how you can\xa0acquire\xa0this little nifty app.\\n\\n**Using your PC**\\n\\n  1. Click on this\xa0<a title=\\"Google Play - Sub Speedo\\" href=\\"https://play.google.com/store/apps/details?id=autorad.subspeedopaid\\" target=\\"_blank\\">Google Play</a>\xa0link\\n  2. To the left, click Buy\xa0(at time of writing it costs $2.70NZD)\\n  3. Enter in your payment details, click Ok\\n  4. Choose the Android device you want it installed to\\n  5. Your Android device will then start downloading the app immediately.\\n\\n**Using Android**\\n\\n  1. Click on Google Play (previously Android Market)\\n  2. Search for: Sub Speedo\\n  3. Select the Sub Speedo (written by: AutoRad Industries)\\n  4. Click Install/Buy\xa0(at time of writing it costs $2.70NZD)\\n\\n  At time of writing there is currently no idevice app\xa0available."},{"id":"/2012/05/10/itunes-could-not-connect-to-the-ipod-passcode","metadata":{"permalink":"/2012/05/10/itunes-could-not-connect-to-the-ipod-passcode","source":"@site/blog/2012-05-10-itunes-could-not-connect-to-the-ipod-passcode.md","title":"iTunes could not connect to the iPod Because It is locked with a Passcode.","description":"1. Turn the iPod off","date":"2012-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.41,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"iTunes could not connect to the iPod Because It is locked with a Passcode.","permalink":"/osx/itunes-could-not-connect-to-the-ipod-passcode/","tags":["Mac OSX","Windows"]},"unlisted":false,"prevItem":{"title":"Speedometer for your Android Mobile","permalink":"/2012/05/10/android-speedo-app"},"nextItem":{"title":"Windows XP to format USB Flash drives as NTFS","permalink":"/2012/05/10/t-win-xp-format-usbdrive-ntfs"}},"content":"1. **Turn** the iPod **off**\\n  2. **Start** **iTunes**\\n  3. **Plug** the **iPod** USB cable into the **computer**\\n  4. **Press** and **hold** the **Home** button down on the iPod forcing the\xa0iPod\xa0to turn on.\\n  5. The iPod will then **boot** into **Recovery** **Mode** and iTunes will prompt with a dialog &#8220;iTunes has discovered an iPod in Recovery Mode, would you like to restore?&#8221;\\n  6. Click Yes\\n  7. Wait for the restore to finish then the iPod will restart and load a freshly updated iOS"},{"id":"/2012/05/10/t-win-xp-format-usbdrive-ntfs","metadata":{"permalink":"/2012/05/10/t-win-xp-format-usbdrive-ntfs","source":"@site/blog/2012-05-10-t-win-xp-format-usbdrive-ntfs.md","title":"Windows XP to format USB Flash drives as NTFS","description":"Windows XP to format USB Flash drives as NTFS","date":"2012-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.255,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows XP to format USB Flash drives as NTFS","permalink":"/win/t-win-xp-format-usbdrive-ntfs/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"iTunes could not connect to the iPod Because It is locked with a Passcode.","permalink":"/2012/05/10/itunes-could-not-connect-to-the-ipod-passcode"},"nextItem":{"title":"How to install a Wifi Max Wifi dongle in Windows Vista","permalink":"/2012/05/10/wifimax-vista"}},"content":"Windows XP to format USB Flash drives as NTFS\\n\\n1. Right click My Computer\\n  \\n2. Click Properties\\n  \\n3. Click Hardware\\n  \\n4. Click Device Manager\\n  \\n5. Click Disk Drives and right click your USB device.\\n  \\n6. Click Properties\\n  \\n7. Click Policies\\n  \\n8. Click Optimize for performance.\\n  \\n9. Click Apply\\n  \\n10. Restart Computer"},{"id":"/2012/05/10/wifimax-vista","metadata":{"permalink":"/2012/05/10/wifimax-vista","source":"@site/blog/2012-05-10-wifimax-vista.md","title":"How to install a Wifi Max Wifi dongle in Windows Vista","description":"How to install a Wifi Max Wifi dongle in Windows Vista","date":"2012-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.955,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install a Wifi Max Wifi dongle in Windows Vista","permalink":"/win/wifimax-vista/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Windows XP to format USB Flash drives as NTFS","permalink":"/2012/05/10/t-win-xp-format-usbdrive-ntfs"},"nextItem":{"title":"Windows 3.1 Nokia n95 Controls","permalink":"/2012/05/10/win-3-1-nokia-n95-controls"}},"content":"How to install a Wifi Max Wifi dongle in Windows Vista\\n\\n  1. Download the latest <a title=\\"WifiMax Vista Drivers\\" href=\\"http://uk.codejunkies.com/support_downloads/zydas_vista_compat.zip\\" target=\\"_blank\\">drivers</a>\\n  2. Run the file and go through the install.\\n  3. Once installation has completed plug the Wi-Fi dongle into a USB port\\n\\n_If you have already plugged the WifiMax dongle in before running the driver software then the WifiMax dongle will have been installed using Vistas default wireless driver. To repair this, follow the instructions below:_\\n\\n  1. _Right click Computer_\\n  2. _Select Properties_\\n  3. _Click the Hardware Tab_\\n  4. _Select Device Manager_\\n  5. _In the list, select Network Adaptors and you should see an Atheros device._\\n  6. _Right click the Atheros device and select Properties_\\n  7. _Click Driver and select Uninstall (make sure Delete the driver software for this device is checked)._\\n  8. _Now go down to Other Devices (still in Device Manager)_\\n  9. _Right click the USB 2.0 WLA device_\\n 10. _Select Update Driver and choose Browse My Computer For Driver_\\n 11. _Browse to the path that the drivers from the previous step were installed on, usually: C:\\\\Program Files\\\\ZyDAS_\\n\\n _It should then find the correct driver and install it._"},{"id":"/2012/05/10/win-3-1-nokia-n95-controls","metadata":{"permalink":"/2012/05/10/win-3-1-nokia-n95-controls","source":"@site/blog/2012-05-10-win-3-1-nokia-n95-controls.md","title":"Windows 3.1 Nokia n95 Controls","description":"Running Windows 3.1 on your Nokia phone in DOSBox and forgot how to control it? The guide is below:","date":"2012-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":1.135,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows 3.1 Nokia n95 Controls","permalink":"/mob/win-3-1-nokia-n95-controls/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"How to install a Wifi Max Wifi dongle in Windows Vista","permalink":"/2012/05/10/wifimax-vista"},"nextItem":{"title":"Windows Thumbnails not Displaying","permalink":"/2012/05/10/windows-thumb-not-displaying"}},"content":"Running Windows 3.1 on your Nokia phone in DOSBox and forgot how to control it? The guide is below:\\n\\nUsing the T9 Keypad\\n\\nReceive Call button -Switching between input modes\\n  \\nEnd Call button &#8211; Exit from DOSBox\\n  \\nPen key\xa0&#8211;\xa0 Shift\\n  \\nDelete key \xa0&#8211;\xa0\xa0Backspace\\n  \\nThumb pad \xa0&#8211;\xa0\xa0Arrows and Enter\\n  \\nRight key \xa0&#8211;\xa0\xa0Escape\\n  \\nLeft key &#8211; Space\\n\\nMode 1:\\n\\nFirst line keys = a d g j m p t w space\\n  \\nFirst line keys + Pen =: A D G J M P T W space\\n  \\nSecond line keys + * = b e h k n q u x .\\n  \\nSecond line keys + \\\\* = Pen + \\\\* B E H K N Q U X\\n  \\nThird line keys + # = c f i l o r v y\\n  \\nThird line keys + Pen + # = C F I L O R V Y |\\n  \\nFourth line keys + 1 = s z\\n  \\nFourth line keys + Pen + 1 = S Z\\n\\nMode 2:\\n\\nNumbers = 1 2 3 4 5 6 7 8 9 0\\n  \\nNumbers + Pen = ! @ # $ % ^ & * ( )\\n\\nMode 3:\\n\\nThumb Pad &#8211; Moving mouse\\n  \\n* = Right mouse button\\n  \\n\\\\# = Left mouse button\\n\\n&nbsp;\\n\\nWant to actually\xa0install\xa0Windows 3.1 on your phone? Follow &#8220;<a title=\\"Windows-31-on-Symbian\\" href=\\"http://www.instructables.com/id/How-to-install-Windows-31-on-Symbian-S60/\\" target=\\"_blank\\">this</a>&#8221; guide from Instructables."},{"id":"/2012/05/10/windows-thumb-not-displaying","metadata":{"permalink":"/2012/05/10/windows-thumb-not-displaying","source":"@site/blog/2012-05-10-windows-thumb-not-displaying.md","title":"Windows Thumbnails not Displaying","description":"Click Start","date":"2012-05-10T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.135,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Thumbnails not Displaying","permalink":"/win/windows-thumb-not-displaying/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Windows 3.1 Nokia n95 Controls","permalink":"/2012/05/10/win-3-1-nokia-n95-controls"},"nextItem":{"title":"Fedora \u201cNo More Mirrors\u201d error","permalink":"/2012/04/26/fedora-no-more-mirrors-error"}},"content":"Click Start\\n  \\nClick Run\\n  \\nType: **_regsvr32 shdocvw.dll_**\\n  \\nPress Enter\\n  \\nClick Start\\n  \\nClick Run\\n  \\nType: **_regsvr32 thumbvw.dll_**\\n  \\nPress Enter\\n  \\nClick Start\\n  \\nClick Run\\n  \\nType: **_regsvr32 shimgvw.dll_**\\n  \\nPress Enter"},{"id":"/2012/04/26/fedora-no-more-mirrors-error","metadata":{"permalink":"/2012/04/26/fedora-no-more-mirrors-error","source":"@site/blog/2012-04-26-fedora-no-more-mirrors-error.md","title":"Fedora \u201cNo More Mirrors\u201d error","description":"1. Open a Terminal","date":"2012-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Fedora \u201cNo More Mirrors\u201d error","permalink":"/linux/fedora-no-more-mirrors-error/","tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Windows Thumbnails not Displaying","permalink":"/2012/05/10/windows-thumb-not-displaying"},"nextItem":{"title":"How to remove Java from OSX","permalink":"/2012/04/26/how-to-remove-java-from-osx"}},"content":"1. Open a Terminal\\n  2. Type:\xa0 **yum clean all** (Press Enter)\\n  3. Type: **yum check-update** (Press Enter)\\n  4. Now attempt to install the package you were trying to install again."},{"id":"/2012/04/26/how-to-remove-java-from-osx","metadata":{"permalink":"/2012/04/26/how-to-remove-java-from-osx","source":"@site/blog/2012-04-26-how-to-remove-java-from-osx.md","title":"How to remove Java from OSX","description":"1. Open Terminal (/Utilities/Terminal)","date":"2012-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.08,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to remove Java from OSX","permalink":"/osx/how-to-remove-java-from-osx/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Fedora \u201cNo More Mirrors\u201d error","permalink":"/2012/04/26/fedora-no-more-mirrors-error"},"nextItem":{"title":"How to Normalize Audio in VLC Media Player","permalink":"/2012/04/26/normalize-audio-vlc"}},"content":"1. Open Terminal (/Utilities/Terminal)\\n  2. Type in:\xa0**sudo rm -rf /System/Library/Frameworks/JavaVM.framework/**\\n  3. Press Enter\\n\\nTested in 10.5.8"},{"id":"/2012/04/26/normalize-audio-vlc","metadata":{"permalink":"/2012/04/26/normalize-audio-vlc","source":"@site/blog/2012-04-26-normalize-audio-vlc.md","title":"How to Normalize Audio in VLC Media Player","description":"1. Open\xa0\xa0VLC Media Player","date":"2012-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.41,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Normalize Audio in VLC Media Player","permalink":"/win/normalize-audio-vlc/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to remove Java from OSX","permalink":"/2012/04/26/how-to-remove-java-from-osx"},"nextItem":{"title":"Skryim crashes when making new game Fix","permalink":"/2012/04/26/skryim-crashes-when-making-new-game-fix"}},"content":"1. Open\xa0\xa0VLC Media Player\\n  2. Click **Tools**\\n  3. **Preferences**\\n  4. Select **Audio** on the left hand settings pane.\\n  5. Check \u201c**_Normalize Volume_**\u201d box\\n  6. **Set** the normalize volume to: **_1.6_**\\n  7. Click **Show All Settings** down the bottom of the dialog.\\n  8. Click **Audio** and select **Filters**\\n  9. Check **_Volume Normalizer_**\\n 10. Click **Filters** and click **_Volume Normalizer_**\\n 11. Set Number of **Audio** Buffers to: **_10_** and set the Maximal **Volume** Level to** _1.6_**\\n 12. Click Save\\n 13. **Restart** VLC Media Player"},{"id":"/2012/04/26/skryim-crashes-when-making-new-game-fix","metadata":{"permalink":"/2012/04/26/skryim-crashes-when-making-new-game-fix","source":"@site/blog/2012-04-26-skryim-crashes-when-making-new-game-fix.md","title":"Skryim crashes when making new game Fix","description":"Having issues with Skyrim, when you attempt to make a new game and it crashes? This is how I fixed it.","date":"2012-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.335,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Skryim crashes when making new game Fix","permalink":"/win/skryim-crashes-when-making-new-game-fix/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to Normalize Audio in VLC Media Player","permalink":"/2012/04/26/normalize-audio-vlc"},"nextItem":{"title":"How to Print life Sized Template From Google Sketchup","permalink":"/2012/04/20/print-life-sized-template-google-sketchup"}},"content":"Having issues with Skyrim, when you attempt to make a new game and it crashes? This is how I fixed it.\\n\\n  1. Click Start\\n  2. Click Control Panel\\n  3. Click Sound (Hardware and Sound)\\n  4. Click on the Sound tab up the top.\\n  5. Select your default Sound playback device and click Properties.\\n  6. Click on Advanced\\n  7. Set the Default Format to:\xa044.1khz 16bit\\n  8. Click Ok"},{"id":"/2012/04/20/print-life-sized-template-google-sketchup","metadata":{"permalink":"/2012/04/20/print-life-sized-template-google-sketchup","source":"@site/blog/2012-04-20-print-life-sized-template-google-sketchup.md","title":"How to Print life Sized Template From Google Sketchup","description":"Make sure that the Sketchup Camera mode is set to Parallel Projection and make sure that the all the standard Camera views are displayed (without these set right Google Sketchup will not print to scale).","date":"2012-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Print life Sized Template From Google Sketchup","permalink":"/misc/print-life-sized-template-google-sketchup/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Skryim crashes when making new game Fix","permalink":"/2012/04/26/skryim-crashes-when-making-new-game-fix"},"nextItem":{"title":"Repair Windows XP Activation","permalink":"/2012/04/20/repair-windows-xp-activation"}},"content":"_Make sure that the Sketchup Camera mode is set to Parallel Projection and make sure that the all the standard Camera views are displayed (without these set right Google Sketchup will not print to scale)._\\n\\n  1. Click Print\\n  2. Uncheck Fit to Page\\n  3. Uncheck Use Modem Extents\\n  4. Set the scale so that 1 inch equals the inches in your Sketchup Project.\\n  5. Print"},{"id":"/2012/04/20/repair-windows-xp-activation","metadata":{"permalink":"/2012/04/20/repair-windows-xp-activation","source":"@site/blog/2012-04-20-repair-windows-xp-activation.md","title":"Repair Windows XP Activation","description":"1. Click Start","date":"2012-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.195,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Repair Windows XP Activation","permalink":"/win/repair-windows-xp-activation/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to Print life Sized Template From Google Sketchup","permalink":"/2012/04/20/print-life-sized-template-google-sketchup"},"nextItem":{"title":"How to set Gmail to be your default Mail client in Firefox","permalink":"/2012/04/13/gmail-default-mail-client-in-firefox"}},"content":"1. Click Start\\n  2. Click Run\\n  3. Type in:\xa0**_regsvr32.exe regwizc.dll_**\\n  4. Press Enter\\n  5. Click Start\\n  6. Click Run\\n  7. Type in:\xa0**_regsvr32.exe licdll.dll_**\\n  8. Press Enter\\n  9. Click Start\\n 10. Click Run\\n 11. Type in:\xa0**_%systemroot%system32oobemsoobe.exe /a_**\\n 12. Press Enter"},{"id":"/2012/04/13/gmail-default-mail-client-in-firefox","metadata":{"permalink":"/2012/04/13/gmail-default-mail-client-in-firefox","source":"@site/blog/2012-04-13-gmail-default-mail-client-in-firefox.md","title":"How to set Gmail to be your default Mail client in Firefox","description":"If you have Gmail and use the Firefox web browser, having gmail as your default email client for typing out emails will be handy and save alot of time. Here is how you do it.","date":"2012-04-13T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to set Gmail to be your default Mail client in Firefox","permalink":"/misc/gmail-default-mail-client-in-firefox/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"Repair Windows XP Activation","permalink":"/2012/04/20/repair-windows-xp-activation"},"nextItem":{"title":"Remove NOD32 Password","permalink":"/2012/04/13/remove-nod32-password"}},"content":"If you have Gmail and use the Firefox web browser, having gmail as your default email client for typing out emails will be handy and save alot of time. Here is how you do it.\\n\\n  1. Open Firefox\\n  2. Left click Tools\\n  3. Left click Options\\n  4. Left Click Applications\\n  5. Go down the list until you find &#8220;mailto&#8221;\\n  6. In the drop down menu to the right of &#8220;mailto&#8221; click Use Gmail\\n  7. Click Ok\\n  8. Restart Firefox and whenever you click a Mailto link you should now be redirected to Gmail&#8217;s compose page!"},{"id":"/2012/04/13/remove-nod32-password","metadata":{"permalink":"/2012/04/13/remove-nod32-password","source":"@site/blog/2012-04-13-remove-nod32-password.md","title":"Remove NOD32 Password","description":"Follow the steps below to remove your Nod32 password, useful when trying to uninstall and cannot remember what it is.","date":"2012-04-13T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.245,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Remove NOD32 Password","permalink":"/win/remove-nod32-password/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to set Gmail to be your default Mail client in Firefox","permalink":"/2012/04/13/gmail-default-mail-client-in-firefox"},"nextItem":{"title":"Handycafe Homepage Firefox","permalink":"/2012/04/10/handycafe-homepage-firefox"}},"content":"Follow the steps below to remove your Nod32 password, useful when trying to uninstall and cannot remember what it is.\\n\\n  1. Click Start\\n  2. Click Run\\n  3. Type: regedit\\n  4. Navigate to: HKEY\\\\_LOCAL\\\\_MACHINE\\\\SOFTWARE\\\\Eset\\\\Nod\\\\CurrentVersion\\n  5. Right click\xa0Package\xa0ID\xa0and select Delete\\n  6. Exit out of Registry Editor and attempt removal of Nod32"},{"id":"/2012/04/10/handycafe-homepage-firefox","metadata":{"permalink":"/2012/04/10/handycafe-homepage-firefox","source":"@site/blog/2012-04-10-handycafe-homepage-firefox.md","title":"Handycafe Homepage Firefox","description":"This usually occurs when HandyCafe uses Firefox permissions/Javascript security to lock the homepage, follow the prompts to repair.","date":"2012-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.44,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Handycafe Homepage Firefox","permalink":"/misc/handycafe-homepage-firefox/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Remove NOD32 Password","permalink":"/2012/04/13/remove-nod32-password"},"nextItem":{"title":"How to Insert Image into Gmail Post","permalink":"/2012/04/10/insert-image-into-gmail-post"}},"content":"This usually occurs when HandyCafe uses Firefox permissions/Javascript security to lock the homepage, follow the prompts to repair.\\n\\n  1. First you need to open your Firefox profile folder. Open Firefox.\\n  2. Click Help\\n  3. Click Troubleshooting Information\\n  4. Click on Open Containing Folder, under the Profile section.\\n  5. Close Firefox\\n  6. Right click user.js and choose Open With, Notepad.\\n  7. The safest bet to remove all locked security options is to select Edit, Select All and delete all the content in the document.\\n  8. Save and close Notepad"},{"id":"/2012/04/10/insert-image-into-gmail-post","metadata":{"permalink":"/2012/04/10/insert-image-into-gmail-post","source":"@site/blog/2012-04-10-insert-image-into-gmail-post.md","title":"How to Insert Image into Gmail Post","description":"1. Login to Gmail","date":"2012-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Insert Image into Gmail Post","permalink":"/misc/insert-image-into-gmail-post/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Handycafe Homepage Firefox","permalink":"/2012/04/10/handycafe-homepage-firefox"},"nextItem":{"title":"Laptop Typing Numbers instead of Letters Fix","permalink":"/2012/04/10/laptop-numbers-instead-of-letters-fix"}},"content":"1. Login to <a title=\\"Gmail\\" href=\\"http://www.gmail.com\\" target=\\"_blank\\">Gmail</a>\\n  2. Click on the Gear (Underneath your profile picture to the right)\\n  3. Click Settings\\n  4. Click Labs\\n  5. Click\xa0Inserting Images\\n  6. Select Enable\\n  7. Select Save Changes (down the bottom of the page)\\n  8. Now when you compose a new email a new button, Insert Image has appeared."},{"id":"/2012/04/10/laptop-numbers-instead-of-letters-fix","metadata":{"permalink":"/2012/04/10/laptop-numbers-instead-of-letters-fix","source":"@site/blog/2012-04-10-laptop-numbers-instead-of-letters-fix.md","title":"Laptop Typing Numbers instead of Letters Fix","description":"If you find that your Laptop&#8217;s letters have started to type numbers, the fix for this is quite simple.","date":"2012-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.24,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Laptop Typing Numbers instead of Letters Fix","permalink":"/misc/laptop-numbers-instead-of-letters-fix/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to Insert Image into Gmail Post","permalink":"/2012/04/10/insert-image-into-gmail-post"},"nextItem":{"title":"MSN Keyport","permalink":"/2012/04/10/msn-keyport"}},"content":"If you find that your Laptop&#8217;s letters have started to type numbers, the fix for this is quite simple.\\n\\n  1. \xa0At the same time press **FN+Numlock**\\n\\n_If that doesn&#8217;t work, try the following:_\\n\\n  1. \xa0At the same time press **Shift+Numlock**\\n\\nThat&#8217;s it, it should be now typing letters."},{"id":"/2012/04/10/msn-keyport","metadata":{"permalink":"/2012/04/10/msn-keyport","source":"@site/blog/2012-04-10-msn-keyport.md","title":"MSN Keyport","description":"1. Click Start","date":"2012-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.21,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"MSN Keyport","permalink":"/win/msn-keyport/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Laptop Typing Numbers instead of Letters Fix","permalink":"/2012/04/10/laptop-numbers-instead-of-letters-fix"},"nextItem":{"title":"RAGE Game Save Location","permalink":"/2012/04/10/rage-game-save-location"}},"content":"1. Click Start\\n  2. Click Run\\n  3. Type in: \xa0**regsvr32 softpub.dll**\\n  4. Press Enter\\n  5. Click Start\\n  6. Click Run\\n  7. Type in: \xa0**regsvr32 mssip32.dll**\\n  8. Press Enter\\n  9. Click Start\\n 10. Click Run\\n 11. Type in: **regsv32 initpki.dll**\\n 12. Press Enter"},{"id":"/2012/04/10/rage-game-save-location","metadata":{"permalink":"/2012/04/10/rage-game-save-location","source":"@site/blog/2012-04-10-rage-game-save-location.md","title":"RAGE Game Save Location","description":"Retail\\\\Users\\\\(your user name)\\\\Saved Game\\\\sid Software\\\\Rage\\\\base\\\\savegame","date":"2012-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.08,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"RAGE Game Save Location","permalink":"/misc/rage-game-save-location/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"MSN Keyport","permalink":"/2012/04/10/msn-keyport"},"nextItem":{"title":"HP Laserjet 3390 Usage Page","permalink":"/2012/04/04/hp-laserjet-3390-usage-page"}},"content":"Retail: C:\\\\Users\\\\(your user name)\\\\Saved Game\\\\sid Software\\\\Rage\\\\base\\\\savegame\\n\\nSTEAM: \xa0Local files\\\\program files\\\\steam\\\\steam\\\\steam apps\\\\(your account)\\\\ rage\\\\look for game saves"},{"id":"/2012/04/04/hp-laserjet-3390-usage-page","metadata":{"permalink":"/2012/04/04/hp-laserjet-3390-usage-page","source":"@site/blog/2012-04-04-hp-laserjet-3390-usage-page.md","title":"HP Laserjet 3390 Usage Page","description":"1. Press &#8220;Menu&#8221; button","date":"2012-04-04T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.135,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"HP Laserjet 3390 Usage Page","permalink":"/misc/hp-laserjet-3390-usage-page/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"RAGE Game Save Location","permalink":"/2012/04/10/rage-game-save-location"},"nextItem":{"title":"HP Printer Error 20","permalink":"/2012/04/04/hp-printer-error-20"}},"content":"1. Press &#8220;**Menu**&#8221; button\\n  2. Use the arrow buttons to select &#8220;**Reports**&#8220;\\n  3. Press &#8220;**Enter**&#8220;\\n  4. Use the arrow keys and select &#8220;**Usage Page**&#8220;\\n  5. Press &#8220;**Enter**&#8220;"},{"id":"/2012/04/04/hp-printer-error-20","metadata":{"permalink":"/2012/04/04/hp-printer-error-20","source":"@site/blog/2012-04-04-hp-printer-error-20.md","title":"HP Printer Error 20","description":"Error 20 on Hewlett Packard printers means that the document you are trying to print is usually too large for the printers on board memory, could be because you have quite alot of jobs in the queue that the printer is storing. Try the following to fix it:","date":"2012-04-04T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.445,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"HP Printer Error 20","permalink":"/misc/hp-printer-error-20/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"HP Laserjet 3390 Usage Page","permalink":"/2012/04/04/hp-laserjet-3390-usage-page"},"nextItem":{"title":"Icons missing in Windows 7","permalink":"/2012/04/04/icons-missing-in-windows-7"}},"content":"Error 20 on Hewlett Packard printers means that the document you are trying to print is usually too large for the printers on board memory, could be because you have quite alot of jobs in the queue that the printer is storing. Try the following to fix it:\\n\\n  1. Press the **Resume** button on the printer (_Usually blinking Orange_)\\n  2. If the above does not work then turn the printer off for 1 minutes to reset its memory and then on again. You should now be able to print."},{"id":"/2012/04/04/icons-missing-in-windows-7","metadata":{"permalink":"/2012/04/04/icons-missing-in-windows-7","source":"@site/blog/2012-04-04-icons-missing-in-windows-7.md","title":"Icons missing in Windows 7","description":"Try the following fixes below:","date":"2012-04-04T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Icons missing in Windows 7","permalink":"/win/icons-missing-in-windows-7/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"HP Printer Error 20","permalink":"/2012/04/04/hp-printer-error-20"},"nextItem":{"title":"Laptop Stuck on Zz Light","permalink":"/2012/04/04/laptop-stuck-on-zz-light"}},"content":"Try the following fixes below:\\n\\n  1. Open **Task** **Manager** (Ctrl+Shift+Esc)\\n  2. Click **Process**\\n  3. **Right** **click** **Explorer**.exe and choose **End** **Task**\\n  4. Click File\\n  5. Click **Run**\\n  6. Type in: **_cmd.exe_** then press Ok\\n  7. Type:\\n  8. **_CD /d %USERPROFILE%_**<wbr>**_AppDataLocal_**\xa0&#8211; Then press **Enter**</wbr>\\n  9. **_DEL IconCache.db /a_** &#8211;\xa0Then press Enter\\n 10. EXIT\xa0&#8211; Then press Enter\\n 11. Click File\\n 12. Click **Run**\\n 13. Type **Explorer**.exe"},{"id":"/2012/04/04/laptop-stuck-on-zz-light","metadata":{"permalink":"/2012/04/04/laptop-stuck-on-zz-light","source":"@site/blog/2012-04-04-laptop-stuck-on-zz-light.md","title":"Laptop Stuck on Zz Light","description":"This usually occurs when the laptop has problems coming out of standby mode.","date":"2012-04-04T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Laptop Stuck on Zz Light","permalink":"/misc/laptop-stuck-on-zz-light/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Icons missing in Windows 7","permalink":"/2012/04/04/icons-missing-in-windows-7"},"nextItem":{"title":"Fix iPad Wifi Issues","permalink":"/2012/04/02/fix-ipad-wifi-issues"}},"content":"This usually occurs when the laptop has problems coming out of standby mode.\\n\\n  1. **Hold down** **Power** Button for **10** **seconds**, then release.\\n  2. Remove the RAM module(s) _(Replace RAM modules into another slot)_"},{"id":"/2012/04/02/fix-ipad-wifi-issues","metadata":{"permalink":"/2012/04/02/fix-ipad-wifi-issues","source":"@site/blog/2012-04-02-fix-ipad-wifi-issues.md","title":"Fix iPad Wifi Issues","description":"Issues with your iPad&#8217;s wireless? Try the following tips below, these can also be used for just normal Wireless issues.","date":"2012-04-02T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":1.325,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Fix iPad Wifi Issues","permalink":"/osx/fix-ipad-wifi-issues/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Laptop Stuck on Zz Light","permalink":"/2012/04/04/laptop-stuck-on-zz-light"},"nextItem":{"title":"XChat Commands","permalink":"/2012/04/01/xchat-commands"}},"content":"Issues with your iPad&#8217;s wireless? Try the following tips below, these can also be used for just normal Wireless issues.\\n\\n**Turning it on or off again**\\n\\nFirst things first &#8220;have you tried turning it on and off again&#8221;.\\n\\n  1. Turn your broadband router off for a few minutes then turn it on to have the device reconnect to the exchange.\\n\\n**Reset Wireless Settings on iPad**\\n\\n  1. Reset the Wireless settings on the iPad\\n  2. Open Settings\\n  3. Go to \xa0\u201cGeneral\u201d in the left-hand menu.\\n  4. Scroll down and tap Reset\\n  5. Select \u201cReset Network Settings.\u201d\\n\\nNote: This will delete any saved Wireless security keys_\\n\\n**Turning off/Changing Wireless Security**\\n\\n  1. You need to access your routers configuration page. Open your web-browser.\\n  2. In the top address field type: 192.168.1.1 (or 10.1.1.1) &#8211; _It also may be something different_\\n  3. Type in the username & password to access the Configuration _(Generally admin,admin)_\\n  4. Look for Wireless Security and go into it.\\n  5. Either change the Wireless Key, or change the security to Open (non secured)\\n  6. Press Save\\n\\n**Adjust your iPads Brightness**\\n\\nOne of the issues with the earlier iPads, is that when on the lowest brightness setting the iPad has problems connecting to the internet. Go to Settings_\\n\\n  1. Select Brightness & Wallpaper\\n  2. Slide the brightness slider up, then Apply.\\n  3. Wait two minutes to see if you have an improvement in Wireless strength.\\n\\n**Turn off Bluetooth**\\n\\nDue to interference issues turning off Bluetooth can sometimes boost your wireless signal._\\n\\n  1. Go to Settings\\n  2. Go to General\\n  3. Select Bluetooth\\n  4. Slide to off."},{"id":"/2012/04/01/xchat-commands","metadata":{"permalink":"/2012/04/01/xchat-commands","source":"@site/blog/2012-04-01-xchat-commands.md","title":"XChat Commands","description":"Enables Auto Rejoin [Client wide] &#8211;\xa0/set irc\\\\auto\\\\rejoin 1","date":"2012-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.04,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"XChat Commands","permalink":"/misc/xchat-commands/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Fix iPad Wifi Issues","permalink":"/2012/04/02/fix-ipad-wifi-issues"},"nextItem":{"title":"How to remove Windows Live Family Safety","permalink":"/2012/03/31/how-to-remove-windows-live-family-safety"}},"content":"Enables Auto Rejoin [Client wide] &#8211;\xa0/set irc\\\\_auto\\\\_rejoin 1"},{"id":"/2012/03/31/how-to-remove-windows-live-family-safety","metadata":{"permalink":"/2012/03/31/how-to-remove-windows-live-family-safety","source":"@site/blog/2012-03-31-how-to-remove-windows-live-family-safety.md","title":"How to remove Windows Live Family Safety","description":"Having problems getting rid of Windows Live Family Safety? Follow the steps below!","date":"2012-03-31T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.49,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to remove Windows Live Family Safety","permalink":"/win/how-to-remove-windows-live-family-safety/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"XChat Commands","permalink":"/2012/04/01/xchat-commands"},"nextItem":{"title":"iATKOS v2.0i Installation","permalink":"/2012/03/31/iatkos-v2-0i-installation"}},"content":"Having problems getting rid of Windows Live Family Safety? Follow the steps below!\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Left Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Left Click <strong>Control Panel</strong>\\n  </li>\\n  <li>\\n    Left Click <strong>Add/Remove Programs</strong>\\n  </li>\\n  <li>\\n    Left Click <strong>Windows Live Essentials</strong>\\n  </li>\\n</ol>\\n\\n_You can also stop Windows Family Safety from running by following the prompts below:_\\n\\n<ol start=\\"1\\">\\n  <li>\\n    Left Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Left click <strong>Run</strong>\\n  </li>\\n  <li>\\n    Type in &#8220;<strong><em>services.msc</em></strong>&#8221; & press Enter\\n  </li>\\n  <li>\\n    Double click <strong>Windows Family Safety</strong>.\\n  </li>\\n  <li>\\n    \xa0In start-up Type select &#8220;<strong><em>Disabled</em></strong>&#8220;\\n  </li>\\n  <li>\\n    Click Ok\\n  </li>\\n  <li>\\n    <strong>Restart</strong> the PC\\n  </li>\\n</ol>"},{"id":"/2012/03/31/iatkos-v2-0i-installation","metadata":{"permalink":"/2012/03/31/iatkos-v2-0i-installation","source":"@site/blog/2012-03-31-iatkos-v2-0i-installation.md","title":"iATKOS v2.0i Installation","description":"Credits goes to Apple and OSX86 community. This is just the instructions for the install.","date":"2012-03-31T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":5.615,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"iATKOS v2.0i Installation","permalink":"/osx/iatkos-v2-0i-installation/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to remove Windows Live Family Safety","permalink":"/2012/03/31/how-to-remove-windows-live-family-safety"},"nextItem":{"title":"igxprd32 display driver has stopped working","permalink":"/2012/03/31/igxprd32"}},"content":"Credits goes to Apple and OSX86 community. This is just the instructions for the install.\\n  \\nAttention:\\n\\n1- This DVD is designed for Intel architectures.\\n  \\nMinimum requirements: Intel SSE3 CPU, 512MB RAM, 12GB free space on target partition.\\n\\n2- This DVD includes Apple&#8217;s OS X Leopard 10.5.2 (9C7010) installation.\\n\\n3- Make sure that the md5 checksum of your iATKOS iso image matches the one posted on our website. Otherwise you may have a faulty DVD image. Use quality media/burner and burn slowly.\\n  \\nInformation:\\n\\n&#8211; This release supports GPT (Guid Partition Table) partition map scheme. You can change your partition table type to GPT via Disk Utility or some other partition managers. We advice Parted Magic Live CD which is based on GParted.\\n\\n&#8211; This release also includes Darwin EFI emulation boot for MBR (Master Boot Record) harddrives. Intel Core CPUs can use stock kernel and stock ACPIPlatform with EFI emulation on MBR and with GPT. This increases stability and performance.\\n\\n&#8211; Intel CoreDuo Processors have appropriate SpeedStep Technology which is stable in OS X. If you have such processor, you can enable throttling feature using the &#8220;SpeedStep&#8221; package which you can find under system drivers section. Note that speedstep kernel is included in iATKOS main system, so don&#8217;t select any of the kernels with speedstep support if you don&#8217;t want to have kernel panic.\\n\\n&#8211; There are some boot options for some various hardware. Boot DVD, press F8 and enter:\\n  \\nnforce -v\\n  \\nfor nForce motherboards with intel SSE3 CPU\\n  \\nnforce_core -v\\n  \\nfor nForce motherboards with Intel Core CPU\\n  \\ntoh -v\\n  \\nfor booting with ToH 9.2.0 kernel which works nice on many types of hardware\\n  \\nInstallation:\\n\\n1- Run Disk Utility via Utilities menu and erase the target partition for clean install. Do partitioning if you need to.\\n\\na- You can choose MBR (Master Boot Record) or GPT (Guid Partition Table) via partitioning options. If you want to change your existing partition table type, know that all your existing data on disk will be gone.\\n  \\nb- For Windows fellows that has not tested this amazing system yet and that does not want to loose their windows software, porn and virus&trojan archive on D:, should use Parted Magic Live CD for preparing an active primary target partition and after that, just erase it via diskutil. Jump to Step 2.\\n\\n2- Select the destination for installation.\\n\\n3- Click customize and select what you need.\\n\\n4- Click Install.\\n\\nInstall time is about 20 minutes.\\n\\n&nbsp;\\n\\nMultiboot:\\n\\niATKOS 2.0i has no integrated procedure for multibooting but preparing a multiboot GPT or MBR system with this release is easier than ever.\\n\\nHere is a &#8221;How To&#8221; for GPT and MBR triple boot including Mac-Vista-Linux.\\n\\n&#8211; MBR (Master Boot Record) triple boot &#8211;\\n\\n&#8211; Change your partition table type to MBR (msdos type) and create 3 primary partitions by using Parted Magic Live CD or iATKOS Disk Utility. HFS+ format for OS X target, Fat32 for the others.\\n  \\nNote: %99.99999 of the PC harddrives already have MBR partition map scheme (msdos type) which you can change it to GPT via Parted Magic CD or iATKOS Disk Utility if you like to.\\n\\n&#8211; Add boot flag to Vista or Linux target and install the OS&#8217; in any order you like. For linux, do not install bootloader to MBR, install it to linux root.\\n\\n&#8211; Boot iATKOS 2.0i DVD and install OS X to its target as usual.\\n\\n&#8211; Now you need to repair Vista. Add boot flag to Vista partition, boot Vista DVD, select repair, add boot flag to OS X partition, thats all.\\n\\nNow you have triple boot on MBR.\\n\\nNote: Adding boot flag to MBR partitions: Boot Parted Magic Live CD, right click to partition, select flags and click to boot.\\n\\n&#8211; GPT (Guid Partition Table) triple boot &#8211;\\n\\n&#8211; Change your partition table type to GPT (Guid Partition Table) and create 3 partitions by using Parted Magic Live CD or iATKOS Disk Utility. HFS+ format for OS X target, Fat32 (msdos) for the others.\\n\\n&#8211; Boot iATKOS 2.0i DVD, install only bootloader to OS X target. \\\\***\\n\\n&#8211; Boot Vista DVD and install it to first Fat32 partition.\\n  \\n(Note: System marks first Fat32 partition or booting Foreign OS&#8217;. If you choose other than first Fat32 as target for Vista, then during installation, system will not continue install process at first Vista boot.)\\n\\n&#8211; Boot Linux media and install the OS. Install the bootloader to linux root.\\n  \\n(Note: Some live linux cd stuff (knoppix, kanotix, parsix etc.) has built-in installer applications and they use gparted for partitioning and formatting targets. At this point, formatting target to linux fs via Gparted is not a good choice, use fdisk just for changing the target partition type from &#8221;c0&#8221; to &#8221;83&#8221; which is linux partition type and then continue installation.)\\n\\n&#8211; Boot iATKOS 2.0i DVD and install OS X as usual.\\n\\nNow you have triple boot on GPT, thats all.\\n\\nAs you see, preparing multiboot system on GPT is more practical and easier.\\n  \\nDarwin on GPT will see other OS&#8217; as Foreign Boot.\\n\\n\\\\*** For pro-users: You can re-install Darwin bootloader by booting the iATKOS DVD with -s. Boot with -s and enter darwin command, follow the instructions. For MBR HD, you can add boot flag via fdisk after darwin installation.\\n  \\nKnown Issues:\\n\\n1- Jmicron IDE controllers are known to have issues with OS X.\\n\\n2- Some 965 chipset MSI and ASUS motherboards (desktop and laptop) have issues with this OS. ACPI errors while booting with DVD and also with hd. There will be patches for those uncompatible devices and there is also a patch for 1.0ir2.\\n\\n3- This dvd may not include all the necessary drivers required by your setup. Additional steps may be needed to be taken by the user to setup and use such components.\\n\\n4- Be sure that the labels of your hfs+ partitions are not similar. ie. Mac 1, MacOS or Leopard X, Leopard Y. Installer may fail to determine the target partition to install the system if the labels match.\\n  \\nUnsuccessful Installation and Related Symptoms:\\n  \\n1- I get the error &#8220;com.apple.Boot.plist is not found&#8221; while booting the DVD\\n\\n&#8211;> You are probably trying to boot from an unsupported controller.\\n  \\n~ You should use usb dvd rom drive, or you may need sata-ide convertor for your dvd rom drive (jmicron).\\n\\n2- System freezes at grey screen by booting the DVD.\\n\\n&#8211;> You probably have an unsupported chipset.\\n  \\n*965 chipset ASUS or MSI motherboard/laptop\\n  \\n*many VIA, SIS, ATI chipset motherboards\\n  \\n~ Change your hardware\\n  \\n~ A patch can be possible for 965 ASUS/MSI hardware, so keep on watching this community.\\n\\n\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\****\\n\\nFor additional Help and instructions, here is the website and irc channel.\\n\\nweb: http://www.uphuck.com\\n  \\nIRC: irc.atlantis-irc.net #uphuck.DVD #osx86.turk\\n\\n\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***\\\\***"},{"id":"/2012/03/31/igxprd32","metadata":{"permalink":"/2012/03/31/igxprd32","source":"@site/blog/2012-03-31-igxprd32.md","title":"igxprd32 display driver has stopped working","description":"&#8220;igxprd32 display driver has stopped working&#8221;","date":"2012-03-31T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.22,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"igxprd32 display driver has stopped working","permalink":"/win/igxprd32/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"iATKOS v2.0i Installation","permalink":"/2012/03/31/iatkos-v2-0i-installation"},"nextItem":{"title":"Using MKVMerge to add subtitles","permalink":"/2012/03/31/using-mkvmerge"}},"content":"&#8220;igxprd32 display driver has stopped working&#8221;\\n\\nThis issue occurs due to a bug with the drivers, in order to repair this issue you have to rollback to a previous driver.\\n\\nThe Intel <a title=\\"Driver_Download\\" href=\\"http://www.bootwin.com/dell-drivers/dell-inspiron-1420-intel-gm965-express-chipset-family-x.html\\" target=\\"_blank\\">6.14.10.4926 A06</a> drivers appear to be the most stable."},{"id":"/2012/03/31/using-mkvmerge","metadata":{"permalink":"/2012/03/31/using-mkvmerge","source":"@site/blog/2012-03-31-using-mkvmerge.md","title":"Using MKVMerge to add subtitles","description":"1. Open MKVMerge","date":"2012-03-31T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.4,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using MKVMerge to add subtitles","permalink":"/win/using-mkvmerge/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"igxprd32 display driver has stopped working","permalink":"/2012/03/31/igxprd32"},"nextItem":{"title":"Install Gnome 3 on Ubuntu 11.04","permalink":"/2012/03/30/gnome-3-ubuntu-11-04"}},"content":"1. Open MKVMerge\\n  2. Add the movie you will like to have the\xa0subtitles\xa0added too.\\n  3. Add the SRT (Subtitle) file for the movie that you would like to add.\\n  4. Click the last track which should be the subtitles.\\n  5. Select &#8220;EN&#8221; for the English language\\n  6. Make sure the Forced Track flag \xa0is set to yes\\n  7. Go to Format Specific Options and make sure the \xa0Character set is: pic ISO-8859-2\\n  8. Give it a new and click Mux"},{"id":"/2012/03/30/gnome-3-ubuntu-11-04","metadata":{"permalink":"/2012/03/30/gnome-3-ubuntu-11-04","source":"@site/blog/2012-03-30-gnome-3-ubuntu-11-04.md","title":"Install Gnome 3 on Ubuntu 11.04","description":"1. Open a Terminal","date":"2012-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.19,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Install Gnome 3 on Ubuntu 11.04","permalink":"/linux/gnome-3-ubuntu-11-04/","tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Using MKVMerge to add subtitles","permalink":"/2012/03/31/using-mkvmerge"},"nextItem":{"title":"How to change your MAC address in Linux","permalink":"/2012/03/30/how-to-change-your-mac-address-in-linux"}},"content":"1. Open a Terminal\\n2. Type: sudo add-apt-repository ppa:gnome3-team/gnome3\\n3. Type: gksu apt-get update\\n4. Type: sudo apt-get dist-upgrade\\n5. Type: sudo apt-get install gnome-shell\\n6. Reboot and at the Login screen,select Gnome 3 as the user interface."},{"id":"/2012/03/30/how-to-change-your-mac-address-in-linux","metadata":{"permalink":"/2012/03/30/how-to-change-your-mac-address-in-linux","source":"@site/blog/2012-03-30-how-to-change-your-mac-address-in-linux.md","title":"How to change your MAC address in Linux","description":"1. Open a Terminal","date":"2012-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.155,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to change your MAC address in Linux","permalink":"/linux/how-to-change-your-mac-address-in-linux/","tags":["Linux"]},"unlisted":false,"prevItem":{"title":"Install Gnome 3 on Ubuntu 11.04","permalink":"/2012/03/30/gnome-3-ubuntu-11-04"},"nextItem":{"title":"Online Virus Scanner services","permalink":"/2012/03/30/online-virus-scanner"}},"content":"1. Open a **Terminal**\\n  2. Type: **_ifconfig eth0 down_**\\n  3. Type: **_ifconfig eth0 hw ether THISISWHEREYOUENTERNEWMACADDRESS_**\\n  4. Type: **_ifconfig eth0 up_**\\n\\nThe mac address change is only temporary, until you restart"},{"id":"/2012/03/30/online-virus-scanner","metadata":{"permalink":"/2012/03/30/online-virus-scanner","source":"@site/blog/2012-03-30-online-virus-scanner.md","title":"Online Virus Scanner services","description":"* Online Virus Scanners ESET Online Scanner &#8211; http://www.eset.com/onlinescan/index.php","date":"2012-03-30T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.47,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Online Virus Scanner services","permalink":"/misc/online-virus-scanner/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to change your MAC address in Linux","permalink":"/2012/03/30/how-to-change-your-mac-address-in-linux"},"nextItem":{"title":"How to install OSX from a USB Drive (PowerPC)","permalink":"/2012/03/28/how-to-install-osx-from-a-usb-drive-powerpc"}},"content":"* <span>Online Virus Scanners ESET Online Scanner &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://www.eset.com/onlinescan/index.php\\">http://www.eset.com/onlinescan/index.php</a> </span>\\n  * <span>McAfee Freescan &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://us.mcafee.com/root/mfs/default.asp\\">http://us.mcafee.com/root/mfs/default.asp</a> </span>\\n  * <span>Panda Activescan &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://www.pandasecurity.com/activescan/\\">http://www.pandasecurity.com/activescan/</a> </span>\\n  * <span>Kaspersky Free Virus Scan &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://www.kaspersky.com/virusscanner\\">http://www.kaspersky.com/virusscanner</a> </span>\\n  * <span>Trend Micro Housecall &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://housecall.trendmicro.com/\\">http://housecall.trendmicro.com/</a> </span>\\n  * <span>Symantec Security Check &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://security.symantec.com/sscv6/WelcomePage.asp\\">http://security.symantec.com/sscv6/WelcomePage.asp</a> </span>\\n  * <span>F-Secure Online Scanner &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://support.f-secure.com/enu/home/ols.shtml\\">http://support.f-secure.com/enu/home/ols.shtml</a> </span>\\n  * <span>Windows Live OneCare Scanner &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://onecare.live.com/site/en-us/default.htm?s_cid=sah\\">http://onecare.live.com/site/en-us/default.htm?s_cid=sah</a> </span>\\n  * <span>BitDefender Online Scanner &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://www.bitdefender.com/scanner/online/free.html\\">http://www.bitdefender.com/scanner/online/free.html</a> </span>\\n  * <span>Virustotal Online Scanner &#8211; <a class=\\"smarterwiki-linkify\\" href=\\"http://www.virustotal.com/\\">http://www.virustotal.com/<br /> </a></span>"},{"id":"/2012/03/28/how-to-install-osx-from-a-usb-drive-powerpc","metadata":{"permalink":"/2012/03/28/how-to-install-osx-from-a-usb-drive-powerpc","source":"@site/blog/2012-03-28-how-to-install-osx-from-a-usb-drive-powerpc.md","title":"How to install OSX from a USB Drive (PowerPC)","description":"1. First you need the DMG of OSX, which you can either create using the Retail DVD you have (Using Disk Utility) or acquire through other means. Place the DMG somewhere you can get too easily, for example the Desktop.","date":"2012-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":1.69,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install OSX from a USB Drive (PowerPC)","permalink":"/osx/how-to-install-osx-from-a-usb-drive-powerpc/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Online Virus Scanner services","permalink":"/2012/03/30/online-virus-scanner"},"nextItem":{"title":"Installing Deki Wiki Skins","permalink":"/2012/03/28/installing-deki-wiki-skins"}},"content":"1. First you need the DMG of OSX, which you can either create using the Retail DVD you have (Using Disk Utility) or acquire through other means. Place the DMG somewhere you can get too easily, for example the Desktop.\\n  2. Connect your USB HDD to the computer & open the Disk Utility (Applications/Utilities/).\\n  3. On the left hand side of the Disk Utility, click Restore.\\n  4. \xa0In the Source field, you need to point it to your OSX DMG. You can either click browse or drag the DMG into the field.\\n  5. In the Restore field you need to choose your USB HDD, you can either click browse or drag the HDD partition (which should be Erased using the Disk Utility) into the field.\\n  6. \xa0Click Restore\\n  7. Once it is completed, you need to dot down the partition information for your external HDD so grab a piece of paper and pen.\\n  8. \xa0Right click on your HDD partition (which should now be labeled the OSX install) and select Information, this will display information on the external HDD and partition information look for partition number, write this down.\\n  9. \xa0Now restart your Mac and hold down Command+Option+O+F when starting, this will open the iMacs firmware, be careful in this area.\\n  \\n    Type: dev / ls (this will open a list of the devices in your computer)\\n 10. \xa0In the list, look for information under usb such as /usb@a/disk@0, once you find this, wrote down the full path for example /@p/pci@1/usb@a/disk@0.\\n 11. \xa0Type: devalias ud (The path you wrote down in the step above)\\n 12. \xa0Type: dir ud:0, \xa0(0 is the partition number, written down in while in the Disk Utility)\\n 13. Now look for a file with tbxi attribute usually \u201cSystemLibraryCoreServicesBootX\u201d\\n 14. Once you find it you need to change to the directory type:\\n 15. dir ud:0,SystemLibraryCoreServices (again the \u201c0\u201d is your partition number)\\n 16. Now is time to boot from the USB HDD type:\\n 17. boot ud:0,SystemLibraryCoreServicesBootX ((again the \u201c0\u201d is your partition number)\\n 18. Press Enter"},{"id":"/2012/03/28/installing-deki-wiki-skins","metadata":{"permalink":"/2012/03/28/installing-deki-wiki-skins","source":"@site/blog/2012-03-28-installing-deki-wiki-skins.md","title":"Installing Deki Wiki Skins","description":"1. Upload the content of your theme folder to var/www/dekiwikiskins (root access required)","date":"2012-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.235,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Installing Deki Wiki Skins","permalink":"/misc/installing-deki-wiki-skins/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to install OSX from a USB Drive (PowerPC)","permalink":"/2012/03/28/how-to-install-osx-from-a-usb-drive-powerpc"},"nextItem":{"title":"Logon Process has failed to create the security options dialog","permalink":"/2012/03/25/logon-process-failed"}},"content":"1. **Upload** the content of your **theme** folder to** var/www/dekiwikiskins** (root access required)\\n  2. **Go** **to** your MindTouch **Control** **Panel** in your web browser (admin required)\\n  3. **Go** to the **Logos** **and Skins** **page**\\n  4. **Select** your **skin** In the Select Skin section\\n  5. Click **Apply** Skin"},{"id":"/2012/03/25/logon-process-failed","metadata":{"permalink":"/2012/03/25/logon-process-failed","source":"@site/blog/2012-03-25-logon-process-failed.md","title":"Logon Process has failed to create the security options dialog","description":"1. Click Start","date":"2012-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.75,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Logon Process has failed to create the security options dialog","permalink":"/win/logon-process-failed/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Installing Deki Wiki Skins","permalink":"/2012/03/28/installing-deki-wiki-skins"},"nextItem":{"title":"Settings for New Zealand Internet Service Providers","permalink":"/2012/03/25/settings-for-new-zealand-internet-service-providers"}},"content":"1. Click Start\\n  2. Click Run\\n  3. Type: msconfig\\n  4. Click Ok\\n  5. This will open \xa0the System Configuration Utility, click Startup tab up the top\\n  6. Go through the startup list and uncheck everything other then your Antivirus and any other software you want starting\\n  7. Press Ok\\n  8. Restart PC\\n\\nIf the fix above, doesn&#8217;t work then follow this tomorrow:\\n\\n  1. Click Start\\n  2. Type: cmd and press Enter\\n  3. Type: sfc /scannow _(You might need the Windows CD as it will attempt to restore non-corrupt files from the CD onto \xa0the computer)._\\n  4. Once completed, restart the computer.\\n\\nIf none of the fixes above work, try the following:\\n\\n  1. Click Start\\n  2. Click Run\\n  3. Type: \xa0devmgmt.msc _(This will open Device Manager, which lists your devices)_\\n  4. In the list, go to System devices\\n  5. Right Click ACPI Fixed Features\\n  6. Choose Uninstall\\n  7. Restart the computer"},{"id":"/2012/03/25/settings-for-new-zealand-internet-service-providers","metadata":{"permalink":"/2012/03/25/settings-for-new-zealand-internet-service-providers","source":"@site/blog/2012-03-25-settings-for-new-zealand-internet-service-providers.md","title":"Settings for New Zealand Internet Service Providers","description":"Ignite Communications (formerly Data Solutions NZ)","date":"2012-03-25T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":3.365,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Settings for New Zealand Internet Service Providers","permalink":"/misc/settings-for-new-zealand-internet-service-providers/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Logon Process has failed to create the security options dialog","permalink":"/2012/03/25/logon-process-failed"},"nextItem":{"title":"How to Logout of PSN Network","permalink":"/2012/03/22/how-to-log-out-of-psn-network"}},"content":"Ignite Communications (formerly Data Solutions NZ)\\n\\n* Domain name server 1 : 123.255.15.131\\n  \\n* Domain name server 2 : 123.255.15.132\\n  \\n* Incoming mail server : pop3.ignite.net.nz\\n  \\n* Outgoing mails server : smtp.ignite.net.nz\\n  \\n* IMAP Server : imap.ignite.net.nz\\n  \\n* Website : www.ignite.net.nz\\n  \\n* Helpdesk : 0800 33 EASY / 0800 333 279\\n\\nClear\\n\\n* Dialup : 086725327 or Auckland 9145656\\n  \\n* ADSL login : username\\\\_adsl@clear.net.nz (or for a wholesale Telecom account being paid on a Clear invoice : username\\\\_adsl@dsl.clear.net.nz)\\n  \\n* Domain Name Server 1 : 203.97.33.1\\n  \\n* Domain Name Server 2 : 203.97.37.14\\n  \\n* Incoming mail server : pop.clear.net.nz\\n  \\n* Outgoing mail server : smtp.clear.net.nz\\n  \\n* Network Status: networkstatus.telstraclear.co.nz/\\n  \\n* News server : news.clear.net.nz\\n  \\n* Website : www.clear.net.nz\\n  \\n* Helpdesk : 0508 888 800\\n\\nXtra\\n\\n* ADSL user name login : xxx.xadsl@xtra.co.nz\\n  \\n* ADSL Settings : PPPoA (RFC2364) VPI: 0 VCI: 100\\n  \\n* Dialup : 087303030\\n  \\n* Domain Name Server 1 : 202.27.184.3\\n  \\n* Domain Name Server 2 : 202.27.184.5\\n  \\n* Incoming mail server : pop3.xtra.co.nz\\n  \\n* Outgoing mail server : send.xtra.co.nz\\n  \\n* News server : news.xtra.co.nz\\n  \\n* Website : www.xtramsn.co.nz\\n  \\n* Helpdesk : 0800 225 598 or for Xtra Jetstream 0800 738 738 or for Xtra business 0800 287 463\\n\\nIhug\\n\\n* ADSL user name login : xxx@adsl.ihug.co.nz\\n  \\n* ADSL Settings : PPPoA (RFC2364) VPI: 0 VCI: 100\\n  \\n* Dialup : 087 300 777\\n  \\n* Domain Name Server 1 :203.109.252.42\\n  \\n* Domain Name Server 2 : 203.109.252.43\\n  \\n* Incoming mail server : pop.ihug.co.nz\\n  \\n* Outgoing mail server : smtp.ihug.co.nz\\n  \\n* News server : news.akl.ihug.co.nz\\n  \\n* Website : www.ihug.co.nz\\n  \\n* Helpdesk : 0800 438 448\\n\\nParadise\\n\\n* Dialup : 086727234 or Auckland 3590101\\n  \\n* Domain Name Server 1 : 203.96.152.4\\n  \\n* Domain Name Server 2 : 203.96.152.12\\n  \\n* Incoming mail server : pop3.paradise.net.nz\\n  \\n* Outgoing mail server : smtp.paradise.net.nz\\n  \\n* News server : news.paradise.net.nz\\n  \\n* Website : www.paradise.net.nz\\n  \\n* Helpdesk : 0800 467 272\\n\\nValuenet (this has been bought by Orcon, but these settings are being maintained in the meantime)\\n\\n* Dialup : 086733444\\n  \\n* Dialup login name : xxx@value.net.nz\\n  \\n* Domain Name Server 1 : 203.96.152.4\\n  \\n* Domain Name Server 2 : 203.96.152.12\\n  \\n* Incoming mail server : pop3.value.net.nz\\n  \\n* Outgoing mail server : smtp.value.net.nz\\n  \\n* Website : www.value.net.nz\\n  \\n* Helpdesk : 0900 939 333 ($2 per minute)\\n\\nSlingshot\\n\\n* Dialup : 086788888\\n  \\n* Incoming mail server : pop3.slingshot.co.nz\\n  \\n* Outgoing mail server : smtp.slingshot.co.nz\\n  \\n* News server : news.slingshot.co.nz\\n  \\n* Website : www.slingshot.co.nz\\n  \\n* Helpdesk : 0900 32 843 ($2 per minute)\\n\\nCompass\\n\\n* Dialup : 086788800\\n  \\n* Domain Name Server 1 : 203.97.99.250\\n  \\n* Domain Name Server 2 : 203.97.100.3\\n  \\n* Incoming mail server : pop3.compass.net.nz\\n  \\n* Outgoing mail server : smtp.compass.net.nz\\n  \\n* Website : www.compass.net.nz\\n  \\n* Helpdesk : 0800 640 840\\n\\nFreenet\\n\\n* Dialup : 086767000\\n  \\n* Domain Name Server 1 : 203.97.99.250\\n  \\n* Domain Name Server 2 : 203.97.100.3\\n  \\n* Incoming mail server : pop3.free.net.nz\\n  \\n* Outgoing mail server : smtp.free.net.nz\\n  \\n* News server : news.free.net.nz\\n  \\n* Website : www.free.net.nz\\n  \\n* Helpdesk : 0900 37336 ($2 per minute)\\n\\nWhoosh\\n\\n* Username login : xxx@woosh.co.nz\\n  \\n* Incoming mail server : pop3.woosh.co.nz (need to login with xxx@woosh.co.nz)\\n  \\n* Outgoing mail server : smtp.woosh.co.nz\\n  \\n* website : www.woosh.co.nz\\n  \\n* Helpdesk : 0800 496 674\\n\\nEThree\\n\\n* Dialup : 087303185\\n  \\n* Domain Name Server 1 : 210.55.24.8\\n  \\n* Domain Name Server 2 : 210.55.24.14\\n  \\n* Incoming mail server : pop3.e3.net.nz\\n  \\n* Outgoing mail server : smtp.e3.net.nz\\n  \\n* Website : www.e3.net.nz\\n  \\n* Helpdesk : 09 977 3570\\n\\nDream Net\\n\\n* Dialup : 086737326\\n  \\n* Domain Name Server 1 : 72.21.35.130\\n  \\n* Domain Name Server 2 : 72.21.35.131\\n  \\n* Incoming mail server : mail.dreamnet.co.nz\\n  \\n* Outgoing mail server : mail.dreamnet.co.nz\\n  \\n* Website : www.dreamnet.co.nz\\n  \\n* Helpdesk : 0800 484 393\\n\\nOrcon Internet\\n  \\n* Dialup : 086755666\\n  \\n* ADSL login : username@fastadsl.net.nz (or jetstart : username@jetstart.jet.net.nz)\\n  \\n* Domain Name Server 1 : 210.55.12.1\\n  \\n* Domain Name Server 2 : 210.55.12.2\\n  \\n* Incoming mail server : pop3.orcon.net.nz (imap.orcon.net.nz)\\n  \\n* Outgoing mail server : smtp.orcon.net.nz\\n  \\n* Website : www.orcon.net.nz\\n  \\n* Helpdesk : 09 444 4414 or 0508 4orcon"},{"id":"/2012/03/22/how-to-log-out-of-psn-network","metadata":{"permalink":"/2012/03/22/how-to-log-out-of-psn-network","source":"@site/blog/2012-03-22-how-to-log-out-of-psn-network.md","title":"How to Logout of PSN Network","description":"1. Go to Playstation Network","date":"2012-03-22T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.15,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Logout of PSN Network","permalink":"/misc/how-to-log-out-of-psn-network/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Settings for New Zealand Internet Service Providers","permalink":"/2012/03/25/settings-for-new-zealand-internet-service-providers"},"nextItem":{"title":"How to Install and Switch Winamp Player skins","permalink":"/2012/03/22/install-and-switch-winamp"}},"content":"1. Go to Playstation **Network**\\n  2. Go to **Account** **Management**\\n  3. Press\xa0**TRIANGLE**\\n  4. Select **Sign** **Out** (here you can also disable AutoLogon to prevent the\xa0play-station\xa0network from logging you in automatically)"},{"id":"/2012/03/22/install-and-switch-winamp","metadata":{"permalink":"/2012/03/22/install-and-switch-winamp","source":"@site/blog/2012-03-22-install-and-switch-winamp.md","title":"How to Install and Switch Winamp Player skins","description":"Ever found a great Winamp Skin, But you did not know how to install it? This tutorial will explain how._","date":"2012-03-22T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.865,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Install and Switch Winamp Player skins","permalink":"/win/install-and-switch-winamp/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to Logout of PSN Network","permalink":"/2012/03/22/how-to-log-out-of-psn-network"},"nextItem":{"title":"OSX Freezing when accessing Network Preferences","permalink":"/2012/03/21/osx-freezing-accessing-network"}},"content":"Ever found a great Winamp Skin, But you did not know how to install it? This tutorial will explain how._\\n\\n**To install Winamp Skins:**\\n\\n  1. Download & save the skin you want\\n  2. Double click the skin file _(usually saved in Documents/Downloads)_\\n  3. When it prompts, whether you want to install skin or not, select Yes.\\n  4. It will then install the skin and switch to it.\\n\\nIf it wont open in Winamp; Right click the skin and click Open With.. if Winamp is not in the list; then Browse for it; Winamp is usually located in: C:Program FilesWinamp. Select winamp.exe and click Open; it will then install the skin._\\n\\n**Switch to another skin**\\n\\n  1. Open Winamp\\n  2. Right click a free space next to one of the toolbars up the top.\\n  3. Click Options\\n  4. Select Skin Browser\\n  5. It will then list any installed skins and you can double click the skin you want to change winamp to the new skin.\\n\\nSome great winamp skins are located &#8220;<a title=\\"Winamp_Skins\\" href=\\"http://www.allwinampskins.com/\\" target=\\"_blank\\">here</a>&#8220;"},{"id":"/2012/03/21/osx-freezing-accessing-network","metadata":{"permalink":"/2012/03/21/osx-freezing-accessing-network","source":"@site/blog/2012-03-21-osx-freezing-accessing-network.md","title":"OSX Freezing when accessing Network Preferences","description":"This usually happens, when some of the OSX settings become corrupted. OSX can regenerate new settings, which should resolve your problem. Follow the settings below:","date":"2012-03-21T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OSX Freezing when accessing Network Preferences","permalink":"/osx/osx-freezing-accessing-network/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to Install and Switch Winamp Player skins","permalink":"/2012/03/22/install-and-switch-winamp"},"nextItem":{"title":"Post to Facebook at the same time as Google+","permalink":"/2012/03/21/post-to-facebook-at-the-same-time-as-google"}},"content":"This usually happens, when some of the OSX settings become corrupted. OSX can regenerate new settings, which should resolve your problem. Follow the settings below:\\n\\n  1. Open **Finder**\\n  2. **Navigate** to your **hard****drive**\\n  3. **Go** to /**Library**/**Prefences**\\n  4. Drag **System****Configuration** folder to **Trash**\\n  5. Now **open****Network****Preferences**, the System Configuration folder should reappear and OSX should not freeze.\\n\\nIf the above doesn&#8217;t work then delete these files:\\n\\n_Library/Preferences/com.apple.systempreferences.plist_\\n\\n_Library/Preferences/com.apple.NetworkUtility.plist_"},{"id":"/2012/03/21/post-to-facebook-at-the-same-time-as-google","metadata":{"permalink":"/2012/03/21/post-to-facebook-at-the-same-time-as-google","source":"@site/blog/2012-03-21-post-to-facebook-at-the-same-time-as-google.md","title":"Post to Facebook at the same time as Google+","description":"Ever wanted to post on Google+ and have it automatic post Facebook as well? This is how you can do it.","date":"2012-03-21T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.495,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Post to Facebook at the same time as Google+","permalink":"/misc/post-to-facebook-at-the-same-time-as-google/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"OSX Freezing when accessing Network Preferences","permalink":"/2012/03/21/osx-freezing-accessing-network"},"nextItem":{"title":"Howto Fix Word not opening","permalink":"/2012/03/21/word-not-opening"}},"content":"Ever wanted to post on Google+ and have it automatic post Facebook as well? This is how you can do it.\\n\\n  1. Find your secret email ID by logging into the <a title=\\"Facebook Mobile\\" href=\\"http://m.facebook.com/\\" target=\\"_blank\\">Facebook</a> Mobile page _(Look for the email address at the bottom of the page)_\\n  2. Go to your <a title=\\"Google+\\" href=\\"https://plus.google.com/\\" target=\\"_blank\\">Google+</a> profile and write something you would like to share.\\n  3. Add your secret email address along with whatever circles you want to share the post with and then hit share.\\n\\nYour post should now be shared with your Facebook account as well."},{"id":"/2012/03/21/word-not-opening","metadata":{"permalink":"/2012/03/21/word-not-opening","source":"@site/blog/2012-03-21-word-not-opening.md","title":"Howto Fix Word not opening","description":"Having problems opening Word? This usually occurs when the template that it uses to generate a new fresh page is corrupted. Follow the instructions below to fix:","date":"2012-03-21T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Howto Fix Word not opening","permalink":"/win/word-not-opening/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Post to Facebook at the same time as Google+","permalink":"/2012/03/21/post-to-facebook-at-the-same-time-as-google"},"nextItem":{"title":"Moshi Voice Commands","permalink":"/2012/03/20/moshi-commands"}},"content":"Having problems opening Word? This usually occurs when the template that it uses to generate a new fresh page is corrupted. Follow the instructions below to fix:\\n\\n  1. Open My Documents\\n  2. Up the top, select Tools\\n  3. Select Folder Options\\n  4. Select View\\n  5. **Enable** **Hidden** Files**/folders**\\n  6. Press Ok\\n  7. Close My Documents\\n  8. Click Start\\n  9. Click **Search**\\n 10. In the search field type: **normal.dot**\\n 11. **Delete** the **Normal.dot** file that appears in the search result.\\n 12. **Open Word**\\n\\nThe file might also be located in the folders below, and is also safe to delete.\\n\\n_C:/Documents and Settings/%user%/Application Data/Microsoft/Templates_\\n\\n_C:/Program Files/Microsoft Office/Templates_"},{"id":"/2012/03/20/moshi-commands","metadata":{"permalink":"/2012/03/20/moshi-commands","source":"@site/blog/2012-03-20-moshi-commands.md","title":"Moshi Voice Commands","description":"This is a list of the Voice Commands for the Moshi voice controlled Alarm clock.","date":"2012-03-20T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.65,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Moshi Voice Commands","permalink":"/misc/moshi-commands/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Howto Fix Word not opening","permalink":"/2012/03/21/word-not-opening"},"nextItem":{"title":"How to Mass Delete Text Messages on Nokia n95","permalink":"/2012/03/19/massdel-nokia-n95"}},"content":"This is a list of the Voice Commands for the Moshi voice controlled Alarm clock.\\n\\n  * **Time** &#8211; _Tells you the current time_\\n  * **Set Time** &#8211; _Set time by voice_\\n  * **Today&#8217;s Date** &#8211; _Tells you today&#8217;s date_\\n  * **Alarm** &#8211; _Tells you the current alarm time_\\n  * **Set Alarm** &#8211; _Set alarm by voice_\\n  * **Alarm Sound** &#8211; _Choose 1 of 3 available alarms (chime, chirp, bell)_\\n  * **Turn Off The Alarm** &#8211; _Turns off alarm and tells current time, date & temperature_\\n  * **Sleep Sound** &#8211; _Choose 1 of 3 available sleep sounds_\\n  * **Play Sleep Sound** &#8211; _Plays 5 minutes of sleep sound_\\n  * **Temperature** &#8211; _Tells you current temperature_\\n  * **Night Light** &#8211; _Turns on night light_\\n  * **Help** &#8211; _Offers help menu for assistance_"},{"id":"/2012/03/19/massdel-nokia-n95","metadata":{"permalink":"/2012/03/19/massdel-nokia-n95","source":"@site/blog/2012-03-19-massdel-nokia-n95.md","title":"How to Mass Delete Text Messages on Nokia n95","description":"1. Go to your phones Inbox","date":"2012-03-19T00:00:00.000Z","tags":[{"inline":true,"label":"Mobile","permalink":"/tags/mobile"}],"readingTime":0.255,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Mass Delete Text Messages on Nokia n95","permalink":"/mob/massdel-nokia-n95/","tags":["Mobile"]},"unlisted":false,"prevItem":{"title":"Moshi Voice Commands","permalink":"/2012/03/20/moshi-commands"},"nextItem":{"title":"Star Wars : The Old Republic Login Service Not Available","permalink":"/2012/03/15/star-wars-the-old-republic-login-service"}},"content":"1. Go to your phones **Inbox**\\n  2. Go into **Options**\\n  3. Choose **Mark**/**Unmark**\\n  4. **Select** **Mark** **All**\\n  5. Press &#8220;**C**&#8221; (or Options, then Delete)\\n\\nYou can also press Edit \xa0while scrolling up or down to select if you want to choose messages then press the C button to remove the messages."},{"id":"/2012/03/15/star-wars-the-old-republic-login-service","metadata":{"permalink":"/2012/03/15/star-wars-the-old-republic-login-service","source":"@site/blog/2012-03-15-star-wars-the-old-republic-login-service.md","title":"Star Wars : The Old Republic Login Service Not Available","description":"Getting the\xa0login\xa0service not\xa0available? Follow the instructions below:","date":"2012-03-15T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.435,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Star Wars : The Old Republic Login Service Not Available","permalink":"/misc/star-wars-the-old-republic-login-service/","tags":["Misc","Windows"]},"unlisted":false,"prevItem":{"title":"How to Mass Delete Text Messages on Nokia n95","permalink":"/2012/03/19/massdel-nokia-n95"},"nextItem":{"title":"Hybserv2 &#8220;Unable to open SET PATH&#8221;","permalink":"/2012/03/14/hybserv2-setpath"}},"content":"Getting the\xa0login\xa0service not\xa0available? Follow the instructions below:\\n\\nYou need to change your Date/Time on the computer so that it is correct, or the login servers will not be able to communicate with your computer.\\n\\n  1. **Close** the **Old Republic** Launcher\\n  2. Click **Start**\\n  3. Click **Control** **Panel**\\n  4. Click **Time**/**Date**\\n  5. Choose **Change**\\n  6. **Set** the **correct** **Time/Date** (also handy to click the Internet tab and make sure that you are getting the latest time from\xa0Microsoft&#8217;s\xa0NTP servers)\\n  7. Click **Ok**\\n  8. Now **open** the **Old Republic** Launcher"},{"id":"/2012/03/14/hybserv2-setpath","metadata":{"permalink":"/2012/03/14/hybserv2-setpath","source":"@site/blog/2012-03-14-hybserv2-setpath.md","title":"Hybserv2 &#8220;Unable to open SET PATH&#8221;","description":"Getting the Hybserv2 &#8220;Unable to open SETPATH&#8221; error? Follow the instructions below to repair.","date":"2012-03-14T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.38,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Hybserv2 &#8220;Unable to open SET PATH&#8221;","permalink":"/linux/hybserv2-setpath/","tags":["Linux","Windows"]},"unlisted":false,"prevItem":{"title":"Star Wars : The Old Republic Login Service Not Available","permalink":"/2012/03/15/star-wars-the-old-republic-login-service"},"nextItem":{"title":"How to enable Keyboard hotkeys for iTunes","permalink":"/2012/03/13/enable-keyboard-hotkeys-for-itunes"}},"content":"Getting the Hybserv2 &#8220;Unable to open SETPATH&#8221; error? Follow the instructions below to repair.\\n\\n**Windows**\\n\\n  1. **Right** **click** on My **Computer**\\n  2. **Left** **click** on **Properties**\\n  3. Left click on **Advanced** **System** **Settings**\\n  4. Left click on **Environment** **Variables**.\\n  5. **Enter** **new** system **variable** with **name** &#8220;**PATH**&#8221; and **value** (The **path** for your **Perl/bin folder**).\\n\\n**Linux**\\n\\n  1. **Open** a **Terminal**\\n  2. **Type**:\xa0**cd include**\\n  3. Press **Enter**\\n  4. **Type\xa0pico config.**h\\n  5. Type:\xa0**\\\\# define SETPATH &#8220;/home/USER/hybserv/etc/settings.conf**\\n  6. **Save\xa0**"},{"id":"/2012/03/13/enable-keyboard-hotkeys-for-itunes","metadata":{"permalink":"/2012/03/13/enable-keyboard-hotkeys-for-itunes","source":"@site/blog/2012-03-13-enable-keyboard-hotkeys-for-itunes.md","title":"How to enable Keyboard hotkeys for iTunes","description":"1. Open iTunes","date":"2012-03-13T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.105,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to enable Keyboard hotkeys for iTunes","permalink":"/win/enable-keyboard-hotkeys-for-itunes/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Hybserv2 &#8220;Unable to open SET PATH&#8221;","permalink":"/2012/03/14/hybserv2-setpath"},"nextItem":{"title":"Howto change the Bit Torrent Upload Speed in Opera","permalink":"/2012/03/13/opera_btupload"}},"content":"1. Open **iTunes**\\n  2. Click **Edit**\\n  3. Click **Preferences**\\n  4. **Check** the **checkbox** titled &#8220;**Enable Full Keyboard Navigation**&#8220;\\n  5. Click **ok**"},{"id":"/2012/03/13/opera_btupload","metadata":{"permalink":"/2012/03/13/opera_btupload","source":"@site/blog/2012-03-13-opera_btupload.md","title":"Howto change the Bit Torrent Upload Speed in Opera","description":"Here is how you edit the Bit Torrent upload speed in Opera 9","date":"2012-03-13T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.415,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Howto change the Bit Torrent Upload Speed in Opera","permalink":"/win/opera_btupload/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to enable Keyboard hotkeys for iTunes","permalink":"/2012/03/13/enable-keyboard-hotkeys-for-itunes"},"nextItem":{"title":"How to Boot from CD/DVD on a Macbook","permalink":"/2012/03/12/boot_macbook"}},"content":"Here is how you edit the Bit Torrent upload speed in Opera 9\\n\\n  1. Open **Opera**\\n  2. In the **Address** **bar** **type**: **opera:config** _(This is a hidden settings panel for opera)\\n  3. **Click** **Bit** **Torent**\\n  4. In the **Max** **Upload** **Rate** **field** **type** the maximum upload rate (in kilobytes) you would like to go to.\\n  5. Click **Save**\\n  6. **Restart** Opera\\n\\nIf you restart your broswer and realise that the torrents are no longer downloading. Check your download folder and reopen them again."},{"id":"/2012/03/12/boot_macbook","metadata":{"permalink":"/2012/03/12/boot_macbook","source":"@site/blog/2012-03-12-boot_macbook.md","title":"How to Boot from CD/DVD on a Macbook","description":"Want to boot from CD/DVD on your Macbook but don&#8217;t know how? The answer is simple!","date":"2012-03-12T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.265,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Boot from CD/DVD on a Macbook","permalink":"/osx/boot_macbook/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"Howto change the Bit Torrent Upload Speed in Opera","permalink":"/2012/03/13/opera_btupload"},"nextItem":{"title":"How to access eMachines recovery partition.","permalink":"/2012/03/12/how-to-access-emachines-recovery-partition"}},"content":"Want to boot from CD/DVD on your Macbook but don&#8217;t know how? The answer is simple!\\n\\n  1. **Hold** the &#8220;**Option**&#8221; key on the keyboard **during\xa0start**&#8211;**up**.\\n  2. **Using** the arrow keys, **select** the **CD**/**DVD** Drive at the **menu** and select **return**.\\n\\nUsing this same method you can switch between Bootcamp and the OSX install."},{"id":"/2012/03/12/how-to-access-emachines-recovery-partition","metadata":{"permalink":"/2012/03/12/how-to-access-emachines-recovery-partition","source":"@site/blog/2012-03-12-how-to-access-emachines-recovery-partition.md","title":"How to access eMachines recovery partition.","description":"Want to restore a Gateway computer back to factory settings? This is how you access the eMachines recovery partition","date":"2012-03-12T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to access eMachines recovery partition.","permalink":"/misc/how-to-access-emachines-recovery-partition/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to Boot from CD/DVD on a Macbook","permalink":"/2012/03/12/boot_macbook"},"nextItem":{"title":"Godaddy Email Settings","permalink":"/2012/03/09/godaddy-email-settings"}},"content":"Want to restore a Gateway computer back to factory settings? This is how you access the eMachines recovery partition\\n\\n  1. Turn **computer** **on**\\n  2. **Press Alt-F10** at same time\\n  3. The **recovery** process **will** **start**.\\n\\nKeep in mind, the recovery process will remove all your user data, including documents & pictures and restore the computer back to when you had first turned it on."},{"id":"/2012/03/09/godaddy-email-settings","metadata":{"permalink":"/2012/03/09/godaddy-email-settings","source":"@site/blog/2012-03-09-godaddy-email-settings.md","title":"Godaddy Email Settings","description":"Godaddy Email Settings","date":"2012-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.645,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Godaddy Email Settings","permalink":"/misc/godaddy-email-settings/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to access eMachines recovery partition.","permalink":"/2012/03/12/how-to-access-emachines-recovery-partition"},"nextItem":{"title":"hal.dll Corrupted/Missing Fix","permalink":"/2012/03/09/hal-dll-corruptedmissing-fix"}},"content":"**Godaddy Email Settings**\\n\\nGodaddy and there resellers (Wild West Domains resellers) use the following servers for incomming and outgoing email, Depending on what datacenter you are in\\n\\n**American datacenters**\\n\\nIncoming (POP3) server: pop.secureserver.net\\n  \\nOutgoing (SMTP) server: smtpout.secureserver.net\\n\\n**Asian Datacenters**\\n\\nIncoming (POP3) server: pop.asia.secureserver.net\\n  \\nOutgoing (SMTP) server: smtpout.asia.secureserver.net\\n  \\nEuropean Datacenters\\n\\nIncoming (POP3) server: pop.europe.secureserver.net\\n  \\nOutgoing (SMTP) server: smtpout.europe.secureserver.net\\n\\nGoDaddy&#8217;s mail servers support Secure and non secure POP/IMAP/SMTP, here are some of the protocols that work\\n\\nUsername\\n  \\nYour full email address\\n  \\nPassword\\n  \\nYour email account password\\n  \\nIncoming Mail Server\\n  \\nYour incoming server.\\n  \\nOutgoing Mail Server\\n  \\nYour outgoing server.\\n  \\nIncoming Port\\n  \\nWithout SSL &#8211; 110\\n  \\nWith SSL &#8211; 995\\n  \\nOutgoing Port\\n  \\nWithout SSL &#8211; one of the following 25, 80, 3535, 587\\n  \\nWith SSL &#8211; 465\\n\\nOriginal resource found &#8220;<a href=\\"http://emailmojo.com/godaddy_and_secureserver_email_settings.html\\" target=\\"_blank\\">here</a>&#8221;"},{"id":"/2012/03/09/hal-dll-corruptedmissing-fix","metadata":{"permalink":"/2012/03/09/hal-dll-corruptedmissing-fix","source":"@site/blog/2012-03-09-hal-dll-corruptedmissing-fix.md","title":"hal.dll Corrupted/Missing Fix","description":"Windows could not start because the following file is missing or corrupt:","date":"2012-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.405,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"hal.dll Corrupted/Missing Fix","permalink":"/win/hal-dll-corruptedmissing-fix/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Godaddy Email Settings","permalink":"/2012/03/09/godaddy-email-settings"},"nextItem":{"title":"How to take Snapshots from Movies","permalink":"/2012/03/09/how-to-take-snapshots-from-movies"}},"content":"Windows could not start because the following file is missing or corrupt:\\n  \\nWindows/system32/hal.dll\\n  \\nPlease re-install a copy of the above file\\n\\n  1. **Boot** using **Windows** Setup **CD**\\n  2. Go into the **Recovery** **Console** by pressing &#8220;R&#8221;.\\n  3. Select the appropriate Windows install\\n  4. Enter the administrator password\\n  5. **Type** the **following** in the console and then press enter (Each line is one different command)\\n  6. **Attrib -H -R -S C:Boot.ini**\\n  7. **Del C:Boot.ini**\\n  8. **BootCfg /Rebuild**\\n  9. **Fixboot**\\n 10. **Restart** PC"},{"id":"/2012/03/09/how-to-take-snapshots-from-movies","metadata":{"permalink":"/2012/03/09/how-to-take-snapshots-from-movies","source":"@site/blog/2012-03-09-how-to-take-snapshots-from-movies.md","title":"How to take Snapshots from Movies","description":"Ever wanted to capture that perfect scene from a movie or video clip? Heres how to do it.","date":"2012-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.46,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to take Snapshots from Movies","permalink":"/win/how-to-take-snapshots-from-movies/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"hal.dll Corrupted/Missing Fix","permalink":"/2012/03/09/hal-dll-corruptedmissing-fix"},"nextItem":{"title":"iMac Beep Codes","permalink":"/2012/03/09/imac-beep-codes"}},"content":"Ever wanted to capture that perfect scene from a movie or video clip? Heres how to do it.\\n\\n**Using Windows Media Player**\\n\\n  1. At the scene you would like to save, press Ctrl-I at the same time to save the snapshot.\\n\\n**Using <a href=\\"http://www.videolan.org/vlc/\\" target=\\"_blank\\">VideoLAN</a> (VLC Media Player)**\\n\\n  1. Pause the scene that you would like to save, by pressing the Space Bar.\\n  2. Click Video\\n  3. Click Snapshot\\n  4. Your snapshot of the scene has now been saved, usually to My Pictures. However the location can be changed in VLC&#8217;s settings."},{"id":"/2012/03/09/imac-beep-codes","metadata":{"permalink":"/2012/03/09/imac-beep-codes","source":"@site/blog/2012-03-09-imac-beep-codes.md","title":"iMac Beep Codes","description":"1 beep = No RAM installed/detected","date":"2012-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.235,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"iMac Beep Codes","permalink":"/osx/imac-beep-codes/","tags":["Mac OSX"]},"unlisted":false,"prevItem":{"title":"How to take Snapshots from Movies","permalink":"/2012/03/09/how-to-take-snapshots-from-movies"},"nextItem":{"title":"How to access Gateways recovery partition.","permalink":"/2012/03/08/gateways-recovery-partition"}},"content":"**1 beep** = No RAM installed/detected\\n  \\n**2 beeps** = Incompatible RAM type installed (for example, EDO)\\n  \\n**3 beeps** = No RAM banks passed memory testing\\n  \\n**4 beeps** = Bad checksum for the remainder of the boot ROM\\n  \\n**5 beeps** = Bad checksum for the ROM boot block"},{"id":"/2012/03/08/gateways-recovery-partition","metadata":{"permalink":"/2012/03/08/gateways-recovery-partition","source":"@site/blog/2012-03-08-gateways-recovery-partition.md","title":"How to access Gateways recovery partition.","description":"Want to restore a Gateway computer back to factory settings? This is how you access the Gateway recovery partition.","date":"2012-03-08T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.275,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to access Gateways recovery partition.","permalink":"/misc/gateways-recovery-partition/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"iMac Beep Codes","permalink":"/2012/03/09/imac-beep-codes"},"nextItem":{"title":"Virtual Box   &#8216;Failed, trying without DKMS&#8221;","permalink":"/2012/03/08/virtual-box-failed-trying-without-dkms"}},"content":"Want to restore a Gateway computer back to factory settings? This is how you access the Gateway recovery partition.\\n\\n  1. Turn **computer** **on**\\n  2. **Tap** **F11** at\xa0start-up to launch the Recovery\xa0Partition\xa0software\\n\\nIf the above doesn&#8217;t work do this instead (more relevant for newer models)\\n\\n  1. Turn **computer** **on**\\n  2. Tap **Alt**+**F10\xa0**at\xa0start-up to launch the Recovery\xa0Partition\xa0software"},{"id":"/2012/03/08/virtual-box-failed-trying-without-dkms","metadata":{"permalink":"/2012/03/08/virtual-box-failed-trying-without-dkms","source":"@site/blog/2012-03-08-virtual-box-failed-trying-without-dkms.md","title":"Virtual Box   &#8216;Failed, trying without DKMS&#8221;","description":"This means simply, you are missing the dkms package.","date":"2012-03-08T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Virtual Box   &#8216;Failed, trying without DKMS&#8221;","permalink":"/linux/virtual-box-failed-trying-without-dkms/","tags":["Linux"]},"unlisted":false,"prevItem":{"title":"How to access Gateways recovery partition.","permalink":"/2012/03/08/gateways-recovery-partition"},"nextItem":{"title":"Failed to install catalog files","permalink":"/2012/03/07/failed-to-install-catalog-files"}},"content":"This means simply, you are missing the <a title=\\"Dynamic Kernel Module Support\\" href=\\"http://en.wikipedia.org/wiki/Dynamic_Kernel_Module_Support\\" target=\\"_blank\\">dkms </a>package.\\n\\n**Ubuntu**\\n\\n  1. **Open** a **Terminal** type: **apt-get install dkms**\\n  2. Press **Enter**\\n  3. Wait for it to **download** & **install**. Try **opening** **Virtual** **Box** Again\\n\\n**Fedora**\\n\\n  1. **Open** a **Terminal** type:\xa0**yum install dkms**\\n  2. Press **Enter**\\n  3. Wait for it to **download** & **install**. Try **opening** **Virtual** **Box** Again"},{"id":"/2012/03/07/failed-to-install-catalog-files","metadata":{"permalink":"/2012/03/07/failed-to-install-catalog-files","source":"@site/blog/2012-03-07-failed-to-install-catalog-files.md","title":"Failed to install catalog files","description":"Getting the\xa0Failed to install catalog files error? This is how you fix it. The issue is related to registry key security settings.","date":"2012-03-07T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.405,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Failed to install catalog files","layout":"post","permalink":"/win/failed-to-install-catalog-files/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Virtual Box   &#8216;Failed, trying without DKMS&#8221;","permalink":"/2012/03/08/virtual-box-failed-trying-without-dkms"},"nextItem":{"title":"How to Import Mass Effect 2 Save into Mass Effect 3","permalink":"/2012/03/07/me2_to_me3"}},"content":"Getting the\xa0Failed to install catalog files error? This is how you fix it. The issue is related to registry key security settings.\\n\\n  1. Click **Start**\\n  2. Click **Run**\\n  3. Type in: **command**\\n  4. Press **enter**\\n  5. In the command prompt **type**:\xa0**secedit /configure /cfg c:Windowsrepairsecsetup.inf /db secsetup.sdb /verbose /areas regkeys**\\n  6. Press **Enter\xa0**\\n\\nIf it reports an error about not knowing where secedit is then you can download it from &#8220;here&#8221;. You need to download it and put it in My Computer/C."},{"id":"/2012/03/07/me2_to_me3","metadata":{"permalink":"/2012/03/07/me2_to_me3","source":"@site/blog/2012-03-07-me2_to_me3.md","title":"How to Import Mass Effect 2 Save into Mass Effect 3","description":"Problems importing your Mass Effect 2 save game into Mass Effect 3, this is caused because Mass Effect 3 actually looks to the Mass Effect 2 folder.","date":"2012-03-07T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.955,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Import Mass Effect 2 Save into Mass Effect 3","permalink":"/win/me2_to_me3/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Failed to install catalog files","permalink":"/2012/03/07/failed-to-install-catalog-files"},"nextItem":{"title":"Epson Aculaser C1100 Printing Darker on Left","permalink":"/2012/03/06/epson-aculaser-c1100"}},"content":"Problems importing your Mass Effect 2 save game into Mass Effect 3, this is caused because Mass Effect 3 actually looks to the Mass Effect 2 folder.\\n\\n  1. Click **Start**\\n  2. Click My Documents/**Documents**\\n  3. Go into the **Bioware** folder\\n  4. Right click, select **Create** **New**, **Folder**\\n  5. **Name** it &#8220;**Mass Effect 2**&#8220;\\n  6. **Go** into the newly **created** Mass Effect 2 **folder** and **make** a **new** **folder** called &#8220;**Save**&#8220;\\n  7. **Go** into the **Save** **Folder**\\n  8. Now **create** a **new** **folder** for your **character** **save** ie\xa0Luke\\\\_30\\\\_Adept_070212\\n  9. **Go** **into** your new **folder**, and **paste** your **save** **game** from Mass Effect 2 into it.\\n 10. Now **run** **Mass** **Effect** **3**, **Go** to **New** **Game**, **Import** **ME2** **Character** & **choose** **Import** **Career**.\\n 11. Now you should be able to **play**!\\n\\n_If the above doesn&#8217;t work. You need to reinstall Mass Effect 2. Copy your save game back to the Mass Effect 2 folder in My Documents/Bioware/Mass Effect 2/Save and load your save game back into Mass Effect 2. Then save your game again. Exit out of Mass Effect 2, load Mass Effect 3 and you should be able to import your character._"},{"id":"/2012/03/06/epson-aculaser-c1100","metadata":{"permalink":"/2012/03/06/epson-aculaser-c1100","source":"@site/blog/2012-03-06-epson-aculaser-c1100.md","title":"Epson Aculaser C1100 Printing Darker on Left","description":"Clean The Print-head","date":"2012-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.495,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Epson Aculaser C1100 Printing Darker on Left","permalink":"/misc/epson-aculaser-c1100/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to Import Mass Effect 2 Save into Mass Effect 3","permalink":"/2012/03/07/me2_to_me3"},"nextItem":{"title":"Epson Stylus RX600 &#8220;scanner error occurred &#8211; please see owners manual&#8221;","permalink":"/2012/03/06/epson-stylus-rx600-scanner-error-occured-please-see-owners-manual"}},"content":"Clean The Print-head\\n\\n  1. Blue lever on right side of printer _(move in and out completely several times.)_\\n\\nChange a\xa0Toner\xa0Cartridge before its expiration.\\n\\n  1. Make sure that Ready or Sleep is displayed in the LCD panel.\\n  2. Select **Change** Toner X (The letters C, M, Y, or K appear in place of X, indicating the colors Cyan, Magenta, Yellow, and Black.) in the **Reset** **Menu** on the control panel.\\n  3. While the cartridge X you selected above **moves** to the **cartridge** **replacement** **position**, Please Wait appears on the LCD panel.\\n  4. **Replace** **Toner** X appears in the LCD panel"},{"id":"/2012/03/06/epson-stylus-rx600-scanner-error-occured-please-see-owners-manual","metadata":{"permalink":"/2012/03/06/epson-stylus-rx600-scanner-error-occured-please-see-owners-manual","source":"@site/blog/2012-03-06-epson-stylus-rx600-scanner-error-occured-please-see-owners-manual.md","title":"Epson Stylus RX600 &#8220;scanner error occurred &#8211; please see owners manual&#8221;","description":"Epson Stylus Photo RX600 - \\"scanner error occured &#8211; please see owners manual\\"","date":"2012-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.235,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Epson Stylus RX600 &#8220;scanner error occurred &#8211; please see owners manual&#8221;","permalink":"/misc/epson-stylus-rx600-scanner-error-occured-please-see-owners-manual/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Epson Aculaser C1100 Printing Darker on Left","permalink":"/2012/03/06/epson-aculaser-c1100"},"nextItem":{"title":"Error Bad Base Code Image","permalink":"/2012/03/06/error-bad-base-code-image"}},"content":"Epson Stylus Photo RX600 - \\"scanner error occured &#8211; please see owners manual\\"\\n\\n  1. **Turn** printer **off**\\n  2. **Open** the scanning **lid**\\n  3. **Look** for a Gray or Orange **switch** with a padlock symbol on it\\n  4. **Set** it to the &#8220;**unlock**&#8221; position\\n  5. **Turn** printer **on**"},{"id":"/2012/03/06/error-bad-base-code-image","metadata":{"permalink":"/2012/03/06/error-bad-base-code-image","source":"@site/blog/2012-03-06-error-bad-base-code-image.md","title":"Error Bad Base Code Image","description":"Caused by Gigabit PCI network card usually 3Com","date":"2012-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.08,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Error Bad Base Code Image","permalink":"/misc/error-bad-base-code-image/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Epson Stylus RX600 &#8220;scanner error occurred &#8211; please see owners manual&#8221;","permalink":"/2012/03/06/epson-stylus-rx600-scanner-error-occured-please-see-owners-manual"},"nextItem":{"title":"Unblock Programs/Services in Windows Firewall","permalink":"/2012/03/06/unblock_apps_winfirewall"}},"content":"Caused by Gigabit PCI network card usually 3Com\\n\\n  1. **Remove** and/or **replace** the PCI **network** **card**"},{"id":"/2012/03/06/unblock_apps_winfirewall","metadata":{"permalink":"/2012/03/06/unblock_apps_winfirewall","source":"@site/blog/2012-03-06-unblock_apps_winfirewall.md","title":"Unblock Programs/Services in Windows Firewall","description":"1. Click Start","date":"2012-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.215,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Unblock Programs/Services in Windows Firewall","layout":"post","permalink":"/win/unblock_apps_winfirewall/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Error Bad Base Code Image","permalink":"/2012/03/06/error-bad-base-code-image"},"nextItem":{"title":"How to install Vinyl Laptop Skins","permalink":"/2012/03/06/vinyl-laptop-skins"}},"content":"1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click **Network and Internet Connections**\\n  4. Click **Network Connections**\\n  5. **Right-click** your Internet **connection** and click **Properties**\\n  6. Click on **Advanced** tab up the top of the dialog\\n  7. Click **Settings**\\n  8. Go to **Exceptions**"},{"id":"/2012/03/06/vinyl-laptop-skins","metadata":{"permalink":"/2012/03/06/vinyl-laptop-skins","source":"@site/blog/2012-03-06-vinyl-laptop-skins.md","title":"How to install Vinyl Laptop Skins","description":"Start with a clean laptop and a clean, flat working surface. To clean your laptop&#8217;s external surface, we recommend a damp lint-free cloth. Basically, you want to make sure your laptop is dry and free from any oil, dirt, or residue before beginning the skin installation.","date":"2012-03-06T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.805,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to install Vinyl Laptop Skins","layout":"post","permalink":"/misc/vinyl-laptop-skins/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Unblock Programs/Services in Windows Firewall","permalink":"/2012/03/06/unblock_apps_winfirewall"},"nextItem":{"title":"56k Modem Strings","permalink":"/2012/03/05/56k-modem-strings"}},"content":"Start with a clean laptop and a clean, flat working surface. To clean your laptop&#8217;s external surface, we recommend a damp lint-free cloth. Basically, you want to make sure your laptop is dry and free from any oil, dirt, or residue before beginning the skin installation.\\n\\n  1. Carefully remove the laptop skin from its backing, with each person holding 2 corners. Hold the skin taut (but don&#8217;t pull!) and line it up over the top of the laptop, aligning the corners.\\n  2. Set the skin down gently and put light pressure in the\xa0centre\xa0of the skin to bond it to the surface.\\n  3. Working from the\xa0centre, gently bond the skin across the laptop from the center to the outside edges, pressing out any bubbles.\\n  4. If you need to trim the skin to fit, bond the skin well around all edges first, then use a sharp utility knife and gently run the blade down the edge that needs to be trimmed.\\n\\n&nbsp;"},{"id":"/2012/03/05/56k-modem-strings","metadata":{"permalink":"/2012/03/05/56k-modem-strings","source":"@site/blog/2012-03-05-56k-modem-strings.md","title":"56k Modem Strings","description":"Want to lower the volume of your 56k Dial-up connection when\xa0connecting? You can adjust the modem options. Follow these instructions.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.515,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"56k Modem Strings","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to install Vinyl Laptop Skins","permalink":"/2012/03/06/vinyl-laptop-skins"},"nextItem":{"title":"Advpack.dll Not Found","permalink":"/2012/03/05/advpack-not-found"}},"content":"Want to lower the volume of your 56k Dial-up connection when\xa0connecting? You can adjust the modem options. Follow these instructions.\\n\\n  1. Click Start\\n  2. Click Control Panel\\n  3. Click Modem or Phone & Modem Options\\n  4. Make sure you are in the General Tab, and choose Properties.\\n  5. Click Connections\\n  6. Click Advanced\\n  7. Insert the following strings, depending on what you want to do and press Ok.\\n\\n**M0** = Speaker always off\\n  \\n**M1** = Speaker on during connection\\n  \\n**M2** = Speaker always on (very noisy)\\n  \\n**L0** = Lowest volume\\n  \\n**L1** = Lowest volume (redundant)\\n  \\n**L2** = Medium volume\\n  \\n**L3** = Maximum volume"},{"id":"/2012/03/05/advpack-not-found","metadata":{"permalink":"/2012/03/05/advpack-not-found","source":"@site/blog/2012-03-05-advpack-not-found.md","title":"Advpack.dll Not Found","description":"Advpack.dll Not Found is only one version of the error messages that Advpack.dll can create \u2013 \u201c&#8221;This application failed to start because advpack.dll was not found. Re-installing the application may fix this problem\u201d is another known error.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.07,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Advpack.dll Not Found","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"56k Modem Strings","permalink":"/2012/03/05/56k-modem-strings"},"nextItem":{"title":"AMD Data change&#8230;update new data to DMI Error","permalink":"/2012/03/05/amd-data"}},"content":"Advpack.dll Not Found is only one version of the error messages that Advpack.dll can create \u2013 \u201c&#8221;This application failed to start because advpack.dll was not found. Re-installing the application may fix this problem\u201d is another known error.\\n\\nAlthough mainly occurring in the older versions of Windows I have seen this error occur from Windows XP through to Windows 7 and is mainly caused by installing the wrong .Net Framework for your computers architecture \u2013 for example attempting to install .Net Framework x86 on a 64 bit version of Windows.\\n\\n<div id=\\"toc_container\\" class=\\"no_bullets\\">\\n  <p class=\\"toc_title\\">\\n    Table of Contents\\n  </p>\\n  \\n  <ul class=\\"toc_list\\">\\n    <li>\\n      <a href=\\"#Fixing_this_error_in_Windows_7\\"><span class=\\"toc_number toc_depth_1\\">1</span> Fixing this error in Windows 7</a>\\n    </li>\\n    <li>\\n      <a href=\\"#Fixing_this_error_in_Windows_XP\\"><span class=\\"toc_number toc_depth_1\\">2</span> Fixing this error in Windows XP</a>\\n    </li>\\n  </ul>\\n</div>\\n\\n#### <span id=\\"Fixing_this_error_in_Windows_7\\">Fixing this error in Windows 7</span>\\n\\n  1. Click Start\\n  2. Click Control Panel\\n  3. Click Uninstall Programs and select \u201cTurn Windows features on or off&#8221; on the top left.\\n  4. Click on &#8220;Microsoft .NET Framework 3.5.1&#8221;.\\n  5. Expand &#8220;Microsoft .NET Framework 3.5.1&#8221; and place a check mark on 2 options listed to enable.\\n  6. Restart the Computer.\\n\\n&nbsp;\\n\\n#### <span id=\\"Fixing_this_error_in_Windows_XP\\">Fixing this error in Windows XP</span>\\n\\n  1. Fixing this error in Windows XP is as usually as easy as\xa0downloading the correct one &#8220;[here](https://www.microsoft.com/net)&#8221; and installing it."},{"id":"/2012/03/05/amd-data","metadata":{"permalink":"/2012/03/05/amd-data","source":"@site/blog/2012-03-05-amd-data.md","title":"AMD Data change&#8230;update new data to DMI Error","description":"Getting the &#8220;AMD Data change&#8230;update new data to DMI&#8221; error? Try these fixes below.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.6,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"AMD Data change&#8230;update new data to DMI Error","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Advpack.dll Not Found","permalink":"/2012/03/05/advpack-not-found"},"nextItem":{"title":"BearShare Opening in New Firefox Tab","permalink":"/2012/03/05/bearshare-opening-in-new-firefox-tab"}},"content":"Getting the &#8220;AMD Data change&#8230;update new data to DMI&#8221; error? Try these fixes below.\\n\\n  1. Press F1, or F2 to enter BIOs\\n  2. Due to the fact that all BIOs are different, look for ACPI options, usually in Power Management.\\n  3. Enable ACPI\\n  4. Make sure the Time/Date settings are correct, and if not correct them.\\n  5. Set HDD from SATA to IDE, in the Controller settings.\\n  6. Restart\\n\\nIf that didn&#8217;t work, you are looking at a BIOS/CMOs Upgrade/Flash. Look at the Motherboard&#8217;s manufacturers website for information on how to do this. Remember this is quite dangerous and if the wrong model or a power cut occurs during flashing, damage to the Motherboard may occur & make it worse."},{"id":"/2012/03/05/bearshare-opening-in-new-firefox-tab","metadata":{"permalink":"/2012/03/05/bearshare-opening-in-new-firefox-tab","source":"@site/blog/2012-03-05-bearshare-opening-in-new-firefox-tab.md","title":"BearShare Opening in New Firefox Tab","description":"Having the problem where BearShare opens when you open a new Firefox tab even after removing BearShare? This is how I fixed it.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.285,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"BearShare Opening in New Firefox Tab","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"AMD Data change&#8230;update new data to DMI Error","permalink":"/2012/03/05/amd-data"},"nextItem":{"title":"BOOTMGR Errors Windows Vista/Windows 7","permalink":"/2012/03/05/bootmgr"}},"content":"Having the problem where BearShare opens when you open a new Firefox tab even after removing BearShare? This is how I fixed it.\\n\\n  1. Open My **Computer** (Double left click on Desktop icon, or go to Start, Computer)\\n  2. **Navigate** (using the left mouse button) to_ **C:Program Files**_\\n  3. Right click the **Bearshare** folder & **Delete**\\n\\n&nbsp;"},{"id":"/2012/03/05/bootmgr","metadata":{"permalink":"/2012/03/05/bootmgr","source":"@site/blog/2012-03-05-bootmgr.md","title":"BOOTMGR Errors Windows Vista/Windows 7","description":"A few BOOTMGR Fixes for those having problems with Windows 7 /Vista startup.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.605,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"BOOTMGR Errors Windows Vista/Windows 7","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"BearShare Opening in New Firefox Tab","permalink":"/2012/03/05/bearshare-opening-in-new-firefox-tab"},"nextItem":{"title":"Brother 5860CN Change Unable 30 Error Fix","permalink":"/2012/03/05/brother-5860"}},"content":"A few BOOTMGR Fixes for those having problems with Windows 7 /Vista startup.\\n\\n**BOOTMGR is compressed.**\\n\\n  1. **Boot** from the **Vista DVD**.\\n  2. Click **&#8216;Repair my Computer**&#8216;\\n  3. At the prompt to select your OS installation, click the button that says &#8216;**Load Drivers**&#8216;. This should bring up a explorer-style drive browser.\\n  4. Then just right-click on the drive you tried to compress, and **deselect &#8216;Compress this Drive&#8217;**\xa0which should still be ticked on. Hit **Apply** & **Apply** to subfolders/files.\\n  5. **Reboot**\\n\\n_**Fixing &#8220;BOOTMGR is missing&#8221; Error While Trying to Boot Windows 7 or Vista**_\\n\\n  1. Press **F8** at Windows\xa0start-up\xa0to get to the Safe Mode menu & select **Startup Repair**\\n  2. Open **Command Prompt**\\n  3. Type the following command: **bootrec /fixboot**\\n  4. **Reboot**"},{"id":"/2012/03/05/brother-5860","metadata":{"permalink":"/2012/03/05/brother-5860","source":"@site/blog/2012-03-05-brother-5860.md","title":"Brother 5860CN Change Unable 30 Error Fix","description":"The problem usually occurs when the Encoder strip a think plastic strip falls out of its guide inside the printer. Here is how to fix it:","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Brother 5860CN Change Unable 30 Error Fix","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"BOOTMGR Errors Windows Vista/Windows 7","permalink":"/2012/03/05/bootmgr"},"nextItem":{"title":"Brother DCP-110C Waste Ink Counter Reset","permalink":"/2012/03/05/brother-dcp-110c"}},"content":"The problem usually occurs when the Encoder strip a think plastic strip falls out of its guide inside the printer. Here is how to fix it:\\n\\n  1. \xa0**Lift scanner cover** and **remove the two plastic panels covering wires**. Mark down position of cables then remove them, and **remove the now-freed top scanner section entirely**\\n  2. \xa0**Remove screws from top plastic shell** _(There&#8217;s two in the back and four in the front, there&#8217;s also two plastic tabs in the front that must be bent aside before you can remove the top shell.)_\\n  3. Gently **put the encoder strip** back in its **plastic guide**.\\n  4. **Restart Printer**"},{"id":"/2012/03/05/brother-dcp-110c","metadata":{"permalink":"/2012/03/05/brother-dcp-110c","source":"@site/blog/2012-03-05-brother-dcp-110c.md","title":"Brother DCP-110C Waste Ink Counter Reset","description":"Resetting the DCP-110C Printer Waste Ink Counter is easy, follow the instructions below:","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.615,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Brother DCP-110C Waste Ink Counter Reset","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Brother 5860CN Change Unable 30 Error Fix","permalink":"/2012/03/05/brother-5860"},"nextItem":{"title":"Brother HL-140 Drum Light is on","permalink":"/2012/03/05/brother-hl-140"}},"content":"Resetting the DCP-110C Printer Waste Ink Counter is easy, follow the instructions below:\\n\\n  1. Press the **Menu & Mono** start keys next press the **Up arrow** key **4 times** to make the machine enter the maintenance mode.\\n  2. **Enter 8 & 0** by using the up arrow key & SET keys.\\n  3. **Press the mono start key several times** until the purge count appears on the LCD.\\n  4. **Enter 2 7 8 & 3** by using the up arrow key & set keys to **reset the purge count to zero**\\n  5. **Press** the **stop/exit** key to return to the initial stage of the\xa0maintenance\xa0mode.\\n  6. **Enter 9 twice** by using the up arrow & set key&#8217;s to exit from maintenance mode\\n  7. **Restart printer**"},{"id":"/2012/03/05/brother-hl-140","metadata":{"permalink":"/2012/03/05/brother-hl-140","source":"@site/blog/2012-03-05-brother-hl-140.md","title":"Brother HL-140 Drum Light is on","description":"Trying to print with your HL-140 and the Drum light is on? Follow the instructions below.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.445,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Brother HL-140 Drum Light is on","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Brother DCP-110C Waste Ink Counter Reset","permalink":"/2012/03/05/brother-dcp-110c"},"nextItem":{"title":"Brother HL-2040 Paper Light Blinking On/Off","permalink":"/2012/03/05/brother-hl-2040"}},"content":"Trying to print with your HL-140 and the Drum light is on? Follow the instructions below.\\n\\n**Reset the Drum**\\n\\n  1. Press the **cover release button** and then **open the front cover** of the printer.\\n  2. **Press & hold down the Go** button until all **4 LEDs are lit**. Once all 4 LEDs are lit, **release the Go button**.\\n  3. **Close the front cover**.\\n  4. Make sure that the Drum LED is now off.\\n\\n**_Caution: Reset the drum counter only when you replaced the drum unit with a new one._**"},{"id":"/2012/03/05/brother-hl-2040","metadata":{"permalink":"/2012/03/05/brother-hl-2040","source":"@site/blog/2012-03-05-brother-hl-2040.md","title":"Brother HL-2040 Paper Light Blinking On/Off","description":"This problem usually occurs due to a Paper Jam in the printer or a faulty Drum unit in the printer. Follow any of the instructions below that matches your problems.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.715,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Brother HL-2040 Paper Light Blinking On/Off","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Brother HL-140 Drum Light is on","permalink":"/2012/03/05/brother-hl-140"},"nextItem":{"title":"c0000135 {Unable to Locate Component}","permalink":"/2012/03/05/c0000135-unable-to-locate-component"}},"content":"_This problem usually occurs due to a Paper Jam in the printer or a faulty Drum unit in the printer. Follow any of the instructions below that matches your problems._\\n\\n**Printing**\\n\\n  1. In your word document/or document you are trying to print\\n  2. Go to File\\n  3. Page Setup\\n  4. Paper\\n  5. Make sure that your paper source is set to Tray 1 or what Tray the correct paper is in.\\n\\n**Reinstalling Toner**\\n\\n  1. Take off the front cover\\n  2. Pull the tabs on the side\\n  3. Pull out the big toner and cartridge\\n  4. Find the Blue tab\\n  5. Slide the tab from right to left\\n  6. Reinsert toner\\n  7. Close the toner door\\n\\n&nbsp;\\n\\n_If none of the above work, it is possible that your Drum unit is faulty and needs to be sent in to the manufacturer for repair._\\n\\n&nbsp;"},{"id":"/2012/03/05/c0000135-unable-to-locate-component","metadata":{"permalink":"/2012/03/05/c0000135-unable-to-locate-component","source":"@site/blog/2012-03-05-c0000135-unable-to-locate-component.md","title":"c0000135 {Unable to Locate Component}","description":"The application has failed to start because CSRSRV.DLL was not found. Reinstalling the application may fix this problem.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.66,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"c0000135 {Unable to Locate Component}","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Brother HL-2040 Paper Light Blinking On/Off","permalink":"/2012/03/05/brother-hl-2040"},"nextItem":{"title":"c000021a Fatal System Error","permalink":"/2012/03/05/c000021a-fatal-system-error"}},"content":"The application has failed to start because CSRSRV.DLL was not found. Reinstalling the application may fix this problem.\\n\\nFirst you need to access the recovery console. Following the instructions below\\n\\n  1. **Boot**\xa0with your original\xa0**Windows XP CD**. You need one which will allow you to get into the Recovery Console.\\n  2. Once booted, choose to use the\xa0**Recovery Console**\\n  3. **Choose**\xa0which\xa0**Windows installation**\xa0you need to log into. if there\u2019s only one on that machine, it will be \u201c1?.\\n  4. **Type**\xa0**in**\xa0the\xa0**Administrator password**.\\n  5. You need to then copy a version of the file that isn&#8217;t corrupted, over the one that is. Type:**\xa0Copy d:i386csrsrv.dll c:windowssystem32csrsrv.dll**\\n  6. **Restart the Computer**\\n\\n_This command also may indicated Harddrive failure. Scan with a Hard Drive tool such as Spinrite or HDD Regenerator to make sure the drive is in good condition._"},{"id":"/2012/03/05/c000021a-fatal-system-error","metadata":{"permalink":"/2012/03/05/c000021a-fatal-system-error","source":"@site/blog/2012-03-05-c000021a-fatal-system-error.md","title":"c000021a Fatal System Error","description":"&#8220;The Session Manager Initialization system process terminated\xa0unexpectedly\xa0with a status of 0xc000026c&#8221;","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"c000021a Fatal System Error","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"c0000135 {Unable to Locate Component}","permalink":"/2012/03/05/c0000135-unable-to-locate-component"},"nextItem":{"title":"Compaq C300 Keyboard Removal","permalink":"/2012/03/05/c300_kb_removal"}},"content":"&#8220;The Session Manager Initialization system process terminated\xa0unexpectedly\xa0with a status of 0xc000026c&#8221;\\n\\nCaused by a recently Windows Update\\n\\nFirst you need to access the recovery console. Following the instructions below\\n\\n  1. **Boot** with your original **Windows XP CD**. You need one which will allow you to get into the Recovery Console.\\n  2. Once booted, choose to use the **Recovery Console**\\n  3. **Choose** which **Windows installation** you need to log into. if there\u2019s only one on that machine, it will be \u201c1?.\\n  4. **Type** **in** the **Administrator password**.\\n  5. Change to the update folder by typing**\xa0cd WINDOWS$NtUninstallKB925902$spuninst**\\n  6. **Run** **this** **command** in the folder: **batch spuninst.txt** _(If the command ran correctly, you should see a spuninst.tag file, using the \u201cdir\u201d command)_\\n  7. **Restart the Computer**\\n\\n_**At this point the computer should boot properly and you should install SP3 and other patches to prevent this problem from happening again.**_\\n\\nIf the **spuninst.tag** file **doesn\u2019t exist**, you may have received a bunch of errors instead. There are a few reasons why this could happen, one of which is if your windows isn\u2019t really installed on the c: drive, even if the Recovery Console always names it that way. To remove the patch manually **type** in these command in the **Command Prompt** _(By going back using the Windows CD instructions above)_\\n\\n_**DEL \u201cc:windows$hf_mig$kb925902mf3216.dll\u201d**_\\n  \\n _ **DEL \u201cc:windows$hf_mig$kb925902gdi32.dll\u201d**_\\n  \\n _ **DEL \u201cc:windows$hf_mig$kb925902user32.dll\u201d**_\\n  \\n _ **DEL \u201cc:windows$hf_mig$kb925902win32k.sys\u201d**_\\n  \\n _ **DEL \u201cc:windowssystem32dllcachegdi32.dll\u201d**_\\n  \\n _ **DEL \u201cc:windowssystem32dllcachemf3216.dll\u201d**_\\n  \\n _ **DEL \u201cc:windowssystem32dllcacheuser32.dll\u201d**_\\n  \\n _ **DEL \u201cc:windowssystem32dllcachewin32k.sys\u201d**_\\n  \\n _ **COPY \u201cc:windows$NtUninstallKB925902$gdi32.dll\u201d \u201cc:windowssystem32gdi32.dll\u201d**_\\n  \\n _ **COPY \u201cc:windows$NtUninstallKB925902$mf3216.dll\u201d \u201cc:windowssystem32mf3216.dll\u201d**_\\n  \\n _ **COPY \u201cc:windows$NtUninstallKB925902$user32.dll\u201d \u201cc:windowssystem32user32.dll\u201d**_\\n  \\n _ **COPY \u201cc:windows$NtUninstallKB925902$win32k.sys\u201d \u201cc:windowssystem32win32k.sys\u201d**_\\n  \\n _ **COPY \u201cc:windows$NtUninstallKB925902$spuninstspuninst.txt\u201d \u201cc:windows$NtUninstallKB925902$spuninstspuninst.tag\u201d**_\\n\\n_Sources taken from <a href=\\"http://www.techforlunch.com/bsod-session-manager-initialization-fails-with-status-0xc000026c/\\" target=\\"_blank\\">TechForLunch</a>_\\n\\n&nbsp;"},{"id":"/2012/03/05/c300_kb_removal","metadata":{"permalink":"/2012/03/05/c300_kb_removal","source":"@site/blog/2012-03-05-c300_kb_removal.md","title":"Compaq C300 Keyboard Removal","description":"1. Remove the switch cover to access the keyboard screws (The switch cover is secured by to screws located in the battery compartment.)","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.205,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Compaq C300 Keyboard Removal","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"c000021a Fatal System Error","permalink":"/2012/03/05/c000021a-fatal-system-error"},"nextItem":{"title":"Canon IP1200 and IP1600 Cartridge Not Recognized","permalink":"/2012/03/05/canon-ip1200"}},"content":"1. **Remove** the **switch** **cover** to access the keyboard screws (The switch cover is secured by to screws located in the battery compartment.)\\n  2. **Remove** the **keyboard** **screws** which are located on the top of the system under the switch cover."},{"id":"/2012/03/05/canon-ip1200","metadata":{"permalink":"/2012/03/05/canon-ip1200","source":"@site/blog/2012-03-05-canon-ip1200.md","title":"Canon IP1200 and IP1600 Cartridge Not Recognized","description":"1. Click Start","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.55,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Canon IP1200 and IP1600 Cartridge Not Recognized","tags":["Misc"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Compaq C300 Keyboard Removal","permalink":"/2012/03/05/c300_kb_removal"},"nextItem":{"title":"Ejecting CD/DVD stuck in an iMac","permalink":"/2012/03/05/cddvd-stuck-in-an-imac"}},"content":"1. Click **Start**\\n  2. Click **Control Panel**\\n  3. Click on **Printers and Faxes**\\n  4. **Right click** on your **printer** (Canon IP1600)\\n  5. Go down to **properties**\\n  6. Then click on **maintenance**\\n  7. Click on \u2018**View Printer Status\u2026..**\u2019\\n  8. \xa0**Click** **on Option**\\n  9. **Click** on **enable** **status** **monitor** _(This will turn off the monitor and allow you to use the\xa0refilled ink cartridges)_\\n 10. If at any time you get the \u2018Ink out\u2019 message on your PC a light under your power button on the\xa0printer will flash.\\n 11. To **continue printing\xa0**press the **button** and click the **close** button on the\xa0warning (marked X). (You may need to do a few things)\\n\\n&nbsp;\\n\\n&nbsp;"},{"id":"/2012/03/05/cddvd-stuck-in-an-imac","metadata":{"permalink":"/2012/03/05/cddvd-stuck-in-an-imac","source":"@site/blog/2012-03-05-cddvd-stuck-in-an-imac.md","title":"Ejecting CD/DVD stuck in an iMac","description":"CD/DVD stuck in your iMac? Try the options below to force it to eject.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Mac OSX","permalink":"/tags/mac-osx"}],"readingTime":0.43,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Ejecting CD/DVD stuck in an iMac","tags":["Mac OSX"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Canon IP1200 and IP1600 Cartridge Not Recognized","permalink":"/2012/03/05/canon-ip1200"},"nextItem":{"title":"Changing the Aion English Client to Korean","permalink":"/2012/03/05/changing-the-aion-english-client-to-korean"}},"content":"CD/DVD stuck in your iMac? Try the options below to force it to eject.\\n\\n  1. Press **Mouse** **Button** at **boot** to force the CD/DVD to eject\\n\\n_If that doesn&#8217;t work, try the following below._\\n\\n**Hold** down the **C** key during **reboot** _(If the CD/DVD doesn&#8217;t have a Boot sector, the iMac will then eject your CD/DVD)_\\n\\n_If that doesn&#8217;t work, try the following below._\\n\\n  1. **Open** up a **Terminal** _(Applications/Utilities/Terminal)_\\n  2. Type:\xa0**drutil tray eject** & press Return\\n  3. Your CD/DVD should now eject.\\n\\n&nbsp;\\n\\n&nbsp;\\n\\n&nbsp;"},{"id":"/2012/03/05/changing-the-aion-english-client-to-korean","metadata":{"permalink":"/2012/03/05/changing-the-aion-english-client-to-korean","source":"@site/blog/2012-03-05-changing-the-aion-english-client-to-korean.md","title":"Changing the Aion English Client to Korean","description":"Short fix:","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"},{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":1.25,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Changing the Aion English Client to Korean","tags":["Misc","Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Ejecting CD/DVD stuck in an iMac","permalink":"/2012/03/05/cddvd-stuck-in-an-imac"},"nextItem":{"title":"How to forcefully clear the Trash in Linux","permalink":"/2012/03/05/clear-trash-linux"}},"content":"Short fix:\\n\\n  1. \xa0Right-click on Aion in your Launcher game list\\n  2. Select Properties\\n  3. Change/update language\\n\\nLong fix:\\n  \\nYou are running a North American client from Europe and may need to reinstall the European client. Odds are you&#8217;ve been playing Aion since way back in the beginning. When you re-install, the version that gets installed is determined by your OS locale settings and Launcher Options. It will take a while, but you will need to remove Aion completely from the Launcher and from your PC and start from square one with the installation. If you have a decent speed internet, this shouldn&#8217;t take all too long unless you do it during prime time.\\n\\nRemove English files fix:\\n\\n  1. Delete the language files in your installation folder AionL10N\\n  2. Make sure your Launcher&#8217;s regional settings are correct\\n  3. Make sure that your Aion language settings in the Launcher are correct\\n  4. Restart the Launcher completely & let it patch the correct language files.\\n\\nRe-install only in Launcher fix:\\n\\n  1. Remove Aion from your launcher by right-clicking and choosing &#8220;Uninstall&#8221;. Do not remove your game files &#8211; only remove it from the Launcher.\\n  2. Restart the Launcher.\\n  3. Right-click on Aion (should now be displayed as &#8220;Not installed&#8221;) and choose &#8220;Install&#8221;.\\n  4. Customize the installation so that all language settings are correct and point it to your Aion game folder from the previous installation.\\n  5. The Launcher will now try to patch any missing language files & hopefully resolve your issue."},{"id":"/2012/03/05/clear-trash-linux","metadata":{"permalink":"/2012/03/05/clear-trash-linux","source":"@site/blog/2012-03-05-clear-trash-linux.md","title":"How to forcefully clear the Trash in Linux","description":"The Trash stores your recently deleted files and folders which you can usually Right click and clear to clear the Trash in Linux \xa0\u2013 however I have ran into problems where even though you clear the Trash the files and folders you want deleted remain. Good news is that you can force the Trash to be cleared by using the Linux terminal.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Linux","permalink":"/tags/linux"}],"readingTime":0.785,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to forcefully clear the Trash in Linux","tags":["Linux"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"Changing the Aion English Client to Korean","permalink":"/2012/03/05/changing-the-aion-english-client-to-korean"},"nextItem":{"title":"Common Antivirus Removal Tools","permalink":"/2012/03/05/common-antivirus-removal-tools"}},"content":"<p class=\\"Style1\\">\\n  The Trash stores your recently deleted files and folders which you can usually Right click and clear to clear the Trash in Linux \xa0\u2013 however I have ran into problems where even though you clear the Trash the files and folders you want deleted remain. Good news is that you can force the Trash to be cleared by using the Linux terminal.\\n</p>\\n\\n  1. Open your Linux Terminal\\n  2. Copy these commands and paste them into the terminal and press Enter\xa0_(this will clear the Trash completely \u2013 anything in the Trash is unable to be recovered)_\\n\\n&nbsp;\\n\\n**`rm -rf ~/.local/share/Trash`**\\n\\n<p class=\\"Style1\\">\\n  <b><code>mkdir ~/.local/share/Trash</code></b>\\n</p>\\n\\n<p class=\\"Style1\\">\\n  <b><code>mkdir ~/.local/share/Trash/expunged</code></b>\\n</p>\\n\\n<p class=\\"Style1\\">\\n  <b><code>mkdir ~/.local/share/Trash/files</code></b>\\n</p>\\n\\n<p class=\\"Style1\\">\\n  <b><code>mkdir ~/.local/share/Trash/info</code></b>\\n</p>\\n\\n<p class=\\"Style1\\">\\n  <b>\xa0</b>\\n</p>\\n\\n<p class=\\"Style1\\">\\n  Note: Tested in Debian/Ubuntu variant \u2013 other Linux distributions may have the Trash location changed and you will need to find the Trash location on your Linux distribution.\\n</p>"},{"id":"/2012/03/05/common-antivirus-removal-tools","metadata":{"permalink":"/2012/03/05/common-antivirus-removal-tools","source":"@site/blog/2012-03-05-common-antivirus-removal-tools.md","title":"Common Antivirus Removal Tools","description":"Ever had problems trying to get rid of Antivirus applications and some little part that causes problems remain? Below is a list to the websites containing the Removal Tools, which can be run to remove the antivirus components that remain on your system.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.39,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Common Antivirus Removal Tools","tags":["Windows"],"date":"2012-03-05 00:00:00 +1300"},"unlisted":false,"prevItem":{"title":"How to forcefully clear the Trash in Linux","permalink":"/2012/03/05/clear-trash-linux"},"nextItem":{"title":"Common Computer Boot Beeps","permalink":"/2012/03/05/common-computer-boot-beeps"}},"content":"Ever had problems trying to get rid of Antivirus applications and some little part that causes problems remain? Below is a list to the websites containing the Removal Tools, which can be run to remove the antivirus components that remain on your system.\\n\\n<a href=\\"https://www-secure.symantec.com/norton-support/jsp/help-solutions.jsp?docid=20080710133834EN&lg=english&ct=united+states&product=home&version=1&pvid=f-home\\" target=\\"_blank\\">Norton Antivirus // Norton Internet Security</a>\\n\\n<a href=\\"http://download.microsoft.com/download/4/c/b/4cb845e7-1076-437b-852a-7842a8ab13c8/OneCareCleanUp.exe\\" target=\\"_blank\\">Windows Live One Care</a>\\n\\n<a href=\\"http://www.ice-kav.com/tools.php\\" target=\\"_blank\\">Kaspersky Antivirus // Kaspersky PURE</a>\\n\\n<a href=\\"http://www.bitdefender.com/support/How-to-uninstall-BitDefender-333.html\\" target=\\"_blank\\">BitDefender</a>\\n\\n<a href=\\"http://kb.eset.com/esetkb/index?page=content&id=SOLN2289\\" target=\\"_blank\\">Eset NOD32</a>\\n\\n<a href=\\"http://esupport.trendmicro.com/solution/en-us/1036064.aspx\\" target=\\"_blank\\">TrendMicro</a>\\n\\n<a href=\\"http://www.avg.com/ww-en/utilities\\" target=\\"_blank\\">AVG Antivirus</a>"},{"id":"/2012/03/05/common-computer-boot-beeps","metadata":{"permalink":"/2012/03/05/common-computer-boot-beeps","source":"@site/blog/2012-03-05-common-computer-boot-beeps.md","title":"Common Computer Boot Beeps","description":"Beep Code","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.355,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Common Computer Boot Beeps","permalink":"/misc/common-computer-boot-beeps/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Common Antivirus Removal Tools","permalink":"/2012/03/05/common-antivirus-removal-tools"},"nextItem":{"title":"How to Contact Blizzard Entertainment","permalink":"/2012/03/05/contact-blizzard-entertainment"}},"content":"Beep Code: Description of Problem:\\n\\n**No Beeps Short** &#8211; No power, Bad CPU/MB, Loose Peripherals\\n  \\n**One Beep** &#8211; Everything is normal\\n  \\n**Two Beeps** &#8211; POST/CMOS Error\\n  \\n**One Long Beep, One Short Beep** &#8211; Motherboard Problem\\n  \\n**One Long Beep, Two Short Beeps** &#8211; Video Problem\\n  \\n**One Long Beep, Three Short Beeps** &#8211; Video Problem\\n  \\n**Three Long Beeps** &#8211; Keyboard Error\\n  \\n**Repeated Long Beeps** &#8211; Memory Error\\n  \\n**Continuous Hi-Lo Beeps** &#8211; CPU Overheating"},{"id":"/2012/03/05/contact-blizzard-entertainment","metadata":{"permalink":"/2012/03/05/contact-blizzard-entertainment","source":"@site/blog/2012-03-05-contact-blizzard-entertainment.md","title":"How to Contact Blizzard Entertainment","description":"Billing and Account Services","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.41,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Contact Blizzard Entertainment","permalink":"/misc/contact-blizzard-entertainment/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Common Computer Boot Beeps","permalink":"/2012/03/05/common-computer-boot-beeps"},"nextItem":{"title":"CPU Critical Temperatures","permalink":"/2012/03/05/cpu-critical-temperatures"}},"content":"**Billing and Account Services**\\n\\n* Email: billing@blizzard.com\\n  \\n* Web form\\n\\nLive Representatives Available Mon-Fri, 8am to 8pm PST\\n\\nFor phone assistance please call: 1 (800) 592-5499 || 1 (800) 59-BLIZZARD\\n  \\n*Australian users should call 1-800-041-378 if the above number doesn\u2019t work.\\n  \\n*Singaporean users should call 1 800-2549-9273 if the above number doesn\u2019t work.\\n\\n**From New Zealand, we need to dial a prefix to call international/USA 1800 numbers.**\\n\\nThe format is: **0168 1 800 592 5499** (for the billing number, as an example)"},{"id":"/2012/03/05/cpu-critical-temperatures","metadata":{"permalink":"/2012/03/05/cpu-critical-temperatures","source":"@site/blog/2012-03-05-cpu-critical-temperatures.md","title":"CPU Critical Temperatures","description":"AMD","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":2.295,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"CPU Critical Temperatures","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"How to Contact Blizzard Entertainment","permalink":"/2012/03/05/contact-blizzard-entertainment"},"nextItem":{"title":"Deactivating Windows XP","permalink":"/2012/03/05/deactivating-winxp"}},"content":"**AMD**\\n\\n_**AMD Athlon Series**_\\n\\nAMD Athlon (socket) up to 1Ghz 90\xb0C\\n  \\nAMD Athlon (slot) all speeds 70\xb0C\\n  \\nAMD Athlon Thunderbird 1.1Ghz+ 95\xb0C\\n  \\nAMD Athlon MP 1.33Ghz+ 95\xb0C\\n  \\nAMD Athlon XP 1.33Ghz+ 90\xb0C\\n  \\nAMD Athlon XP T-Bred up to 2100+ 90\xb0C\\n  \\nAMD Athlon XP T-Bred over 2100+ 85\xb0C\\n  \\nAMD Athlon XP Barton 85\xb0C\\n  \\nAMD Athlon 64 70\xb0C\\n  \\nAMD Athlon 64 (Socket 939, 1.4 volts) 65\xb0C\\n  \\nAMD Athlon 64 FX (sledgehammer) 70\xb0C\\n  \\nAMD Athlon FX (San Diego + Toledo + Windsor) 63\xb0C\\n  \\nAMD Athlon X2 (Manchester + Toledo) 65\xb0C\\n  \\nAMD Athlon X2 (Windsor) 70\xb0C-72\xb0C (1.35v &#8211; 1.25v)\\n  \\nAMD Athlon X2 (Brisbane) 78\xb0C (1.25v)\\n\\n_**AMD Phenom**_ \\n\\nAMD Phenom 70\xb0C\\n  \\nAMD Phenom X3 70\xb0C\\n  \\nAMD Phenom X4 (9100, 9750, 9850) 61\xb0C\\n  \\nAMD Phenom X4 (9550, 9650) 70\xb0C\\n\\n_**AMD Sempron**_ \\n\\nAMD Sempron (T-bred/Barton core) 90\xb0C\\n  \\nAMD Sempron (Paris core) 70\xb0C\\n  \\nAMD Sempron (Manila) 69\xb0C &#8211; 78\xb0C\\n  \\nAMD Sempron (Sparta) 65\xb0C &#8211; 75\xb0C\\n  \\nAMD Sempron (Sargus) 63\xb0C\\n  \\nAMD Mobile Sempron 95\xb0C\\n\\n_**AMD Athlon II**_\\n  \\nAthlon II X2 (Regor) 72\xb0C &#8211; 81\xb0C\\n  \\nAthlon II X3 (Rana) 71\xb0C &#8211; 75\xb0C\\n  \\nAthlon II X4 (Propus) 70\xb0C &#8211; 72\xb0C\\n\\n_**AMD Phenom II**_ \\n\\nPhenom II X2 (Callisto) 70\xb0C\\n  \\nPhenom II X3 (Heka) 72\xb0C\\n  \\nPhenom II X4 (Denab) 62\xb0C &#8211; 71\xb0C\\n  \\nPhenom II X6 (Thuban) 62\xb0C &#8211; 71\xb0C\\n\\n**Intel CPU&#8217;s**\\n\\n_**Intel Pentium D**_ \\n\\nPentium D (Smithfield 805, 820) 63\xb0C\\n  \\nPentium D (Smithfield 830, 840) 69.8\xb0C\\n  \\nPentium D (Presler 915, 920, 930, 945, 960) 63.4\xb0C\\n  \\nPentium D (Presler 940, 950) 68.6\xb0C\\n\\n_**Intel Celeron Serie**s_\\n\\nCeleron D (Prescott) 67\xb0C\\n  \\nCeleron D (Cedar Mill) 69.2\xb0C\\n  \\nMobile Celeron 100\xb0C\\n\\n_**Intel Core 2 Duo**_ \\n\\nIntel core 2 Duo (Conroe E4300, E4400, E6300, E6400) 61.4\xb0C\\n  \\nIntel Core 2 Duo (Conroe E4500, E4600, E4700) 73.3\xb0C\\n  \\nIntel Core 2 Duo (Conroe E6320, E6420, E6540, E6550, E6600, E6700, E6750, E6850) 60.1\xb0C\\n  \\nIntel Core 2 Duo (Wolfdale) 72.4\xb0C\\n  \\nMobile Core 2 Duo 100\xb0C\\n\\n_**Intel Core 2 Extreme**_ \\n\\nIntel Core 2 Extreme (Conroe) 60.4\xb0C\\n  \\nIntel Core 2 Extreme (Kentsfield Q6700) 71\xb0C\\n  \\nIntel Core 2 Extreme (Kentsfield Q6600) 62.2\xb0C\\n  \\nIntel Core 2 Extreme (Kentsfield QX6700, QX6850) 64.5\xb0C\\n  \\nIntel Core 2 Extreme (Kentsfield QX6800) 54.8\xb0C\\n  \\nIntel Core 2 Extreme (Yorkfiled Q9300, Q9450, Q9550) 71.4\xb0C\\n  \\nIntel Core 2 Extreme (Yorkfield QX9650) 64.5\xb0C\\n  \\nIntel Core 2 Extreme (Yorkfield QX9770) 55.5\xb0C\\n  \\nIntel Core 2 Extreme (Hypertown QX9775) 63\xb0C\\n\\n_**Intel Itanium 2**_ \\n\\nIntel Itanium 2 below 1Ghz 66\xb0C\\n  \\nIntel Itanium 2 1Ghz &#8211; 1.6Ghz 83\xb0C\\n\\n**_Intel Core i3 / i5 / i7_** \\n\\nIntel Core i3 (Clarkdale) 72.6\xb0C\\n  \\nIntel Core i3 (Sandy-Bridge) 65\xb0C &#8211; 69\xb0C\\n  \\nIntel Core i5 (Clarkdale) 72.6\xb0C\\n  \\nIntel Core i5 (Clarkdale) (Model 661) 69.8\xb0C\\n  \\nIntel Core i5 (Lynfield) 72.7\xb0C\\n  \\nIntel Core i5 (Lynfield) (Model 750s) 68.9\xb0C\\n  \\nIntel Core i5 (Sandy-Bridge) 69\xb0C &#8211; 73\xb0C\\n  \\nIntel Core i7 (Lynfield) 69\xb0C &#8211; 73\xb0C\\n  \\nIntel Core i7 (Gulftown) 67.9\xb0C\\n  \\nIntel Core i7 (Sandy-Bridge) 72.6\xb0C\\n  \\nIntel Core i7 Extreme (Bloomfield) 67.9\xb0C"},{"id":"/2012/03/05/deactivating-winxp","metadata":{"permalink":"/2012/03/05/deactivating-winxp","source":"@site/blog/2012-03-05-deactivating-winxp.md","title":"Deactivating Windows XP","description":"Deactivating Windows XP can be useful especially when moving the Windows XP install from one computer to another as it makes Windows activation easier and more automated.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.625,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Deactivating Windows XP","permalink":"/win/deactivating-winxp/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"CPU Critical Temperatures","permalink":"/2012/03/05/cpu-critical-temperatures"},"nextItem":{"title":"Disable New Hardware Wizard in Windows XP to Enable Automatic Device Driver Installation","permalink":"/2012/03/05/disable-new-hardware-wizard"}},"content":"Deactivating Windows XP can be useful especially when moving the Windows XP install from one computer to another as it makes Windows activation easier and more automated.\\n\\nFollow the simple guide below to make the necessary Registry change to deactivate your Windows XP version.\\n\\n**Deactivating Windows XP**\\n\\n<ol style=\\"font-family: sans-serif; font-size: medium; font-style: normal; line-height: normal;\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Run</strong>\\n  </li>\\n  <li>\\n    Type &#8216;<strong>regedit</strong>&#8216;\\n  </li>\\n  <li>\\n    Navigate to\\n  </li>\\n  <li>\\n    <strong><em>HKEY_LOCAL_MACHINE/Software/Microsoft/WindowsNT/Current Version/WPAEvents</em></strong>\\n  </li>\\n  <li>\\n    <strong>Double</strong>&#8211;<strong>click <em>OOBETimer</em></strong>, when the box comes up, <strong>change</strong> the <strong>LAST</strong> <strong>TWO</strong> HEX <strong>VALUES</strong> to nulls(<strong><em>00 00</em></strong>)\\n  </li>\\n</ol>\\n\\n**Reactivating Windows XP**\\n\\n<ol style=\\"font-family: sans-serif; font-size: medium; font-style: normal; line-height: normal;\\">\\n  <li>\\n    Click <strong>Start</strong>\\n  </li>\\n  <li>\\n    Click <strong>Run</strong>\\n  </li>\\n  <li>\\n    Type &#8220;<strong>c:\\\\windows\\\\system32\\\\oobe\\\\msoobe.exe /a</strong>&#8220;\\n  </li>\\n  <li>\\n    Press <strong>Enter</strong>\\n  </li>\\n</ol>"},{"id":"/2012/03/05/disable-new-hardware-wizard","metadata":{"permalink":"/2012/03/05/disable-new-hardware-wizard","source":"@site/blog/2012-03-05-disable-new-hardware-wizard.md","title":"Disable New Hardware Wizard in Windows XP to Enable Automatic Device Driver Installation","description":"Want to skip straight to automatic hardware installation? Follow the instructions below to edit the registry.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.465,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Disable New Hardware Wizard in Windows XP to Enable Automatic Device Driver Installation","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Deactivating Windows XP","permalink":"/2012/03/05/deactivating-winxp"},"nextItem":{"title":"Export &#038; Import Windows XP Theme","permalink":"/2012/03/05/export-import-windows-xp-theme"}},"content":"Want to skip straight to automatic hardware installation? Follow the instructions below to edit the registry.\\n\\n  1. Click **Start**\\n  2. Click **Programs**\\n  3. Click Accessories\\n  4. Click **Notepad**\\n  5. Copy the script below:\\n\\n<div>\\n  <p>\\n    <em>REGEDIT4</em>\\n  </p>\\n  \\n  <p>\\n    <em>[HKEY_LOCAL_MACHINESOFTWAREPoliciesMicrosoftWindowsDriverSearching]</em><br /> <em> &#8220;DontSearchWindowsUpdate&#8221;=dword:00000001</em><br /> <em> &#8220;DontPromptForWindowsUpdate&#8221;=dword:00000001</em><br /> <em> &#8220;DontSearchCD&#8221;=dword:00000001</em><br /> <em> &#8220;DontSearchFloppies&#8221;=dword:00000001</em>\\n  </p>\\n  \\n  <p>\\n    <em>[HKEY_CURRENT_USERSoftwarePoliciesMicrosoftWindows NTDriver Signing]</em><br /> <em> &#8220;behaviorOnFailedVerify&#8221;=dword:00000000</em>\\n  </p>\\n  \\n  <p>\\n    <em>[HKEY_USERS.DEFAULTSoftwarePoliciesMicrosoftWindows NTDriver Signing]</em><br /> <em> &#8220;behaviorOnFailedVerify&#8221;=dword:00000000</em>\\n  </p>\\n</div>\\n\\n  1. **Paste** into Notepad\\n  2. Click File\\n  3. Click **Save As**\\n  4. Name the file &#8220;**_DisableHardware.bat_**&#8220;\\n  5. **Run** the file & accept all changes."},{"id":"/2012/03/05/export-import-windows-xp-theme","metadata":{"permalink":"/2012/03/05/export-import-windows-xp-theme","source":"@site/blog/2012-03-05-export-import-windows-xp-theme.md","title":"Export &#038; Import Windows XP Theme","description":"Export","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.59,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Export &#038; Import Windows XP Theme","permalink":"/win/export-import-windows-xp-theme/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Disable New Hardware Wizard in Windows XP to Enable Automatic Device Driver Installation","permalink":"/2012/03/05/disable-new-hardware-wizard"},"nextItem":{"title":"How to create a Windows XP Picture Screensaver","permalink":"/2012/03/05/how-to-create-a-windows-xp-picture-screensaver"}},"content":"**Export**\\n\\n  1. Right-click the desktop\\n  2. Select Properties\\n  3. Go to the Themes tab\\n  4. Select your theme\\n  5. Click the Save As button\\n  6. Save the file to the My Documents folder _(or folder of your choice)_\\n  7. Navigate to the saved theme location ie My Documents\\n  8. Locate your theme\\n  9. Copy it to a floppy disk or USB thumb drive.\\n\\n**Import**\\n\\n  1. Right-click the desktop and select Properties\\n  2. On the Themes tab, click the Theme drop-down list and select Browse.\\n  3. In the Open Theme dialog box, access the My Documents folder, locate your theme file, and double-click it.\\n  4. Click OK to load the new theme and close the Display Properties dialog box."},{"id":"/2012/03/05/how-to-create-a-windows-xp-picture-screensaver","metadata":{"permalink":"/2012/03/05/how-to-create-a-windows-xp-picture-screensaver","source":"@site/blog/2012-03-05-how-to-create-a-windows-xp-picture-screensaver.md","title":"How to create a Windows XP Picture Screensaver","description":"How to create a Windows XP Picture Screensaver","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.335,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to create a Windows XP Picture Screensaver","permalink":"/win/how-to-create-a-windows-xp-picture-screensaver/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Export &#038; Import Windows XP Theme","permalink":"/2012/03/05/export-import-windows-xp-theme"},"nextItem":{"title":"How to disable the Windows XP Boot Disk","permalink":"/2012/03/05/how-to-disable-the-windows-xp-boot-disk"}},"content":"How to create a Windows XP Picture Screensaver\\n\\n  1. Right-click an empty spot on your desktop and then click **Properties**.\\n  2. Click the **Screen** **Saver** tab.\\n  3. In the Screen saver list, **click** My **Pictures** **Slideshow**.\\n  4. Click Settings to make any adjustments\\n  5. Click **OK**.\\n  6. Click the Preview button to see how it will look.\\n  7. Click OK if you are happy with the results."},{"id":"/2012/03/05/how-to-disable-the-windows-xp-boot-disk","metadata":{"permalink":"/2012/03/05/how-to-disable-the-windows-xp-boot-disk","source":"@site/blog/2012-03-05-how-to-disable-the-windows-xp-boot-disk.md","title":"How to disable the Windows XP Boot Disk","description":"Disable the Windows XP Boot Disk","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to disable the Windows XP Boot Disk","permalink":"/win/how-to-disable-the-windows-xp-boot-disk/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to create a Windows XP Picture Screensaver","permalink":"/2012/03/05/how-to-create-a-windows-xp-picture-screensaver"},"nextItem":{"title":"How To: Fix &#8216;Cryptographic Service&#8217; error when installing Windows XP Updates","permalink":"/2012/03/05/how-to-fix-cryptographic-service-error-when-installing-windows-xp-updates"}},"content":"Disable the Windows XP Boot Disk\\n\\n  1. Click **Start** and then click **Run**; **type** in: **msconfig**\\n  2. **Click BOOT.INI** from the top tabs.\\n  3. Go down to **Boot** **Options**, and **tick**: **/NOGUIBOOT**\\n  4. Click **Apply** then **Ok** to close msconfig.\\n  5. Restart the computer\\n\\nThe splash screen will be gone. It can be re-enabled by unticking /NOGUIBOOT back in the msconfig._"},{"id":"/2012/03/05/how-to-fix-cryptographic-service-error-when-installing-windows-xp-updates","metadata":{"permalink":"/2012/03/05/how-to-fix-cryptographic-service-error-when-installing-windows-xp-updates","source":"@site/blog/2012-03-05-how-to-fix-cryptographic-service-error-when-installing-windows-xp-updates.md","title":"How To: Fix &#8216;Cryptographic Service&#8217; error when installing Windows XP Updates","description":"Open a command prompt window (click Start, Run, type cmd and hit Enter) and then type the following commands:","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.465,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How To: Fix &#8216;Cryptographic Service&#8217; error when installing Windows XP Updates","permalink":"/win/how-to-fix-cryptographic-service-error-when-installing-windows-xp-updates/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to disable the Windows XP Boot Disk","permalink":"/2012/03/05/how-to-disable-the-windows-xp-boot-disk"},"nextItem":{"title":"How to Guide to using Nero Startsmart to burn CDs and DVDs","permalink":"/2012/03/05/howto-guide-to-using-nero-startsmart-to-burn-cds-and-dvds"}},"content":"Open a command prompt window _(click Start, Run, type cmd and hit Enter)_ and then type the following commands:\\n\\n_**regsvr32 softpub.dll**_\\n  \\n_ **regsvr32 wintrust.dll**_\\n  \\n_ **regsvr32 initpki.dll**_\\n  \\n_ **regsvr32 dssenh.dll**_\\n  \\n_ **regsvr32 rsaenh.dll**_\\n  \\n_ **regsvr32 gpkcsp.dll**_\\n  \\n_ **regsvr32 sccbase.dll**_\\n  \\n_ **regsvr32 slbcsp.dll**_\\n  \\n_ **regsvr32 cryptdlg.dll**_\\n\\nRestart Cryptographic Services by clicking the Start button again, and try installing the service pack or security patch once more.\\n\\nIf the above solution&#8217;s do not work. You can use a tool called [Dial A Fix](http://wiki.lunarsoft.net/wiki/Dial-a-fix) to repair the Cryptographic & Windows Update services with a few clicks."},{"id":"/2012/03/05/howto-guide-to-using-nero-startsmart-to-burn-cds-and-dvds","metadata":{"permalink":"/2012/03/05/howto-guide-to-using-nero-startsmart-to-burn-cds-and-dvds","source":"@site/blog/2012-03-05-howto-guide-to-using-nero-startsmart-to-burn-cds-and-dvds.md","title":"How to Guide to using Nero Startsmart to burn CDs and DVDs","description":"To Open Nero","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.815,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Guide to using Nero Startsmart to burn CDs and DVDs","permalink":"/win/howto-guide-to-using-nero-startsmart-to-burn-cds-and-dvds/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How To: Fix &#8216;Cryptographic Service&#8217; error when installing Windows XP Updates","permalink":"/2012/03/05/how-to-fix-cryptographic-service-error-when-installing-windows-xp-updates"},"nextItem":{"title":"How to Create a Directory list to Notepad","permalink":"/2012/03/05/listdir"}},"content":"**To Open Nero**\\n\\n  1. Click **Start**\\n  2. Click **All Programs**\\n  3. Click **Nero**\\n  4. Click **Nero StartSmart**\\n\\n**Burning a CD using Nero StartSmart**\\n\\n  1. At the **top** of Nero make sure you have &#8220;**CD/DVD**&#8221; selected.\\n  2. To make an **Audio or Data CD select the relevant icon**.\\n  3. **Click Make Data / Make Audio CD**.\\n  4. Click **Add**\\n  5. **Select** the **files** you would like to burn to the **CD** and select **Add**.\\n  6. Select **Finished** when you have **finished** **adding** **the** **files** you want to burn.\\n  7. Click **Next**\\n  8. **Change the Disc Name** to something relevant.\\n  9. Click **Burn** to burn CD/DVD.\\n\\nIf you keep having problems burning the CD/DVDs you can lower the burning speed to improve an accurate burn at the same window as the Disc Name.\\n\\nIf you have problems ejecting the CD/DVD drive after burning it might be faulty and you can try ejecting by going into My Computer, Right click the CD/DVD drive and select Eject."},{"id":"/2012/03/05/listdir","metadata":{"permalink":"/2012/03/05/listdir","source":"@site/blog/2012-03-05-listdir.md","title":"How to Create a Directory list to Notepad","description":"This is how you make a list of directories/files on your computer using Command Prompt,","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.23,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"How to Create a Directory list to Notepad","permalink":"/win/listdir/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to Guide to using Nero Startsmart to burn CDs and DVDs","permalink":"/2012/03/05/howto-guide-to-using-nero-startsmart-to-burn-cds-and-dvds"},"nextItem":{"title":"Error Loading PID Generator DLL while attempting to install Age of Empires 3","permalink":"/2012/03/05/mfc42"}},"content":"This is how you make a list of directories/files on your computer using Command Prompt,\\n\\n  1. Click Start\\n  2. Click Programs\\n  3. Click Accessories\\n  4. Click Command Prompt\\n  5. Type in the following command:\xa0**dir /a /b /-p /o:gen >filelisting.txt**\\n  6. It will then create a list."},{"id":"/2012/03/05/mfc42","metadata":{"permalink":"/2012/03/05/mfc42","source":"@site/blog/2012-03-05-mfc42.md","title":"Error Loading PID Generator DLL while attempting to install Age of Empires 3","description":"Usually caused by a missing/damaged &#8220;mfc42.dll&#8221; file. You can repair this by doing the following.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.43,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Error Loading PID Generator DLL while attempting to install Age of Empires 3","permalink":"/win/mfc42/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"How to Create a Directory list to Notepad","permalink":"/2012/03/05/listdir"},"nextItem":{"title":"Adobe Photoshop CS4 148:3","permalink":"/2012/03/05/photoshop_cs4"}},"content":"Usually caused by a missing/damaged &#8220;mfc42.dll&#8221; file. You can repair this by doing the following.\\n\\n  1. Download <a href=\\"http://www.dlldump.com/download-dll-files_new.php/dllfiles/M/mfc42.dll/6.0.400/download.html\\" target=\\"_blank\\">mfc42.dll</a>\xa0(Remember don&#8217;t click any of the Ads, look for &#8220;Click Here to Download mfc42.dll&#8221;)\\n  2. Now go to the location of your newly downloaded file (Usually in Downloads) and right click the file & choose Copy.\\n  3. Open My Computer\\n  4. Go to C:\\\\Windows\\\\System32\\n  5. Right click and select Paste\\n  6. Choose Yes if you are asked to overwrite.\\n  7. Attempt Age of Empires 3 install again."},{"id":"/2012/03/05/photoshop_cs4","metadata":{"permalink":"/2012/03/05/photoshop_cs4","source":"@site/blog/2012-03-05-photoshop_cs4.md","title":"Adobe Photoshop CS4 148:3","description":"This error is caused by the Flexnet Licensing service being disabled. This is how you fix it","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Adobe Photoshop CS4 148:3","permalink":"/win/photoshop_cs4/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Error Loading PID Generator DLL while attempting to install Age of Empires 3","permalink":"/2012/03/05/mfc42"},"nextItem":{"title":"Reset the Waste Ink Counter on the Canon Pixma 1200","permalink":"/2012/03/05/reset-the-waste-ink-counter"}},"content":"This error is caused by the Flexnet Licensing service being disabled. This is how you fix it\\n\\n  1. Click Start\\n  2. Click Run, or in Search Box\\n  3. Type **_services.msc_** press Enter\\n  4. The Windows services list will open, look for** _FLEXNet\xa0Licensing_**\\n  5. Open it, choose **Automatic**\\n  6. Selection Start\\n  7. Press **Ok**\\n  8. **Open** _**Photoshop**_"},{"id":"/2012/03/05/reset-the-waste-ink-counter","metadata":{"permalink":"/2012/03/05/reset-the-waste-ink-counter","source":"@site/blog/2012-03-05-reset-the-waste-ink-counter.md","title":"Reset the Waste Ink Counter on the Canon Pixma 1200","description":"1. Disconnect printer from AC power","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.375,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Reset the Waste Ink Counter on the Canon Pixma 1200","permalink":"/misc/reset-the-waste-ink-counter/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Adobe Photoshop CS4 148:3","permalink":"/2012/03/05/photoshop_cs4"},"nextItem":{"title":"Using Define Word extension to search Auction Websites","permalink":"/2012/03/05/search_auction_firefox"}},"content":"1. **Disconnect** printer from AC **power**\\n  2. Press and **HOLD** the **Power** button, and **connect** **printer** to AC **power**\\n  3. **Release** the the **Power** button\\n  4. **Press** the **Power** button again\\n\\nIf the above method does not work, try the following:\\n\\n  1. **Press** the **resume** button and then **press** **power** button\\n  2. **Release** the **resume** **button** and **press** **2** more times then **release** **both**\\n  3. **Press** and **release** **resume** **4** times and **power** **2** times."},{"id":"/2012/03/05/search_auction_firefox","metadata":{"permalink":"/2012/03/05/search_auction_firefox","source":"@site/blog/2012-03-05-search_auction_firefox.md","title":"Using Define Word extension to search Auction Websites","description":"Using the Define Word extension, you can select it to search Auction sites such as Trademe by\xa0hi-lighting\xa0a word.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.535,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Using Define Word extension to search Auction Websites","permalink":"/misc/search_auction_firefox/","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"Reset the Waste Ink Counter on the Canon Pixma 1200","permalink":"/2012/03/05/reset-the-waste-ink-counter"},"nextItem":{"title":"An error occurred while using SSL configuration for socket address Error","permalink":"/2012/03/05/sslsocket"}},"content":"Using the Define Word extension, you can select it to search Auction sites such as Trademe by\xa0hi-lighting\xa0a word.\\n\\n  1. <a href=\\"https://addons.mozilla.org/en-US/firefox/addon/define-word/\\" target=\\"_blank\\">Download Extension\xa0</a>\xa0_(Restart Firefox after install)_\\n  2. Click Tools\\n  3. Click Extensions\\n  4. Click Define Word\\n  5. Click Options\\n  6. Click New\\n  7. Choose what engine name you want I will use Trademe as an example.\\n  8. Engine Name: Trademe\\n  9. Action: http://www.trademe.co.nz/Browse/SearchResults.aspx?searchtype=GENERAL&searchstring=search}&go.x=0&go.y=0&searchregion=100\\n 10. Now you can select on a word, right click, Define and Trademe it will then search trademe for the name you want.\\n 11. It should work with other search sites, and auction sites like eBay, just replace the term you searched with: {search}"},{"id":"/2012/03/05/sslsocket","metadata":{"permalink":"/2012/03/05/sslsocket","source":"@site/blog/2012-03-05-sslsocket.md","title":"An error occurred while using SSL configuration for socket address Error","description":"&#8220;An error occurred while using SSL configuration for socket address %2. The error status code is contained within the returned data.&#8221;","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.54,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"An error occurred while using SSL configuration for socket address Error","permalink":"/win/sslsocket/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Using Define Word extension to search Auction Websites","permalink":"/2012/03/05/search_auction_firefox"},"nextItem":{"title":"STOP 0x0000007B(0xF78DA63C,0x0000034,0X x0000000,0x0000000 Blue Screen of Death Windows XP","permalink":"/2012/03/05/stop-0x0000007b0xf78da63c0x00000340x-x00000000x0000000-blue-screen-of-death-windows-xp"}},"content":"&#8220;An error occurred while using SSL configuration for socket address %2. The error status code is contained within the returned data.&#8221;\\n\\nThis error occurs when the SSL certificates that allow you to access secure content, such as Emails & Internet Banking becomes damaged or corrupted. Follow the instructions below to fix.\\n\\n  1. Click Start click All Programs, click Accessories\\n  2. **Right**&#8211;**click** **Command** **Prompt** & click **Run** **as** **Administrator** \xa0then click Continue.\\n  3. Type **_netsh http show sslcert_**, \xa0press Enter to view the installed certificates.\\n  4. Type **_netsh http delete sslcert_**, press Enter to delete the incorrectly installed certificate.\\n  5. Type **_netsh http add sslcert_**, press Enter to\xa0re-install\xa0the certificate."},{"id":"/2012/03/05/stop-0x0000007b0xf78da63c0x00000340x-x00000000x0000000-blue-screen-of-death-windows-xp","metadata":{"permalink":"/2012/03/05/stop-0x0000007b0xf78da63c0x00000340x-x00000000x0000000-blue-screen-of-death-windows-xp","source":"@site/blog/2012-03-05-stop-0x0000007b0xf78da63c0x00000340x-x00000000x0000000-blue-screen-of-death-windows-xp.md","title":"STOP 0x0000007B(0xF78DA63C,0x0000034,0X x0000000,0x0000000 Blue Screen of Death Windows XP","description":"Trying to boot Windows XP and getting the\xa0STOP 0x0000007B BSOD?","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.56,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"STOP 0x0000007B(0xF78DA63C,0x0000034,0X x0000000,0x0000000 Blue Screen of Death Windows XP","permalink":"/win/stop-0x0000007b0xf78da63c0x00000340x-x00000000x0000000-blue-screen-of-death-windows-xp/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"An error occurred while using SSL configuration for socket address Error","permalink":"/2012/03/05/sslsocket"},"nextItem":{"title":"US Internet Service Providers Settings","permalink":"/2012/03/05/usisp"}},"content":"Trying to boot Windows XP and getting the\xa0STOP 0x0000007B BSOD?\\n\\nThis usually occurs when Windows is trying to repair/load from a SATA drive, without the\xa0appropriate\xa0SATA drivers. This is the way I fixed it, due to the fact that many BIOs are different they are generic instructions which should hopefully be easy to follow in your particular circumstance.\\n\\n  1. Enter the Computer&#8217;s BIOs (Usually it is F1 or F2 at computer boot)\\n  2. Go into the HDD Section or an area\xa0indicating\xa0any Controller options (Using the Arrow Keys & the Enter button to navigate)\\n  3. **Change** the HDD **controller** from AHCI/SATA or Raid to **_ATA or IDE_**\\n  4. **Restart** the computer\xa0boot up\xa0or Windows install."},{"id":"/2012/03/05/usisp","metadata":{"permalink":"/2012/03/05/usisp","source":"@site/blog/2012-03-05-usisp.md","title":"US Internet Service Providers Settings","description":"Below are email settings for some US ISPs (Internet Service Provider)","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Misc","permalink":"/tags/misc"}],"readingTime":0.855,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"US Internet Service Providers Settings","tags":["Misc"]},"unlisted":false,"prevItem":{"title":"STOP 0x0000007B(0xF78DA63C,0x0000034,0X x0000000,0x0000000 Blue Screen of Death Windows XP","permalink":"/2012/03/05/stop-0x0000007b0xf78da63c0x00000340x-x00000000x0000000-blue-screen-of-death-windows-xp"},"nextItem":{"title":"Change the Windows 7 Logon Background","permalink":"/2012/03/05/win7_logon_back"}},"content":"Below are email settings for some US ISPs (Internet Service Provider)\\n\\n**AT&T Worldnet:**\\n\\nPOP (incoming mail server): ipostoffice.worldnet.att.net\\n  \\nSMTP (outgoing mail server: imailhost.worldnet.att.net\\n  \\n**Charter Communications**\\n\\nPOP (incoming mail): pop.charter.net\\n  \\nSMTP (outgoing mail): smtp.charter.net\\n\\n**Comcast:**\\n\\nPOP (incoming mail): mail.comcast.net\\n  \\nSMTP (outgoing mail): smtp.comcast.net\\n  \\n**Earthlink:**\\n\\nPOP (incoming mail): pop.earthlink.net\\n  \\nSMTP (outgoing mail): smtpauth.earthlink.net\\n\\n**Insight Broadband**\\n\\nPOP3 (Incoming Mail) Server: mail.insightbb.com\\n  \\nSMTP (Outgoing Mail) Server: mail.insightbb.com\\n\\n**Qwest**\\n\\nBoth the POP and SMTP servers vary based on your physical location.\\n\\nPOP (incoming mail): pop.broadband.rogers.com\\n  \\nSMTP (outgoing mail): smtp.broadband.rogers.com\\n\\n**Shaw:**\\n\\nPOP: (incoming mail) shawmail (note: if you&#8217;re using a router or firewall, you may need to use the &#8220;fully qualified domain name&#8221; instead of just &#8220;shawmail&#8221;\\n  \\nSMTP: shawmail (note: if you&#8217;re using a router or firewall, you may need to use the &#8220;fully qualified domain name&#8221; instead of just &#8220;shawmail&#8221;\\n\\n**Time Warner/RoadRunner:**\\n\\nPOP (incoming mail server)\\n  \\nSMTP (outgoing mail server)\\n\\n**Verizon:**\\n\\nVerizon DSL and/or FIOS:\\n  \\nPOP (incoming mail server): incoming.verizon.net\\n  \\nSMTP (outgoing mail server): outgoing.verizon.net\\n  \\n**Verizon/Yahoo!:**\\n  \\nPOP (incoming mail server): incoming.yahoo.verizon.net\\n  \\nSMTP (outgoing mail server): outgoing.yahoo.verizon.net"},{"id":"/2012/03/05/win7_logon_back","metadata":{"permalink":"/2012/03/05/win7_logon_back","source":"@site/blog/2012-03-05-win7_logon_back.md","title":"Change the Windows 7 Logon Background","description":"Want to know how to change your Windows 7 Logon background to something a bit more modern/fancy? Follow the options below.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.47,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Change the Windows 7 Logon Background","permalink":"/win/win7_logon_back/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"US Internet Service Providers Settings","permalink":"/2012/03/05/usisp"},"nextItem":{"title":"Windows Mail Connection Tab Frozen","permalink":"/2012/03/05/win_mail_tab"}},"content":"Want to know how to change your Windows 7 Logon background to something a bit more modern/fancy? Follow the options below.\\n\\n  1. Launch **Regedit**, and browse to &#8216;**HKEY\\\\_LOCAL\\\\_MACHINESoftwareMicrosoftWindowsCurrentVersionAuthenticationLogonUIBackground**&#8216;.\\n  2. Double click the dword key called &#8216;**OEM Background**&#8216; (c_reate it if not there_) and set its value to **1**. **Locate** the log-in background image you&#8217;d like to use. It must be a JPG file that&#8217;s less than 245kB in size.\\n  3. **Copy** the **image** you want to use to **C:Windowssystem32oobeinfobackgrounds**\xa0folder.\\n  4. **Rename** it to **backgroundDefault.jpg**\\n  5. Restart computer and check out your new handy work."},{"id":"/2012/03/05/win_mail_tab","metadata":{"permalink":"/2012/03/05/win_mail_tab","source":"@site/blog/2012-03-05-win_mail_tab.md","title":"Windows Mail Connection Tab Frozen","description":"This usually happens after deleting or renaming\xa0Dial-up\xa0accounts, especially when the email client is set to check your mail with a certain account.","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Mail Connection Tab Frozen","permalink":"/win/win_mail_tab/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Change the Windows 7 Logon Background","permalink":"/2012/03/05/win7_logon_back"},"nextItem":{"title":"Windows Vista Black Screen &#038; Slow to Boot Repair","permalink":"/2012/03/05/winvistablack"}},"content":"This usually happens after deleting or renaming\xa0Dial-up\xa0accounts, especially when the email client is set to check your mail with a certain account.\\n\\n  1. **Recreate** the **account** _(Make sure that you have all the details that you need, including your Incoming & Outgoing Mail settings, ISP Username & Password)_\\n  2. Create the mail account from scratch\\n  3. **Delete** the **old** mail **account**"},{"id":"/2012/03/05/winvistablack","metadata":{"permalink":"/2012/03/05/winvistablack","source":"@site/blog/2012-03-05-winvistablack.md","title":"Windows Vista Black Screen &#038; Slow to Boot Repair","description":"Ever had the problem where Windows Vista has a Black screen, while trying to boot? I have. This is how I fixed it (For Advanced users only).","date":"2012-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Windows","permalink":"/tags/windows"}],"readingTime":0.94,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Windows Vista Black Screen &#038; Slow to Boot Repair","permalink":"/win/winvistablack/","tags":["Windows"]},"unlisted":false,"prevItem":{"title":"Windows Mail Connection Tab Frozen","permalink":"/2012/03/05/win_mail_tab"}},"content":"Ever had the problem where Windows Vista has a Black screen, while trying to boot? I have. This is how I fixed it (For Advanced users only).\\n\\n  1. Delete the following folders/files\\n  2. _**C:/windows/system32/drivers/pcmcia.sys**_\\n  3. _**C:/windows/system32/drivers/1394bus.sys**_\\n  4. _**C:/windows/system32/driversohci1394.sys**_\\n  5. _**C:/windows/system32/driverstore/filerepository/pcmcia.***_\\n  6. _**C:/windows/system32/driverstore/filerepository/1394.infblablabla**_\\n  7. _**C:/windows/system32/driverstore/filerepository/sdbus.infblablabla***_\\n  8. _**C:/windows/infsdbus.inf***_\\n  9. _**C:/windows/infsdbus.PNF***_\\n 10. Restart your computer and choose to edit boot options. You will see a screen which shows default windows boot options: /noexecute=optin. After optin, _**add /debug**_ command and press esc. (There needs to be a space between optin and /debug.)\\n 11. Find and delete the following files\\n 12. Delete _**sptd.sys**_\\n 13. Delete _**sptd.sy**_\\n 14. Delete _**sptd.sys**_\\n 15. Delete _**sptd.sys**_\\n 16. Go to a command prompt and run the following commands\\n 17. _**cd windows**_\\n 18. _**del \\\\*pcmcia\\\\*.* /s/p**_\\n 19. _**del \\\\*1394\\\\*.* /s/p**_\\n 20. Restart computer.\\n\\nStill not working? Try these additional options below.\\n\\n  1. Press **F2** to enter **BIOS** mode\\n  2. 2. Under &#8220;**Onboard Devices**&#8221; select &#8220;**Flash Cache Module**&#8221; and set it to &#8220;**Off**&#8221; (you cannot disable SATA unless this is set to &#8220;Off&#8221; mode.\\n  3. Move up to **&#8220;SATA Operation**&#8221; and change from &#8220;AHCI&#8221; to &#8220;**ATA**&#8220;.\\n  4. Restart the computer."}]}}')}}]);